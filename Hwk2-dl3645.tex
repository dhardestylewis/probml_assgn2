\documentclass[11pt]{article}
\usepackage{amsmath,amssymb,amsthm}
\usepackage{geometry}
\usepackage{enumitem}
\geometry{margin=1in}

\begin{document}

\begin{center}
{\Large STCS 6701: Probabilistic Machine Learning\\
Homework 2}

\vspace{1em}

Due: Nov 14 at 11:59 pm ET
\end{center}

\section*{Instructions}

\begin{itemize}
\item All homework should be typeset using \LaTeX. Box your answers whenever appropriate.

\item Standard late policy applies. Everyone has a total five late days throughout the semester. You are free to use them for whatever reason, no need to inform course staff.

\item The homework should be turned in via Gradescope before the deadline (more details will be announced closer to the deadline).

\item Turn in the code as well as the writeup.

\item You can use any programming language you like.
\end{itemize}

\newpage

\section*{Problem 1: Ideal Point Model}

\textbf{Motivation} This problem is intended to give you an introduction to one of the most widely used latent variable models in political and social sciences. We consider a dataset of roll-call votes from the 113th U.S. Senate.

Your task is to uncover hidden ideological structure from these binary voting patterns. We will use a probit ideal-point model (a variant of Item Response Theory) that represents both senators and bills on a shared latent axis. This model is widely used to study polarization and party structure in real legislatures.

\subsection*{Question 1.1: The model}

\textbf{Data:} $y_{ij} \in \{0, 1, \text{missing}\}$ = vote of senator $i$ on bill $j$.

\textbf{Latent Utility:} We conceptualize the voting process as composed of the following elements

\begin{itemize}
\item Each senator has an ideological position on a hidden \emph{left-right} axis.

\item Each bill has a \emph{location} on that axis: some are left-leaning, some are right-leaning, some are centrist.

\item Some bills are more polarizing than others ---a tax reform bill may split the chamber almost perfectly, while a ceremonial resolution passes nearly unanimously.

\item A senator casts a ``yea'' if their \textbf{latent support} for a bill crosses some internal threshold, otherwise the senator casts a ``nay'' vote.
\end{itemize}

This hidden ``support'' for a bill is not observed directly, we only get to see yea/nay/didn't vote. So we imagine that behind every vote there is an unobserved continuous variable ---a latent utility $z_{ij}$--- that represents how strongly senator $i$ supports bill $j$.

\begin{enumerate}
\item If $z_{ij} > 0$, the senator votes ``yea.''

\item If $z_{ij} < 0$, the senator votes ``nay.''
\end{enumerate}

Now we want to connect $z_{ij}$ to parameters that describe senators and bills.

\begin{enumerate}
\item \textbf{Senator position.} Suppose each senator has a hidden ideology $\theta_i$ on a \emph{left-right} axis. If $\theta_i$ is large and positive, the senator is more conservative; if it is negative, more liberal.

\item \textbf{Bill location.} Suppose each bill has a threshold $\beta_j$, placing it on the same axis. A bill with $\beta_j = 0$ is centrist; a bill with $\beta_j = +2$ (i.e., some ``large'' arbitrary number) is very conservative; a bill with $\beta_j = -2$ (i.e., some ``small'' arbitrary number) is very liberal.

\item \textbf{Discrimination.} Not all bills are equally informative to a senator's ideology. Some bills divide senators sharply, others less so. To capture this we introduce a discrimination parameter $\alpha_j$.
\end{enumerate}

Therefore, latent utility takes the form
\begin{align}
z_{ij} &= \alpha_j(\theta_i - \beta_j) + \epsilon_{ij}, \qquad \epsilon_{ij} \sim \mathcal{N}(0, 1). \label{eq:latent}\\
y_{ij} &= \mathbb{1}(z_{ij} > 0) \label{eq:vote}
\end{align}

\begin{enumerate}[label=\alph*)]
\item \textbf{Explain in words:} If a senator's $\theta_i$ is far larger than a bill's $\beta_j$ (and $\alpha_j > 0$), what does the model predict about the vote? What if $\theta_i$ is smaller?

We are given
\begin{align*}
z_{ij} &= \alpha_j(\theta_i - \beta_j) + \epsilon_{ij}, && \epsilon_{ij} \sim \mathcal{N}(0, 1),\\
y_{ij} &= \mathbb{1}(z_{ij} > 0).
\end{align*}
Define the mean of the latent utility
\[
\mu_{ij} \triangleq \alpha_j(\theta_i - \beta_j).
\]
Then we can rewrite $z_{ij} = \mu_{ij} + \epsilon_{ij}$, so conditional on $(\theta_i, \alpha_j, \beta_j)$ we have
\[
z_{ij} \mid \theta_i, \alpha_j, \beta_j \sim \mathcal{N}(\mu_{ij}, 1).
\]
Before proceeding with Question~1.1(a), let us derive the probability that the model produces a ``yea'':
\begin{align*}
\Pr(y_{ij}=1 \mid \theta_i, \alpha_j, \beta_j)
&= \Pr(z_{ij} > 0 \mid \theta_i, \alpha_j, \beta_j)\\
&= \Pr(\mu_{ij} + \epsilon_{ij} > 0)\\
&= \Pr(\epsilon_{ij} > -\mu_{ij})\\
&= 1 - \Phi(-\mu_{ij}) = \Phi(\mu_{ij}) = \Phi\!\big(\alpha_j(\theta_i - \beta_j)\big),
\end{align*}
where $\Phi(\cdot)$ is the standard normal CDF and we used the symmetry $\Phi(-x)=1-\Phi(x)$. Similarly,
\[
\Pr(y_{ij}=0 \mid \theta_i, \alpha_j, \beta_j) = 1 - \Phi\!\big(\alpha_j(\theta_i - \beta_j)\big).
\]
\textbf{Case 1: $\theta_i$ is far larger than $\beta_j$.} Assume $\theta_i \gg \beta_j$ and $\alpha_j > 0$. Then $\theta_i - \beta_j \gg 0$ and hence $\mu_{ij} = \alpha_j(\theta_i - \beta_j) \gg 0$. Because $\Phi(\mu_{ij})$ is very close to $1$ when $\mu_{ij}$ is large and positive, the model predicts $y_{ij}=1$ with probability near one: the latent utility is almost surely positive, so the senator is very likely to vote ``yea.''

\textbf{Case 2: $\theta_i$ is far smaller than $\beta_j$.} Assume $\theta_i \ll \beta_j$ and $\alpha_j > 0$. Then $\theta_i - \beta_j \ll 0$, implying $\mu_{ij} \ll 0$. Now $\Phi(\mu_{ij})$ is close to $0$, so $\Pr(y_{ij}=1\mid \theta_i,\alpha_j,\beta_j)$ is near zero. In this situation the latent utility is very likely negative, so the senator will almost surely vote ``nay.''

\item \textbf{Role of $\alpha_j$:} Compare two bills with the same $\beta_j$ but different $\alpha_j$: Which bill is more \emph{polarizing}?

For a fixed senator $i$ and bill $j$, the latent utility satisfies
\begin{align*}
z_{ij} &= \alpha_j(\theta_i - \beta_j) + \epsilon_{ij}, && \epsilon_{ij} \sim \mathcal{N}(0, 1),\\
y_{ij} &= \mathbb{1}(z_{ij} > 0).
\end{align*}
From the preliminary derivation we already have
\[
\Pr(y_{ij}=1 \mid \theta_i, \alpha_j, \beta_j)=\Phi\big(\alpha_j(\theta_i - \beta_j)\big).
\]
Define $p_j(\theta_i) \triangleq \Pr(y_{ij}=1 \mid \theta_i, \alpha_j, \beta_j)$ and introduce $u=\alpha_j(\theta_i - \beta_j)$ so that $p_j(\theta_i)=\Phi(u)$. Differentiating with respect to $\theta_i$ gives
\[
\frac{\partial p_j(\theta_i)}{\partial \theta_i} = \phi(u)\,\alpha_j = \alpha_j\,\phi\big(\alpha_j(\theta_i - \beta_j)\big),
\]
where $\phi(\cdot)$ is the standard normal density. Thus $\alpha_j$ controls how sharply the vote probability $p_j(\theta_i)$ rises or falls along the ideological axis.

Now compare two bills $j$ and $k$ with the same location $\beta$ but different discriminations $\alpha_j>\alpha_k>0$. Their vote probabilities for senator $i$ are
\[
p_j(\theta_i) = \Phi\big(\alpha_j(\theta_i - \beta)\big), \qquad
p_k(\theta_i) = \Phi\big(\alpha_k(\theta_i - \beta)\big).
\]

\textbf{Case 1: $\theta_i$ near the bill location $\beta$.} When $\theta_i=\beta$ exactly we obtain $p_j(\beta)=p_k(\beta)=\Phi(0)=0.5$. For $\theta_i$ close (but not equal) to $\beta$, the arguments $\alpha_j(\theta_i-\beta)$ and $\alpha_k(\theta_i-\beta)$ remain close to zero, so both probabilities stay near $0.5$. However, the derivatives at $\theta_i=\beta$ are
\[
\left.\frac{\partial p_j(\theta_i)}{\partial \theta_i}\right|_{\theta_i=\beta} = \alpha_j \phi(0), \qquad
\left.\frac{\partial p_k(\theta_i)}{\partial \theta_i}\right|_{\theta_i=\beta} = \alpha_k \phi(0),
\]
and since $\alpha_j>\alpha_k$, the curve $p_j(\theta_i)$ changes more rapidly with ideology near the cutpoint. In other words, a small ideological shift around $\beta$ causes a larger swing in the probability of voting ``yea'' for bill $j$ than for bill $k$; bill $j$ is therefore more polarizing.

\textbf{Case 2: $\theta_i$ far from $\beta$.} If $|\theta_i-\beta| \to \infty$, then $\alpha_j(\theta_i-\beta)$ and $\alpha_k(\theta_i-\beta)$ tend to $\pm\infty$ with the same sign, so
\[
p_j(\theta_i) \to \mathbb{1}(\theta_i>\beta), \qquad p_k(\theta_i) \to \mathbb{1}(\theta_i>\beta),
\]
and both probabilities saturate at 0 or 1. In this regime the derivatives shrink to zero because the Gaussian density $\phi\big(\alpha_\ell(\theta_i-\beta)\big)$ vanishes as its argument diverges. The main difference between $j$ and $k$ therefore lies in how quickly they transition between the extremes: larger $\alpha_j$ yields a steeper sigmoid, so $p_j$ leaves the ambiguous middle region more abruptly.

Overall, holding $\beta_j=\beta_k=\beta$ and $\alpha_j>\alpha_k>0$, bill $j$ is more polarizing than bill $k$ because its discrimination parameter makes the probability curve $p_j(\theta_i)$ change more steeply (and thus more abruptly) from near 0 to near 1 as a senatorâ€™s ideology moves across $\beta$.

\item \textbf{Missing Votes:} How should we handle $y_{ij} = \text{missing}$ in this setup?


We start from the model, for each $(i,j)$:
\[
z_{ij} = \alpha_j(\theta_i - \beta_j) + \epsilon_{ij}, \qquad \epsilon_{ij} \sim \mathcal{N}(0,1), \qquad y_{ij} = \mathbb{1}(z_{ij} > 0).
\]
From the earlier derivation, we obtained an explicit expression for the voting probability:
\[
\Pr(y_{ij}=1 \mid \theta_i, \alpha_j, \beta_j) = \Pr(z_{ij} > 0 \mid \theta_i, \alpha_j, \beta_j) = \Phi\big(\alpha_j(\theta_i - \beta_j)\big),
\]
where $\Phi(\cdot)$ is the standard normal CDF. Therefore,
\[
\Pr(y_{ij}=0 \mid \theta_i, \alpha_j, \beta_j) = 1 - \Pr(y_{ij}=1 \mid \theta_i, \alpha_j, \beta_j) = 1 - \Phi\big(\alpha_j(\theta_i - \beta_j)\big).
\]
\textbf{Bernoulli representation.} Define $p_{ij} := \Pr(y_{ij}=1 \mid \theta_i, \alpha_j, \beta_j) = \Phi\big(\alpha_j(\theta_i - \beta_j)\big)$. Then, for a single observed vote $y_{ij} \in \{0,1\}$,
\[
\Pr(y_{ij} \mid \theta_i, \alpha_j, \beta_j) = p_{ij}^{\,y_{ij}} (1-p_{ij})^{1-y_{ij}}.
\]
Substituting $p_{ij} = \Phi\big(\alpha_j(\theta_i - \beta_j)\big)$ gives
\[
\Pr(y_{ij} \mid \theta_i, \alpha_j, \beta_j) = [\Phi\big(\alpha_j(\theta_i - \beta_j)\big)]^{y_{ij}} [1-\Phi\big(\alpha_j(\theta_i - \beta_j)\big)]^{1-y_{ij}}.
\]
\textbf{Joint likelihood.} Define the index set of non-missing votes $O := \{ (i,j) : y_{ij} \in \{0,1\} \}$. Assuming conditional independence of votes given $\{\theta_i\}$ and $\{\alpha_j, \beta_j\}$,
\[
p(\{y_{ij}\}_{(i,j)\in O} \mid \{\theta_i\}, \{\alpha_j, \beta_j\}) = \prod_{(i,j)\in O} \Pr(y_{ij} \mid \theta_i, \alpha_j, \beta_j),
\]
which by the Bernoulli form becomes
\[
p(\{y_{ij}\}_{(i,j)\in O} \mid \{\theta_i\}, \{\alpha_j, \beta_j\}) = \prod_{(i,j)\in O} [\Phi\big(\alpha_j(\theta_i - \beta_j)\big)]^{y_{ij}} [1-\Phi\big(\alpha_j(\theta_i - \beta_j)\big)]^{1-y_{ij}}.
\]

\item \textbf{Sketch:} Sketch the graphical model using plate notation.

\item \textbf{Sketch:} Draw a 1D lineshowing senators at positions $\theta_i$, bills at positions $\beta_j$, and explain the threshold rule with a simple picture.

\item \textbf{Setting the Prior:} Suppose you choose a zero-centered prior for $\theta_i$ and $\beta_j$. How would you choose the prior variance(s) using the held-out data?

\textbf{1. Specify the priors with unknown variances.} Choose zero-centered Gaussian priors for the latent senator and bill locations:
\begin{align*}
\theta_i &\sim \mathcal{N}(0, \sigma^2_\theta), \quad i = 1, \ldots, N_{\text{sen}},\\
\beta_j &\sim \mathcal{N}(0, \sigma^2_\beta), \quad j = 1, \ldots, N_{\text{bill}}.
\end{align*}
Treat $\sigma^2_\theta, \sigma^2_\beta$ as hyperparameters that we will choose using the held-out votes.

\textbf{2. Split the data: training vs held-out.} Let $O$ be the set of observed (non-missing) votes as before, and split it into
\begin{itemize}
\item a training set $O_{\text{train}}$,
\item a held-out set $O_{\text{val}}$,
\end{itemize}
with $O_{\text{train}} \cap O_{\text{val}} = \emptyset$ and $O_{\text{train}} \cup O_{\text{val}} = O$.

We will fit the model on $O_{\text{train}}$ for each candidate choice of $(\sigma^2_\theta, \sigma^2_\beta)$, and evaluate predictive performance on $O_{\text{val}}$.

\textbf{3. For fixed variances, fit $\theta, \beta$ on the training set.} Recall the Bernoulli form:
\begin{align*}
p_{ij}(\theta_i, \beta_j) &:= \Pr(y_{ij} = 1 \mid \theta_i, \alpha_j, \beta_j) = \Phi\!\big(\alpha_j(\theta_i - \beta_j)\big),\\
\Pr(y_{ij} \mid \theta_i, \alpha_j, \beta_j) &= p_{ij}^{\,y_{ij}} (1 - p_{ij})^{1 - y_{ij}}.
\end{align*}
For a fixed pair $(\sigma^2_\theta, \sigma^2_\beta)$, the training log-posterior (up to an additive constant) is
\begin{align*}
\ell_{\text{train}}(\theta, \beta \mid \sigma^2_\theta, \sigma^2_\beta) &= \sum_{(i,j) \in O_{\text{train}}} \left[ y_{ij} \log p_{ij}(\theta_i, \beta_j) + (1 - y_{ij}) \log(1 - p_{ij}(\theta_i, \beta_j)) \right]\\
&\quad - \frac{1}{2\sigma^2_\theta} \sum_i \theta_i^2 - \frac{1}{2\sigma^2_\beta} \sum_j \beta_j^2.
\end{align*}
For that choice of $(\sigma^2_\theta, \sigma^2_\beta)$, compute the MAP estimates
\[
\hat{\theta}(\sigma^2_\theta, \sigma^2_\beta), \quad \hat{\beta}(\sigma^2_\theta, \sigma^2_\beta)
\]
by maximizing $\ell_{\text{train}}$.

\textbf{4. Compute held-out predictive log-likelihood.} Using $\hat{\theta}, \hat{\beta}$, compute predicted probabilities on held-out pairs:
\[
\hat{p}_{ij} := \Phi\!\big(\alpha_j(\hat{\theta}_i - \hat{\beta}_j)\big), \quad (i,j) \in O_{\text{val}}.
\]
The held-out log-likelihood for this choice of variances is
\[
L_{\text{val}}(\sigma^2_\theta, \sigma^2_\beta) := \sum_{(i,j) \in O_{\text{val}}} \left[ y_{ij} \log \hat{p}_{ij} + (1 - y_{ij}) \log(1 - \hat{p}_{ij}) \right].
\]

\textbf{5. Choose the prior variances by maximizing held-out performance.} Finally, treat $\sigma^2_\theta, \sigma^2_\beta$ as tuning parameters and choose them to maximize the held-out log-likelihood:
\[
(\sigma^{2*}_\theta, \sigma^{2*}_\beta) = \arg\max_{(\sigma^2_\theta, \sigma^2_\beta)} L_{\text{val}}(\sigma^2_\theta, \sigma^2_\beta).
\]
In practice one would:
\begin{itemize}
\item pick a grid of candidate values for $\sigma^2_\theta$ and $\sigma^2_\beta$,
\item for each pair, fit MAP $\hat{\theta}, \hat{\beta}$ on $O_{\text{train}}$,
\item compute $L_{\text{val}}$,
\item select the pair that yields the largest $L_{\text{val}}$.
\end{itemize}
\end{enumerate}

\newpage

\subsection*{Question 1.2: Putting priors on the parameters}

Right now, the latent parameters $\theta_i, \beta_j, \alpha_j$ are free-floating. To complete the model, we need to place prior distributions on these quantities.

\textbf{Priors for $\theta_i, \beta_j, \alpha_j$.} Throughout, assume the following priors

\begin{itemize}
\item $\theta_i \sim \mathcal{N}(\mu_\theta, \sigma^2_\theta)$: senators' latent ideologies. (e.g., where a senator is in the political spectrum)

\item $\beta_j \sim \mathcal{N}(\mu_\beta, \sigma^2_\beta)$: bills' latent positions on the ideological axis. (e.g., where a bill is in the political spectrum).

\item $\alpha_j \sim \mathcal{N}(\mu_\alpha, \sigma^2_\alpha)$: how strongly a bill separates senators into yea/nay camps.
\end{itemize}

\textbf{Identifiability.} The model likelihood depends only on the product $\alpha_j(\theta_i - \beta_j)$. This leads to certain \emph{symmetries} in the parameters:

\begin{enumerate}
\item \textbf{Translation:} Show that if we add a constant $c$ to all senator positions and all bill positions,
\[
(\theta_i - \beta_j) = (\theta_i + c) - (\beta_j + c),
\]
the likelihood is unchanged. What does this mean about the absolute location of the ideological axis?

We already have, from 1.1, for each $(i,j)$,
\[
\Pr(y_{ij}=1 \mid \theta_i,\alpha_j,\beta_j) = \Phi\!\big(\alpha_j(\theta_i - \beta_j)\big),
\]
and
\[
\Pr(y_{ij}=0 \mid \theta_i,\alpha_j,\beta_j) = 1 - \Phi\!\big(\alpha_j(\theta_i - \beta_j)\big).
\]
So the likelihood depends on the parameters only through the term $\alpha_j(\theta_i - \beta_j)$.

\textbf{Show translation invariance.} Consider a constant $c \in \mathbb{R}$ and define transformed parameters
\[
\theta_i' = \theta_i + c, \qquad \beta_j' = \beta_j + c, \qquad \alpha_j' = \alpha_j.
\]
Compute the transformed difference:
\[
\theta_i' - \beta_j' = (\theta_i + c) - (\beta_j + c) = \theta_i - \beta_j.
\]
Then the probit argument under the transformed parameters is
\[
\alpha_j'(\theta_i' - \beta_j') = \alpha_j(\theta_i - \beta_j),
\]
so the ``yea'' probability is unchanged:
\[
\Pr(y_{ij}=1 \mid \theta_i',\alpha_j',\beta_j') = \Phi\!\big(\alpha_j'(\theta_i' - \beta_j')\big) = \Phi\!\big(\alpha_j(\theta_i - \beta_j)\big) = \Pr(y_{ij}=1 \mid \theta_i,\alpha_j,\beta_j).
\]
The same holds for $\Pr(y_{ij}=0\mid\cdot)$, so for each $(i,j)$,
\[
\Pr(y_{ij}\mid \theta_i',\alpha_j',\beta_j') = \Pr(y_{ij}\mid \theta_i,\alpha_j,\beta_j).
\]
Because the joint likelihood is the product over $(i,j)$ of these terms, the entire likelihood is unchanged by the transformation
\[
\theta_i \mapsto \theta_i + c, \qquad \beta_j \mapsto \beta_j + c, \qquad \alpha_j \text{ unchanged}.
\]

This shows that the absolute location (origin) of the ideological axis is not identified by the data: only relative positions $\theta_i - \beta_j$ matter. In other words, the model can only recover ideologies and bill locations up to an additive constant; choosing where ``0'' is on the axis is a matter of convention (e.g., setting $\mu_\theta=0$, or fixing one $\beta_j=0$).

\item \textbf{Scaling:} Show that if we multiply all senator and bill positions by $k > 0$ and divide all discriminations by $k$,
\[
\alpha_j(\theta_i - \beta_j) = \frac{\alpha_j}{k}(k\theta_i - k\beta_j),
\]
the likelihood is unchanged. What does this mean about the scale of the ideological axis?

We already know from 1.1 that, for each $(i,j)$,
\[
\Pr(y_{ij}=1 \mid \theta_i,\alpha_j,\beta_j) = \Phi\!\big(\alpha_j(\theta_i - \beta_j)\big),
\]
\[
\Pr(y_{ij}=0 \mid \theta_i,\alpha_j,\beta_j) = 1 - \Phi\!\big(\alpha_j(\theta_i - \beta_j)\big).
\]
So again the likelihood depends on the parameters only through the product
\[
\alpha_j(\theta_i - \beta_j).
\]

\textbf{Show scaling invariance.} Take a constant $k>0$ and define the transformed parameters
\[
\theta_i' = k\,\theta_i, \qquad \beta_j' = k\,\beta_j, \qquad \alpha_j' = \frac{\alpha_j}{k}.
\]
First compute the transformed difference:
\[
\theta_i' - \beta_j' = k\theta_i - k\beta_j = k(\theta_i - \beta_j).
\]
Now plug into the probit argument with the transformed discrimination:
\[
\alpha_j'(\theta_i' - \beta_j') = \frac{\alpha_j}{k}\,\big[k(\theta_i - \beta_j)\big] = \alpha_j(\theta_i - \beta_j).
\]
Therefore, for every $(i,j)$,
\[
\Pr(y_{ij}=1 \mid \theta_i',\alpha_j',\beta_j') = \Phi\!\big(\alpha_j'(\theta_i' - \beta_j')\big) = \Phi\!\big(\alpha_j(\theta_i - \beta_j)\big) = \Pr(y_{ij}=1 \mid \theta_i,\alpha_j,\beta_j),
\]
and similarly
\[
\Pr(y_{ij}=0 \mid \theta_i',\alpha_j',\beta_j') = \Pr(y_{ij}=0 \mid \theta_i,\alpha_j,\beta_j).
\]
Thus for each observation,
\[
\Pr(y_{ij}\mid \theta_i',\alpha_j',\beta_j') = \Pr(y_{ij}\mid \theta_i,\alpha_j,\beta_j),
\]
and the product over all $(i,j)$ (the joint likelihood) is unchanged by the transformation
\[
\theta_i \mapsto k\theta_i, \qquad \beta_j \mapsto k\beta_j, \qquad \alpha_j \mapsto \alpha_j/k.
\]

This shows that the scale (units) of the ideological axis is not identified by the data: if we stretch or compress all $\theta_i$ and $\beta_j$ by a positive factor $k$ and compensate by shrinking $\alpha_j$ by $1/k$, the likelihood is exactly the same. The model only identifies ideological positions up to a multiplicative constant; choosing the ``unit'' of ideology (e.g. fixing $\sigma_\theta^2 = 1$ or constraining the variance of $\theta_i$) is a matter of convention needed to fix the scale.

\item \textbf{Interpretation:} Do these symmetries affect how you interpret the parameters? Are there any other symmetries in the parameters?

The translation and scaling symmetries mean that the absolute origin and units of the ideological axis are not identified by the data. One can shift all $\theta_i$ and $\beta_j$ by a constant, or multiply them all by a positive constant and rescale $\alpha_j$ accordingly, without changing any vote probabilities. As a result, $\theta_i$ and $\beta_j$ are only interpretable up to an affine transformation (shift and rescale); what is identified are relative positions, orderings, and distances, not the raw numerical values themselves.

If the sign of $\alpha_j$ is not constrained, there is one more symmetry: a global sign flip
\[
\theta_i \mapsto -\theta_i, \qquad \beta_j \mapsto -\beta_j, \qquad \alpha_j \mapsto -\alpha_j
\]
also leaves the likelihood unchanged. This means the orientation of the ideological axis (``which side is left/right'') is arbitrary until one imposes a convention (for example, fixing some known liberal to have $\theta_i>0$ or requiring most $\alpha_j>0$).
\end{enumerate}

\newpage

\subsection*{Question 1.3: CAVI}

In this section you will derive CAVI updates for this model.

\begin{enumerate}[label=\alph*)]
\item \textbf{Joint distribution:} Using the priors on the previous question write down the joint distribution $p(y, z, \alpha, \theta, \beta)$.

\textbf{Model pieces.} From before, for each pair $(i,j)$:
\begin{align*}
z_{ij} &= \alpha_j(\theta_i - \beta_j) + \varepsilon_{ij}, \qquad \varepsilon_{ij} \sim \mathcal{N}(0,1),\\
y_{ij} &= \mathbb{1}(z_{ij} > 0).
\end{align*}

\textbf{Priors:}

Senators' ideologies:
\[
\theta_i \sim \mathcal{N}(\mu_\theta, \sigma^2_\theta), \quad i = 1, \ldots, N.
\]

Bills' locations:
\[
\beta_j \sim \mathcal{N}(\mu_\beta, \sigma^2_\beta), \quad j = 1, \ldots, J.
\]

Bills' discriminations:
\[
\alpha_j \sim \mathcal{N}(\mu_\alpha, \sigma^2_\alpha), \quad j = 1, \ldots, J.
\]

For CAVI, it is convenient to make explicit the conditional distributions:

Latent utilities:
\[
z_{ij} \mid \theta_i, \alpha_j, \beta_j \sim \mathcal{N}\!\big(\alpha_j(\theta_i - \beta_j), \, 1\big).
\]

Observed votes are deterministic given $z_{ij}$:
\[
p(y_{ij} \mid z_{ij}) = \mathbb{1}\{y_{ij} = 1, \, z_{ij} > 0\} + \mathbb{1}\{y_{ij} = 0, \, z_{ij} \le 0\}.
\]

\textbf{Factorization of the joint.} Collect notation:
\begin{align*}
y &= \{y_{ij}\}, \quad z = \{z_{ij}\}, \quad \theta = \{\theta_i\}, \quad \beta = \{\beta_j\}, \quad \alpha = \{\alpha_j\}.
\end{align*}

Using conditional independence (given $\theta, \beta, \alpha$, all $z_{ij}$ are independent across $i,j$; given $z_{ij}$, the $y_{ij}$ are independent), the joint distribution factors as
\[
p(y, z, \alpha, \theta, \beta) = \left[\prod_{i=1}^N p(\theta_i)\right] \left[\prod_{j=1}^J p(\beta_j)\right] \left[\prod_{j=1}^J p(\alpha_j)\right] \left[\prod_{i=1}^N \prod_{j=1}^J p(z_{ij} \mid \theta_i, \alpha_j, \beta_j)\right] \left[\prod_{i=1}^N \prod_{j=1}^J p(y_{ij} \mid z_{ij})\right].
\]

Now plug in the specific normal and indicator forms:

Priors:
\[
p(\theta_i) = \mathcal{N}(\theta_i \mid \mu_\theta, \sigma^2_\theta), \quad p(\beta_j) = \mathcal{N}(\beta_j \mid \mu_\beta, \sigma^2_\beta), \quad p(\alpha_j) = \mathcal{N}(\alpha_j \mid \mu_\alpha, \sigma^2_\alpha),
\]

Latent utilities:
\[
p(z_{ij} \mid \theta_i, \alpha_j, \beta_j) = \mathcal{N}\!\big(z_{ij} \mid \alpha_j(\theta_i - \beta_j), \, 1\big),
\]

Vote indicators:
\[
p(y_{ij} \mid z_{ij}) = \mathbb{1}\{y_{ij} = 1, \, z_{ij} > 0\} + \mathbb{1}\{y_{ij} = 0, \, z_{ij} \le 0\}.
\]

So the full joint is
\begin{align*}
p(y, z, \alpha, \theta, \beta) &= \left[\prod_{i=1}^N \mathcal{N}(\theta_i \mid \mu_\theta, \sigma^2_\theta)\right] \left[\prod_{j=1}^J \mathcal{N}(\beta_j \mid \mu_\beta, \sigma^2_\beta)\right] \left[\prod_{j=1}^J \mathcal{N}(\alpha_j \mid \mu_\alpha, \sigma^2_\alpha)\right]\\
&\quad \times \left[\prod_{i=1}^N \prod_{j=1}^J \mathcal{N}\!\big(z_{ij} \mid \alpha_j(\theta_i - \beta_j), \, 1\big)\right] \left[\prod_{i=1}^N \prod_{j=1}^J p(y_{ij} \mid z_{ij})\right].
\end{align*}

\item \textbf{Latent utilities $z_{ij}$.} Recall that $z_{ij} \mid \theta_i, \beta_j, \alpha_j \sim \mathcal{N}(\alpha_j(\theta_i - \beta_j), 1)$. Show that conditioning on $y_{ij}$ leads to a truncated Normal:
\[
z_{ij} \mid y_{ij}, \theta, \beta, \alpha \sim
\begin{cases}
\mathcal{N}(\mu_{ij}, 1) \text{ truncated to } (0, \infty), & y_{ij} = 1,\\
\mathcal{N}(\mu_{ij}, 1) \text{ truncated to } (-\infty, 0], & y_{ij} = 0,
\end{cases}
\]
where $\mu_{ij} = \alpha_j(\theta_i - \beta_j)$. If $y_{ij} = -1$ (missing), explain why no $z_{ij}$ is drawn.

We start from the latent utility model, for each $(i,j)$:

Latent utility:
\[
z_{ij} \mid \theta_i, \beta_j, \alpha_j \sim \mathcal{N}(\mu_{ij}, 1),
\]
where
\[
\mu_{ij} := \alpha_j(\theta_i - \beta_j).
\]

Observed vote:
\[
y_{ij} = \mathbb{1}(z_{ij} > 0).
\]

Equivalently, the conditional distribution of $y_{ij}$ given $z_{ij}$ is
\[
p(y_{ij} \mid z_{ij}) = \mathbb{1}\{y_{ij} = 1, \, z_{ij} > 0\} + \mathbb{1}\{y_{ij} = 0, \, z_{ij} \le 0\}.
\]

\textbf{Conditional distribution of $z_{ij}$ given $y_{ij}$.} We want $p(z_{ij} \mid y_{ij}, \theta, \beta, \alpha)$. By Bayes' rule (up to a proportionality constant):
\[
p(z_{ij} \mid y_{ij}, \theta, \beta, \alpha) \;\propto\; p(y_{ij} \mid z_{ij})\, p(z_{ij} \mid \theta_i, \beta_j, \alpha_j).
\]

But we know
\[
p(z_{ij} \mid \theta_i, \beta_j, \alpha_j) = \mathcal{N}(z_{ij} \mid \mu_{ij}, 1).
\]

Now consider two cases.

\textbf{Case 1: $y_{ij} = 1$.} If $y_{ij} = 1$, then by the definition of $y_{ij}$,
\[
p(y_{ij} = 1 \mid z_{ij}) = \mathbb{1}\{z_{ij} > 0\}.
\]

So the conditional density is
\[
p(z_{ij} \mid y_{ij} = 1, \theta, \beta, \alpha) \;\propto\; \mathbb{1}\{z_{ij} > 0\}\, \mathcal{N}(z_{ij} \mid \mu_{ij}, 1).
\]

This is exactly a Normal $\mathcal{N}(\mu_{ij}, 1)$ density restricted to the domain $(0, \infty)$ and renormalized. Therefore:
\[
z_{ij} \mid y_{ij} = 1, \theta, \beta, \alpha \sim \mathcal{N}(\mu_{ij}, 1) \text{ truncated to } (0, \infty).
\]

\textbf{Case 2: $y_{ij} = 0$.} If $y_{ij} = 0$, then
\[
p(y_{ij} = 0 \mid z_{ij}) = \mathbb{1}\{z_{ij} \le 0\}.
\]

So
\[
p(z_{ij} \mid y_{ij} = 0, \theta, \beta, \alpha) \;\propto\; \mathbb{1}\{z_{ij} \le 0\}\, \mathcal{N}(z_{ij} \mid \mu_{ij}, 1),
\]
which is a Normal $\mathcal{N}(\mu_{ij}, 1)$ restricted to $(-\infty, 0]$. Hence:
\[
z_{ij} \mid y_{ij} = 0, \theta, \beta, \alpha \sim \mathcal{N}(\mu_{ij}, 1) \text{ truncated to } (-\infty, 0].
\]

Putting both cases together:
\[
z_{ij} \mid y_{ij}, \theta, \beta, \alpha \sim
\begin{cases}
\mathcal{N}(\mu_{ij}, 1) \text{ truncated to } (0, \infty), & y_{ij} = 1,\\[4pt]
\mathcal{N}(\mu_{ij}, 1) \text{ truncated to } (-\infty, 0], & y_{ij} = 0,
\end{cases}
\]
with $\mu_{ij} = \alpha_j(\theta_i - \beta_j)$.

\textbf{Missing case: $y_{ij} = -1$.} Recall the augmented model for a single $(i,j)$:
\begin{align*}
z_{ij} \mid \theta_i, \beta_j, \alpha_j &\sim \mathcal{N}(\mu_{ij}, 1), \qquad \mu_{ij} := \alpha_j(\theta_i - \beta_j),\\
y_{ij} &= \mathbb{1}(z_{ij} > 0), \quad y_{ij} \in \{0, 1\}.
\end{align*}

For observed votes $y_{ij} \in \{0, 1\}$, we showed
\[
z_{ij} \mid y_{ij}, \theta, \beta, \alpha \sim
\begin{cases}
\mathcal{N}(\mu_{ij}, 1) \text{ truncated to } (0, \infty), & y_{ij} = 1,\\[4pt]
\mathcal{N}(\mu_{ij}, 1) \text{ truncated to } (-\infty, 0], & y_{ij} = 0.
\end{cases}
\]

Now consider the missing case $y_{ij} = -1$. Define the index set of non-missing votes
\[
O := \{(i,j): y_{ij} \in \{0, 1\}\}.
\]

Start from the full augmented joint over all pairs $(i,j)$:
\begin{align*}
p\big(\{y_{ij}, z_{ij}\}_{i,j}, \theta, \beta, \alpha\big) &= \Bigg[\prod_{(i,j) \in O} p(y_{ij} \mid z_{ij})\, p(z_{ij} \mid \theta_i, \beta_j, \alpha_j)\Bigg]\\
&\quad \times \Bigg[\prod_{(i,j) \notin O} p(z_{ij} \mid \theta_i, \beta_j, \alpha_j)\Bigg] p(\theta)\, p(\beta)\, p(\alpha).
\end{align*}

To obtain the joint distribution involving only observed votes and their latent utilities, integrate out the $\{z_{ij}\}_{(i,j) \notin O}$:
\begin{align*}
p\big(\{y_{ij}, z_{ij}\}_{(i,j) \in O}, \theta, \beta, \alpha\big) &= \int \Bigg[\prod_{(i,j) \in O} p(y_{ij} \mid z_{ij})\, p(z_{ij} \mid \theta_i, \beta_j, \alpha_j)\Bigg]\\
&\quad \times \Bigg[\prod_{(i,j) \notin O} p(z_{ij} \mid \theta_i, \beta_j, \alpha_j)\Bigg] p(\theta)\, p(\beta)\, p(\alpha)\, \prod_{(i,j) \notin O} dz_{ij}\\[4pt]
&= \Bigg[\prod_{(i,j) \in O} p(y_{ij} \mid z_{ij})\, p(z_{ij} \mid \theta_i, \beta_j, \alpha_j)\Bigg] p(\theta)\, p(\beta)\, p(\alpha),
\end{align*}
because each factor $\int p(z_{ij} \mid \theta_i, \beta_j, \alpha_j)\, dz_{ij} = 1$ for $(i,j) \notin O$.

Thus the joint distribution over observed votes, their latent utilities, and the parameters is
\[
p\big(\{y_{ij}, z_{ij}\}_{(i,j) \in O}, \theta, \beta, \alpha\big) = \Bigg[\prod_{(i,j) \in O} p(y_{ij} \mid z_{ij})\, p(z_{ij} \mid \theta_i, \beta_j, \alpha_j)\Bigg] p(\theta)\, p(\beta)\, p(\alpha).
\]

Pairs $(i,j)$ with $y_{ij} = -1$ are not in $O$, so they do not appear in this product and there is no likelihood term involving $y_{ij}$. Introducing a corresponding $z_{ij}$ that appears only in its prior conditional $p(z_{ij} \mid \theta_i, \beta_j, \alpha_j)$ would integrate out to 1 and would not change the posterior. Therefore, when $y_{ij} = -1$ (missing), we simply omit the latent utility and no $z_{ij}$ is drawn.

\item \textbf{Senator positions $\theta_i$.} Derive the conditional distribution of $\theta_i$ given $z, \beta, \alpha$ under your chosen prior from 2.1. Show it is Gaussian, and write down its mean and variance.

\textbf{Step 1: Write the conditional for a single senator $i$.}

Prior (from 2.1):
\[
\theta_i \sim \mathcal{N}(\mu_\theta, \sigma^2_\theta), \quad \text{independently across } i.
\]

Latent utilities (for all bills $j$ with a non-missing vote from senator $i$):
\[
z_{ij} \mid \theta_i, \beta_j, \alpha_j \sim \mathcal{N}(\mu_{ij}, 1), \qquad \mu_{ij} := \alpha_j(\theta_i - \beta_j).
\]

Define the set of bills for which senator $i$ has a non-missing vote:
\[
O_i := \{j : (i,j) \in O\}.
\]

Conditional on $z, \beta, \alpha$, the full conditional for $\theta_i$ is
\[
p(\theta_i \mid z, \beta, \alpha) \;\propto\; p(\theta_i)\, \prod_{j \in O_i} p(z_{ij} \mid \theta_i, \beta_j, \alpha_j).
\]

Plug in the Gaussian forms:

Prior:
\[
p(\theta_i) \propto \exp\!\left\{-\frac{1}{2\sigma^2_\theta}(\theta_i - \mu_\theta)^2\right\}.
\]

Likelihood terms (for each $j \in O_i$):
\[
p(z_{ij} \mid \theta_i, \beta_j, \alpha_j) \propto \exp\!\left\{-\frac{1}{2}(z_{ij} - \alpha_j(\theta_i - \beta_j))^2\right\}.
\]

Thus
\[
p(\theta_i \mid z, \beta, \alpha) \;\propto\; \exp\!\left\{-\frac{1}{2\sigma^2_\theta}(\theta_i - \mu_\theta)^2 - \frac{1}{2}\sum_{j \in O_i} (z_{ij} - \alpha_j(\theta_i - \beta_j))^2\right\}.
\]

We now show this kernel is Gaussian in $\theta_i$ by expanding the exponent and completing the square.

\textbf{Step 2: Expand the quadratic in $\theta_i$.}

First expand the prior term:
\[
-\frac{1}{2\sigma^2_\theta}(\theta_i - \mu_\theta)^2 = -\frac{1}{2\sigma^2_\theta}(\theta_i^2 - 2\mu_\theta \theta_i + \mu_\theta^2).
\]

Next expand each likelihood term. Rewrite the mean:
\[
z_{ij} - \alpha_j(\theta_i - \beta_j) = z_{ij} - \alpha_j \theta_i + \alpha_j \beta_j = \alpha_j \theta_i - (z_{ij} + \alpha_j \beta_j)\,(-1),
\]
so
\[
(z_{ij} - \alpha_j(\theta_i - \beta_j))^2 = (\alpha_j \theta_i - (z_{ij} + \alpha_j \beta_j))^2 = \alpha_j^2 \theta_i^2 - 2\alpha_j \theta_i (z_{ij} + \alpha_j \beta_j) + (z_{ij} + \alpha_j \beta_j)^2.
\]

Sum over $j \in O_i$:
\begin{align*}
\sum_{j \in O_i} (z_{ij} - \alpha_j(\theta_i - \beta_j))^2 &= \left(\sum_{j \in O_i} \alpha_j^2\right) \theta_i^2 - 2\theta_i \sum_{j \in O_i} \alpha_j(z_{ij} + \alpha_j \beta_j) + \sum_{j \in O_i} (z_{ij} + \alpha_j \beta_j)^2.
\end{align*}

Now the exponent in $p(\theta_i \mid \cdot)$ is
\begin{align*}
\log p(\theta_i \mid z, \beta, \alpha) &= \text{const} - \frac{1}{2\sigma^2_\theta}(\theta_i^2 - 2\mu_\theta \theta_i + \mu_\theta^2) - \frac{1}{2}\sum_{j \in O_i} (z_{ij} - \alpha_j(\theta_i - \beta_j))^2\\
&= \text{const} - \frac{1}{2}\left[\left(\frac{1}{\sigma^2_\theta} + \sum_{j \in O_i} \alpha_j^2\right) \theta_i^2 - 2\theta_i\left(\frac{\mu_\theta}{\sigma^2_\theta} + \sum_{j \in O_i} \alpha_j(z_{ij} + \alpha_j \beta_j)\right)\right],
\end{align*}
where ``const'' collects all terms independent of $\theta_i$.

Define the posterior precision and linear term:
\[
A_i := \frac{1}{\sigma^2_\theta} + \sum_{j \in O_i} \alpha_j^2, \qquad B_i := \frac{\mu_\theta}{\sigma^2_\theta} + \sum_{j \in O_i} \alpha_j(z_{ij} + \alpha_j \beta_j).
\]

Then
\[
\log p(\theta_i \mid z, \beta, \alpha) = \text{const} - \frac{1}{2}(A_i \theta_i^2 - 2B_i \theta_i).
\]

\textbf{Step 3: Complete the square and identify the Gaussian.}

Write the quadratic in completed-square form:
\[
A_i \theta_i^2 - 2B_i \theta_i = A_i\left(\theta_i^2 - 2\frac{B_i}{A_i}\theta_i\right) = A_i\left(\theta_i - \frac{B_i}{A_i}\right)^2 - A_i\left(\frac{B_i}{A_i}\right)^2.
\]

So
\[
\log p(\theta_i \mid z, \beta, \alpha) = \text{const}' - \frac{A_i}{2}\left(\theta_i - \frac{B_i}{A_i}\right)^2,
\]
where $\text{const}'$ is another constant not depending on $\theta_i$.

This is exactly the kernel of a Normal distribution with

variance
\[
\text{Var}(\theta_i \mid z, \beta, \alpha) = A_i^{-1} = \left(\frac{1}{\sigma^2_\theta} + \sum_{j \in O_i} \alpha_j^2\right)^{-1},
\]

mean
\[
\mathbb{E}[\theta_i \mid z, \beta, \alpha] = \frac{B_i}{A_i} = \left(\frac{1}{\sigma^2_\theta} + \sum_{j \in O_i} \alpha_j^2\right)^{-1} \left(\frac{\mu_\theta}{\sigma^2_\theta} + \sum_{j \in O_i} \alpha_j(z_{ij} + \alpha_j \beta_j)\right).
\]

For each senator $i$, the conditional distribution of $\theta_i$ given $z, \beta, \alpha$ is Gaussian:
\[
\theta_i \mid z, \beta, \alpha \sim \mathcal{N}(m_{\theta_i}, v_{\theta_i}),
\]
with
\[
v_{\theta_i} = \left(\frac{1}{\sigma^2_\theta} + \sum_{j \in O_i} \alpha_j^2\right)^{-1},
\]
\[
m_{\theta_i} = v_{\theta_i}\left(\frac{\mu_\theta}{\sigma^2_\theta} + \sum_{j \in O_i} \alpha_j(z_{ij} + \alpha_j \beta_j)\right),
\]
where $O_i = \{j : (i,j) \in O\}$ indexes the bills on which senator $i$ cast a non-missing vote.

\item \textbf{Bill locations $\beta_j$.} Derive the conditional distribution of $\beta_j$ given $z, \theta, \alpha$ under your chosen prior from 2.1. Show it is Gaussian, and write down its mean and variance.

We proceed exactly as for the $\theta_i$ case, but now treating $\beta_j$ as the unknown and $(\theta, \alpha, z)$ as given.

\textbf{Step 1: Prior and conditional likelihood for a single bill $j$.}

From 2.1, the prior on the bill location is
\[
\beta_j \sim \mathcal{N}(\mu_\beta, \sigma^2_\beta),
\]
independently across $j$.

For all senators $i$ with a non-missing vote on bill $j$, the latent utilities satisfy
\[
z_{ij} \mid \theta_i, \beta_j, \alpha_j \sim \mathcal{N}(\mu_{ij}, 1), \qquad \mu_{ij} := \alpha_j(\theta_i - \beta_j).
\]

Let
\[
O_j := \{i : (i,j) \in O\}
\]
be the set of senators who (non-missingly) voted on bill $j$.

Given $z, \theta, \alpha$, the full conditional for $\beta_j$ is
\[
p(\beta_j \mid z, \theta, \alpha) \;\propto\; p(\beta_j)\, \prod_{i \in O_j} p(z_{ij} \mid \theta_i, \beta_j, \alpha_j).
\]

Plug in the Gaussian pieces:

Prior:
\[
p(\beta_j) \propto \exp\!\left\{-\frac{1}{2\sigma^2_\beta}(\beta_j - \mu_\beta)^2\right\}.
\]

For each $i \in O_j$,
\[
p(z_{ij} \mid \theta_i, \beta_j, \alpha_j) \propto \exp\!\left\{-\frac{1}{2}(z_{ij} - \alpha_j(\theta_i - \beta_j))^2\right\}.
\]

Thus
\[
p(\beta_j \mid z, \theta, \alpha) \;\propto\; \exp\!\left\{-\frac{1}{2\sigma^2_\beta}(\beta_j - \mu_\beta)^2 - \frac{1}{2}\sum_{i \in O_j} (z_{ij} - \alpha_j(\theta_i - \beta_j))^2\right\}.
\]

We now expand the exponent as a quadratic function of $\beta_j$.

\textbf{Step 2: Expand the quadratic in $\beta_j$.}

First expand the prior term:
\[
-\frac{1}{2\sigma^2_\beta}(\beta_j - \mu_\beta)^2 = -\frac{1}{2\sigma^2_\beta}(\beta_j^2 - 2\mu_\beta \beta_j + \mu_\beta^2).
\]

For the likelihood terms, write
\[
z_{ij} - \alpha_j(\theta_i - \beta_j) = z_{ij} - \alpha_j \theta_i + \alpha_j \beta_j = \alpha_j \beta_j + c_{ij}, \quad c_{ij} := z_{ij} - \alpha_j \theta_i.
\]

Then
\[
(z_{ij} - \alpha_j(\theta_i - \beta_j))^2 = (\alpha_j \beta_j + c_{ij})^2 = \alpha_j^2 \beta_j^2 + 2\alpha_j \beta_j c_{ij} + c_{ij}^2.
\]

Summing over $i \in O_j$:
\[
\sum_{i \in O_j} (z_{ij} - \alpha_j(\theta_i - \beta_j))^2 = \left(\sum_{i \in O_j} \alpha_j^2\right) \beta_j^2 + 2\beta_j \sum_{i \in O_j} \alpha_j c_{ij} + \sum_{i \in O_j} c_{ij}^2.
\]

Because $\alpha_j$ does not depend on $i$,
\[
\sum_{i \in O_j} \alpha_j^2 = |O_j|\,\alpha_j^2.
\]

Now collect the terms that depend on $\beta_j$ in the exponent of $p(\beta_j \mid \cdot)$:
\begin{align*}
\log p(\beta_j \mid z, \theta, \alpha) &= \text{const} - \frac{1}{2\sigma^2_\beta}(\beta_j^2 - 2\mu_\beta \beta_j + \mu_\beta^2) - \frac{1}{2}\sum_{i \in O_j} (\alpha_j^2 \beta_j^2 + 2\alpha_j \beta_j c_{ij} + c_{ij}^2)\\
&= \text{const} - \frac{1}{2}\left[\left(\frac{1}{\sigma^2_\beta} + |O_j|\alpha_j^2\right) \beta_j^2 - 2\beta_j\left(\frac{\mu_\beta}{\sigma^2_\beta} - \alpha_j \sum_{i \in O_j} c_{ij}\right)\right],
\end{align*}
where ``const'' collects all terms that do not depend on $\beta_j$.

Define
\[
A_j := \frac{1}{\sigma^2_\beta} + |O_j|\alpha_j^2, \qquad B_j := \frac{\mu_\beta}{\sigma^2_\beta} - \alpha_j \sum_{i \in O_j} c_{ij} = \frac{\mu_\beta}{\sigma^2_\beta} - \alpha_j \sum_{i \in O_j} (z_{ij} - \alpha_j \theta_i).
\]

Then
\[
\log p(\beta_j \mid z, \theta, \alpha) = \text{const} - \frac{1}{2}(A_j \beta_j^2 - 2B_j \beta_j).
\]

\textbf{Step 3: Complete the square and identify the Gaussian.}

Complete the square in $\beta_j$:
\[
A_j \beta_j^2 - 2B_j \beta_j = A_j\left(\beta_j - \frac{B_j}{A_j}\right)^2 - A_j\left(\frac{B_j}{A_j}\right)^2.
\]

So
\[
\log p(\beta_j \mid z, \theta, \alpha) = \text{const}' - \frac{A_j}{2}\left(\beta_j - \frac{B_j}{A_j}\right)^2,
\]
which is the kernel of a Normal distribution with

variance
\[
\text{Var}(\beta_j \mid z, \theta, \alpha) = A_j^{-1} = \left(\frac{1}{\sigma^2_\beta} + |O_j|\alpha_j^2\right)^{-1},
\]

mean
\[
\mathbb{E}[\beta_j \mid z, \theta, \alpha] = \frac{B_j}{A_j} = \left(\frac{1}{\sigma^2_\beta} + |O_j|\alpha_j^2\right)^{-1} \left(\frac{\mu_\beta}{\sigma^2_\beta} - \alpha_j \sum_{i \in O_j} (z_{ij} - \alpha_j \theta_i)\right).
\]

\textbf{Final form.} For each bill $j$, the conditional distribution of $\beta_j$ given $z, \theta, \alpha$ is Gaussian:
\[
\beta_j \mid z, \theta, \alpha \sim \mathcal{N}(m_{\beta_j}, v_{\beta_j}),
\]
with
\[
v_{\beta_j} = \left(\frac{1}{\sigma^2_\beta} + |O_j|\alpha_j^2\right)^{-1},
\]
\[
m_{\beta_j} = v_{\beta_j}\left(\frac{\mu_\beta}{\sigma^2_\beta} - \alpha_j \sum_{i \in O_j} (z_{ij} - \alpha_j \theta_i)\right),
\]
where $O_j = \{i : (i,j) \in O\}$ is the set of senators who cast a non-missing vote on bill $j$.

\item \textbf{Bill discriminations $\alpha_j$.} Assume the prior you proposed in 2.1 for $\alpha_j$. Derive the conditional distribution of $\alpha_j$ given $z, \theta, \beta$. If you chose a Normal prior, it will be Normal; if you chose a truncated Normal prior, it will be truncated Normal (see Useful Formulas for information on the Truncated Normal).

We proceed just like for $\theta_i$ and $\beta_j$, now treating $\alpha_j$ as the unknown with $(z, \theta, \beta)$ given. Suppose the prior is
\[
\alpha_j \sim \mathcal{N}(\mu_\alpha, \sigma^2_\alpha).
\]

\textbf{Step 1: Write the joint in exponential form.}

For a given bill $j$, the complete conditional is proportional to the prior on $\alpha_j$ times the likelihood of the observed latent utilities $z_{ij}$ for all senators $i \in O_j$ who voted on bill $j$:
\[
p(\alpha_j \mid z, \theta, \beta) \propto p(\alpha_j) \prod_{i \in O_j} p(z_{ij} \mid \theta_i, \beta_j, \alpha_j).
\]

The latent utility model is
\[
z_{ij} \mid \theta_i, \beta_j, \alpha_j \sim \mathcal{N}(\alpha_j(\theta_i - \beta_j), 1).
\]

Let
\[
x_{ij} := \theta_i - \beta_j,
\]
so
\[
z_{ij} \mid \alpha_j \sim \mathcal{N}(\alpha_j x_{ij}, 1).
\]

Then
\begin{align*}
\log p(\alpha_j \mid z, \theta, \beta) &= \text{const} - \frac{1}{2\sigma^2_\alpha}(\alpha_j - \mu_\alpha)^2 - \frac{1}{2}\sum_{i \in O_j} (z_{ij} - \alpha_j x_{ij})^2\\
&= \text{const} - \frac{1}{2\sigma^2_\alpha}(\alpha_j^2 - 2\mu_\alpha \alpha_j) - \frac{1}{2}\sum_{i \in O_j} (z_{ij}^2 - 2z_{ij} \alpha_j x_{ij} + \alpha_j^2 x_{ij}^2).
\end{align*}

\textbf{Step 2: Collect quadratic terms in $\alpha_j$.}

The coefficient of $\alpha_j^2$ in the exponent is
\[
-\frac{1}{2}\left[\frac{1}{\sigma^2_\alpha} + \sum_{i \in O_j} x_{ij}^2\right].
\]

The coefficient of $\alpha_j$ is
\[
\frac{\mu_\alpha}{\sigma^2_\alpha} + \sum_{i \in O_j} z_{ij} x_{ij}.
\]

Define
\[
A_j := \frac{1}{\sigma^2_\alpha} + \sum_{i \in O_j} (\theta_i - \beta_j)^2, \qquad B_j := \frac{\mu_\alpha}{\sigma^2_\alpha} + \sum_{i \in O_j} z_{ij}(\theta_i - \beta_j).
\]

Then
\[
\log p(\alpha_j \mid z, \theta, \beta) = \text{const} - \frac{1}{2}(A_j \alpha_j^2 - 2B_j \alpha_j).
\]

\textbf{Step 3: Complete the square.}

Complete the square in $\alpha_j$:
\[
A_j \alpha_j^2 - 2B_j \alpha_j = A_j\left(\alpha_j - \frac{B_j}{A_j}\right)^2 - A_j\left(\frac{B_j}{A_j}\right)^2.
\]

So
\[
\log p(\alpha_j \mid z, \theta, \beta) = \text{const}' - \frac{A_j}{2}\left(\alpha_j - \frac{B_j}{A_j}\right)^2,
\]
which is the kernel of a Normal distribution.

\textbf{Final form (Normal prior).} For each bill $j$, the conditional distribution is Gaussian:
\[
\alpha_j \mid z, \theta, \beta \sim \mathcal{N}(m_{\alpha_j}, v_{\alpha_j}),
\]
with
\[
v_{\alpha_j} = \left(\frac{1}{\sigma^2_\alpha} + \sum_{i \in O_j} (\theta_i - \beta_j)^2\right)^{-1},
\]
\[
m_{\alpha_j} = v_{\alpha_j}\left(\frac{\mu_\alpha}{\sigma^2_\alpha} + \sum_{i \in O_j} z_{ij}(\theta_i - \beta_j)\right),
\]
where $O_j = \{i : (i,j) \in O\}$ is the set of senators who cast a non-missing vote on bill $j$.

\textbf{Truncated Normal prior.} If instead the prior is
\[
\alpha_j \sim \mathcal{N}(\mu_\alpha, \sigma^2_\alpha) \text{ truncated to } (0, \infty),
\]
the conditional is the same Normal $\mathcal{N}(m_{\alpha_j}, v_{\alpha_j})$ truncated to $(0, \infty)$.

\item \textbf{Marginalizing out $z$.} Write down $p(y, \alpha, \theta, \beta)$. What is $p(y \mid \alpha, \theta, \beta)$?

\textbf{Step 1: Start from the joint with $z$.}

Restricting to non-missing entries $(i,j) \in O$,
\[
p(y, z, \alpha, \theta, \beta) = \left[\prod_{(i,j) \in O} p(y_{ij} \mid z_{ij}) \, p(z_{ij} \mid \theta_i, \beta_j, \alpha_j)\right] \, p(\theta) \, p(\beta) \, p(\alpha),
\]
where
\[
z_{ij} \mid \theta_i, \beta_j, \alpha_j \sim \mathcal{N}\!\big(\mu_{ij}, 1\big), \quad \mu_{ij} := \alpha_j(\theta_i - \beta_j),
\]
and
\[
y_{ij} = \mathbf{1}(z_{ij} > 0), \quad y_{ij} \in \{0, 1\}.
\]

The conditional for $y_{ij}$ given $z_{ij}$ is
\[
p(y_{ij} \mid z_{ij}) = \begin{cases}
1, & y_{ij} = 1, \, z_{ij} > 0,\\
1, & y_{ij} = 0, \, z_{ij} \le 0,\\
0, & \text{otherwise}.
\end{cases}
\]

Equivalently,
\[
p(y_{ij} \mid z_{ij}) = [\mathbf{1}(z_{ij} > 0)]^{y_{ij}} [\mathbf{1}(z_{ij} \le 0)]^{1 - y_{ij}}.
\]

\textbf{Step 2: Marginalize out $z$.}

We want
\[
p(y, \alpha, \theta, \beta) = \int p(y, z, \alpha, \theta, \beta) \, dz.
\]

Plug in the factorized joint and separate the integral:
\begin{align*}
p(y, \alpha, \theta, \beta) &= \int \left[\prod_{(i,j) \in O} p(y_{ij} \mid z_{ij}) \, p(z_{ij} \mid \theta_i, \beta_j, \alpha_j)\right] \, p(\theta) \, p(\beta) \, p(\alpha) \, dz\\
&= p(\theta) \, p(\beta) \, p(\alpha) \, \prod_{(i,j) \in O} \int p(y_{ij} \mid z_{ij}) \, p(z_{ij} \mid \theta_i, \beta_j, \alpha_j) \, dz_{ij}.
\end{align*}

For each fixed $(i,j)$,
\[
\int p(y_{ij} \mid z_{ij}) \, p(z_{ij} \mid \theta_i, \beta_j, \alpha_j) \, dz_{ij}.
\]

\textbf{Step 3: Evaluate the inner integral.}

Recall
\[
z_{ij} \mid \theta_i, \beta_j, \alpha_j \sim \mathcal{N}(\mu_{ij}, 1), \quad \mu_{ij} = \alpha_j(\theta_i - \beta_j).
\]

So the density is
\[
p(z_{ij} \mid \theta_i, \beta_j, \alpha_j) = \phi(z_{ij} - \mu_{ij}) = \frac{1}{\sqrt{2\pi}} \exp\!\left(-\frac{(z_{ij} - \mu_{ij})^2}{2}\right).
\]

We treat the two cases $y_{ij} = 1$ and $y_{ij} = 0$ separately.

\textit{Case A: $y_{ij} = 1$.}

Then
\[
p(y_{ij} = 1 \mid \theta_i, \alpha_j, \beta_j) = \int p(y_{ij} = 1 \mid z_{ij}) \, p(z_{ij} \mid \theta_i, \beta_j, \alpha_j) \, dz_{ij}.
\]

But $p(y_{ij} = 1 \mid z_{ij}) = \mathbf{1}(z_{ij} > 0)$, so
\begin{align*}
p(y_{ij} = 1 \mid \theta_i, \alpha_j, \beta_j) &= \int_0^\infty \phi(z_{ij} - \mu_{ij}) \, dz_{ij}\\
&= \Pr(z_{ij} > 0 \mid \theta_i, \alpha_j, \beta_j).
\end{align*}

Standardize: let $U = z_{ij} - \mu_{ij} \sim \mathcal{N}(0, 1)$. Then
\[
\Pr(z_{ij} > 0) = \Pr(U > -\mu_{ij}) = 1 - \Phi(-\mu_{ij}) = \Phi(\mu_{ij}),
\]
using symmetry of the standard normal CDF. Hence
\[
p(y_{ij} = 1 \mid \theta_i, \alpha_j, \beta_j) = \Phi(\mu_{ij}) = \Phi(\alpha_j(\theta_i - \beta_j)),
\]
which matches what we had in 1.1.

\textit{Case B: $y_{ij} = 0$.}

Similarly,
\[
p(y_{ij} = 0 \mid \theta_i, \alpha_j, \beta_j) = \int p(y_{ij} = 0 \mid z_{ij}) \, p(z_{ij} \mid \theta_i, \beta_j, \alpha_j) \, dz_{ij}.
\]

Now $p(y_{ij} = 0 \mid z_{ij}) = \mathbf{1}(z_{ij} \le 0)$, so
\begin{align*}
p(y_{ij} = 0 \mid \theta_i, \alpha_j, \beta_j) &= \int_{-\infty}^0 \phi(z_{ij} - \mu_{ij}) \, dz_{ij}\\
&= \Pr(z_{ij} \le 0 \mid \theta_i, \alpha_j, \beta_j).
\end{align*}

Again standardize $U = z_{ij} - \mu_{ij}$:
\[
\Pr(z_{ij} \le 0) = \Pr(U \le -\mu_{ij}) = \Phi(-\mu_{ij}) = 1 - \Phi(\mu_{ij}),
\]
so
\[
p(y_{ij} = 0 \mid \theta_i, \alpha_j, \beta_j) = 1 - \Phi(\alpha_j(\theta_i - \beta_j)).
\]

We can combine both cases into the Bernoulli-pmf form
\[
p(y_{ij} \mid \theta_i, \alpha_j, \beta_j) = [\Phi(\alpha_j(\theta_i - \beta_j))]^{y_{ij}} \, [1 - \Phi(\alpha_j(\theta_i - \beta_j))]^{1 - y_{ij}}.
\]

Thus we have explicitly shown that
\[
\int p(y_{ij} \mid z_{ij}) \, p(z_{ij} \mid \theta_i, \beta_j, \alpha_j) \, dz_{ij} = p(y_{ij} \mid \theta_i, \alpha_j, \beta_j).
\]

\textbf{Step 4: Plug back into the marginal.}

Return to
\[
p(y, \alpha, \theta, \beta) = p(\theta) \, p(\beta) \, p(\alpha) \, \prod_{(i,j) \in O} \int p(y_{ij} \mid z_{ij}) \, p(z_{ij} \mid \theta_i, \beta_j, \alpha_j) \, dz_{ij}.
\]

By the calculation above, each integral is $p(y_{ij} \mid \theta_i, \alpha_j, \beta_j)$, so
\[
p(y, \alpha, \theta, \beta) = p(\theta) \, p(\beta) \, p(\alpha) \, \prod_{(i,j) \in O} p(y_{ij} \mid \theta_i, \alpha_j, \beta_j),
\]
i.e.
\[
p(y, \alpha, \theta, \beta) = p(\theta) \, p(\beta) \, p(\alpha) \, \prod_{(i,j) \in O} [\Phi(\alpha_j(\theta_i - \beta_j))]^{y_{ij}} [1 - \Phi(\alpha_j(\theta_i - \beta_j))]^{1 - y_{ij}}.
\]

\textbf{Step 5: Make $p(y \mid \alpha, \theta, \beta)$ explicit.}

By the definition of conditional density,
\[
p(y \mid \alpha, \theta, \beta) = \frac{p(y, \alpha, \theta, \beta)}{p(\alpha, \theta, \beta)} = \frac{p(y, \alpha, \theta, \beta)}{p(\theta) \, p(\beta) \, p(\alpha)}.
\]

Using the expression we just derived for $p(y, \alpha, \theta, \beta)$,
\[
p(y \mid \alpha, \theta, \beta) = \prod_{(i,j) \in O} p(y_{ij} \mid \theta_i, \alpha_j, \beta_j) = \prod_{(i,j) \in O} [\Phi(\alpha_j(\theta_i - \beta_j))]^{y_{ij}} [1 - \Phi(\alpha_j(\theta_i - \beta_j))]^{1 - y_{ij}}.
\]

Then, equivalently,
\[
p(y, \alpha, \theta, \beta) = p(\theta) \, p(\beta) \, p(\alpha) \, p(y \mid \alpha, \theta, \beta).
\]

\item \textbf{Variational family.} Assume a factorization
\[
q(\theta, \beta, \alpha, z) = \left(\prod_{i=1}^n q(\theta_i)\right) \left(\prod_{j=1}^d q(\beta_j) q(\alpha_j)\right) \left(\prod_{i=1}^n \prod_{j=1}^p q(z_{ij})\right),
\]
where $q(\theta_i)$ and $q(\beta_j)$ are Normal, $q(\alpha_j)$ is either Normal or truncated Normal (depending on your chosen prior), and $q(z_{ij})$ is a truncated Normal as in 1.2(b). Write the full family explicitly and state which moments of each factor you will need for updates.

Assume the mean-field factorization
\[
q(\theta, \beta, \alpha, z) = \left(\prod_{i=1}^n q(\theta_i)\right) \left(\prod_{j=1}^d q(\beta_j)\,q(\alpha_j)\right) \left(\prod_{i=1}^n \prod_{j=1}^p q(z_{ij})\right),
\]
with each factor in a parametric family as follows.

\textbf{Senator positions $\theta_i$.} For each $i=1,\dots,n$,
\[
q(\theta_i) = \mathcal{N}\!\big(m_{\theta_i},\,s_{\theta_i}^2\big),
\]
with free variational parameters $m_{\theta_i}\in\mathbb{R}$ and $s_{\theta_i}^2>0$.

\textbf{Bill locations $\beta_j$.} For each $j=1,\dots,d$,
\[
q(\beta_j) = \mathcal{N}\!\big(m_{\beta_j},\,s_{\beta_j}^2\big),
\]
with variational parameters $m_{\beta_j}\in\mathbb{R}$, $s_{\beta_j}^2>0$.

\textbf{Bill discriminations $\alpha_j$.} For each $j=1,\dots,d$,
\begin{itemize}
\item if the prior on $\alpha_j$ is Normal,
\[
q(\alpha_j) = \mathcal{N}\!\big(m_{\alpha_j},\,s_{\alpha_j}^2\big),
\]
\item if the prior on $\alpha_j$ is truncated Normal (e.g.\ $\alpha_j>0$),
\[
q(\alpha_j) = \mathcal{N}\!\big(m_{\alpha_j},\,s_{\alpha_j}^2\big) \;\text{truncated to}\;(0,\infty),
\]
\end{itemize}
with variational parameters $m_{\alpha_j}\in\mathbb{R}$, $s_{\alpha_j}^2>0$ in either case.

\textbf{Latent utilities $z_{ij}$.} For each observed vote $(i,j)\in O$ with $y_{ij}\in\{0,1\}$,
\[
q(z_{ij}) = \begin{cases}
\mathcal{N}\!\big(m_{z_{ij}},\,s_{z_{ij}}^2\big)\ \text{truncated to }(0,\infty), & y_{ij}=1,\\[4pt]
\mathcal{N}\!\big(m_{z_{ij}},\,s_{z_{ij}}^2\big)\ \text{truncated to }(-\infty,0], & y_{ij}=0,
\end{cases}
\]
with variational parameters $m_{z_{ij}}\in\mathbb{R}$, $s_{z_{ij}}^2>0$.

For missing votes $y_{ij}=-1$, no $q(z_{ij})$ factor is introduced (those pairs are excluded from the product).

Putting this together, the full family is
\begin{align*}
q(\theta,\beta,\alpha,z)
&=
\left[\prod_{i=1}^n \mathcal{N}\!\big(\theta_i\mid m_{\theta_i},s_{\theta_i}^2\big)\right]
\left[\prod_{j=1}^d \mathcal{N}\!\big(\beta_j\mid m_{\beta_j},s_{\beta_j}^2\big)\,q(\alpha_j)\right] \\
&\quad\times
\prod_{(i,j)\in O}
q(z_{ij}),
\end{align*}
with $q(\alpha_j)$ and $q(z_{ij})$ as specified above.

\textbf{Moments needed for CAVI updates.}

From these factors, the coordinate updates will require the following expectations:
\begin{itemize}
\item \textbf{For each senator $i$}:
\begin{itemize}
\item $\mathbb{E}_q[\theta_i] = m_{\theta_i}$
\item $\mathbb{E}_q[\theta_i^2] = s_{\theta_i}^2 + m_{\theta_i}^2$
\end{itemize}

\item \textbf{For each bill location $j$}:
\begin{itemize}
\item $\mathbb{E}_q[\beta_j] = m_{\beta_j}$
\item $\mathbb{E}_q[\beta_j^2] = s_{\beta_j}^2 + m_{\beta_j}^2$
\end{itemize}

\item \textbf{For each bill discrimination $j$}:
\begin{itemize}
\item $\mathbb{E}_q[\alpha_j]$
\item $\mathbb{E}_q[\alpha_j^2]$
\end{itemize}
(If $q(\alpha_j)$ is Normal, these are $m_{\alpha_j}$ and $s_{\alpha_j}^2 + m_{\alpha_j}^2$;
if truncated Normal, they are given by the standard truncated Normal moment formulas.)

\item \textbf{For each latent utility $z_{ij}$ with $(i,j)\in O$}:
\begin{itemize}
\item $\mathbb{E}_q[z_{ij}]$
\item $\mathbb{E}_q[z_{ij}^2]$
\end{itemize}
(Again, computed using the truncated Normal formulas for the appropriate truncation region.)
\end{itemize}

Because of the mean-field factorization,
\[
q(\theta,\beta,\alpha,z) = q(\theta)\,q(\beta)\,q(\alpha)\,q(z),
\]
all mixed expectations factor into products of first moments, for example:
\begin{itemize}
\item $\mathbb{E}_q[\alpha_j\,\theta_i] = \mathbb{E}_q[\alpha_j]\,\mathbb{E}_q[\theta_i]$,
\item $\mathbb{E}_q[\alpha_j\,\beta_j] = \mathbb{E}_q[\alpha_j]\,\mathbb{E}_q[\beta_j]$,
\item $\mathbb{E}_q[\alpha_j\,(\theta_i-\beta_j)] = \mathbb{E}_q[\alpha_j]\,(\mathbb{E}_q[\theta_i]-\mathbb{E}_q[\beta_j])$,
\end{itemize}
so all coordinate updates can be written in terms of the one- and two-moment quantities listed above.

\item \textbf{Coordinate updates.} Using the identity
\[
\log q^*(v) \propto \mathbb{E}_{-v}[\log p(y, z, \theta, \beta, \alpha)],
\]
derive expressions for the optimal factors up to Normal/truncated Normal forms. Specifically:
\begin{enumerate}[label=(\roman*)]
\item $q(z_{ij})$: update the mean parameter $\bar{\mu}_{ij}$ and give a formula for $\mathbb{E}_q[z_{ij}]$ using standard truncated-Normal moments (see Useful Formulas).
\item $q(\theta_i)$ and $q(\beta_j)$: write the precision and mean in terms of expectations $\mathbb{E}_q[\alpha_j]$, $\mathbb{E}_q[\alpha_j^2]$, and $\mathbb{E}_q[z_{ij}]$.
\item $q(\alpha_j)$: treat $\{z_{ij}\}_{i=1}^n$ as responses in a linear regression on $(\theta_i - \beta_j)$. Write the resulting mean and variance, and note how the update changes.
\end{enumerate}

Throughout, let
\begin{itemize}
\item $O = \{(i,j) : y_{ij} \in \{0,1\}\}$ be the set of observed votes,
\item $O_i = \{j : (i,j) \in O\}$ votes observed for senator $i$,
\item $O_j = \{i : (i,j) \in O\}$ votes observed on bill $j$.
\end{itemize}

We also keep the priors
\[
\theta_i \sim \mathcal{N}(\mu_\theta, \sigma_\theta^2), \quad \beta_j \sim \mathcal{N}(\mu_\beta, \sigma_\beta^2), \quad \alpha_j \sim \mathcal{N}(\mu_\alpha, \sigma_\alpha^2).
\]

The augmented model has, for $(i,j) \in O$,
\[
z_{ij} \mid \theta_i, \beta_j, \alpha_j \sim \mathcal{N}(\mu_{ij}, 1), \quad \mu_{ij} := \alpha_j(\theta_i - \beta_j),
\]
\[
y_{ij} = \mathbf{1}(z_{ij} > 0), \quad y_{ij} \in \{0,1\}.
\]

The variational family factorizes as
\[
q(\theta, \beta, \alpha, z) = \left(\prod_i q(\theta_i)\right) \left(\prod_j q(\beta_j)\,q(\alpha_j)\right) \left(\prod_{(i,j) \in O} q(z_{ij})\right).
\]

We use the standard CAVI identity
\[
\log q^*(v) \propto \mathbb{E}_{-v}[\log p(y, z, \theta, \beta, \alpha)].
\]

\textbf{(i) Update for $q(z_{ij})$.}

We isolate terms of the joint that involve a given $z_{ij}$:
\[
\log p(y, z, \theta, \beta, \alpha) = \log p(y_{ij} \mid z_{ij}) + \log p(z_{ij} \mid \theta_i, \beta_j, \alpha_j) + \text{(terms not involving $z_{ij}$)}.
\]

So
\[
\log q^*(z_{ij}) \propto \mathbb{E}_{-z_{ij}}[\log p(y_{ij} \mid z_{ij})] + \mathbb{E}_{-z_{ij}}[\log p(z_{ij} \mid \theta_i, \beta_j, \alpha_j)].
\]

The likelihood term enforces the truncation:
\begin{itemize}
\item If $y_{ij} = 1$, support is $z_{ij} > 0$,
\item If $y_{ij} = 0$, support is $z_{ij} \le 0$.
\end{itemize}

Within that support, $\log p(y_{ij} \mid z_{ij})$ is constant in $z_{ij}$.

The Gaussian prior conditional:
\[
p(z_{ij} \mid \theta_i, \beta_j, \alpha_j) = \mathcal{N}(z_{ij} \mid \mu_{ij}, 1), \quad \mu_{ij} = \alpha_j(\theta_i - \beta_j).
\]

Ignoring constants in $z_{ij}$, we have inside the support
\[
\log q^*(z_{ij}) \propto \mathbb{E}_{-z_{ij}}\left[-\frac{1}{2}(z_{ij} - \mu_{ij})^2\right] = -\frac{1}{2}\left(z_{ij}^2 - 2z_{ij}\,\mathbb{E}_q[\mu_{ij}]\right) + \text{const}.
\]

Thus, up to truncation, the optimal factor is a Normal with
\[
\bar{\mu}_{ij} := \mathbb{E}_q[\mu_{ij}] = \mathbb{E}_q[\alpha_j(\theta_i - \beta_j)].
\]

Under the mean-field factorization,
\[
\mathbb{E}_q[\alpha_j(\theta_i - \beta_j)] = \mathbb{E}_q[\alpha_j]\,(\mathbb{E}_q[\theta_i] - \mathbb{E}_q[\beta_j]).
\]

So
\[
q(z_{ij}) \propto \mathcal{N}(z_{ij} \mid \bar{\mu}_{ij}, 1) \times \begin{cases}
\mathbf{1}(z_{ij} > 0), & y_{ij} = 1,\\
\mathbf{1}(z_{ij} \le 0), & y_{ij} = 0.
\end{cases}
\]

That is,
\begin{itemize}
\item If $y_{ij} = 1$: $q(z_{ij})$ is $\mathcal{N}(\bar{\mu}_{ij}, 1)$ truncated to $(0,\infty)$.
\item If $y_{ij} = 0$: $q(z_{ij})$ is $\mathcal{N}(\bar{\mu}_{ij}, 1)$ truncated to $(-\infty,0]$.
\end{itemize}

\textbf{Moment $\mathbb{E}_q[z_{ij}]$.}

Let $\phi(\cdot)$ and $\Phi(\cdot)$ be the standard Normal pdf and cdf.
\begin{itemize}
\item If $y_{ij} = 1$ (truncate to $(0,\infty)$):
\[
\mathbb{E}_q[z_{ij}] = \bar{\mu}_{ij} + \frac{\phi(\bar{\mu}_{ij})}{\Phi(\bar{\mu}_{ij})}.
\]
\item If $y_{ij} = 0$ (truncate to $(-\infty,0]$):
\[
\mathbb{E}_q[z_{ij}] = \bar{\mu}_{ij} - \frac{\phi(\bar{\mu}_{ij})}{1 - \Phi(\bar{\mu}_{ij})}.
\]
\end{itemize}

These are the standard first moments for a Normal($\bar{\mu}_{ij}, 1$) truncated above or below at 0, respectively.

\textbf{(ii) Updates for $q(\theta_i)$ and $q(\beta_j)$.}

\textit{$q(\theta_i)$.}

We collect all terms involving $\theta_i$: the prior and the likelihood pieces for $z_{ij}$ with $j \in O_i$.
\[
\log p(\theta_i \mid \cdot) \propto \log p(\theta_i) + \sum_{j \in O_i} \log p(z_{ij} \mid \theta_i, \beta_j, \alpha_j).
\]

Using
\[
p(\theta_i) = \mathcal{N}(\theta_i \mid \mu_\theta, \sigma_\theta^2), \quad p(z_{ij} \mid \theta_i, \beta_j, \alpha_j) = \mathcal{N}(z_{ij} \mid \alpha_j(\theta_i - \beta_j), 1),
\]
we can write
\[
\log q^*(\theta_i) \propto -\frac{1}{2\sigma_\theta^2}(\theta_i - \mu_\theta)^2 - \frac{1}{2}\sum_{j \in O_i} (z_{ij} - \alpha_j(\theta_i - \beta_j))^2.
\]

Expand the quadratic in $\theta_i$, then take expectation over $\alpha_j, \beta_j, z_{ij}$ (all independent of $\theta_i$ under $q$). The resulting form is
\[
\log q^*(\theta_i) = -\frac{1}{2}\,\lambda_{\theta_i}\,\theta_i^2 + \eta_{\theta_i}\,\theta_i + \text{const},
\]
with
\[
\lambda_{\theta_i} = \frac{1}{\sigma_\theta^2} + \sum_{j \in O_i} \mathbb{E}_q[\alpha_j^2],
\]
\[
\eta_{\theta_i} = \frac{\mu_\theta}{\sigma_\theta^2} + \sum_{j \in O_i} (\mathbb{E}_q[\alpha_j]\,\mathbb{E}_q[z_{ij}] + \mathbb{E}_q[\alpha_j^2]\,\mathbb{E}_q[\beta_j]).
\]

Thus $q(\theta_i)$ is Normal,
\[
q(\theta_i) = \mathcal{N}(m_{\theta_i}, v_{\theta_i}),
\]
with
\[
v_{\theta_i} = \lambda_{\theta_i}^{-1}, \quad m_{\theta_i} = v_{\theta_i}\,\eta_{\theta_i}.
\]

So the update for $q(\theta_i)$ uses the moments $\mathbb{E}_q[\alpha_j]$, $\mathbb{E}_q[\alpha_j^2]$, $\mathbb{E}_q[z_{ij}]$, and $\mathbb{E}_q[\beta_j]$.

\textit{$q(\beta_j)$.}

Similarly, for each bill $j$, collect terms involving $\beta_j$: its prior and the likelihood pieces for $z_{ij}$ with $i \in O_j$:
\[
\log p(\beta_j \mid \cdot) \propto \log p(\beta_j) + \sum_{i \in O_j} \log p(z_{ij} \mid \theta_i, \beta_j, \alpha_j).
\]

With
\[
p(\beta_j) = \mathcal{N}(\beta_j \mid \mu_\beta, \sigma_\beta^2),
\]
and the same Normal likelihood, expanding in $\beta_j$ and taking expectations gives
\[
\log q^*(\beta_j) = -\frac{1}{2}\,\lambda_{\beta_j}\,\beta_j^2 + \eta_{\beta_j}\,\beta_j + \text{const},
\]
where
\[
\lambda_{\beta_j} = \frac{1}{\sigma_\beta^2} + \sum_{i \in O_j} \mathbb{E}_q[\alpha_j^2],
\]
\[
\eta_{\beta_j} = \frac{\mu_\beta}{\sigma_\beta^2} + \sum_{i \in O_j} (\mathbb{E}_q[\alpha_j^2]\,\mathbb{E}_q[\theta_i] - \mathbb{E}_q[\alpha_j]\,\mathbb{E}_q[z_{ij}]).
\]

Thus
\[
q(\beta_j) = \mathcal{N}(m_{\beta_j}, v_{\beta_j}),
\]
with
\[
v_{\beta_j} = \lambda_{\beta_j}^{-1}, \quad m_{\beta_j} = v_{\beta_j}\,\eta_{\beta_j}.
\]

Again, the update depends on $\mathbb{E}_q[\alpha_j]$, $\mathbb{E}_q[\alpha_j^2]$, $\mathbb{E}_q[z_{ij}]$, $\mathbb{E}_q[\theta_i]$.

\textbf{(iii) Update for $q(\alpha_j)$.}

For a fixed bill $j$, the terms involving $\alpha_j$ are its prior and the likelihood contributions $p(z_{ij} \mid \theta_i, \beta_j, \alpha_j)$ for $i \in O_j$:
\[
\log p(\alpha_j \mid \cdot) \propto \log p(\alpha_j) + \sum_{i \in O_j} \log p(z_{ij} \mid \theta_i, \beta_j, \alpha_j).
\]

Using the Normal prior
\[
p(\alpha_j) = \mathcal{N}(\alpha_j \mid \mu_\alpha, \sigma_\alpha^2),
\]
and writing the likelihood as a linear regression:
\[
z_{ij} = \alpha_j x_{ij} + \varepsilon_{ij}, \quad x_{ij} := \theta_i - \beta_j, \quad \varepsilon_{ij} \sim \mathcal{N}(0, 1),
\]
the log-likelihood term is
\[
\sum_{i \in O_j} \log \mathcal{N}(z_{ij} \mid \alpha_j x_{ij}, 1) \propto -\frac{1}{2}\sum_{i \in O_j} (z_{ij} - \alpha_j x_{ij})^2.
\]

Expanding in $\alpha_j$ and taking expectation over $z_{ij}, \theta_i, \beta_j$ (but not $\alpha_j$) gives
\[
\log q^*(\alpha_j) = -\frac{1}{2}\,\lambda_{\alpha_j}\,\alpha_j^2 + \eta_{\alpha_j}\,\alpha_j + \text{const},
\]
where
\[
\lambda_{\alpha_j} = \frac{1}{\sigma_\alpha^2} + \sum_{i \in O_j} \mathbb{E}_q[x_{ij}^2], \quad x_{ij} = \theta_i - \beta_j,
\]
\[
\eta_{\alpha_j} = \frac{\mu_\alpha}{\sigma_\alpha^2} + \sum_{i \in O_j} \mathbb{E}_q[z_{ij} x_{ij}].
\]

Under the mean-field factorization, $z_{ij}$, $\theta_i$, and $\beta_j$ are independent under $q$, so
\[
\mathbb{E}_q[x_{ij}] = \mathbb{E}_q[\theta_i] - \mathbb{E}_q[\beta_j],
\]
\[
\mathbb{E}_q[x_{ij}^2] = \mathbb{E}_q[\theta_i^2] - 2\,\mathbb{E}_q[\theta_i]\,\mathbb{E}_q[\beta_j] + \mathbb{E}_q[\beta_j^2],
\]
\[
\mathbb{E}_q[z_{ij} x_{ij}] = \mathbb{E}_q[z_{ij}]\,(\mathbb{E}_q[\theta_i] - \mathbb{E}_q[\beta_j]).
\]

So $q(\alpha_j)$ is Gaussian in the unconstrained case:
\[
q(\alpha_j) = \mathcal{N}(m_{\alpha_j}, v_{\alpha_j}),
\]
with
\[
v_{\alpha_j} = \lambda_{\alpha_j}^{-1}, \quad m_{\alpha_j} = v_{\alpha_j}\,\eta_{\alpha_j}.
\]

\end{enumerate}

\newpage

\section*{Problem 2: Gaussian Matrix Factorization (MFVI from conditionals + features)}

\subsection*{\S Model}

Let $X \in \mathbb{R}^{n \times m}$ be a partially observed ratings matrix with observed index set $\Omega \subseteq \{1, \ldots, n\} \times \{1, \ldots, m\}$. Fix latent dimension $K$. For users $i$ and items $j$ we have factors $\theta_i, \beta_j \in \mathbb{R}^K$.
\[
\theta_i \sim \mathcal{N}(0, \eta^2_\theta I_K), \quad \beta_j \sim \mathcal{N}(0, \eta^2_B I_K), \quad x_{ij} \mid \theta_i^\top \beta_j \sim \mathcal{N}(\theta_i^\top \beta_j, \sigma^2), \quad (i, j) \in \Omega.
\]

Define $\Omega_i = \{j : (i, j) \in \Omega\}$ and $\Omega^j = \{i : (i, j) \in \Omega\}$. Throughout, $\sigma^2, \eta^2_\theta, \eta^2_B$ are known.

\textbf{(Given) Complete conditionals.} You may use the following (conjugate) complete conditionals in your derivations.
\begin{align}
p(\theta_i \mid \{\beta_j\}, X, \sigma^2, \eta^2_\theta) &= \mathcal{N}(\mu_{\theta_i}, \Sigma_{\theta_i}), \quad \Sigma^{-1}_{\theta_i} = \eta^{-2}_\theta I_K + \sigma^{-2} \sum_{j \in \Omega_i} \beta_j \beta_j^\top, \quad \mu_{\theta_i} = \Sigma_{\theta_i} \sigma^{-2} \sum_{j \in \Omega_i} \beta_j x_{ij},\\
p(\beta_j \mid \{\theta_i\}, X, \sigma^2, \eta^2_B) &= \mathcal{N}(\mu_{\beta_j}, \Sigma_{\beta_j}), \quad \Sigma^{-1}_{\beta_j} = \eta^{-2}_B I_K + \sigma^{-2} \sum_{i \in \Omega^j} \theta_i \theta_i^\top, \quad \mu_{\beta_j} = \Sigma_{\beta_j} \sigma^{-2} \sum_{i \in \Omega^j} \theta_i x_{ij}.
\end{align}

\subsection*{Question 2.1: Mean-field VI (CAVI from the conditionals)}

We approximate the posterior with a factorized family
\[
q(\Theta, B) = \prod_{i=1}^n q(\theta_i) \prod_{j=1}^m q(\beta_j), \quad q(\theta_i) = \mathcal{N}(m_{\theta_i}, V_{\theta_i}), \quad q(\beta_j) = \mathcal{N}(m_{\beta_j}, V_{\beta_j}).
\]

\begin{enumerate}[label=\alph*)]
\item \textbf{ELBO pieces.} Write the ELBO $\mathcal{L}(q)$ and list the expectations it comprises.

We use the mean-field family
\[
q(\Theta, B) = \prod_{i=1}^n q(\theta_i) \prod_{j=1}^m q(\beta_j), \quad q(\theta_i) = \mathcal{N}(m_{\theta_i}, V_{\theta_i}), \quad q(\beta_j) = \mathcal{N}(m_{\beta_j}, V_{\beta_j}).
\]

The joint model:
\[
\theta_i \sim \mathcal{N}(0, \eta_\theta^2 I_K), \quad \beta_j \sim \mathcal{N}(0, \eta_B^2 I_K),
\]
\[
x_{ij} \mid \theta_i, \beta_j \sim \mathcal{N}(\theta_i^\top \beta_j, \sigma^2), \quad (i,j) \in \Omega.
\]

So the ELBO is
\[
\mathcal{L}(q) = \mathbb{E}_q[\log p(X, \Theta, B)] - \mathbb{E}_q[\log q(\Theta, B)].
\]

Break it into pieces.

\textbf{Likelihood term.}
\[
\log p(X \mid \Theta, B) = \sum_{(i,j) \in \Omega} \log \mathcal{N}(x_{ij} \mid \theta_i^\top \beta_j, \sigma^2).
\]

Up to an additive constant in $q$,
\[
\log \mathcal{N}(x_{ij} \mid \theta_i^\top \beta_j, \sigma^2) = -\frac{1}{2\sigma^2}(x_{ij} - \theta_i^\top \beta_j)^2 + \text{const}.
\]

So
\[
\mathbb{E}_q[\log p(X \mid \Theta, B)] = -\frac{1}{2\sigma^2} \sum_{(i,j) \in \Omega} \mathbb{E}_q[(x_{ij} - \theta_i^\top \beta_j)^2] + \text{const},
\]
with
\[
\mathbb{E}_q[(x_{ij} - \theta_i^\top \beta_j)^2] = x_{ij}^2 - 2x_{ij}\,\mathbb{E}_q[\theta_i^\top \beta_j] + \mathbb{E}_q[(\theta_i^\top \beta_j)^2].
\]

\textbf{Prior terms.}
\[
\log p(\Theta) = \sum_{i=1}^n \log \mathcal{N}(\theta_i \mid 0, \eta_\theta^2 I_K),
\]
\[
\log p(B) = \sum_{j=1}^m \log \mathcal{N}(\beta_j \mid 0, \eta_B^2 I_K).
\]

Up to constants,
\[
\mathbb{E}_q[\log p(\Theta)] = -\frac{1}{2\eta_\theta^2} \sum_{i=1}^n \mathbb{E}_q[\theta_i^\top \theta_i] + \text{const},
\]
\[
\mathbb{E}_q[\log p(B)] = -\frac{1}{2\eta_B^2} \sum_{j=1}^m \mathbb{E}_q[\beta_j^\top \beta_j] + \text{const}.
\]

\textbf{Entropy (variational) terms.}
\[
\mathbb{E}_q[\log q(\Theta, B)] = \sum_{i=1}^n \mathbb{E}_q[\log q(\theta_i)] + \sum_{j=1}^m \mathbb{E}_q[\log q(\beta_j)].
\]

For each Gaussian factor,
\[
\mathbb{E}_q[\log q(\theta_i)] = -\frac{1}{2}\big(\log |V_{\theta_i}| + K(1 + \log 2\pi)\big),
\]
\[
\mathbb{E}_q[\log q(\beta_j)] = -\frac{1}{2}\big(\log |V_{\beta_j}| + K(1 + \log 2\pi)\big).
\]

\textbf{Putting it together (up to constants independent of $q$):}
\begin{align*}
\mathcal{L}(q) &= -\frac{1}{2\sigma^2} \sum_{(i,j) \in \Omega} \big(x_{ij}^2 - 2x_{ij}\,\mathbb{E}_q[\theta_i^\top \beta_j] + \mathbb{E}_q[(\theta_i^\top \beta_j)^2]\big)\\
&\quad -\frac{1}{2\eta_\theta^2} \sum_i \mathbb{E}_q[\theta_i^\top \theta_i] - \frac{1}{2\eta_B^2} \sum_j \mathbb{E}_q[\beta_j^\top \beta_j]\\
&\quad - \sum_i \mathbb{E}_q[\log q(\theta_i)] - \sum_j \mathbb{E}_q[\log q(\beta_j)] + \text{const}.
\end{align*}

\textbf{Expectations needed under $q$.}

Under the mean-field Gaussians,

\textit{First and second moments of user factors:}
\[
\mathbb{E}_q[\theta_i] = m_{\theta_i}, \quad \mathbb{E}_q[\theta_i \theta_i^\top] = V_{\theta_i} + m_{\theta_i} m_{\theta_i}^\top.
\]

\textit{First and second moments of item factors:}
\[
\mathbb{E}_q[\beta_j] = m_{\beta_j}, \quad \mathbb{E}_q[\beta_j \beta_j^\top] = V_{\beta_j} + m_{\beta_j} m_{\beta_j}^\top.
\]

\textit{Cross terms for likelihood:}
\[
\mathbb{E}_q[\theta_i^\top \beta_j] = m_{\theta_i}^\top m_{\beta_j},
\]
\[
\mathbb{E}_q[(\theta_i^\top \beta_j)^2] = \mathbb{E}_q[\beta_j^\top (\theta_i \theta_i^\top) \beta_j] = \text{tr}\!\big((V_{\theta_i} + m_{\theta_i} m_{\theta_i}^\top)(V_{\beta_j} + m_{\beta_j} m_{\beta_j}^\top)\big).
\]

\textit{Entropy pieces for each Gaussian factor:}

$\mathbb{E}_q[\log q(\theta_i)]$, $\mathbb{E}_q[\log q(\beta_j)]$ as above in terms of $V_{\theta_i}, V_{\beta_j}$.

\item \textbf{CAVI updates.} Using the identity $\log q^\star(x_v) \propto \mathbb{E}_{-v}[\log p(x_v \mid x_{-v})]$ and the \emph{given} complete conditionals 3 and 4, derive the optimal CAVI updates by replacing unknowns with their $q$-expectations. State the updates for $V^{-1}_{\theta_i}$, $V^{-1}_{\beta_j}$, $m_{\theta_i}$, and $m_{\beta_j}$.

Let the mean-field family be
\[
q(\Theta, B) = \prod_{i=1}^n q(\theta_i) \prod_{j=1}^m q(\beta_j)
\]
with
\[
q(\theta_i) = \mathcal{N}(m_{\theta_i}, V_{\theta_i}), \quad q(\beta_j) = \mathcal{N}(m_{\beta_j}, V_{\beta_j}).
\]

Define the index sets
\[
\Omega_i := \{j : (i,j) \in \Omega\}, \quad \Omega_j := \{i : (i,j) \in \Omega\}.
\]

The given complete conditionals are
\[
p(\theta_i \mid \{\beta_j\}, X) = \mathcal{N}(\mu_{\theta_i}, \Sigma_{\theta_i}), \quad \Sigma_{\theta_i}^{-1} = \eta_\theta^{-2} I_K + \sigma^{-2} \sum_{j \in \Omega_i} \beta_j \beta_j^\top,
\]
\[
\mu_{\theta_i} = \Sigma_{\theta_i}\,\sigma^{-2} \sum_{j \in \Omega_i} \beta_j x_{ij},
\]
and
\[
p(\beta_j \mid \{\theta_i\}, X) = \mathcal{N}(\mu_{\beta_j}, \Sigma_{\beta_j}), \quad \Sigma_{\beta_j}^{-1} = \eta_B^{-2} I_K + \sigma^{-2} \sum_{i \in \Omega_j} \theta_i \theta_i^\top,
\]
\[
\mu_{\beta_j} = \Sigma_{\beta_j}\,\sigma^{-2} \sum_{i \in \Omega_j} \theta_i x_{ij}.
\]

Using
\[
\log q^\star(x_v) \propto \mathbb{E}_{-v}[\log p(x_v \mid x_{-v})],
\]
the optimal variational factor has the same Gaussian form, with each occurrence of $\beta_j$, $\beta_j \beta_j^\top$ and $\theta_i$, $\theta_i \theta_i^\top$ replaced by their expectations under $q$.

Because
\[
\mathbb{E}_q[\beta_j] = m_{\beta_j}, \quad \mathbb{E}_q[\beta_j \beta_j^\top] = V_{\beta_j} + m_{\beta_j} m_{\beta_j}^\top,
\]
\[
\mathbb{E}_q[\theta_i] = m_{\theta_i}, \quad \mathbb{E}_q[\theta_i \theta_i^\top] = V_{\theta_i} + m_{\theta_i} m_{\theta_i}^\top,
\]
the CAVI updates are:

\textbf{User factors $\theta_i$:}

\textit{Precision:}
\[
V_{\theta_i}^{-1} = \eta_\theta^{-2} I_K + \sigma^{-2} \sum_{j \in \Omega_i} \mathbb{E}_q[\beta_j \beta_j^\top] = \eta_\theta^{-2} I_K + \sigma^{-2} \sum_{j \in \Omega_i} (V_{\beta_j} + m_{\beta_j} m_{\beta_j}^\top).
\]

\textit{Mean:}
\[
m_{\theta_i} = V_{\theta_i}\,\sigma^{-2} \sum_{j \in \Omega_i} \mathbb{E}_q[\beta_j]\,x_{ij} = V_{\theta_i}\,\sigma^{-2} \sum_{j \in \Omega_i} m_{\beta_j}\,x_{ij}.
\]

\textbf{Item factors $\beta_j$:}

\textit{Precision:}
\[
V_{\beta_j}^{-1} = \eta_B^{-2} I_K + \sigma^{-2} \sum_{i \in \Omega_j} \mathbb{E}_q[\theta_i \theta_i^\top] = \eta_B^{-2} I_K + \sigma^{-2} \sum_{i \in \Omega_j} (V_{\theta_i} + m_{\theta_i} m_{\theta_i}^\top).
\]

\textit{Mean:}
\[
m_{\beta_j} = V_{\beta_j}\,\sigma^{-2} \sum_{i \in \Omega_j} \mathbb{E}_q[\theta_i]\,x_{ij} = V_{\beta_j}\,\sigma^{-2} \sum_{i \in \Omega_j} m_{\theta_i}\,x_{ij}.
\]

\item \textbf{Algorithm sketch.} Give pseudocode for one CAVI sweep:
\[
\{q(\theta_i)\}_{i=1}^n \to \{q(\beta_j)\}_{j=1}^m,
\]
including which expectations are recomputed and a convergence criterion (e.g., ELBO monotone ascent or small parameter change).

\textbf{INPUT:}
\begin{itemize}
\item $X$ -- ratings matrix (with missing entries)
\item $\Omega$ -- set of observed indices $(i, j)$
\item $\Omega_i$ -- for each $i$, $\Omega_i = \{j : (i, j) \in \Omega\}$
\item $\Omega_j$ -- for each $j$, $\Omega_j = \{i : (i, j) \in \Omega\}$
\item $\sigma^2$ -- noise variance
\item $\eta_\theta^2, \eta_B^2$ -- prior variances
\item $K$ -- latent dimension
\item $\texttt{tol}$ -- convergence tolerance (e.g., $10^{-4}$)
\item $\texttt{max\_iters}$ -- maximum CAVI iterations
\end{itemize}

\textbf{INITIALIZE:}
\begin{itemize}
\item For $i = 1$ to $n$:
\begin{itemize}
\item $m_{\theta_i} \gets$ random $K$-vector
\item $V_{\theta_i} \gets I_K$ ($K \times K$ matrix)
\end{itemize}
\item For $j = 1$ to $m$:
\begin{itemize}
\item $m_{\beta_j} \gets$ random $K$-vector
\item $V_{\beta_j} \gets I_K$
\end{itemize}
\end{itemize}

\textbf{REPEAT} for $t = 1, 2, \ldots, \texttt{max\_iters}$:

\quad \textit{// Store old means for convergence check}

\quad $m_{\theta}^{\text{old}} \gets \{m_{\theta_i}\}$ for all $i$

\quad $m_{\beta}^{\text{old}} \gets \{m_{\beta_j}\}$ for all $j$

\quad \textbf{1. Compute expectations under $q(\beta_j)$ for current iteration}

\quad For $j = 1$ to $m$:
\begin{itemize}
\item $\mathbb{E}_\beta[j] \gets m_{\beta_j}$ \quad \textit{// $\mathbb{E}_q[\beta_j]$}
\item $\mathbb{E}_{\beta\text{-outer}}[j] \gets V_{\beta_j} + m_{\beta_j} m_{\beta_j}^\top$ \quad \textit{// $\mathbb{E}_q[\beta_j \beta_j^\top]$}
\end{itemize}

\quad \textbf{2. Update $q(\theta_i)$ block}

\quad For $i = 1$ to $n$:
\begin{itemize}
\item $\text{Prec}_{\theta_i} \gets \eta_\theta^{-2} I_K$ \quad \textit{// precision (inverse covariance) of $\theta_i$}
\item $b_{\theta_i} \gets \mathbf{0}_K$ \quad \textit{// linear term for $\theta_i$ mean}
\item For each $j \in \Omega_i$:
\begin{itemize}
\item $\text{Prec}_{\theta_i} \gets \text{Prec}_{\theta_i} + \sigma^{-2} \mathbb{E}_{\beta\text{-outer}}[j]$
\item $b_{\theta_i} \gets b_{\theta_i} + \sigma^{-2} \mathbb{E}_\beta[j] \cdot X[i, j]$
\end{itemize}
\item $V_{\theta_i} \gets \text{Prec}_{\theta_i}^{-1}$
\item $m_{\theta_i} \gets V_{\theta_i} \cdot b_{\theta_i}$
\end{itemize}

\quad \textbf{3. Compute expectations under updated $q(\theta_i)$}

\quad For $i = 1$ to $n$:
\begin{itemize}
\item $\mathbb{E}_\theta[i] \gets m_{\theta_i}$ \quad \textit{// $\mathbb{E}_q[\theta_i]$}
\item $\mathbb{E}_{\theta\text{-outer}}[i] \gets V_{\theta_i} + m_{\theta_i} m_{\theta_i}^\top$ \quad \textit{// $\mathbb{E}_q[\theta_i \theta_i^\top]$}
\end{itemize}

\quad \textbf{4. Update $q(\beta_j)$ block}

\quad For $j = 1$ to $m$:
\begin{itemize}
\item $\text{Prec}_{\beta_j} \gets \eta_B^{-2} I_K$ \quad \textit{// precision (inverse covariance) of $\beta_j$}
\item $b_{\beta_j} \gets \mathbf{0}_K$ \quad \textit{// linear term for $\beta_j$ mean}
\item For each $i \in \Omega_j$:
\begin{itemize}
\item $\text{Prec}_{\beta_j} \gets \text{Prec}_{\beta_j} + \sigma^{-2} \mathbb{E}_{\theta\text{-outer}}[i]$
\item $b_{\beta_j} \gets b_{\beta_j} + \sigma^{-2} \mathbb{E}_\theta[i] \cdot X[i, j]$
\end{itemize}
\item $V_{\beta_j} \gets \text{Prec}_{\beta_j}^{-1}$
\item $m_{\beta_j} \gets V_{\beta_j} \cdot b_{\beta_j}$
\end{itemize}

\quad \textbf{5. Convergence criterion}
\begin{itemize}
\item $\Delta_\theta \gets \max_i \|m_{\theta_i} - m_{\theta_i}^{\text{old}}\|_2$
\item $\Delta_\beta \gets \max_j \|m_{\beta_j} - m_{\beta_j}^{\text{old}}\|_2$
\item $\Delta \gets \max(\Delta_\theta, \Delta_\beta)$
\item If $\Delta < \texttt{tol}$: \textbf{break}
\end{itemize}

\textbf{OUTPUT:}

$\{m_{\theta_i}, V_{\theta_i}\}$ for $i = 1, \ldots, n$

$\{m_{\beta_j}, V_{\beta_j}\}$ for $j = 1, \ldots, m$

\item \textbf{Flagging uncertain recommendations} Using $q$ derive the approximate posterior predictive variance $\text{Var}(x_{ij} \mid \text{data})$ on a holdout set (i.e., $(i, j) \notin \Omega$) and explain how you would use this variance to flag uncertain recommendations.

We consider a single held-out user-item pair $(i,j) \notin \Omega$.

The generative model for the rating is
\[
x_{ij} \mid \theta_i, \beta_j \sim \mathcal{N}(\theta_i^\top \beta_j, \sigma^2).
\]

Under the variational posterior,
\[
q(\theta_i) = \mathcal{N}(m_{\theta_i}, V_{\theta_i}), \qquad q(\beta_j) = \mathcal{N}(m_{\beta_j}, V_{\beta_j}),
\]
and, by the mean-field assumption,
\[
q(\theta_i, \beta_j) = q(\theta_i)\,q(\beta_j),
\]
so $\theta_i$ and $\beta_j$ are independent under $q$.

The posterior predictive for $x_{ij}$ is approximated by integrating out $\theta_i, \beta_j$ under $q$:
\[
p(x_{ij} \mid \text{data}) \approx \iint p(x_{ij} \mid \theta_i, \beta_j)\,q(\theta_i)\,q(\beta_j)\,d\theta_i\,d\beta_j.
\]

We now derive $\operatorname{Var}(x_{ij} \mid \text{data})$ under this approximation.

\textbf{Step 1: Law of total variance.}

Define the latent mean function
\[
\mu_{ij}(\theta_i, \beta_j) := \theta_i^\top \beta_j.
\]

The law of total variance gives
\[
\operatorname{Var}(x_{ij} \mid \text{data}) \approx \mathbb{E}_q\!\big[\operatorname{Var}(x_{ij} \mid \theta_i, \beta_j)\big] + \operatorname{Var}_q\!\big(\mathbb{E}[x_{ij} \mid \theta_i, \beta_j]\big),
\]
where the expectation and variance are with respect to $q(\theta_i, \beta_j)$.

From the Gaussian likelihood,
\[
\operatorname{Var}(x_{ij} \mid \theta_i, \beta_j) = \sigma^2, \qquad \mathbb{E}[x_{ij} \mid \theta_i, \beta_j] = \mu_{ij}(\theta_i, \beta_j) = \theta_i^\top \beta_j.
\]

Therefore,
\[
\mathbb{E}_q\!\big[\operatorname{Var}(x_{ij} \mid \theta_i, \beta_j)\big] = \mathbb{E}_q[\sigma^2] = \sigma^2,
\]
and
\[
\operatorname{Var}_q\!\big(\mathbb{E}[x_{ij} \mid \theta_i, \beta_j]\big) = \operatorname{Var}_q(\theta_i^\top \beta_j).
\]

So
\[
\operatorname{Var}(x_{ij} \mid \text{data}) \approx \sigma^2 + \operatorname{Var}_q(\theta_i^\top \beta_j).
\]

The problem is now reduced to computing $\operatorname{Var}_q(\theta_i^\top \beta_j)$.

\textbf{Step 2: Moments of $\theta_i^\top \beta_j$ under $q$.}

Under $q$,
\[
\theta_i \sim \mathcal{N}(m_{\theta_i}, V_{\theta_i}), \qquad \beta_j \sim \mathcal{N}(m_{\beta_j}, V_{\beta_j}),
\]
with $\theta_i \perp \beta_j$ (independent).

We will compute:
\begin{itemize}
\item $\mathbb{E}_q[\theta_i^\top \beta_j]$,
\item $\mathbb{E}_q[(\theta_i^\top \beta_j)^2]$,
\item $\operatorname{Var}_q(\theta_i^\top \beta_j) = \mathbb{E}_q[(\theta_i^\top \beta_j)^2] - \big(\mathbb{E}_q[\theta_i^\top \beta_j]\big)^2$.
\end{itemize}

\textit{Step 2.1: First moment $\mathbb{E}_q[\theta_i^\top \beta_j]$.}

Using linearity of expectation and independence:
\[
\mathbb{E}_q[\theta_i^\top \beta_j] = \mathbb{E}_q[\theta_i]^\top \mathbb{E}_q[\beta_j] = m_{\theta_i}^\top m_{\beta_j}.
\]

\textit{Step 2.2: Second moment $\mathbb{E}_q[(\theta_i^\top \beta_j)^2]$.}

Start from the scalar square:
\[
(\theta_i^\top \beta_j)^2 = (\beta_j^\top \theta_i)(\theta_i^\top \beta_j) = \beta_j^\top(\theta_i \theta_i^\top)\beta_j.
\]

Use the identity for quadratic forms in terms of trace:
\[
x^\top A x = \operatorname{tr}(A x x^\top).
\]

Here, take $x = \beta_j$, $A = \theta_i \theta_i^\top$. Then
\[
\beta_j^\top(\theta_i \theta_i^\top)\beta_j = \operatorname{tr}(\theta_i \theta_i^\top \,\beta_j \beta_j^\top).
\]

Thus
\[
(\theta_i^\top \beta_j)^2 = \operatorname{tr}(\theta_i \theta_i^\top \,\beta_j \beta_j^\top).
\]

Now take expectation with respect to $q$:
\[
\mathbb{E}_q[(\theta_i^\top \beta_j)^2] = \mathbb{E}_q\big[\operatorname{tr}(\theta_i \theta_i^\top \,\beta_j \beta_j^\top)\big].
\]

Use linearity of trace and expectation:
\[
\mathbb{E}_q\big[\operatorname{tr}(\theta_i \theta_i^\top \,\beta_j \beta_j^\top)\big] = \operatorname{tr}\big(\mathbb{E}_q[\theta_i \theta_i^\top \,\beta_j \beta_j^\top]\big).
\]

Under the mean-field factorization, $\theta_i$ and $\beta_j$ are independent, so
\[
\mathbb{E}_q[\theta_i \theta_i^\top \,\beta_j \beta_j^\top] = \mathbb{E}_q[\theta_i \theta_i^\top]\;\mathbb{E}_q[\beta_j \beta_j^\top].
\]

Define the second-moment matrices
\[
S_{\theta_i} := \mathbb{E}_q[\theta_i \theta_i^\top], \qquad S_{\beta_j} := \mathbb{E}_q[\beta_j \beta_j^\top].
\]

Then
\[
\mathbb{E}_q[(\theta_i^\top \beta_j)^2] = \operatorname{tr}\big(S_{\theta_i} S_{\beta_j}\big).
\]

For Gaussian factors,
\[
S_{\theta_i} = V_{\theta_i} + m_{\theta_i} m_{\theta_i}^\top, \qquad S_{\beta_j} = V_{\beta_j} + m_{\beta_j} m_{\beta_j}^\top.
\]

\textit{Step 2.3: Variance $\operatorname{Var}_q(\theta_i^\top \beta_j)$.}

By definition,
\[
\operatorname{Var}_q(\theta_i^\top \beta_j) = \mathbb{E}_q[(\theta_i^\top \beta_j)^2] - \big(\mathbb{E}_q[\theta_i^\top \beta_j]\big)^2.
\]

We already have:
\begin{itemize}
\item $\mathbb{E}_q[(\theta_i^\top \beta_j)^2] = \operatorname{tr}(S_{\theta_i} S_{\beta_j})$,
\item $\mathbb{E}_q[\theta_i^\top \beta_j] = m_{\theta_i}^\top m_{\beta_j}$.
\end{itemize}

Thus
\[
\operatorname{Var}_q(\theta_i^\top \beta_j) = \operatorname{tr}(S_{\theta_i} S_{\beta_j}) - (m_{\theta_i}^\top m_{\beta_j})^2.
\]

Using $S_{\theta_i} = V_{\theta_i} + m_{\theta_i} m_{\theta_i}^\top$ and $S_{\beta_j} = V_{\beta_j} + m_{\beta_j} m_{\beta_j}^\top$, expanding and simplifying yields the equivalent expression
\[
\operatorname{Var}_q(\theta_i^\top \beta_j) = \operatorname{tr}(V_{\theta_i} V_{\beta_j}) + m_{\beta_j}^\top V_{\theta_i} m_{\beta_j} + m_{\theta_i}^\top V_{\beta_j} m_{\theta_i}.
\]

\textbf{Step 3: Posterior predictive variance.}

Putting everything together:
\[
\operatorname{Var}(x_{ij} \mid \text{data}) \approx \sigma^2 + \operatorname{Var}_q(\theta_i^\top \beta_j).
\]

So the two equivalent formulas are:

\textit{Compact form (via second-moment matrices):}
\[
\boxed{
\operatorname{Var}(x_{ij} \mid \text{data}) \approx \sigma^2 + \operatorname{tr}\big(S_{\theta_i} S_{\beta_j}\big) - (m_{\theta_i}^\top m_{\beta_j})^2
}
\]
with
\[
S_{\theta_i} = V_{\theta_i} + m_{\theta_i} m_{\theta_i}^\top, \qquad S_{\beta_j} = V_{\beta_j} + m_{\beta_j} m_{\beta_j}^\top.
\]

\textit{Expanded form (in terms of $V$ and $m$ only):}
\[
\boxed{
\operatorname{Var}(x_{ij} \mid \text{data}) \approx \sigma^2 + \operatorname{tr}(V_{\theta_i} V_{\beta_j}) + m_{\beta_j}^\top V_{\theta_i} m_{\beta_j} + m_{\theta_i}^\top V_{\beta_j} m_{\theta_i}
}
\]

Here $m_{\theta_i}, V_{\theta_i}, m_{\beta_j}, V_{\beta_j}$ are the current variational means and covariances from the MFVI algorithm.

\textbf{Using this variance to flag uncertain recommendations.}

For a held-out pair $(i,j) \notin \Omega$:
\begin{itemize}
\item \textit{Predictive mean:}
\[
\hat{x}_{ij} := \mathbb{E}_q[x_{ij} \mid \text{data}] \approx m_{\theta_i}^\top m_{\beta_j}.
\]
\item \textit{Predictive variance:}
\[
\widehat{\operatorname{Var}}(x_{ij} \mid \text{data}) \approx \sigma^2 + \operatorname{Var}_q(\theta_i^\top \beta_j),
\]
using either of the formulas above.
\end{itemize}

To flag uncertain recommendations, one can:
\begin{enumerate}
\item Compute $\widehat{\operatorname{Var}}(x_{ij} \mid \text{data})$ for all candidate $(i,j)$ in the recommendation pool.
\item Choose a threshold $\tau$ (e.g., the 80th or 90th percentile of the variance values).
\item Mark all pairs with
\[
\widehat{\operatorname{Var}}(x_{ij} \mid \text{data}) > \tau
\]
as high-uncertainty.
\item Use this flag to:
\begin{itemize}
\item avoid recommending high-uncertainty items,
\item or surface them only as ``exploration'' queries,
\item or ask the user for more feedback on those items.
\end{itemize}
\end{enumerate}

Pairs with lower variance are treated as more reliable, high-confidence recommendations.

\item \textbf{Setting hyperparameters} Explain how you could set $\eta^2_\theta$, $\eta^2_B$, and $\sigma^2$ using heldout data.
\end{enumerate}

\subsection*{Question 2.3: Adding item features (e.g., Genre) and updating CAVI}

Let $g_j \in \mathbb{R}^p$ be a (possibly multi-hot) feature vector for item $j$ (e.g., genres).

\textbf{(additive side term).} Augment the likelihood with an additive linear effect of features:
\[
x_{ij} \mid \theta_i, \beta_j, \gamma \sim \mathcal{N}(\theta_i^\top \beta_j + g_j^\top \gamma, \sigma^2), \quad \gamma \sim \mathcal{N}(0, \eta^2_\gamma I_p).
\]

\textbf{(Given) Conditional for $\gamma$.} You may use
\begin{align*}
p(\gamma \mid \Theta, B, X, G) &= \mathcal{N}(\mu_\gamma, \Sigma_\gamma)\\
\Sigma^{-1}_\gamma &= \eta^{-2}_\gamma I_p + \sigma^{-2} \sum_{(i,j) \in \Omega} g_j g_j^\top\\
\mu_\gamma &= \Sigma_\gamma \sigma^{-2} \sum_{(i,j) \in \Omega} g_j (x_{ij} - \theta_i^\top \beta_j).
\end{align*}

\begin{enumerate}[label=\alph*)]
\item \textbf{Mean-field extension.} Extend the variational family to include $q(\gamma) = \mathcal{N}(m_\gamma, V_\gamma)$. Derive the CAVI update for $q(\gamma)$ by replacing $\theta_i^\top \beta_j$ with $\mathbb{E}_q[\theta_i]^\top \mathbb{E}_q[\beta_j]$.

\item \textbf{How do $\theta, \beta$ updates change?} Show that the precisions $V^{-1}_{\theta_i}$ and $V^{-1}_{\beta_j}$ are unchanged, and only the means are residualized by the feature term:
\[
m_{\theta_i} = V_{\theta_i} \sigma^{-2} \sum_{j \in \Omega_i} m_{\beta_j} (x_{ij} - g_j^\top m_\gamma), \quad m_{\beta_j} = V_{\beta_j} \sigma^{-2} \sum_{i \in \Omega^j} m_{\theta_i} (x_{ij} - g_j^\top m_\gamma).
\]
Explain why this residualization is the only change.

\item \textbf{User features} Suppose we are also given user features $f_i \in \mathbb{R}^p$. Suggest a way of extending the model to account for this feature. Which CAVI updates would you expect to change and why?
\end{enumerate}

\newpage

\section*{Question 3: Implementation \& Report (choose \emph{one})}

Pick exactly one of the following and implement it end-to-end:\footnote{For MAP and coordinatewise MAP implementations, feel free to use automatic differentiation frameworks like torch, pyro, jax, or tensorflow.}

\begin{itemize}
\item \textbf{Option A (Ideal Point Model; Probit IRT, 1D):} Implement a CAVI (using \S1.2) on the 113th Senate dataset (votes.csv, senators.txt).

\item \textbf{Option B (Gaussian Matrix Factorization):} Implement CAVI updates for the model in \S2 on a standard explicit-feedback dataset (e.g. MovieLens 100K). Use the coordinate-wise updates you derived and evaluate out-of-sample.

\item \textbf{Option C (Project-aligned Probabilistic Latent Factor Model):} If your final project involves a \emph{probabilistic latent factor model}---interpreted broadly to include both linear matrix factorization and nonlinear variants such as variational autoencoders (VAEs) or other probabilistic latent-variable models---implement a minimal working version on a real dataset of your choice. Clearly describe the dataset and the generative process (likelihood, priors, and inference or optimization method). Be sure to \textbf{introduce the model and its probabilistic assumptions}, and \textbf{interpret the latent variables} (e.g., what structure or semantics they capture). Include both quantitative evaluation (e.g., held-out log-likelihood or predictive metrics) and qualitative visualizations that illuminate the learned latent structure.
\end{itemize}

\subsection*{What to implement (minimal checklist)}

\textbf{Common to both options}

\begin{enumerate}[label=\alph*)]
\item \textbf{Posterior predictive.}
\begin{itemize}
\item \emph{Option A:} For held-out $(i, j)$, compute $\hat{p}_{ij} = \Phi(\hat{\alpha}_j(\hat{\theta}_i - \hat{\beta}_j))$.
\item \emph{Option B:} For held-out $(i, j)$, compute $\hat{x}_{ij} = \hat{\theta}_i^\top \hat{\beta}_j$.
\end{itemize}

\item \textbf{Metrics (pick at least two).}
\begin{itemize}
\item \emph{Binary (Option A):} average log-posterior-predictive on the held-out set + one metric of your choosing.
\item \emph{Ratings (Option B):} Average log-posterior-predictive on a held-out set + one metric of your choosing.
\end{itemize}

\item \textbf{Convergence diagnostics.}
\end{enumerate}

\textbf{Option A specific (Ideal Point)}

\begin{enumerate}[label=\alph*),resume]
\item \textbf{Substantive plots.}
\begin{itemize}
\item \textbf{Ideology axis:} posterior means of $\theta_i$ with intervals, colored by party; label $\sim$5 outliers.
\item \textbf{Bill landscape:} scatter of $(\hat{\beta}_j, \hat{\alpha}_j)$; annotate $\sim$5 highest $\hat{\alpha}_j$ bills.
\end{itemize}

\item \textbf{Implementation details.} Plot the ELBO vs iteration, number of iterations to convergence etc,.
\end{enumerate}

\textbf{Option B specific (Matrix Factorization)}

\begin{enumerate}[label=\alph*),resume]
\setcounter{enumi}{5}
\item \textbf{Latent dimension sweep.} Run for $K \in \{2, 5, 10, 20\}$ (or a comparable set) and report RMSE/MAE vs. $K$.

\item \textbf{Performance with vs without genre as a feature.}

\item \textbf{Uncertainty (first-order).} Estimate $\text{Var}(x_{ij} \mid \text{data})$, and discuss how you could use this to flag uncertain recommendations (estimate this analytically or by drawing samples from the predictive).

\item \textbf{Visualization (for $K = 2$).} Scatter plots of $\{\hat{\theta}_i\}$ and $\{\hat{\beta}_j\}$; comment on clusters (genres/users).

\item \textbf{Convergence checks} Plot the ELBO vs iteration.
\end{enumerate}

\subsection*{What to turn in}

Please submit a short report (\textbf{2--4 pages of main text}; this is a soft limit, but avoid going significantly over 4 pages). Your writeup should read like a concise research note describing what you set out to do, how you did it, and what you found. Your report must include all the elements listed in the \emph{What to implement} section above.

The structure below is intended as a set of \emph{guidelines} to help organize your report, rather than a rigid template.

\begin{enumerate}[label=\roman*.]
\item \textbf{Introduction.} State the problem you are addressing and briefly motivate why the modeling approach is appropriate.

\item \textbf{Model.} Write down the full joint distribution for your generative model, specifying all distributions and parameterizations (e.g., Gamma shape--rate). Include a graphical model if relevant.

\item \textbf{Inference.} Summarize your inference approach and main implementation choices in the main text (details may go in an Appendix).

\item \textbf{Data \& setup.} Describe your dataset or image, preprocessing steps, choice of $K$, priors, and any design decisions. Compare alternative choices where appropriate.

\item \textbf{Results.} Present key outcomes: number of clusters in factors, posterior summaries of component parameters, and representative visualizations of the latent factors, quantitative checks of model fits etc.
\end{enumerate}

\newpage

\section*{Question 4: Final Project}

Write an ``aspirational abstract'' for your final project. Note you are not committed to deliver everything you mention on the abstract. Rather, preparing the abstract is a chance to think concretely and envision a successful final project.

\newpage

\section*{Formulas \& Identities You May Find Useful}

\textbf{Notation.} $\phi(\cdot)$ and $\Phi(\cdot)$ denote the standard Normal pdf and cdf. For a Normal truncated to $(a, b)$ we use the shorthands $\alpha = (a - \mu)/\sigma$, $\beta = (b - \mu)/\sigma$, and $Z = \Phi(\beta) - \Phi(\alpha)$.

\subsection*{A. Probit link and data augmentation}

If $z \sim \mathcal{N}(\mu, 1)$ and $y = \mathbb{I}\{z > 0\}$, then
\[
P(y = 1) = \Phi(\mu), \quad \log p(y \mid \mu) = y \log \Phi(\mu) + (1 - y) \log(1 - \Phi(\mu)).
\]

\subsection*{B. Truncated Normal moments (general and one-sided)}

Let $Y \sim \mathcal{N}(\mu, \sigma^2)$ truncated to $(a, b)$ with the $\alpha, \beta, Z$ above. Then
\begin{align*}
\mathbb{E}[Y] &= \mu + \sigma \cdot \lambda, \quad \text{where } \lambda = \frac{\phi(\alpha) - \phi(\beta)}{Z},\\
\text{Var}(Y) &= \sigma^2 \left[1 + \frac{\alpha\phi(\alpha) - \beta\phi(\beta)}{Z} - \lambda^2\right], \quad \mathbb{E}[Y^2] = \text{Var}(Y) + \mathbb{E}[Y]^2.
\end{align*}

\textbf{One-sided cases:}
\begin{align*}
Y \sim \mathcal{N}(\mu, \sigma^2) \text{ truncated to } (0, \infty) &: \quad \alpha = \frac{-\mu}{\sigma}, \quad Z = 1 - \Phi(\alpha), \quad \lambda = \frac{\phi(\alpha)}{Z}.\\
Y \sim \mathcal{N}(\mu, \sigma^2) \text{ truncated to } (-\infty, 0] &: \quad \alpha = \frac{0-\mu}{\sigma}, \quad Z = \Phi(\alpha), \quad \lambda = \frac{-\phi(\alpha)}{Z}.
\end{align*}

\textbf{Half-Normal (special case).} If $\mu = 0$ and truncation is $(0,\infty)$, $Y$ is Half-Normal: $\mathbb{E}[Y] = \sigma\sqrt{2/\pi}$, $\text{Var}(Y) = \sigma^2(1 - 2/\pi)$. \emph{Note:} Using $|X|$ with $X \sim \mathcal{N}(0, \sigma^2)$ samples this case correctly. For $\mu \neq 0$, $|X|$ does not produce the correct truncated Normal---use inverse-CDF, rejection sampling, or an off-the-shelf routine.

\subsection*{C. Sampling from a truncated Normal}

Sampling is typically done with either rejection sampling using the inverse cdf or using exponential rejection schemes. However, in practice, you may just want to use an off-the-shelf sampler like \texttt{scipy.stats.truncnorm}.

\subsection*{D. Gaussian identities for CAVI}

If $x \sim \mathcal{N}(m, V)$ then
\[
\mathbb{E}[x] = m, \quad \mathbb{E}[xx^\top] = V + mm^\top.
\]

If $x \sim \mathcal{N}(m_x, V_x)$ and $y \sim \mathcal{N}(m_y, V_y)$ are independent, then
\[
\mathbb{E}[x^\top y] = m_x^\top m_y, \quad \text{Var}(x^\top y) = m_x^\top V_y m_x + m_y^\top V_x m_y + \text{tr}(V_x V_y).
\]

\end{document}
