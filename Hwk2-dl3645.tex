\documentclass[11pt]{article}
\usepackage{amsmath,amssymb,amsthm}
\usepackage{geometry}
\usepackage{enumitem}
\geometry{margin=1in}

\begin{document}

\begin{center}
{\Large STCS 6701: Probabilistic Machine Learning\\
Homework 2}

\vspace{1em}

Due: Nov 14 at 11:59 pm ET
\end{center}

\section*{Instructions}

\begin{itemize}
\item All homework should be typeset using \LaTeX. Box your answers whenever appropriate.

\item Standard late policy applies. Everyone has a total five late days throughout the semester. You are free to use them for whatever reason, no need to inform course staff.

\item The homework should be turned in via Gradescope before the deadline (more details will be announced closer to the deadline).

\item Turn in the code as well as the writeup.

\item You can use any programming language you like.
\end{itemize}

\newpage

\section*{Problem 1: Ideal Point Model}

\textbf{Motivation} This problem is intended to give you an introduction to one of the most widely used latent variable models in political and social sciences. We consider a dataset of roll-call votes from the 113th U.S. Senate.

Your task is to uncover hidden ideological structure from these binary voting patterns. We will use a probit ideal-point model (a variant of Item Response Theory) that represents both senators and bills on a shared latent axis. This model is widely used to study polarization and party structure in real legislatures.

\subsection*{Question 1.1: The model}

\textbf{Data:} $y_{ij} \in \{0, 1, \text{missing}\}$ = vote of senator $i$ on bill $j$.

\textbf{Latent Utility:} We conceptualize the voting process as composed of the following elements

\begin{itemize}
\item Each senator has an ideological position on a hidden \emph{left-right} axis.

\item Each bill has a \emph{location} on that axis: some are left-leaning, some are right-leaning, some are centrist.

\item Some bills are more polarizing than others ---a tax reform bill may split the chamber almost perfectly, while a ceremonial resolution passes nearly unanimously.

\item A senator casts a ``yea'' if their \textbf{latent support} for a bill crosses some internal threshold, otherwise the senator casts a ``nay'' vote.
\end{itemize}

This hidden ``support'' for a bill is not observed directly, we only get to see yea/nay/didn't vote. So we imagine that behind every vote there is an unobserved continuous variable ---a latent utility $z_{ij}$--- that represents how strongly senator $i$ supports bill $j$.

\begin{enumerate}
\item If $z_{ij} > 0$, the senator votes ``yea.''

\item If $z_{ij} < 0$, the senator votes ``nay.''
\end{enumerate}

Now we want to connect $z_{ij}$ to parameters that describe senators and bills.

\begin{enumerate}
\item \textbf{Senator position.} Suppose each senator has a hidden ideology $\theta_i$ on a \emph{left-right} axis. If $\theta_i$ is large and positive, the senator is more conservative; if it is negative, more liberal.

\item \textbf{Bill location.} Suppose each bill has a threshold $\beta_j$, placing it on the same axis. A bill with $\beta_j = 0$ is centrist; a bill with $\beta_j = +2$ (i.e., some ``large'' arbitrary number) is very conservative; a bill with $\beta_j = -2$ (i.e., some ``small'' arbitrary number) is very liberal.

\item \textbf{Discrimination.} Not all bills are equally informative to a senator's ideology. Some bills divide senators sharply, others less so. To capture this we introduce a discrimination parameter $\alpha_j$.
\end{enumerate}

Therefore, latent utility takes the form
\begin{align}
z_{ij} &= \alpha_j(\theta_i - \beta_j) + \epsilon_{ij}, \qquad \epsilon_{ij} \sim \mathcal{N}(0, 1). \label{eq:latent}\\
y_{ij} &= \mathbb{1}(z_{ij} > 0) \label{eq:vote}
\end{align}

\begin{enumerate}[label=\alph*)]
\item \textbf{Explain in words:} If a senator's $\theta_i$ is far larger than a bill's $\beta_j$ (and $\alpha_j > 0$), what does the model predict about the vote? What if $\theta_i$ is smaller?

We are given
\begin{align*}
z_{ij} &= \alpha_j(\theta_i - \beta_j) + \epsilon_{ij}, && \epsilon_{ij} \sim \mathcal{N}(0, 1),\\
y_{ij} &= \mathbb{1}(z_{ij} > 0).
\end{align*}
Define the mean of the latent utility
\[
\mu_{ij} \triangleq \alpha_j(\theta_i - \beta_j).
\]
Then we can rewrite $z_{ij} = \mu_{ij} + \epsilon_{ij}$, so conditional on $(\theta_i, \alpha_j, \beta_j)$ we have
\[
z_{ij} \mid \theta_i, \alpha_j, \beta_j \sim \mathcal{N}(\mu_{ij}, 1).
\]
Before proceeding with Question~1.1(a), let us derive the probability that the model produces a ``yea'':
\begin{align*}
\Pr(y_{ij}=1 \mid \theta_i, \alpha_j, \beta_j)
&= \Pr(z_{ij} > 0 \mid \theta_i, \alpha_j, \beta_j)\\
&= \Pr(\mu_{ij} + \epsilon_{ij} > 0)\\
&= \Pr(\epsilon_{ij} > -\mu_{ij})\\
&= 1 - \Phi(-\mu_{ij}) = \Phi(\mu_{ij}) = \Phi\!\big(\alpha_j(\theta_i - \beta_j)\big),
\end{align*}
where $\Phi(\cdot)$ is the standard normal CDF and we used the symmetry $\Phi(-x)=1-\Phi(x)$. Similarly,
\[
\Pr(y_{ij}=0 \mid \theta_i, \alpha_j, \beta_j) = 1 - \Phi\!\big(\alpha_j(\theta_i - \beta_j)\big).
\]
\textbf{Case 1: $\theta_i$ is far larger than $\beta_j$.} Assume $\theta_i \gg \beta_j$ and $\alpha_j > 0$. Then $\theta_i - \beta_j \gg 0$ and hence $\mu_{ij} = \alpha_j(\theta_i - \beta_j) \gg 0$. Because $\Phi(\mu_{ij})$ is very close to $1$ when $\mu_{ij}$ is large and positive, the model predicts $y_{ij}=1$ with probability near one: the latent utility is almost surely positive, so the senator is very likely to vote ``yea.''

\textbf{Case 2: $\theta_i$ is far smaller than $\beta_j$.} Assume $\theta_i \ll \beta_j$ and $\alpha_j > 0$. Then $\theta_i - \beta_j \ll 0$, implying $\mu_{ij} \ll 0$. Now $\Phi(\mu_{ij})$ is close to $0$, so $\Pr(y_{ij}=1\mid \theta_i,\alpha_j,\beta_j)$ is near zero. In this situation the latent utility is very likely negative, so the senator will almost surely vote ``nay.''

\item \textbf{Role of $\alpha_j$:} Compare two bills with the same $\beta_j$ but different $\alpha_j$: Which bill is more \emph{polarizing}?

For a fixed senator $i$ and bill $j$, the latent utility satisfies
\begin{align*}
z_{ij} &= \alpha_j(\theta_i - \beta_j) + \epsilon_{ij}, && \epsilon_{ij} \sim \mathcal{N}(0, 1),\\
y_{ij} &= \mathbb{1}(z_{ij} > 0).
\end{align*}
From the preliminary derivation we already have
\[
\Pr(y_{ij}=1 \mid \theta_i, \alpha_j, \beta_j)=\Phi\big(\alpha_j(\theta_i - \beta_j)\big).
\]
Define $p_j(\theta_i) \triangleq \Pr(y_{ij}=1 \mid \theta_i, \alpha_j, \beta_j)$ and introduce $u=\alpha_j(\theta_i - \beta_j)$ so that $p_j(\theta_i)=\Phi(u)$. Differentiating with respect to $\theta_i$ gives
\[
\frac{\partial p_j(\theta_i)}{\partial \theta_i} = \phi(u)\,\alpha_j = \alpha_j\,\phi\big(\alpha_j(\theta_i - \beta_j)\big),
\]
where $\phi(\cdot)$ is the standard normal density. Thus $\alpha_j$ controls how sharply the vote probability $p_j(\theta_i)$ rises or falls along the ideological axis.

Now compare two bills $j$ and $k$ with the same location $\beta$ but different discriminations $\alpha_j>\alpha_k>0$. Their vote probabilities for senator $i$ are
\[
p_j(\theta_i) = \Phi\big(\alpha_j(\theta_i - \beta)\big), \qquad
p_k(\theta_i) = \Phi\big(\alpha_k(\theta_i - \beta)\big).
\]

\textbf{Case 1: $\theta_i$ near the bill location $\beta$.} When $\theta_i=\beta$ exactly we obtain $p_j(\beta)=p_k(\beta)=\Phi(0)=0.5$. For $\theta_i$ close (but not equal) to $\beta$, the arguments $\alpha_j(\theta_i-\beta)$ and $\alpha_k(\theta_i-\beta)$ remain close to zero, so both probabilities stay near $0.5$. However, the derivatives at $\theta_i=\beta$ are
\[
\left.\frac{\partial p_j(\theta_i)}{\partial \theta_i}\right|_{\theta_i=\beta} = \alpha_j \phi(0), \qquad
\left.\frac{\partial p_k(\theta_i)}{\partial \theta_i}\right|_{\theta_i=\beta} = \alpha_k \phi(0),
\]
and since $\alpha_j>\alpha_k$, the curve $p_j(\theta_i)$ changes more rapidly with ideology near the cutpoint. In other words, a small ideological shift around $\beta$ causes a larger swing in the probability of voting ``yea'' for bill $j$ than for bill $k$; bill $j$ is therefore more polarizing.

\textbf{Case 2: $\theta_i$ far from $\beta$.} If $|\theta_i-\beta| \to \infty$, then $\alpha_j(\theta_i-\beta)$ and $\alpha_k(\theta_i-\beta)$ tend to $\pm\infty$ with the same sign, so
\[
p_j(\theta_i) \to \mathbb{1}(\theta_i>\beta), \qquad p_k(\theta_i) \to \mathbb{1}(\theta_i>\beta),
\]
and both probabilities saturate at 0 or 1. In this regime the derivatives shrink to zero because the Gaussian density $\phi\big(\alpha_\ell(\theta_i-\beta)\big)$ vanishes as its argument diverges. The main difference between $j$ and $k$ therefore lies in how quickly they transition between the extremes: larger $\alpha_j$ yields a steeper sigmoid, so $p_j$ leaves the ambiguous middle region more abruptly.

Overall, holding $\beta_j=\beta_k=\beta$ and $\alpha_j>\alpha_k>0$, bill $j$ is more polarizing than bill $k$ because its discrimination parameter makes the probability curve $p_j(\theta_i)$ change more steeply (and thus more abruptly) from near 0 to near 1 as a senatorâ€™s ideology moves across $\beta$.

\item \textbf{Missing Votes:} How should we handle $y_{ij} = \text{missing}$ in this setup?


We start from the model, for each $(i,j)$:
\[
z_{ij} = \alpha_j(\theta_i - \beta_j) + \epsilon_{ij}, \qquad \epsilon_{ij} \sim \mathcal{N}(0,1), \qquad y_{ij} = \mathbb{1}(z_{ij} > 0).
\]
From the earlier derivation, we obtained an explicit expression for the voting probability:
\[
\Pr(y_{ij}=1 \mid \theta_i, \alpha_j, \beta_j) = \Pr(z_{ij} > 0 \mid \theta_i, \alpha_j, \beta_j) = \Phi\big(\alpha_j(\theta_i - \beta_j)\big),
\]
where $\Phi(\cdot)$ is the standard normal CDF. Therefore,
\[
\Pr(y_{ij}=0 \mid \theta_i, \alpha_j, \beta_j) = 1 - \Pr(y_{ij}=1 \mid \theta_i, \alpha_j, \beta_j) = 1 - \Phi\big(\alpha_j(\theta_i - \beta_j)\big).
\]
\textbf{Bernoulli representation.} Define $p_{ij} := \Pr(y_{ij}=1 \mid \theta_i, \alpha_j, \beta_j) = \Phi\big(\alpha_j(\theta_i - \beta_j)\big)$. Then, for a single observed vote $y_{ij} \in \{0,1\}$,
\[
\Pr(y_{ij} \mid \theta_i, \alpha_j, \beta_j) = p_{ij}^{\,y_{ij}} (1-p_{ij})^{1-y_{ij}}.
\]
Substituting $p_{ij} = \Phi\big(\alpha_j(\theta_i - \beta_j)\big)$ gives
\[
\Pr(y_{ij} \mid \theta_i, \alpha_j, \beta_j) = [\Phi\big(\alpha_j(\theta_i - \beta_j)\big)]^{y_{ij}} [1-\Phi\big(\alpha_j(\theta_i - \beta_j)\big)]^{1-y_{ij}}.
\]
\textbf{Joint likelihood.} Define the index set of non-missing votes $O := \{ (i,j) : y_{ij} \in \{0,1\} \}$. Assuming conditional independence of votes given $\{\theta_i\}$ and $\{\alpha_j, \beta_j\}$,
\[
p(\{y_{ij}\}_{(i,j)\in O} \mid \{\theta_i\}, \{\alpha_j, \beta_j\}) = \prod_{(i,j)\in O} \Pr(y_{ij} \mid \theta_i, \alpha_j, \beta_j),
\]
which by the Bernoulli form becomes
\[
p(\{y_{ij}\}_{(i,j)\in O} \mid \{\theta_i\}, \{\alpha_j, \beta_j\}) = \prod_{(i,j)\in O} [\Phi\big(\alpha_j(\theta_i - \beta_j)\big)]^{y_{ij}} [1-\Phi\big(\alpha_j(\theta_i - \beta_j)\big)]^{1-y_{ij}}.
\]

\item \textbf{Sketch:} Sketch the graphical model using plate notation.

\item \textbf{Sketch:} Draw a 1D lineshowing senators at positions $\theta_i$, bills at positions $\beta_j$, and explain the threshold rule with a simple picture.

\item \textbf{Setting the Prior:} Suppose you choose a zero-centered prior for $\theta_i$ and $\beta_j$. How would you choose the prior variance(s) using the held-out data?

\textbf{1. Specify the priors with unknown variances.} Choose zero-centered Gaussian priors for the latent senator and bill locations:
\begin{align*}
\theta_i &\sim \mathcal{N}(0, \sigma^2_\theta), \quad i = 1, \ldots, N_{\text{sen}},\\
\beta_j &\sim \mathcal{N}(0, \sigma^2_\beta), \quad j = 1, \ldots, N_{\text{bill}}.
\end{align*}
Treat $\sigma^2_\theta, \sigma^2_\beta$ as hyperparameters that we will choose using the held-out votes.

\textbf{2. Split the data: training vs held-out.} Let $O$ be the set of observed (non-missing) votes as before, and split it into
\begin{itemize}
\item a training set $O_{\text{train}}$,
\item a held-out set $O_{\text{val}}$,
\end{itemize}
with $O_{\text{train}} \cap O_{\text{val}} = \emptyset$ and $O_{\text{train}} \cup O_{\text{val}} = O$.

We will fit the model on $O_{\text{train}}$ for each candidate choice of $(\sigma^2_\theta, \sigma^2_\beta)$, and evaluate predictive performance on $O_{\text{val}}$.

\textbf{3. For fixed variances, fit $\theta, \beta$ on the training set.} Recall the Bernoulli form (from your previous part):
\begin{align*}
p_{ij}(\theta_i, \beta_j) &:= \Pr(y_{ij} = 1 \mid \theta_i, \alpha_j, \beta_j) = \Phi\!\big(\alpha_j(\theta_i - \beta_j)\big),\\
\Pr(y_{ij} \mid \theta_i, \alpha_j, \beta_j) &= p_{ij}^{\,y_{ij}} (1 - p_{ij})^{1 - y_{ij}}.
\end{align*}
For a fixed pair $(\sigma^2_\theta, \sigma^2_\beta)$, the training log-posterior (up to an additive constant) is
\begin{align*}
\ell_{\text{train}}(\theta, \beta \mid \sigma^2_\theta, \sigma^2_\beta) &= \sum_{(i,j) \in O_{\text{train}}} \left[ y_{ij} \log p_{ij}(\theta_i, \beta_j) + (1 - y_{ij}) \log(1 - p_{ij}(\theta_i, \beta_j)) \right]\\
&\quad - \frac{1}{2\sigma^2_\theta} \sum_i \theta_i^2 - \frac{1}{2\sigma^2_\beta} \sum_j \beta_j^2.
\end{align*}
For that choice of $(\sigma^2_\theta, \sigma^2_\beta)$, compute the MAP estimates
\[
\hat{\theta}(\sigma^2_\theta, \sigma^2_\beta), \quad \hat{\beta}(\sigma^2_\theta, \sigma^2_\beta)
\]
by maximizing $\ell_{\text{train}}$.

\textbf{4. Compute held-out predictive log-likelihood.} Using $\hat{\theta}, \hat{\beta}$, compute predicted probabilities on held-out pairs:
\[
\hat{p}_{ij} := \Phi\!\big(\alpha_j(\hat{\theta}_i - \hat{\beta}_j)\big), \quad (i,j) \in O_{\text{val}}.
\]
The held-out log-likelihood for this choice of variances is
\[
L_{\text{val}}(\sigma^2_\theta, \sigma^2_\beta) := \sum_{(i,j) \in O_{\text{val}}} \left[ y_{ij} \log \hat{p}_{ij} + (1 - y_{ij}) \log(1 - \hat{p}_{ij}) \right].
\]

\textbf{5. Choose the prior variances by maximizing held-out performance.} Finally, treat $\sigma^2_\theta, \sigma^2_\beta$ as tuning parameters and choose them to maximize the held-out log-likelihood:
\[
(\sigma^{2*}_\theta, \sigma^{2*}_\beta) = \arg\max_{(\sigma^2_\theta, \sigma^2_\beta)} L_{\text{val}}(\sigma^2_\theta, \sigma^2_\beta).
\]
In practice you would:
\begin{itemize}
\item pick a grid of candidate values for $\sigma^2_\theta$ and $\sigma^2_\beta$,
\item for each pair, fit MAP $\hat{\theta}, \hat{\beta}$ on $O_{\text{train}}$,
\item compute $L_{\text{val}}$,
\item select the pair that yields the largest $L_{\text{val}}$.
\end{itemize}
\end{enumerate}

\newpage

\subsection*{Question 1.2: Putting priors on the parameters}

Right now, the latent parameters $\theta_i, \beta_j, \alpha_j$ are free-floating. To complete the model, we need to place prior distributions on these quantities.

\textbf{Priors for $\theta_i, \beta_j, \alpha_j$.} Throughout, assume the following priors

\begin{itemize}
\item $\theta_i \sim \mathcal{N}(\mu_\theta, \sigma^2_\theta)$: senators' latent ideologies. (e.g., where a senator is in the political spectrum)

\item $\beta_j \sim \mathcal{N}(\mu_\beta, \sigma^2_\beta)$: bills' latent positions on the ideological axis. (e.g., where a bill is in the political spectrum).

\item $\alpha_j \sim \mathcal{N}(\mu_\alpha, \sigma^2_\alpha)$: how strongly a bill separates senators into yea/nay camps.
\end{itemize}

\textbf{Identifiability.} The model likelihood depends only on the product $\alpha_j(\theta_i - \beta_j)$. This leads to certain \emph{symmetries} in the parameters:

\begin{enumerate}
\item \textbf{Translation:} Show that if we add a constant $c$ to all senator positions and all bill positions,
\[
(\theta_i - \beta_j) = (\theta_i + c) - (\beta_j + c),
\]
the likelihood is unchanged. What does this mean about the absolute location of the ideological axis?

We already have, from 1.1, for each $(i,j)$,
\[
\Pr(y_{ij}=1 \mid \theta_i,\alpha_j,\beta_j) = \Phi\!\big(\alpha_j(\theta_i - \beta_j)\big),
\]
and
\[
\Pr(y_{ij}=0 \mid \theta_i,\alpha_j,\beta_j) = 1 - \Phi\!\big(\alpha_j(\theta_i - \beta_j)\big).
\]
So the likelihood depends on the parameters only through the term $\alpha_j(\theta_i - \beta_j)$.

\textbf{Show translation invariance.} Consider a constant $c \in \mathbb{R}$ and define transformed parameters
\[
\theta_i' = \theta_i + c, \qquad \beta_j' = \beta_j + c, \qquad \alpha_j' = \alpha_j.
\]
Compute the transformed difference:
\[
\theta_i' - \beta_j' = (\theta_i + c) - (\beta_j + c) = \theta_i - \beta_j.
\]
Then the probit argument under the transformed parameters is
\[
\alpha_j'(\theta_i' - \beta_j') = \alpha_j(\theta_i - \beta_j),
\]
so the ``yea'' probability is unchanged:
\[
\Pr(y_{ij}=1 \mid \theta_i',\alpha_j',\beta_j') = \Phi\!\big(\alpha_j'(\theta_i' - \beta_j')\big) = \Phi\!\big(\alpha_j(\theta_i - \beta_j)\big) = \Pr(y_{ij}=1 \mid \theta_i,\alpha_j,\beta_j).
\]
The same holds for $\Pr(y_{ij}=0\mid\cdot)$, so for each $(i,j)$,
\[
\Pr(y_{ij}\mid \theta_i',\alpha_j',\beta_j') = \Pr(y_{ij}\mid \theta_i,\alpha_j,\beta_j).
\]
Because the joint likelihood is the product over $(i,j)$ of these terms, the entire likelihood is unchanged by the transformation
\[
\theta_i \mapsto \theta_i + c, \qquad \beta_j \mapsto \beta_j + c, \qquad \alpha_j \text{ unchanged}.
\]

This shows that the absolute location (origin) of the ideological axis is not identified by the data: only relative positions $\theta_i - \beta_j$ matter. In other words, the model can only recover ideologies and bill locations up to an additive constant; choosing where ``0'' is on the axis is a matter of convention (e.g., setting $\mu_\theta=0$, or fixing one $\beta_j=0$).

\item \textbf{Scaling:} Show that if we multiply all senator and bill positions by $k > 0$ and divide all discriminations by $k$,
\[
\alpha_j(\theta_i - \beta_j) = \frac{\alpha_j}{k}(k\theta_i - k\beta_j),
\]
the likelihood is unchanged. What does this mean about the scale of the ideological axis?

\item \textbf{Interpretation:} Do these symmetries affect how you interpret the parameters? Are there any other symmetries in the parameters?
\end{enumerate}

\newpage

\subsection*{Question 1.3: CAVI}

In this section you will derive CAVI updates for this model.

\begin{enumerate}[label=\alph*)]
\item \textbf{Joint distribution:} Using the priors on the previous question write down the joint distribution $p(y, z, \alpha, \theta, \beta)$.

\item \textbf{Latent utilities $z_{ij}$.} Recall that $z_{ij} \mid \theta_i, \beta_j, \alpha_j \sim \mathcal{N}(\alpha_j(\theta_i - \beta_j), 1)$. Show that conditioning on $y_{ij}$ leads to a truncated Normal:
\[
z_{ij} \mid y_{ij}, \theta, \beta, \alpha \sim
\begin{cases}
\mathcal{N}(\mu_{ij}, 1) \text{ truncated to } (0, \infty), & y_{ij} = 1,\\
\mathcal{N}(\mu_{ij}, 1) \text{ truncated to } (-\infty, 0], & y_{ij} = 0,
\end{cases}
\]
where $\mu_{ij} = \alpha_j(\theta_i - \beta_j)$. If $y_{ij} = -1$ (missing), explain why no $z_{ij}$ is drawn.

\item \textbf{Senator positions $\theta_i$.} Derive the conditional distribution of $\theta_i$ given $z, \beta, \alpha$ under your chosen prior from 2.1. Show it is Gaussian, and write down its mean and variance.

\item \textbf{Bill locations $\beta_j$.} Derive the conditional distribution of $\beta_j$ given $z, \theta, \alpha$ under your chosen prior from 2.1. Show it is Gaussian, and write down its mean and variance.

\item \textbf{Bill discriminations $\alpha_j$.} Assume the prior you proposed in 2.1 for $\alpha_j$. Derive the conditional distribution of $\alpha_j$ given $z, \theta, \beta$. If you chose a Normal prior, it will be Normal; if you chose a truncated Normal prior, it will be truncated Normal (see Useful Formulas for information on the Truncated Normal).

\item \textbf{Marginalizing out $z$.} Write down $p(y, \alpha, \theta, \beta)$. What is $p(y \mid \alpha, \theta, \beta)$?

\item \textbf{Variational family.} Assume a factorization
\[
q(\theta, \beta, \alpha, z) = \left(\prod_{i=1}^n q(\theta_i)\right) \left(\prod_{j=1}^d q(\beta_j) q(\alpha_j)\right) \left(\prod_{i=1}^n \prod_{j=1}^p q(z_{ij})\right),
\]
where $q(\theta_i)$ and $q(\beta_j)$ are Normal, $q(\alpha_j)$ is either Normal or truncated Normal (depending on your chosen prior), and $q(z_{ij})$ is a truncated Normal as in 1.2(b). Write the full family explicitly and state which moments of each factor you will need for updates.

\item \textbf{Coordinate updates.} Using the identity
\[
\log q^*(v) \propto \mathbb{E}_{-v}[\log p(y, z, \theta, \beta, \alpha)],
\]
derive expressions for the optimal factors up to Normal/truncated Normal forms. Specifically:
\begin{enumerate}[label=(\roman*)]
\item $q(z_{ij})$: update the mean parameter $\bar{\mu}_{ij}$ and give a formula for $\mathbb{E}_q[z_{ij}]$ using standard truncated-Normal moments (see Useful Formulas).
\item $q(\theta_i)$ and $q(\beta_j)$: write the precision and mean in terms of expectations $\mathbb{E}_q[\alpha_j]$, $\mathbb{E}_q[\alpha_j^2]$, and $\mathbb{E}_q[z_{ij}]$.
\item $q(\alpha_j)$: treat $\{z_{ij}\}_{i=1}^n$ as responses in a linear regression on $(\theta_i - \beta_j)$. Write the resulting mean and variance, and note how the update changes.
\end{enumerate}
\end{enumerate}

\newpage

\section*{Problem 2: Gaussian Matrix Factorization (MFVI from conditionals + features)}

\subsection*{\S Model}

Let $X \in \mathbb{R}^{n \times m}$ be a partially observed ratings matrix with observed index set $\Omega \subseteq \{1, \ldots, n\} \times \{1, \ldots, m\}$. Fix latent dimension $K$. For users $i$ and items $j$ we have factors $\theta_i, \beta_j \in \mathbb{R}^K$.
\[
\theta_i \sim \mathcal{N}(0, \eta^2_\theta I_K), \quad \beta_j \sim \mathcal{N}(0, \eta^2_B I_K), \quad x_{ij} \mid \theta_i^\top \beta_j \sim \mathcal{N}(\theta_i^\top \beta_j, \sigma^2), \quad (i, j) \in \Omega.
\]

Define $\Omega_i = \{j : (i, j) \in \Omega\}$ and $\Omega^j = \{i : (i, j) \in \Omega\}$. Throughout, $\sigma^2, \eta^2_\theta, \eta^2_B$ are known.

\textbf{(Given) Complete conditionals.} You may use the following (conjugate) complete conditionals in your derivations.
\begin{align}
p(\theta_i \mid \{\beta_j\}, X, \sigma^2, \eta^2_\theta) &= \mathcal{N}(\mu_{\theta_i}, \Sigma_{\theta_i}), \quad \Sigma^{-1}_{\theta_i} = \eta^{-2}_\theta I_K + \sigma^{-2} \sum_{j \in \Omega_i} \beta_j \beta_j^\top, \quad \mu_{\theta_i} = \Sigma_{\theta_i} \sigma^{-2} \sum_{j \in \Omega_i} \beta_j x_{ij},\\
p(\beta_j \mid \{\theta_i\}, X, \sigma^2, \eta^2_B) &= \mathcal{N}(\mu_{\beta_j}, \Sigma_{\beta_j}), \quad \Sigma^{-1}_{\beta_j} = \eta^{-2}_B I_K + \sigma^{-2} \sum_{i \in \Omega^j} \theta_i \theta_i^\top, \quad \mu_{\beta_j} = \Sigma_{\beta_j} \sigma^{-2} \sum_{i \in \Omega^j} \theta_i x_{ij}.
\end{align}

\subsection*{Question 2.1: Mean-field VI (CAVI from the conditionals)}

We approximate the posterior with a factorized family
\[
q(\Theta, B) = \prod_{i=1}^n q(\theta_i) \prod_{j=1}^m q(\beta_j), \quad q(\theta_i) = \mathcal{N}(m_{\theta_i}, V_{\theta_i}), \quad q(\beta_j) = \mathcal{N}(m_{\beta_j}, V_{\beta_j}).
\]

\begin{enumerate}[label=\alph*)]
\item \textbf{ELBO pieces.} Write the ELBO $\mathcal{L}(q)$ and list the expectations it comprises.

\item \textbf{CAVI updates.} Using the identity $\log q^\star(x_v) \propto \mathbb{E}_{-v}[\log p(x_v \mid x_{-v})]$ and the \emph{given} complete conditionals 3 and 4, derive the optimal CAVI updates by replacing unknowns with their $q$-expectations. State the updates for $V^{-1}_{\theta_i}$, $V^{-1}_{\beta_j}$, $m_{\theta_i}$, and $m_{\beta_j}$.

\item \textbf{Algorithm sketch.} Give pseudocode for one CAVI sweep:
\[
\{q(\theta_i)\}_{i=1}^n \to \{q(\beta_j)\}_{j=1}^m,
\]
including which expectations are recomputed and a convergence criterion (e.g., ELBO monotone ascent or small parameter change).

\item \textbf{Flagging uncertain recommendations} Using $q$ derive the approximate posterior predictive variance $\text{Var}(x_{ij} \mid \text{data})$ on a holdout set (i.e., $(i, j) \notin \Omega$) and explain how you would use this variance to flag uncertain recommendations.

\item \textbf{Setting hyperparameters} Explain how you could set $\eta^2_\theta$, $\eta^2_B$, and $\sigma^2$ using heldout data.
\end{enumerate}

\subsection*{Question 2.3: Adding item features (e.g., Genre) and updating CAVI}

Let $g_j \in \mathbb{R}^p$ be a (possibly multi-hot) feature vector for item $j$ (e.g., genres).

\textbf{(additive side term).} Augment the likelihood with an additive linear effect of features:
\[
x_{ij} \mid \theta_i, \beta_j, \gamma \sim \mathcal{N}(\theta_i^\top \beta_j + g_j^\top \gamma, \sigma^2), \quad \gamma \sim \mathcal{N}(0, \eta^2_\gamma I_p).
\]

\textbf{(Given) Conditional for $\gamma$.} You may use
\begin{align*}
p(\gamma \mid \Theta, B, X, G) &= \mathcal{N}(\mu_\gamma, \Sigma_\gamma)\\
\Sigma^{-1}_\gamma &= \eta^{-2}_\gamma I_p + \sigma^{-2} \sum_{(i,j) \in \Omega} g_j g_j^\top\\
\mu_\gamma &= \Sigma_\gamma \sigma^{-2} \sum_{(i,j) \in \Omega} g_j (x_{ij} - \theta_i^\top \beta_j).
\end{align*}

\begin{enumerate}[label=\alph*)]
\item \textbf{Mean-field extension.} Extend the variational family to include $q(\gamma) = \mathcal{N}(m_\gamma, V_\gamma)$. Derive the CAVI update for $q(\gamma)$ by replacing $\theta_i^\top \beta_j$ with $\mathbb{E}_q[\theta_i]^\top \mathbb{E}_q[\beta_j]$.

\item \textbf{How do $\theta, \beta$ updates change?} Show that the precisions $V^{-1}_{\theta_i}$ and $V^{-1}_{\beta_j}$ are unchanged, and only the means are residualized by the feature term:
\[
m_{\theta_i} = V_{\theta_i} \sigma^{-2} \sum_{j \in \Omega_i} m_{\beta_j} (x_{ij} - g_j^\top m_\gamma), \quad m_{\beta_j} = V_{\beta_j} \sigma^{-2} \sum_{i \in \Omega^j} m_{\theta_i} (x_{ij} - g_j^\top m_\gamma).
\]
Explain why this residualization is the only change.

\item \textbf{User features} Suppose we are also given user features $f_i \in \mathbb{R}^p$. Suggest a way of extending the model to account for this feature. Which CAVI updates would you expect to change and why?
\end{enumerate}

\newpage

\section*{Question 3: Implementation \& Report (choose \emph{one})}

Pick exactly one of the following and implement it end-to-end:\footnote{For MAP and coordinatewise MAP implementations, feel free to use automatic differentiation frameworks like torch, pyro, jax, or tensorflow.}

\begin{itemize}
\item \textbf{Option A (Ideal Point Model; Probit IRT, 1D):} Implement a CAVI (using \S1.2) on the 113th Senate dataset (votes.csv, senators.txt).

\item \textbf{Option B (Gaussian Matrix Factorization):} Implement CAVI updates for the model in \S2 on a standard explicit-feedback dataset (e.g. MovieLens 100K). Use the coordinate-wise updates you derived and evaluate out-of-sample.

\item \textbf{Option C (Project-aligned Probabilistic Latent Factor Model):} If your final project involves a \emph{probabilistic latent factor model}---interpreted broadly to include both linear matrix factorization and nonlinear variants such as variational autoencoders (VAEs) or other probabilistic latent-variable models---implement a minimal working version on a real dataset of your choice. Clearly describe the dataset and the generative process (likelihood, priors, and inference or optimization method). Be sure to \textbf{introduce the model and its probabilistic assumptions}, and \textbf{interpret the latent variables} (e.g., what structure or semantics they capture). Include both quantitative evaluation (e.g., held-out log-likelihood or predictive metrics) and qualitative visualizations that illuminate the learned latent structure.
\end{itemize}

\subsection*{What to implement (minimal checklist)}

\textbf{Common to both options}

\begin{enumerate}[label=\alph*)]
\item \textbf{Posterior predictive.}
\begin{itemize}
\item \emph{Option A:} For held-out $(i, j)$, compute $\hat{p}_{ij} = \Phi(\hat{\alpha}_j(\hat{\theta}_i - \hat{\beta}_j))$.
\item \emph{Option B:} For held-out $(i, j)$, compute $\hat{x}_{ij} = \hat{\theta}_i^\top \hat{\beta}_j$.
\end{itemize}

\item \textbf{Metrics (pick at least two).}
\begin{itemize}
\item \emph{Binary (Option A):} average log-posterior-predictive on the held-out set + one metric of your choosing.
\item \emph{Ratings (Option B):} Average log-posterior-predictive on a held-out set + one metric of your choosing.
\end{itemize}

\item \textbf{Convergence diagnostics.}
\end{enumerate}

\textbf{Option A specific (Ideal Point)}

\begin{enumerate}[label=\alph*),resume]
\item \textbf{Substantive plots.}
\begin{itemize}
\item \textbf{Ideology axis:} posterior means of $\theta_i$ with intervals, colored by party; label $\sim$5 outliers.
\item \textbf{Bill landscape:} scatter of $(\hat{\beta}_j, \hat{\alpha}_j)$; annotate $\sim$5 highest $\hat{\alpha}_j$ bills.
\end{itemize}

\item \textbf{Implementation details.} Plot the ELBO vs iteration, number of iterations to convergence etc,.
\end{enumerate}

\textbf{Option B specific (Matrix Factorization)}

\begin{enumerate}[label=\alph*),resume]
\setcounter{enumi}{5}
\item \textbf{Latent dimension sweep.} Run for $K \in \{2, 5, 10, 20\}$ (or a comparable set) and report RMSE/MAE vs. $K$.

\item \textbf{Performance with vs without genre as a feature.}

\item \textbf{Uncertainty (first-order).} Estimate $\text{Var}(x_{ij} \mid \text{data})$, and discuss how you could use this to flag uncertain recommendations (estimate this analytically or by drawing samples from the predictive).

\item \textbf{Visualization (for $K = 2$).} Scatter plots of $\{\hat{\theta}_i\}$ and $\{\hat{\beta}_j\}$; comment on clusters (genres/users).

\item \textbf{Convergence checks} Plot the ELBO vs iteration.
\end{enumerate}

\subsection*{What to turn in}

Please submit a short report (\textbf{2--4 pages of main text}; this is a soft limit, but avoid going significantly over 4 pages). Your writeup should read like a concise research note describing what you set out to do, how you did it, and what you found. Your report must include all the elements listed in the \emph{What to implement} section above.

The structure below is intended as a set of \emph{guidelines} to help organize your report, rather than a rigid template.

\begin{enumerate}[label=\roman*.]
\item \textbf{Introduction.} State the problem you are addressing and briefly motivate why the modeling approach is appropriate.

\item \textbf{Model.} Write down the full joint distribution for your generative model, specifying all distributions and parameterizations (e.g., Gamma shape--rate). Include a graphical model if relevant.

\item \textbf{Inference.} Summarize your inference approach and main implementation choices in the main text (details may go in an Appendix).

\item \textbf{Data \& setup.} Describe your dataset or image, preprocessing steps, choice of $K$, priors, and any design decisions. Compare alternative choices where appropriate.

\item \textbf{Results.} Present key outcomes: number of clusters in factors, posterior summaries of component parameters, and representative visualizations of the latent factors, quantitative checks of model fits etc.
\end{enumerate}

\newpage

\section*{Question 4: Final Project}

Write an ``aspirational abstract'' for your final project. Note you are not committed to deliver everything you mention on the abstract. Rather, preparing the abstract is a chance to think concretely and envision a successful final project.

\newpage

\section*{Formulas \& Identities You May Find Useful}

\textbf{Notation.} $\phi(\cdot)$ and $\Phi(\cdot)$ denote the standard Normal pdf and cdf. For a Normal truncated to $(a, b)$ we use the shorthands $\alpha = (a - \mu)/\sigma$, $\beta = (b - \mu)/\sigma$, and $Z = \Phi(\beta) - \Phi(\alpha)$.

\subsection*{A. Probit link and data augmentation}

If $z \sim \mathcal{N}(\mu, 1)$ and $y = \mathbb{I}\{z > 0\}$, then
\[
P(y = 1) = \Phi(\mu), \quad \log p(y \mid \mu) = y \log \Phi(\mu) + (1 - y) \log(1 - \Phi(\mu)).
\]

\subsection*{B. Truncated Normal moments (general and one-sided)}

Let $Y \sim \mathcal{N}(\mu, \sigma^2)$ truncated to $(a, b)$ with the $\alpha, \beta, Z$ above. Then
\begin{align*}
\mathbb{E}[Y] &= \mu + \sigma \cdot \lambda, \quad \text{where } \lambda = \frac{\phi(\alpha) - \phi(\beta)}{Z},\\
\text{Var}(Y) &= \sigma^2 \left[1 + \frac{\alpha\phi(\alpha) - \beta\phi(\beta)}{Z} - \lambda^2\right], \quad \mathbb{E}[Y^2] = \text{Var}(Y) + \mathbb{E}[Y]^2.
\end{align*}

\textbf{One-sided cases:}
\begin{align*}
Y \sim \mathcal{N}(\mu, \sigma^2) \text{ truncated to } (0, \infty) &: \quad \alpha = \frac{-\mu}{\sigma}, \quad Z = 1 - \Phi(\alpha), \quad \lambda = \frac{\phi(\alpha)}{Z}.\\
Y \sim \mathcal{N}(\mu, \sigma^2) \text{ truncated to } (-\infty, 0] &: \quad \alpha = \frac{0-\mu}{\sigma}, \quad Z = \Phi(\alpha), \quad \lambda = \frac{-\phi(\alpha)}{Z}.
\end{align*}

\textbf{Half-Normal (special case).} If $\mu = 0$ and truncation is $(0,\infty)$, $Y$ is Half-Normal: $\mathbb{E}[Y] = \sigma\sqrt{2/\pi}$, $\text{Var}(Y) = \sigma^2(1 - 2/\pi)$. \emph{Note:} Using $|X|$ with $X \sim \mathcal{N}(0, \sigma^2)$ samples this case correctly. For $\mu \neq 0$, $|X|$ does not produce the correct truncated Normal---use inverse-CDF, rejection sampling, or an off-the-shelf routine.

\subsection*{C. Sampling from a truncated Normal}

Sampling is typically done with either rejection sampling using the inverse cdf or using exponential rejection schemes. However, in practice, you may just want to use an off-the-shelf sampler like \texttt{scipy.stats.truncnorm}.

\subsection*{D. Gaussian identities for CAVI}

If $x \sim \mathcal{N}(m, V)$ then
\[
\mathbb{E}[x] = m, \quad \mathbb{E}[xx^\top] = V + mm^\top.
\]

If $x \sim \mathcal{N}(m_x, V_x)$ and $y \sim \mathcal{N}(m_y, V_y)$ are independent, then
\[
\mathbb{E}[x^\top y] = m_x^\top m_y, \quad \text{Var}(x^\top y) = m_x^\top V_y m_x + m_y^\top V_x m_y + \text{tr}(V_x V_y).
\]

\end{document}
