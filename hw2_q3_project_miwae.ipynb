{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jmageqXLxPhe",
        "outputId": "15582b0e-4160-4208-d73e-ccd77c74da8f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7PtgrurvxkQB",
        "outputId": "2c62ff31-49f6-4a71-dccf-13e6cfe5a795"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (2.0.2)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.12/dist-packages (2.2.2)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.12/dist-packages (1.6.1)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.12/dist-packages (1.16.3)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.12/dist-packages (3.10.0)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.12/dist-packages (2.9.0+cu126)\n",
            "Requirement already satisfied: seaborn in /usr/local/lib/python3.12/dist-packages (0.13.2)\n",
            "Collecting faiss-cpu\n",
            "  Downloading faiss_cpu-1.13.0-cp39-abi3-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (7.7 kB)\n",
            "Collecting scikit-dimension\n",
            "  Downloading scikit_dimension-0.3.4-py3-none-any.whl.metadata (1.3 kB)\n",
            "Collecting python-picard\n",
            "  Downloading python-picard-0.8.tar.gz (64 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m64.4/64.4 kB\u001b[0m \u001b[31m5.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: hdbscan in /usr/local/lib/python3.12/dist-packages (0.8.40)\n",
            "Requirement already satisfied: geopandas in /usr/local/lib/python3.12/dist-packages (1.1.1)\n",
            "Collecting OpenTSNE\n",
            "  Downloading opentsne-1.0.4-cp312-cp312-manylinux_2_24_x86_64.manylinux_2_28_x86_64.whl.metadata (8.1 kB)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn) (1.5.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn) (3.6.0)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (1.3.3)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (4.60.1)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (1.4.9)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (25.0)\n",
            "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (11.3.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (3.2.5)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch) (3.20.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.12/dist-packages (from torch) (4.15.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch) (75.2.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch) (1.13.3)\n",
            "Requirement already satisfied: networkx>=2.5.1 in /usr/local/lib/python3.12/dist-packages (from torch) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch) (3.1.6)\n",
            "Requirement already satisfied: fsspec>=0.8.5 in /usr/local/lib/python3.12/dist-packages (from torch) (2025.3.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.80)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch) (9.10.2.21)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.4.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch) (11.3.0.4)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch) (10.3.7.77)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch) (11.7.1.2)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch) (12.5.4.2)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch) (0.7.1)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.27.5 in /usr/local/lib/python3.12/dist-packages (from torch) (2.27.5)\n",
            "Requirement already satisfied: nvidia-nvshmem-cu12==3.3.20 in /usr/local/lib/python3.12/dist-packages (from torch) (3.3.20)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.85)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch) (1.11.1.6)\n",
            "Requirement already satisfied: triton==3.5.0 in /usr/local/lib/python3.12/dist-packages (from torch) (3.5.0)\n",
            "Requirement already satisfied: numba in /usr/local/lib/python3.12/dist-packages (from scikit-dimension) (0.60.0)\n",
            "Requirement already satisfied: pyogrio>=0.7.2 in /usr/local/lib/python3.12/dist-packages (from geopandas) (0.11.1)\n",
            "Requirement already satisfied: pyproj>=3.5.0 in /usr/local/lib/python3.12/dist-packages (from geopandas) (3.7.2)\n",
            "Requirement already satisfied: shapely>=2.0.0 in /usr/local/lib/python3.12/dist-packages (from geopandas) (2.1.2)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.12/dist-packages (from pyogrio>=0.7.2->geopandas) (2025.11.12)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch) (3.0.3)\n",
            "Requirement already satisfied: llvmlite<0.44,>=0.43.0dev0 in /usr/local/lib/python3.12/dist-packages (from numba->scikit-dimension) (0.43.0)\n",
            "Downloading faiss_cpu-1.13.0-cp39-abi3-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (23.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m23.6/23.6 MB\u001b[0m \u001b[31m100.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading scikit_dimension-0.3.4-py3-none-any.whl (66 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m66.4/66.4 kB\u001b[0m \u001b[31m6.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading opentsne-1.0.4-cp312-cp312-manylinux_2_24_x86_64.manylinux_2_28_x86_64.whl (3.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.5/3.5 MB\u001b[0m \u001b[31m92.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hBuilding wheels for collected packages: python-picard\n",
            "  Building wheel for python-picard (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for python-picard: filename=python_picard-0.8-py3-none-any.whl size=16394 sha256=f50288fb84062f392a3ec013899124af0ab8c9cea2a51b365e7918b1fa07a5fc\n",
            "  Stored in directory: /root/.cache/pip/wheels/c3/43/5e/501cefd2d8a40f4fff96a84acc25ddf803ca44c6ec54d8b273\n",
            "Successfully built python-picard\n",
            "Installing collected packages: faiss-cpu, scikit-dimension, python-picard, OpenTSNE\n",
            "Successfully installed OpenTSNE-1.0.4 faiss-cpu-1.13.0 python-picard-0.8 scikit-dimension-0.3.4\n"
          ]
        }
      ],
      "source": [
        "!pip install numpy pandas scikit-learn scipy matplotlib torch seaborn faiss-cpu scikit-dimension python-picard hdbscan geopandas OpenTSNE"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_fVTbL0FxqEV"
      },
      "outputs": [],
      "source": [
        "# File: utils.py\n",
        "import warnings\n",
        "import time\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from scipy.stats import skew, kurtosis # type: ignore\n",
        "import logging # Import logging directly\n",
        "from typing import NamedTuple # Ensure NamedTuple is imported\n",
        "\n",
        "# Optional imports to check at runtime\n",
        "try:\n",
        "    import faiss # type: ignore\n",
        "    FAISS_AVAILABLE = True\n",
        "except ImportError:\n",
        "    FAISS_AVAILABLE = False\n",
        "\n",
        "try:\n",
        "    from skdim.id import TwoNN # type: ignore\n",
        "    SKDIM_AVAILABLE = True\n",
        "except ImportError:\n",
        "    SKDIM_AVAILABLE = False\n",
        "\n",
        "try:\n",
        "    from picard import picard # type: ignore\n",
        "    PICARD_AVAILABLE = True\n",
        "except ImportError:\n",
        "    PICARD_AVAILABLE = False\n",
        "\n",
        "try:\n",
        "    import hdbscan # type: ignore\n",
        "    HDBSCAN_AVAILABLE = True\n",
        "except ImportError:\n",
        "    HDBSCAN_AVAILABLE = False\n",
        "\n",
        "# REMOVED: from .utils import get_logger (This was the problematic self-import)\n",
        "\n",
        "def get_logger(name: str, verbose: bool = True) -> logging.Logger:\n",
        "    \"\"\"Basic logger.\"\"\"\n",
        "    logger_instance = logging.getLogger(name)\n",
        "    if not logger_instance.handlers: # Avoid duplicate handlers\n",
        "        handler = logging.StreamHandler()\n",
        "        formatter = logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s')\n",
        "        handler.setFormatter(formatter)\n",
        "        logger_instance.addHandler(handler)\n",
        "    logger_instance.setLevel(logging.INFO if verbose else logging.WARNING)\n",
        "    return logger_instance\n",
        "\n",
        "logger = get_logger(__name__) # Now this is fine, as get_logger is defined above.\n",
        "\n",
        "def check_available_methods(verbose: bool = True): # Renamed in import alias\n",
        "    \"\"\"Check which advanced methods are available and log their status.\"\"\"\n",
        "    log_func = logger.info if verbose else lambda x: None\n",
        "    log_func(\"\\nChecking available advanced methods:\")\n",
        "    log_func(f\"  FAISS (fast KNN): {'Available' if FAISS_AVAILABLE else 'Not available'}\")\n",
        "    log_func(f\"  scikit-dim (TwoNN): {'Available' if SKDIM_AVAILABLE else 'Not available'}\")\n",
        "    log_func(f\"  Picard (ICA): {'Available' if PICARD_AVAILABLE else 'Not available'}\")\n",
        "    log_func(f\"  HDBSCAN: {'Available' if HDBSCAN_AVAILABLE else 'Not available'}\")\n",
        "    log_func(\"\")\n",
        "\n",
        "class PicardResult(NamedTuple):\n",
        "    \"\"\"\n",
        "    Stores results from Picard ICA.\n",
        "    components_ (K): The unmixing matrix (n_components, n_features).\n",
        "    mixing_ (W): The mixing matrix (n_features, n_components).\n",
        "    \"\"\"\n",
        "    components_: np.ndarray\n",
        "    mixing_: np.ndarray\n",
        "    n_iter_: int"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2h0TDNiGze-U"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import logging\n",
        "\n",
        "# Try to use Picard ICA for performance; fallback to scikit-learn FastICA\n",
        "try:\n",
        "    from picard import picard\n",
        "    PICARD_AVAILABLE = True\n",
        "except ImportError:\n",
        "    PICARD_AVAILABLE = False\n",
        "    from sklearn.decomposition import FastICA\n",
        "\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "global ICA_MODULE_AVAILABLE\n",
        "\n",
        "ICA_MODULE_AVAILABLE = True\n",
        "\n",
        "class ICAModule:\n",
        "    \"\"\"\n",
        "    Wrapper for Independent Component Analysis (ICA).\n",
        "    Uses Picard if available, otherwise falls back to scikit-learn's FastICA.\n",
        "    \"\"\"\n",
        "    def __init__(self, random_state: int = None, verbose: bool = False):\n",
        "        self.random_state = random_state\n",
        "        self.verbose = verbose\n",
        "        if PICARD_AVAILABLE:\n",
        "            logger.info(\"ICAModule: Using Picard ICA implementation.\")\n",
        "        else:\n",
        "            logger.info(\"ICAModule: Picard not available, falling back to scikit-learn FastICA.\")\n",
        "\n",
        "    def run_ica(self,\n",
        "                X: np.ndarray,\n",
        "                n_components: int,\n",
        "                feature_names: list = None\n",
        "               ) -> dict:\n",
        "        \"\"\"\n",
        "        Perform ICA on the input data X.\n",
        "\n",
        "        Parameters:\n",
        "        -------------\n",
        "        X : np.ndarray of shape (n_samples, n_features)\n",
        "            Input data matrix.\n",
        "        n_components : int\n",
        "            Number of independent components to estimate.\n",
        "        feature_names : list, optional\n",
        "            Names of the original features (for metadata).\n",
        "\n",
        "        Returns:\n",
        "        --------\n",
        "        result : dict\n",
        "            Dictionary containing:\n",
        "            - 'ica_sources': np.ndarray of shape (n_samples, n_components)\n",
        "            - 'ica_mixing_matrix': np.ndarray of shape (n_features, n_components)\n",
        "            - 'ica_model': underlying model or info dict\n",
        "        \"\"\"\n",
        "        n_samples, n_features = X.shape\n",
        "        if n_samples < 2 or n_features < 1:\n",
        "            logger.warning(\"ICAModule.run_ica: Insufficient data for ICA. Returning empty sources.\")\n",
        "            return {'ica_sources': np.empty((n_samples, 0)),\n",
        "                    'ica_mixing_matrix': np.empty((n_features, 0)),\n",
        "                    'ica_model': None}\n",
        "\n",
        "        # Center the data\n",
        "        X_centered = X - np.mean(X, axis=0, keepdims=True)\n",
        "\n",
        "        if PICARD_AVAILABLE:\n",
        "            # Picard expects shape (n_features, n_samples)\n",
        "            try:\n",
        "                W, S, info = picard(\n",
        "                    Y=X_centered.T,\n",
        "                    ortho=False,\n",
        "                    orth_wts=False,\n",
        "                    fun=\"logcosh\",  # or another contrast function\n",
        "                    max_iter=200,\n",
        "                    tol=1e-6,\n",
        "                    random_state=self.random_state,\n",
        "                    verbose=self.verbose\n",
        "                )\n",
        "                # S has shape (n_components, n_samples)\n",
        "                sources = S.T[:, :n_components]\n",
        "                mixing = np.linalg.pinv(W).T[:, :n_components]\n",
        "                model = {'unmixing_matrix': W, 'info': info}\n",
        "            except Exception as e:\n",
        "                logger.error(f\"ICAModule.run_ica: Picard ICA failed: {e}\", exc_info=True)\n",
        "                return {'ica_sources': np.empty((n_samples, 0)),\n",
        "                        'ica_mixing_matrix': np.empty((n_features, 0)),\n",
        "                        'ica_model': None}\n",
        "        else:\n",
        "            # FastICA fallback\n",
        "            try:\n",
        "                ica = FastICA(n_components=n_components,\n",
        "                              random_state=self.random_state,\n",
        "                              max_iter=200,\n",
        "                              tol=1e-6)\n",
        "                sources = ica.fit_transform(X_centered)\n",
        "                mixing = ica.mixing_\n",
        "                model = ica\n",
        "            except Exception as e:\n",
        "                logger.error(f\"ICAModule.run_ica: FastICA failed: {e}\", exc_info=True)\n",
        "                return {'ica_sources': np.empty((n_samples, 0)),\n",
        "                        'ica_mixing_matrix': np.empty((n_features, 0)),\n",
        "                        'ica_model': None}\n",
        "\n",
        "        # Optionally attach feature names if provided\n",
        "        result = {\n",
        "            'ica_sources': sources,\n",
        "            'ica_mixing_matrix': mixing,\n",
        "            'ica_model': model\n",
        "        }\n",
        "        if feature_names is not None:\n",
        "            result['feature_names'] = feature_names[:n_features]\n",
        "\n",
        "        logger.info(f\"ICAModule: Extracted {sources.shape[1]} independent components.\")\n",
        "        return result"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jwxaK2UCxtj5"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import warnings\n",
        "from typing import Tuple, List, Optional, Dict, Any, Union\n",
        "import logging\n",
        "from collections import defaultdict\n",
        "\n",
        "# Scikit-learn imports\n",
        "from sklearn.preprocessing import StandardScaler, QuantileTransformer, PowerTransformer\n",
        "from sklearn.ensemble import IsolationForest # For robust outlier detection\n",
        "from scipy.stats import skew, kurtosis\n",
        "\n",
        "\n",
        "class DataPreprocessor:\n",
        "    \"\"\"\n",
        "    Handles preprocessing of real estate data, including cleaning, transformation,\n",
        "    feature engineering (BBL), scaling, and robust outlier/percentile detection for prices.\n",
        "    Retains all records, marking sale_price and log_sale_price as NaN for missing/non-positive original prices.\n",
        "    \"\"\"\n",
        "    def __init__(self,\n",
        "                 price_col: str = 'sale_price',\n",
        "                 log_price_col_name: str = 'log_sale_price',\n",
        "                 price_outlier_col_name: str = 'is_price_outlier', # Flag for detected outliers\n",
        "                 outlier_method: str = 'isolation_forest', # Method for price outlier detection ('isolation_forest', 'percentile', 'both', 'none')\n",
        "                 outlier_contamination: Union[str, float] = 'auto', # Contamination for IsolationForest\n",
        "                 min_price_percentile_filter: Optional[float] = None, # e.g., 0.01 for 1st percentile (lower bound)\n",
        "                 max_price_percentile_filter: Optional[float] = None, # e.g., 0.99 for 99th percentile (upper bound)\n",
        "                 skewness_threshold: float = 1.0, # Threshold for auto-transforming other numeric features\n",
        "                 default_transform_type: str = 'quantile', # Default transform for skewed features\n",
        "                 random_state: Optional[int] = None,\n",
        "                 verbose: bool = True):\n",
        "\n",
        "        self.price_col = price_col\n",
        "        self.log_price_col = log_price_col_name # This will be the name of the log-transformed price column\n",
        "        self.price_outlier_col = price_outlier_col_name\n",
        "        self.outlier_method = outlier_method.lower() if isinstance(outlier_method, str) else 'none'\n",
        "        self.outlier_contamination = outlier_contamination\n",
        "\n",
        "        self.min_price_percentile_filter = min_price_percentile_filter\n",
        "        self.max_price_percentile_filter = max_price_percentile_filter\n",
        "        self.fitted_min_price_cutoff_value: Optional[float] = None\n",
        "        self.fitted_max_price_cutoff_value: Optional[float] = None\n",
        "        self.fitted_min_percentile_threshold_used: Optional[float] = None\n",
        "        self.fitted_max_percentile_threshold_used: Optional[float] = None\n",
        "\n",
        "        self.skewness_threshold = abs(skewness_threshold)\n",
        "        self.default_transform_type = default_transform_type.lower()\n",
        "        self.random_state = random_state\n",
        "        self.verbose = verbose\n",
        "        self.logger = get_logger(f\"{self.__class__.__name__}\", verbose) # Instance logger\n",
        "\n",
        "        self.feature_transformers: Dict[str, Any] = {}\n",
        "        self.scaler: Optional[StandardScaler] = None\n",
        "        self.isolation_forest_model: Optional[IsolationForest] = None\n",
        "\n",
        "        self.transformed_features_list: List[str] = []\n",
        "        self.scaled_features_list: List[str] = []\n",
        "        self.feature_type_metadata: Dict[str, str] = {}\n",
        "        self.price_analysis_stats: Dict[str, Any] = {}\n",
        "\n",
        "        self.fitted = False\n",
        "\n",
        "        valid_outlier_methods = ['isolation_forest', 'percentile', 'both', 'none']\n",
        "        if self.outlier_method not in valid_outlier_methods:\n",
        "            raise ValueError(f\"Unsupported outlier_method: {self.outlier_method}. Supported: {valid_outlier_methods}.\")\n",
        "\n",
        "        self._log(f\"DataPreprocessor initialized. Original price column: '{self.price_col}', \"\n",
        "                  f\"Log-transformed price column to be created/used: '{self.log_price_col}', \"\n",
        "                  f\"Skew Threshold: {self.skewness_threshold}, \"\n",
        "                  f\"Default Feature Transform: '{self.default_transform_type}', Price Outlier Method: '{self.outlier_method}', \"\n",
        "                  f\"Default Min Price Percentile: {self.min_price_percentile_filter}, \"\n",
        "                  f\"Default Max Price Percentile: {self.max_price_percentile_filter})\", level=\"info\")\n",
        "\n",
        "    def _log(self, message: str, level: str = \"info\", exc_info: bool = False):\n",
        "        \"\"\"Helper for conditional logging via the instance's logger.\"\"\"\n",
        "        # Ensure logger is correctly set for verbosity at the time of logging\n",
        "        current_log_level = logging.DEBUG if level == \"debug\" else logging.INFO\n",
        "        if not self.verbose and current_log_level < logging.WARNING: # Skip if not verbose and trying to log below WARNING\n",
        "             pass\n",
        "        else:\n",
        "            log_func = getattr(self.logger, level, self.logger.info)\n",
        "            log_func(message, exc_info=exc_info)\n",
        "\n",
        "\n",
        "    def clean_and_transform_price(self,\n",
        "                                  df: pd.DataFrame,\n",
        "                                  min_price_pct_threshold_to_apply: Optional[float] = None,\n",
        "                                  max_price_pct_threshold_to_apply: Optional[float] = None\n",
        "                                 ) -> pd.DataFrame:\n",
        "        \"\"\"\n",
        "        Cleans the price column (specified by `self.price_col`),\n",
        "        creates/applies log1p to positive prices into `self.log_price_col`,\n",
        "        and flags/filters outliers on positive prices.\n",
        "        Rows with initially missing or non-positive prices will have NaN in `self.price_col` (if coerced)\n",
        "        and in `self.log_price_col`. These rows are NOT dropped.\n",
        "        \"\"\"\n",
        "        self._log(f\"Starting price processing. Original price column: '{self.price_col}', \"\n",
        "                  f\"Target log-price column: '{self.log_price_col}'. Initial shape: {df.shape}\", level=\"info\")\n",
        "        _df = df.copy()\n",
        "\n",
        "        if self.price_col not in _df.columns:\n",
        "            self._log(f\"Original price column '{self.price_col}' not found in DataFrame. Cannot process prices.\", level=\"error\")\n",
        "            # Ensure log_price_col is created as all NaNs if price_col is missing, so downstream steps don't break.\n",
        "            if self.log_price_col not in _df.columns:\n",
        "                 _df[self.log_price_col] = np.nan\n",
        "                 self._log(f\"Created empty (all NaN) log-price column '{self.log_price_col}' as original price column was missing.\", level=\"warning\")\n",
        "            return _df\n",
        "\n",
        "        # Coerce to numeric. Non-numeric prices become NaN. Original NaNs remain NaN.\n",
        "        original_nan_count = _df[self.price_col].isnull().sum()\n",
        "        _df[self.price_col] = pd.to_numeric(_df[self.price_col], errors='coerce')\n",
        "        coerced_nan_count = _df[self.price_col].isnull().sum()\n",
        "        newly_coerced_nans = coerced_nan_count - original_nan_count\n",
        "        if newly_coerced_nans > 0:\n",
        "            self._log(f\"{newly_coerced_nans} entries in '{self.price_col}' were coerced to NaN (non-numeric).\", level=\"info\")\n",
        "        self._log(f\"Total {coerced_nan_count} NaN values in '{self.price_col}' after numeric coercion.\", level=\"info\")\n",
        "\n",
        "        # Initialize log_price_col. It will remain NaN for rows where price_col is NaN or non-positive.\n",
        "        # This ensures the column self.log_price_col is always created.\n",
        "        _df[self.log_price_col] = np.nan\n",
        "        self._log(f\"Initialized column '{self.log_price_col}' with NaNs. It will be populated with log1p of positive values from '{self.price_col}'.\", level=\"debug\")\n",
        "\n",
        "\n",
        "        # Initialize outlier column if any price processing is expected\n",
        "        if self.outlier_method != 'none' or \\\n",
        "           self.fitted_min_price_cutoff_value is not None or \\\n",
        "           min_price_pct_threshold_to_apply is not None or \\\n",
        "           self.fitted_max_price_cutoff_value is not None or \\\n",
        "           max_price_pct_threshold_to_apply is not None:\n",
        "            _df[self.price_outlier_col] = False # Default: not an outlier\n",
        "\n",
        "        # Identify rows with valid, positive prices for transformation and outlier detection\n",
        "        positive_price_mask = (_df[self.price_col].fillna(-1) > 0) # Fill NaN with a non-positive value for the mask\n",
        "        n_positive = positive_price_mask.sum()\n",
        "        n_total_rows = len(_df)\n",
        "        n_missing_or_non_positive = n_total_rows - n_positive\n",
        "        self._log(f\"Found {n_positive} positive prices in '{self.price_col}' (out of {n_total_rows} total rows). \"\n",
        "                  f\"{n_missing_or_non_positive} rows have missing, zero, or negative prices in '{self.price_col}'.\", level=\"info\")\n",
        "\n",
        "        current_min_cutoff = self.fitted_min_price_cutoff_value\n",
        "        current_max_cutoff = self.fitted_max_price_cutoff_value\n",
        "\n",
        "        # Percentile cutoffs are determined *only* from positive prices in self.price_col\n",
        "        if n_positive > 0:\n",
        "            if min_price_pct_threshold_to_apply is not None:\n",
        "                current_min_cutoff = np.percentile(_df.loc[positive_price_mask, self.price_col], min_price_pct_threshold_to_apply * 100)\n",
        "                self.fitted_min_price_cutoff_value = current_min_cutoff\n",
        "                self.fitted_min_percentile_threshold_used = min_price_pct_threshold_to_apply\n",
        "                self.price_analysis_stats['fitted_min_price_cutoff_value'] = current_min_cutoff\n",
        "                self.price_analysis_stats['fitted_min_percentile_threshold_used'] = min_price_pct_threshold_to_apply\n",
        "                self._log(f\"Calculated min price cutoff for '{self.price_col}' at {min_price_pct_threshold_to_apply*100:.1f}th percentile of positive prices: {current_min_cutoff}\", level=\"info\")\n",
        "\n",
        "            if max_price_pct_threshold_to_apply is not None:\n",
        "                current_max_cutoff = np.percentile(_df.loc[positive_price_mask, self.price_col], max_price_pct_threshold_to_apply * 100)\n",
        "                self.fitted_max_price_cutoff_value = current_max_cutoff\n",
        "                self.fitted_max_percentile_threshold_used = max_price_pct_threshold_to_apply\n",
        "                self.price_analysis_stats['fitted_max_price_cutoff_value'] = current_max_cutoff\n",
        "                self.price_analysis_stats['fitted_max_percentile_threshold_used'] = max_price_pct_threshold_to_apply\n",
        "                self._log(f\"Calculated max price cutoff for '{self.price_col}' at {max_price_pct_threshold_to_apply*100:.1f}th percentile of positive prices: {current_max_cutoff}\", level=\"info\")\n",
        "\n",
        "        percentile_filtered_indices = pd.Series(False, index=_df.index)\n",
        "        if current_min_cutoff is not None:\n",
        "            percentile_filtered_indices |= (_df[self.price_col] < current_min_cutoff) & positive_price_mask\n",
        "        if current_max_cutoff is not None:\n",
        "            percentile_filtered_indices |= (_df[self.price_col] > current_max_cutoff) & positive_price_mask\n",
        "\n",
        "        if ('percentile' in self.outlier_method or 'both' in self.outlier_method) and self.price_outlier_col in _df.columns:\n",
        "            if percentile_filtered_indices.any():\n",
        "                _df.loc[percentile_filtered_indices, self.price_outlier_col] = True\n",
        "                self._log(f\"Flagged {percentile_filtered_indices.sum()} positive prices in '{self.price_col}' as outliers based on percentile cutoffs.\", level=\"info\")\n",
        "\n",
        "        # Apply log1p transform to positive prices, storing in self.log_price_col\n",
        "        if n_positive > 0:\n",
        "            _df.loc[positive_price_mask, self.log_price_col] = np.log1p(_df.loc[positive_price_mask, self.price_col])\n",
        "            self._log(f\"Applied log1p transform to {n_positive} positive values from '{self.price_col}' and stored in '{self.log_price_col}'. \"\n",
        "                      f\"Non-positive or NaN original prices in '{self.price_col}' result in NaN in '{self.log_price_col}'.\", level=\"debug\")\n",
        "\n",
        "            # Isolation Forest outlier detection on log-transformed positive prices (from self.log_price_col)\n",
        "            if ('isolation_forest' in self.outlier_method or 'both' in self.outlier_method) and self.price_outlier_col in _df.columns:\n",
        "                log_positive_prices = _df.loc[positive_price_mask, self.log_price_col].values.reshape(-1, 1)\n",
        "                finite_log_mask = np.isfinite(log_positive_prices.flatten())\n",
        "\n",
        "                if not np.any(finite_log_mask):\n",
        "                    self._log(f\"No finite log-positive prices in '{self.log_price_col}' available for IsolationForest.\", level=\"warning\")\n",
        "                    if min_price_pct_threshold_to_apply is not None: # i.e. during fit\n",
        "                        self.isolation_forest_model = None\n",
        "                else:\n",
        "                    log_positive_prices_finite = log_positive_prices[finite_log_mask]\n",
        "                    if log_positive_prices_finite.shape[0] < 2:\n",
        "                        self._log(f\"Too few finite positive log-price samples ({log_positive_prices_finite.shape[0]}) in '{self.log_price_col}' for IsolationForest. Skipping.\", level=\"warning\")\n",
        "                        if min_price_pct_threshold_to_apply is not None:\n",
        "                            self.isolation_forest_model = None\n",
        "                    else:\n",
        "                        try:\n",
        "                            if min_price_pct_threshold_to_apply is not None or self.isolation_forest_model is None:\n",
        "                                self._log(f\"Fitting IsolationForest (contamination='{self.outlier_contamination}') on {log_positive_prices_finite.shape[0]} finite values from '{self.log_price_col}'.\", level=\"debug\")\n",
        "                                if_model = IsolationForest(contamination=self.outlier_contamination, random_state=self.random_state, n_estimators=100)\n",
        "                                if_model.fit(log_positive_prices_finite)\n",
        "                                self.isolation_forest_model = if_model\n",
        "\n",
        "                            if self.isolation_forest_model:\n",
        "                                outlier_preds_finite = self.isolation_forest_model.predict(log_positive_prices_finite)\n",
        "                                temp_if_outlier_flags = pd.Series(False, index=_df.loc[positive_price_mask].index)\n",
        "                                indices_finite = _df.loc[positive_price_mask].index[finite_log_mask]\n",
        "                                temp_if_outlier_flags.loc[indices_finite] = (outlier_preds_finite == -1)\n",
        "\n",
        "                                if 'both' in self.outlier_method:\n",
        "                                    _df.loc[positive_price_mask, self.price_outlier_col] |= temp_if_outlier_flags\n",
        "                                else: # Overwrite/set with IF flags\n",
        "                                    _df.loc[positive_price_mask, self.price_outlier_col] = temp_if_outlier_flags\n",
        "\n",
        "                                num_if_outliers = temp_if_outlier_flags.sum()\n",
        "                                self._log(f\"IsolationForest identified {num_if_outliers} potential price outliers using '{self.log_price_col}'. Updated '{self.price_outlier_col}'.\", level=\"info\")\n",
        "                            else:\n",
        "                                self._log(\"Isolation Forest model not available for prediction.\", level=\"warning\")\n",
        "                        except Exception as e:\n",
        "                            self._log(f\"Error during IsolationForest on '{self.log_price_col}': {e}\", level=\"error\", exc_info=True)\n",
        "                            if min_price_pct_threshold_to_apply is not None:\n",
        "                                self.isolation_forest_model = None\n",
        "        else: # n_positive == 0\n",
        "            self._log(f\"No positive prices found in '{self.price_col}'. '{self.log_price_col}' will contain all NaNs. No outlier detection on prices performed.\", level=\"info\")\n",
        "\n",
        "        try:\n",
        "            valid_original_prices = _df[self.price_col].dropna()\n",
        "            if not valid_original_prices.empty: self.price_analysis_stats['original_skew'] = skew(valid_original_prices)\n",
        "\n",
        "            finite_log_prices = _df[self.log_price_col].replace([np.inf, -np.inf], np.nan).dropna()\n",
        "            if not finite_log_prices.empty:\n",
        "                self.price_analysis_stats['log_skew'] = skew(finite_log_prices)\n",
        "                self.price_analysis_stats['log_kurtosis'] = kurtosis(finite_log_prices)\n",
        "                self.price_analysis_stats['log_heavy_tailed'] = self.price_analysis_stats.get('log_kurtosis', 0) > 1.0\n",
        "                self._log(f\"Stats for '{self.price_col}': OrigSkew={self.price_analysis_stats.get('original_skew', 'N/A'):.2f}. \"\n",
        "                          f\"Stats for '{self.log_price_col}': LogSkew={self.price_analysis_stats.get('log_skew', 'N/A'):.2f}, \"\n",
        "                          f\"LogKurt={self.price_analysis_stats.get('log_kurtosis', 'N/A'):.2f} \"\n",
        "                          f\"(HeavyTailed: {self.price_analysis_stats.get('log_heavy_tailed', False)})\", level=\"debug\")\n",
        "            else:\n",
        "                self._log(f\"No finite values in '{self.log_price_col}' to calculate log skew/kurtosis.\", level=\"debug\")\n",
        "                self.price_analysis_stats.update({'log_skew': np.nan, 'log_kurtosis': np.nan, 'log_heavy_tailed': False})\n",
        "        except Exception as stat_e:\n",
        "            self._log(f\"Could not calculate price skew/kurtosis for '{self.price_col}' or '{self.log_price_col}': {stat_e}\", level=\"warning\")\n",
        "\n",
        "        self.feature_type_metadata[self.price_col] = 'numeric_price_original'\n",
        "        self.feature_type_metadata[self.log_price_col] = 'numeric_price_log_transformed_or_nan'\n",
        "        if self.price_outlier_col in _df.columns:\n",
        "            self.feature_type_metadata[self.price_outlier_col] = 'boolean_price_outlier_flag'\n",
        "\n",
        "        # Final check on the log_price_col status\n",
        "        if self.log_price_col in _df.columns:\n",
        "            num_log_price_final_nan = _df[self.log_price_col].isnull().sum()\n",
        "            self._log(f\"Price processing complete for original price column '{self.price_col}'. \"\n",
        "                      f\"Log-transformed price column '{self.log_price_col}' created/updated. \"\n",
        "                      f\"'{self.log_price_col}' contains {num_log_price_final_nan} NaNs out of {len(_df)} rows. \"\n",
        "                      f\"Shape after: {_df.shape}\", level=\"info\")\n",
        "        else:\n",
        "            self._log(f\"Price processing attempted, but log-price column '{self.log_price_col}' is unexpectedly missing. \"\n",
        "                      f\"Shape after: {_df.shape}\", level=\"error\")\n",
        "\n",
        "        return _df\n",
        "\n",
        "    def create_bbl_identifier(self, df: pd.DataFrame) -> pd.DataFrame:\n",
        "        \"\"\"Creates a standard BBL (Borough-Block-Lot) identifier string.\"\"\"\n",
        "        self._log(\"Attempting to create BBL identifier...\", level=\"debug\")\n",
        "        _df = df.copy()\n",
        "        bbl_components = ['borough', 'block', 'lot']\n",
        "\n",
        "        if 'bbl' in _df.columns and _df['bbl'].notna().all(): # Check if 'bbl' already exists and is reasonably populated\n",
        "            self._log(\"BBL column already exists and seems populated. Skipping creation.\", level=\"debug\")\n",
        "            if 'bbl' not in self.feature_type_metadata: # Ensure metadata is set\n",
        "                self.feature_type_metadata['bbl'] = 'identifier_existing'\n",
        "            return _df\n",
        "\n",
        "        missing_bbl_cols = [col for col in bbl_components if col not in _df.columns]\n",
        "        if missing_bbl_cols:\n",
        "            self._log(f\"BBL component columns missing: {missing_bbl_cols}. Cannot create BBL identifier.\", level=\"warning\")\n",
        "            return _df\n",
        "\n",
        "        try:\n",
        "            # Convert components to string, handling NaNs by treating them as invalid (-1 before str conversion)\n",
        "            _df['borough_str'] = _df['borough'].fillna(-1).astype(float).astype(int).astype(str).str.strip()\n",
        "            _df['block_str']   = _df['block'].fillna(-1).astype(float).astype(int).astype(str).str.strip().str.zfill(5)\n",
        "            _df['lot_str']     = _df['lot'].fillna(-1).astype(float).astype(int).astype(str).str.strip().str.zfill(4)\n",
        "\n",
        "            # Create BBL only where all components were valid (not resulting in '-1' based strings)\n",
        "            # and do not contain hyphens (which might indicate parsing issues or actual negative numbers)\n",
        "            valid_bbl_mask = ~(_df['borough_str'].isin(['-1', 'nan'])) & \\\n",
        "                             ~(_df['block_str'].isin([('-1').zfill(5), ('nan').zfill(5)])) & \\\n",
        "                             ~(_df['lot_str'].isin([('-1').zfill(4), ('nan').zfill(4)])) & \\\n",
        "                             ~(_df['borough_str'].str.contains('-')) & \\\n",
        "                             ~(_df['block_str'].str.contains('-')) & \\\n",
        "                             ~(_df['lot_str'].str.contains('-'))\n",
        "\n",
        "\n",
        "            _df['bbl'] = pd.NA # Initialize with pandas NA for string type\n",
        "            _df.loc[valid_bbl_mask, 'bbl'] = _df['borough_str'] + _df['block_str'] + _df['lot_str']\n",
        "\n",
        "            num_created = valid_bbl_mask.sum()\n",
        "            num_failed = len(_df) - num_created\n",
        "            self._log(f\"Created {num_created} BBL identifiers. {num_failed} rows had invalid/missing BBL components.\", level=\"info\")\n",
        "\n",
        "            _df.drop(columns=['borough_str', 'block_str', 'lot_str'], inplace=True)\n",
        "            self.feature_type_metadata['bbl'] = 'identifier_created'\n",
        "        except Exception as e:\n",
        "            self._log(f\"Error creating BBL identifier: {e}\", level=\"error\", exc_info=True)\n",
        "            if 'bbl' not in _df.columns: _df['bbl'] = pd.NA # Ensure column exists even if failed\n",
        "        return _df\n",
        "\n",
        "    def apply_feature_transformations(self,\n",
        "                                      df: pd.DataFrame,\n",
        "                                      numeric_cols: List[str],\n",
        "                                      skewness_threshold_to_use: float,\n",
        "                                      default_transform_type_to_use: str,\n",
        "                                      auto_transform_skewed: bool = True\n",
        "                                     ) -> Tuple[pd.DataFrame, Dict[str, Any]]:\n",
        "        _df = df.copy()\n",
        "        applied_transformers: Dict[str, Any] = {}\n",
        "        transformed_cols_run: List[str] = []\n",
        "\n",
        "        valid_numeric_cols = [col for col in numeric_cols if col in _df.columns and pd.api.types.is_numeric_dtype(_df[col])]\n",
        "        if not valid_numeric_cols:\n",
        "            self._log(\"No valid numeric columns provided for transformation.\", level=\"warning\")\n",
        "            return _df, applied_transformers\n",
        "\n",
        "        cols_to_transform_map: Dict[str, str] = {}\n",
        "        effective_transform_type = default_transform_type_to_use.lower()\n",
        "        effective_skew_threshold = abs(skewness_threshold_to_use)\n",
        "\n",
        "        if auto_transform_skewed:\n",
        "            self._log(f\"Auto-detecting skewed features (Skew threshold > {effective_skew_threshold}). Applying '{effective_transform_type}'.\", level=\"info\")\n",
        "            for col in valid_numeric_cols:\n",
        "                col_data = _df[col].dropna()\n",
        "                if len(col_data) > 2: # Skew calculation needs at least 3 non-NaN points\n",
        "                    try:\n",
        "                        col_skew = skew(col_data)\n",
        "                        if abs(col_skew) > effective_skew_threshold:\n",
        "                            cols_to_transform_map[col] = effective_transform_type\n",
        "                    except Exception as e: self._log(f\"Skew calculation failed for '{col}': {e}\", level=\"warning\")\n",
        "            self._log(f\"Found {len(cols_to_transform_map)} skewed columns for auto-transformation: {list(cols_to_transform_map.keys())}\", level=\"info\")\n",
        "        elif effective_transform_type != 'none': # Apply to all if not auto and type is not none\n",
        "            self._log(f\"Applying '{effective_transform_type}' to all {len(valid_numeric_cols)} provided numeric columns (auto_transform_skewed=False).\", level=\"info\")\n",
        "            for col in valid_numeric_cols: cols_to_transform_map[col] = effective_transform_type\n",
        "\n",
        "        # Group columns by transformation type to apply batch transformations if possible\n",
        "        grouped_cols: Dict[str, List[str]] = defaultdict(list)\n",
        "        for col, t_type in cols_to_transform_map.items(): grouped_cols[t_type].append(col)\n",
        "\n",
        "        for t_type, cols in grouped_cols.items():\n",
        "            if not cols: continue\n",
        "            self._log(f\"Applying '{t_type}' transform to columns: {cols}\", level=\"debug\")\n",
        "            X_subset = _df[cols].copy() # Operate on a copy for this transformation group\n",
        "\n",
        "            # Impute NaNs before transformation\n",
        "            if X_subset.isnull().any().any():\n",
        "                self._log(f\"NaNs detected before '{t_type}' transform for {cols}. Imputing with median.\", level=\"warning\")\n",
        "                for c_name in cols: # Impute each column in the subset individually\n",
        "                    if X_subset[c_name].isnull().any():\n",
        "                        median_val = X_subset[c_name].median()\n",
        "                        X_subset[c_name].fillna(median_val if not pd.isna(median_val) else 0.0, inplace=True)\n",
        "\n",
        "            # Replace Infs after imputation\n",
        "            if not np.all(np.isfinite(X_subset.values)):\n",
        "                self._log(f\"Non-finite values (Inf) detected after imputation for '{t_type}' transform for {cols}. Replacing with 0.\", level=\"warning\")\n",
        "                for c_name in cols: # Replace in each column of the subset\n",
        "                    X_subset[c_name].replace([np.inf, -np.inf], 0, inplace=True)\n",
        "\n",
        "            transformer_instance = None; transformed_data = None\n",
        "            # Unique key for the transformer instance based on type and columns it applies to\n",
        "            # Useful if different groups of columns get the same type but fitted differently (though current logic fits per group)\n",
        "            transformer_key = f\"{t_type}_transformer_for_{'_'.join(sorted(cols))}\"\n",
        "\n",
        "            try:\n",
        "                if t_type == 'quantile':\n",
        "                    n_samples_subset = X_subset.shape[0]\n",
        "                    # Adjust n_quantiles: must be <= n_samples. If n_samples is small, use n_samples-1 or a small default.\n",
        "                    n_q = min(1000, max(10, n_samples_subset -1 if n_samples_subset > 1 else 1))\n",
        "                    if n_q >= n_samples_subset and n_samples_subset > 0 : n_q = n_samples_subset -1 # strictly less\n",
        "                    if n_q <=0 : n_q = min(10, n_samples_subset if n_samples_subset > 0 else 1) # smallest possible if very few samples\n",
        "\n",
        "                    transformer_instance = QuantileTransformer(output_distribution='normal', n_quantiles=n_q, random_state=self.random_state, subsample=min(100_000, n_samples_subset))\n",
        "                elif t_type == 'yeo-johnson':\n",
        "                    transformer_instance = PowerTransformer(method='yeo-johnson', standardize=False) # standardize=False as we scale later\n",
        "                elif t_type == 'box-cox':\n",
        "                    # Box-Cox requires all data to be positive.\n",
        "                    if np.any(X_subset.values <= 0):\n",
        "                        self._log(f\"Clipping non-positive values to a small positive number (e.g., 1e-6) for Box-Cox on {cols}.\", level=\"warning\")\n",
        "                        X_subset = X_subset.clip(lower=1e-6)\n",
        "                    transformer_instance = PowerTransformer(method='box-cox', standardize=False)\n",
        "                elif t_type == 'log1p': # Direct numpy transform, not a sklearn transformer\n",
        "                    if np.any(X_subset.values < 0): self._log(f\"log1p encountered negative values in {cols}. Resulting NaNs will be cleaned post-transform.\", level=\"warning\")\n",
        "                    transformed_data = np.log1p(X_subset.values)\n",
        "                    # Clean NaNs/Infs that might result from log1p (e.g., log1p(-1) = -inf for complex, or from prior Infs)\n",
        "                    transformed_data = np.nan_to_num(transformed_data, nan=0.0, posinf=0.0, neginf=0.0) # Replace with 0\n",
        "                else:\n",
        "                    self._log(f\"Unsupported transform_type: '{t_type}'. Skipping for columns: {cols}.\", level=\"warning\"); continue\n",
        "\n",
        "                if transformer_instance: # If a sklearn transformer was instantiated\n",
        "                    transformed_data = transformer_instance.fit_transform(X_subset)\n",
        "                    # Store the fitted transformer instance\n",
        "                    self.feature_transformers[transformer_key] = transformer_instance\n",
        "\n",
        "                if transformed_data is not None:\n",
        "                    _df[cols] = transformed_data # Update original DataFrame subset\n",
        "                    transformed_cols_run.extend(cols)\n",
        "                    for col_name in cols: self.feature_type_metadata[col_name] = f'numeric_transformed_{t_type}'\n",
        "            except Exception as e:\n",
        "                self._log(f\"Error during '{t_type}' transform for columns {cols}: {e}\", level=\"error\", exc_info=True)\n",
        "\n",
        "        self.transformed_features_list = list(set(self.transformed_features_list + transformed_cols_run))\n",
        "        # Ensure all numeric cols processed get some metadata type\n",
        "        for col in valid_numeric_cols:\n",
        "            if col not in transformed_cols_run and col not in self.feature_type_metadata: # If not transformed and no prior type\n",
        "                self.feature_type_metadata[col] = 'numeric_original_untransformed' # More specific\n",
        "        return _df, self.feature_transformers # Return transformers for `transform` method\n",
        "\n",
        "    def scale_features(self, df: pd.DataFrame, features_to_scale: List[str]) -> Tuple[pd.DataFrame, Optional[StandardScaler]]:\n",
        "        self._log(f\"Applying StandardScaler to {len(features_to_scale)} features: {features_to_scale[:5]}...\", level=\"info\")\n",
        "        _df = df.copy()\n",
        "        valid_features = [f for f in features_to_scale if f in _df.columns and pd.api.types.is_numeric_dtype(_df[f])]\n",
        "        if not valid_features:\n",
        "            self._log(\"No valid numeric features found for scaling.\", level=\"warning\"); return _df, None\n",
        "\n",
        "        X_to_scale = _df[valid_features].copy() # Operate on a copy\n",
        "        # Impute NaNs before scaling\n",
        "        if X_to_scale.isnull().any().any():\n",
        "            cols_with_nan = X_to_scale.columns[X_to_scale.isnull().any()].tolist()\n",
        "            self._log(f\"NaNs detected before scaling columns: {cols_with_nan}. Imputing with median.\", level=\"warning\")\n",
        "            for col in valid_features: # Impute each column individually\n",
        "                if X_to_scale[col].isnull().any():\n",
        "                    median_val = X_to_scale[col].median()\n",
        "                    X_to_scale[col].fillna(median_val if not pd.isna(median_val) else 0.0, inplace=True)\n",
        "\n",
        "        # Replace Infs after imputation\n",
        "        if not np.all(np.isfinite(X_to_scale.values)):\n",
        "            self._log(\"Non-finite values (Inf) detected before scaling after NaN imputation. Replacing with 0.\", level=\"error\")\n",
        "            X_to_scale.replace([np.inf, -np.inf], 0, inplace=True) # Replace in the subset\n",
        "\n",
        "        # Check for near-constant columns before scaling\n",
        "        variances = X_to_scale.var()\n",
        "        constant_cols = variances[variances < 1e-10].index.tolist()\n",
        "        if constant_cols:\n",
        "            self._log(f\"Near-constant columns detected before scaling: {constant_cols}. Their variance is < 1e-10. StandardScaler will result in zeros for these.\", level=\"warning\")\n",
        "\n",
        "        current_scaler = StandardScaler()\n",
        "        try:\n",
        "            scaled_values = current_scaler.fit_transform(X_to_scale)\n",
        "            # Handle NaNs that might result from scaling constant columns (variance is 0)\n",
        "            if np.isnan(scaled_values).any():\n",
        "                nan_cols_after_scale = X_to_scale.columns[np.isnan(scaled_values).any(axis=0)].tolist()\n",
        "                self._log(f\"NaNs generated by StandardScaler for columns (likely due to zero variance): {nan_cols_after_scale}. Replacing these NaNs with 0.\", level=\"warning\")\n",
        "                scaled_values = np.nan_to_num(scaled_values, nan=0.0) # Replace NaNs with 0\n",
        "\n",
        "            _df[valid_features] = scaled_values # Update original DataFrame\n",
        "            self.scaler = current_scaler # Store the fitted scaler\n",
        "            self.scaled_features_list = list(set(self.scaled_features_list + valid_features))\n",
        "            self._log(f\"StandardScaler applied to {len(valid_features)} features.\", level=\"info\")\n",
        "            for col in valid_features: # Update metadata\n",
        "                current_type = self.feature_type_metadata.get(col, 'numeric') # Default if no prior transform type\n",
        "                if '_scaled' not in current_type: self.feature_type_metadata[col] = current_type + '_scaled'\n",
        "            return _df, self.scaler\n",
        "        except Exception as e:\n",
        "            self._log(f\"Error scaling features {valid_features}: {e}\", level=\"error\", exc_info=True)\n",
        "            return df.copy(), None # Return original df on error\n",
        "\n",
        "    def fit_transform(self, df: pd.DataFrame,\n",
        "                      numeric_features_to_process: Optional[List[str]] = None,\n",
        "                      auto_transform_skewed: bool = True,\n",
        "                      scale_numeric: bool = True,\n",
        "                      create_bbl: bool = True,\n",
        "                      min_price_percentile: Optional[float] = None,\n",
        "                      max_price_percentile: Optional[float] = None,\n",
        "                      default_transform_type_override: Optional[str] = None,\n",
        "                      transform_skew_threshold_override: Optional[float] = None,\n",
        "                      **kwargs # Allow extra kwargs\n",
        "                     ) -> Optional[pd.DataFrame]:\n",
        "        self.logger.info(f\"--- Starting Preprocessing Pipeline (fit_transform) on DataFrame with shape {df.shape} ---\")\n",
        "        if kwargs:\n",
        "            self._log(f\"Received unexpected keyword arguments in fit_transform: {kwargs}. These will be ignored.\", level=\"warning\")\n",
        "\n",
        "        # Reset all fitted states\n",
        "        self.fitted = False; self.feature_transformers = {}; self.transformed_features_list = []\n",
        "        self.scaler = None; self.scaled_features_list = []; self.feature_type_metadata = {}\n",
        "        self.isolation_forest_model = None; self.price_analysis_stats = {}\n",
        "        self.fitted_min_price_cutoff_value = None; self.fitted_max_price_cutoff_value = None\n",
        "        self.fitted_min_percentile_threshold_used = None; self.fitted_max_percentile_threshold_used = None\n",
        "\n",
        "        # Determine effective parameters for this run\n",
        "        eff_min_price_pct_thresh = min_price_percentile if min_price_percentile is not None else self.min_price_percentile_filter\n",
        "        eff_max_price_pct_thresh = max_price_percentile if max_price_percentile is not None else self.max_price_percentile_filter\n",
        "        eff_skew_thresh = transform_skew_threshold_override if transform_skew_threshold_override is not None else self.skewness_threshold\n",
        "        eff_transform_type = default_transform_type_override if default_transform_type_override is not None else self.default_transform_type\n",
        "\n",
        "        self._log(f\"Effective parameters for fit_transform: MinPricePct={eff_min_price_pct_thresh}, MaxPricePct={eff_max_price_pct_thresh}, \"\n",
        "                  f\"PriceCol='{self.price_col}', LogPriceCol='{self.log_price_col}', \"\n",
        "                  f\"SkewThreshold={eff_skew_thresh}, DefaultTransformType='{eff_transform_type}'\", level=\"debug\")\n",
        "\n",
        "        _df = self.clean_and_transform_price(df.copy(), # Operate on a copy\n",
        "                                             min_price_pct_threshold_to_apply=eff_min_price_pct_thresh,\n",
        "                                             max_price_pct_threshold_to_apply=eff_max_price_pct_thresh)\n",
        "        if _df is None or _df.empty:\n",
        "            self.logger.error(\"DataFrame empty after price cleaning. Aborting fit_transform.\"); return None\n",
        "\n",
        "        if create_bbl: _df = self.create_bbl_identifier(_df)\n",
        "\n",
        "        # Define columns to exclude from numeric_features_to_process\n",
        "        price_related_cols = [self.price_col, self.log_price_col, self.price_outlier_col]\n",
        "        # Define a more comprehensive list of potential ID columns to exclude from general numeric processing\n",
        "        potential_id_cols = [\n",
        "            'bbl', 'borough', 'block', 'lot', 'zip_code', 'zipcode', # Common BBL and zip\n",
        "            'cd', 'council', 'schooldist', 'healtharea', 'policeprct', # Admin/district IDs\n",
        "            'xcoord', 'ycoord', 'latitude', 'longitude', # Coordinates\n",
        "            'index', 'id', 'rowid', 'geoid', 'gid', 'uniqueid', 'parcelid', 'parid', 'parcelnumb', # Generic ID names\n",
        "            'condono', 'appbbl', 'tbl', 'year', 'sale_year', 'sale_month', 'sale_day' # Other common IDs/time parts\n",
        "        ]\n",
        "        # Filter this list to only include those present in the DataFrame, case-insensitively\n",
        "        df_cols_lower_map = {c.lower(): c for c in _df.columns}\n",
        "        id_cols_in_df_cased = [df_cols_lower_map[id_col_lower] for id_col_lower in potential_id_cols if id_col_lower in df_cols_lower_map]\n",
        "\n",
        "        cols_to_exclude_from_num_processing = list(set(price_related_cols + id_cols_in_df_cased))\n",
        "\n",
        "\n",
        "        if numeric_features_to_process is None: # Auto-detect if not provided\n",
        "            process_cols = [col for col in _df.select_dtypes(include=np.number).columns\n",
        "                            if col not in cols_to_exclude_from_num_processing]\n",
        "            self._log(f\"Auto-detected {len(process_cols)} numeric features for transformation/scaling: {process_cols[:10]}...\", level=\"debug\")\n",
        "        else: # Use provided list, but filter out non-numeric, price/ID cols\n",
        "            process_cols = [f for f in numeric_features_to_process\n",
        "                            if f in _df.columns and pd.api.types.is_numeric_dtype(_df[f])\n",
        "                            and f not in cols_to_exclude_from_num_processing]\n",
        "            self._log(f\"Using specified list: {len(process_cols)} valid numeric features for transformation/scaling: {process_cols[:10]}...\", level=\"debug\")\n",
        "\n",
        "        if process_cols:\n",
        "            _df, fitted_transformers = self.apply_feature_transformations(\n",
        "                _df, process_cols,\n",
        "                skewness_threshold_to_use=eff_skew_thresh,\n",
        "                default_transform_type_to_use=eff_transform_type,\n",
        "                auto_transform_skewed=auto_transform_skewed\n",
        "            )\n",
        "            self.feature_transformers = fitted_transformers # Store fitted transformers\n",
        "        else: self._log(\"No numeric features to apply transformations to (after exclusions).\", level=\"info\")\n",
        "\n",
        "        if scale_numeric and process_cols:\n",
        "            _df, fitted_scaler = self.scale_features(_df, features_to_scale=process_cols)\n",
        "            self.scaler = fitted_scaler # Store fitted scaler\n",
        "        elif not process_cols: self._log(\"No numeric features to scale (after exclusions).\", level=\"info\")\n",
        "        else: # scale_numeric is False\n",
        "            self._log(\"Scaling of numeric features skipped as per 'scale_numeric=False' configuration.\", level=\"info\")\n",
        "            # Ensure metadata is set for unscaled processed columns\n",
        "            for col in process_cols:\n",
        "                if col in self.feature_type_metadata and '_scaled' not in self.feature_type_metadata[col]: pass # Already typed by transform\n",
        "                elif col not in self.feature_type_metadata: self.feature_type_metadata[col] = 'numeric_processed_unscaled'\n",
        "\n",
        "\n",
        "        self.fitted = True\n",
        "        self.logger.info(f\"--- Preprocessing Pipeline (fit_transform) Completed. Final shape: {_df.shape} ---\")\n",
        "\n",
        "        # Sanity checks post-processing\n",
        "        if self.log_price_col in _df.columns:\n",
        "            num_log_price_nan = _df[self.log_price_col].isnull().sum()\n",
        "            if num_log_price_nan == len(_df) and n_positive > 0 : # Only warn if there *were* positive prices initially\n",
        "                self.logger.warning(f\"CRITICAL WARNING: Column '{self.log_price_col}' is all NaNs post-preprocessing, \"\n",
        "                                    f\"even though positive prices were present in '{self.price_col}'!\")\n",
        "            elif num_log_price_nan > 0 :\n",
        "                self._log(f\"Note: Log-price column '{self.log_price_col}' contains {num_log_price_nan} NaNs \"\n",
        "                          f\"post-preprocessing (expected for non-positive/missing original prices in '{self.price_col}').\", level=\"info\")\n",
        "        else:\n",
        "             self.logger.error(f\"CRITICAL: Log-price column '{self.log_price_col}' is MISSING from DataFrame after fit_transform!\")\n",
        "\n",
        "\n",
        "        final_check_cols = [c for c in process_cols if c in _df.columns] # Re-check for existence\n",
        "        if final_check_cols and _df[final_check_cols].isnull().any().any():\n",
        "            nan_counts_final = _df[final_check_cols].isnull().sum()\n",
        "            cols_with_nans_final = nan_counts_final[nan_counts_final > 0].index.tolist()\n",
        "            self.logger.critical(f\"CRITICAL: NaNs found in final processed numeric modeling columns: {cols_with_nans_final}. \"\n",
        "                                 f\"Counts: {nan_counts_final[nan_counts_final > 0].to_dict()}. This may indicate issues in imputation within transform/scale.\")\n",
        "        return _df\n",
        "\n",
        "    def transform(self, df: pd.DataFrame) -> Optional[pd.DataFrame]:\n",
        "        if not self.fitted:\n",
        "            raise RuntimeError(\"Preprocessor must be fitted using 'fit_transform' before calling 'transform'.\")\n",
        "\n",
        "        self.logger.info(f\"--- Applying Fitted Preprocessing Pipeline (transform) to new data. Shape: {df.shape} ---\")\n",
        "        _df = df.copy() # Operate on a copy\n",
        "\n",
        "        # Price cleaning uses fitted cutoffs (if any) and fitted IF model. It also creates/updates log_price_col.\n",
        "        _df = self.clean_and_transform_price(_df)\n",
        "        if _df is None or _df.empty:\n",
        "            self.logger.warning(\"DataFrame empty after price cleaning in transform. Returning as is.\"); return _df\n",
        "\n",
        "        # BBL creation if it was done during fit\n",
        "        if self.feature_type_metadata.get('bbl') == 'identifier_created':\n",
        "            _df = self.create_bbl_identifier(_df)\n",
        "\n",
        "        processed_in_transform = set() # Keep track of columns processed by specific transformers\n",
        "        # Apply fitted feature transformers\n",
        "        for transformer_key, transformer_instance in self.feature_transformers.items():\n",
        "            # Infer columns this transformer was fitted on from its key\n",
        "            # Assuming key format like \"quantile_transformer_for_colA_colB\"\n",
        "            try:\n",
        "                # Robustly extract columns from the key\n",
        "                key_parts = transformer_key.split('_for_')\n",
        "                transform_type_from_key = key_parts[0].split('_transformer')[0] if key_parts else \"unknown\"\n",
        "                cols_str = key_parts[1] if len(key_parts) > 1 else ''\n",
        "\n",
        "                # Find actual columns this transformer was fitted on (from self.transformed_features_list and metadata)\n",
        "                # This is tricky if keys are not perfectly parseable. A better way is to store (transformer, cols_list) in self.feature_transformers.\n",
        "                # For now, let's try to find relevant columns for this specific instance:\n",
        "                cols_to_apply_transform = []\n",
        "                if cols_str: # If key has _for_colA_colB structure\n",
        "                    potential_cols = cols_str.split('_')\n",
        "                    cols_to_apply_transform = [c for c in potential_cols if c in _df.columns and c not in processed_in_transform]\n",
        "                else: # Fallback: if key is just 'quantile_transformer' (less specific)\n",
        "                    cols_to_apply_transform = [\n",
        "                        col for col, meta in self.feature_type_metadata.items()\n",
        "                        if f'transformed_{transform_type_from_key}' in meta and col in _df.columns and col not in processed_in_transform\n",
        "                        and col in self.transformed_features_list # Ensure it was one of the transformed columns\n",
        "                    ]\n",
        "                cols_to_apply_transform = list(set(cols_to_apply_transform)) # Unique\n",
        "\n",
        "            except Exception as e_key_parse:\n",
        "                 self._log(f\"Could not parse columns from transformer key '{transformer_key}': {e_key_parse}. Skipping this transformer.\", level=\"warning\")\n",
        "                 continue\n",
        "\n",
        "            if not cols_to_apply_transform:\n",
        "                self._log(f\"No columns found or remaining in input df for transformer key '{transformer_key}'. Skipping.\", level=\"debug\")\n",
        "                continue\n",
        "\n",
        "            self._log(f\"Applying fitted transformer '{transformer_key}' to columns: {cols_to_apply_transform}\", level=\"debug\")\n",
        "            X_new_subset = _df[cols_to_apply_transform].copy() # Operate on copy of relevant columns\n",
        "\n",
        "            # Impute NaNs using median of the current subset (as during fit)\n",
        "            if X_new_subset.isnull().any().any():\n",
        "                self._log(f\"NaNs detected in new data for transform '{transformer_key}' for columns {cols_to_apply_transform}. Imputing with median of subset.\", level=\"warning\")\n",
        "                for c in cols_to_apply_transform:\n",
        "                    if X_new_subset[c].isnull().any():\n",
        "                        median_val = X_new_subset[c].median()\n",
        "                        X_new_subset[c].fillna(median_val if not pd.isna(median_val) else 0.0, inplace=True)\n",
        "\n",
        "            # Replace Infs after imputation\n",
        "            if not np.all(np.isfinite(X_new_subset.values)):\n",
        "                self._log(f\"Non-finite values detected after imputation for '{transformer_key}' on {cols_to_apply_transform}. Replacing with 0.\", level=\"warning\")\n",
        "                X_new_subset.replace([np.inf, -np.inf], 0, inplace=True)\n",
        "\n",
        "            # Special handling for Box-Cox (requires positive values)\n",
        "            if isinstance(transformer_instance, PowerTransformer) and transformer_instance.method == 'box-cox':\n",
        "                if np.any(X_new_subset.values <= 0):\n",
        "                    self._log(f\"Clipping non-positive values to 1e-6 for Box-Cox transform on {cols_to_apply_transform} in transform().\", level=\"warning\")\n",
        "                    X_new_subset = X_new_subset.clip(lower=1e-6)\n",
        "            try:\n",
        "                transformed_values_new = transformer_instance.transform(X_new_subset)\n",
        "                _df[cols_to_apply_transform] = transformed_values_new\n",
        "                processed_in_transform.update(cols_to_apply_transform)\n",
        "            except Exception as e:\n",
        "                self._log(f\"Error applying fitted transform '{transformer_key}' to {cols_to_apply_transform}: {e}. Data for these columns might remain untransformed.\", level=\"error\", exc_info=True)\n",
        "\n",
        "        # Handle direct log1p if it wasn't a sklearn transformer (based on metadata from fit_transform)\n",
        "        log1p_cols_from_meta = [col for col, meta in self.feature_type_metadata.items()\n",
        "                                if meta == 'numeric_transformed_log1p' and col in _df.columns and col not in processed_in_transform]\n",
        "        if log1p_cols_from_meta:\n",
        "            self._log(f\"Applying direct log1p transform (as done in fit) to: {log1p_cols_from_meta}\", level=\"debug\")\n",
        "            for col in log1p_cols_from_meta:\n",
        "                col_data = _df[col].copy()\n",
        "                if np.any(col_data.values < 0): self._log(f\"log1p (transform) encountered negative values in {col}. NaNs will be cleaned.\", level=\"warning\")\n",
        "                _df[col] = np.log1p(col_data)\n",
        "                _df[col] = np.nan_to_num(_df[col], nan=0.0, posinf=0.0, neginf=0.0) # Clean after log1p\n",
        "                processed_in_transform.update([col])\n",
        "\n",
        "\n",
        "        # Apply fitted scaler to all columns that were scaled during fit_transform\n",
        "        if self.scaler and self.scaled_features_list:\n",
        "            cols_to_scale_new = [f for f in self.scaled_features_list if f in _df.columns] # Only scale if present\n",
        "            if cols_to_scale_new:\n",
        "                X_to_scale_new = _df[cols_to_scale_new].copy() # Operate on copy\n",
        "\n",
        "                # Impute NaNs using fitted means from the scaler if available, else median of current subset\n",
        "                if X_to_scale_new.isnull().any().any():\n",
        "                    cols_with_nan_transform = X_to_scale_new.columns[X_to_scale_new.isnull().any()].tolist()\n",
        "                    self._log(f\"NaNs detected before scaling new data: {cols_with_nan_transform}. Imputing...\", level=\"warning\")\n",
        "                    for i, col_name in enumerate(cols_to_scale_new):\n",
        "                        if X_to_scale_new[col_name].isnull().any():\n",
        "                            impute_val = 0.0 # Default imputation\n",
        "                            try: # Try to find original index for this column as per fitting order\n",
        "                                original_training_idx = self.scaled_features_list.index(col_name)\n",
        "                                if hasattr(self.scaler, 'mean_') and self.scaler.mean_ is not None and original_training_idx < len(self.scaler.mean_):\n",
        "                                    impute_val = self.scaler.mean_[original_training_idx]\n",
        "                                else: # Fallback to current median if mean_ not suitable or col not found\n",
        "                                    impute_val = X_to_scale_new[col_name].median()\n",
        "                            except (ValueError, IndexError):\n",
        "                                impute_val = X_to_scale_new[col_name].median() # Fallback\n",
        "\n",
        "                            X_to_scale_new[col_name].fillna(impute_val if not pd.isna(impute_val) else 0.0, inplace=True)\n",
        "\n",
        "                # Replace Infs after imputation\n",
        "                if not np.all(np.isfinite(X_to_scale_new.values)):\n",
        "                    self._log(\"Non-finite values (Inf) detected before scaling (transform) after NaN imputation. Replacing with 0.\", level=\"error\")\n",
        "                    X_to_scale_new.replace([np.inf, -np.inf], 0, inplace=True)\n",
        "                try:\n",
        "                    scaled_values_new = self.scaler.transform(X_to_scale_new)\n",
        "                    if np.isnan(scaled_values_new).any(): # Handle NaNs from scaling (e.g. constant cols in new data)\n",
        "                        self._log(\"NaNs generated by scaler.transform (likely constant cols in new data or during fit). Replacing with 0.\", level=\"warning\")\n",
        "                        scaled_values_new = np.nan_to_num(scaled_values_new, nan=0.0)\n",
        "                    _df[cols_to_scale_new] = scaled_values_new\n",
        "                    self._log(f\"Applied fitted scaler to {len(cols_to_scale_new)} features.\", level=\"info\")\n",
        "                except Exception as e:\n",
        "                    self._log(f\"Error applying fitted scaler: {e}. Data for these columns might remain unscaled.\", level=\"error\", exc_info=True)\n",
        "            else: self._log(\"No features designated for scaling found in the new data.\", level=\"debug\")\n",
        "        else: self._log(\"No scaler fitted or no features were designated for scaling during fit. Skipping scaling in transform.\", level=\"debug\")\n",
        "\n",
        "        self.logger.info(f\"--- Preprocessing Pipeline (transform) Completed. Final shape: {_df.shape} ---\")\n",
        "        return _df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lUnkDw6ax8UG"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.utils import resample\n",
        "from sklearn.neighbors import KernelDensity\n",
        "from typing import Tuple, Optional, Dict, List, Union\n",
        "import logging\n",
        "\n",
        "\n",
        "logger = get_logger(__name__) # Module-level logger\n",
        "\n",
        "class SamplingUtils:\n",
        "    \"\"\"\n",
        "    Provides utility functions for data sampling, including downsampling,\n",
        "    upsampling with optional KDE synthesis, and stratified sampling.\n",
        "    Includes functionality to return sample weights after rebalancing.\n",
        "    \"\"\"\n",
        "    def __init__(self, random_state: Optional[int] = None, verbose: bool = True):\n",
        "        self.random_state = random_state\n",
        "        self.rng = np.random.RandomState(random_state) # Compatible with older numpy\n",
        "        self.verbose = verbose\n",
        "        # Instance logger, configured by the verbose flag\n",
        "        self.logger = get_logger(f\"{self.__class__.__name__}\", verbose=self.verbose)\n",
        "\n",
        "\n",
        "    def _log(self, message: str, level: str = \"info\"):\n",
        "        \"\"\"Helper for conditional logging using the instance logger.\"\"\"\n",
        "        log_func = getattr(self.logger, level, self.logger.info)\n",
        "        log_func(message)\n",
        "\n",
        "\n",
        "    def downsample(self,\n",
        "                   df: pd.DataFrame,\n",
        "                   n_samples: int,\n",
        "                   stratify_col: Optional[str] = None,\n",
        "                   replace: bool = False) -> Tuple[pd.DataFrame, np.ndarray]:\n",
        "        \"\"\"\n",
        "        Downsamples a DataFrame, with optional stratification.\n",
        "\n",
        "        Args:\n",
        "            df: Input DataFrame.\n",
        "            n_samples: Desired number of samples.\n",
        "            stratify_col: Column for stratified sampling.\n",
        "            replace: Sample with replacement (False for standard downsampling).\n",
        "\n",
        "        Returns:\n",
        "            Tuple: (downsampled DataFrame, indices of sampled rows from original df).\n",
        "        \"\"\"\n",
        "        n_total = len(df)\n",
        "        if n_samples >= n_total and not replace:\n",
        "            self._log(f\"Requested n_samples ({n_samples}) >= total ({n_total}) and replace=False. Returning original.\")\n",
        "            return df.copy(), df.index.values # Return original indices\n",
        "\n",
        "        if n_samples <= 0:\n",
        "            self._log(\"n_samples <= 0. Returning empty DataFrame.\", level=\"warning\")\n",
        "            return pd.DataFrame(columns=df.columns), np.array([])\n",
        "\n",
        "        sampled_indices: np.ndarray = np.array([], dtype=df.index.dtype) # Match index type\n",
        "\n",
        "        if stratify_col and stratify_col in df.columns and df[stratify_col].nunique() > 1:\n",
        "            self._log(f\"Stratified downsampling by '{stratify_col}' to {n_samples} samples.\")\n",
        "            try:\n",
        "                sampled_indices_list = []\n",
        "                stratum_counts = df[stratify_col].value_counts()\n",
        "                target_props = stratum_counts / n_total\n",
        "                target_samples_float = target_props * n_samples\n",
        "                target_samples_int = np.floor(target_samples_float).astype(int)\n",
        "                remainder = n_samples - target_samples_int.sum()\n",
        "\n",
        "                # Distribute remainder based on fractional parts\n",
        "                fractional_parts = target_samples_float - target_samples_int\n",
        "                # Get indices of strata to add remainder to (those with largest fractional parts)\n",
        "                # These are indices *within* the `target_samples_int` Series, corresponding to stratum labels\n",
        "                add_indices_order = fractional_parts.index[np.argsort(-fractional_parts.values)]\n",
        "\n",
        "                for i in range(remainder):\n",
        "                    stratum_to_increment = add_indices_order[i % len(add_indices_order)] # Cycle if remainder > num_strata\n",
        "                    target_samples_int[stratum_to_increment] += 1\n",
        "\n",
        "                total_sampled_count = 0\n",
        "                for stratum_val, target_stratum_samples in target_samples_int.items():\n",
        "                    group = df[df[stratify_col] == stratum_val]\n",
        "                    stratum_n_total = len(group)\n",
        "                    stratum_n_to_sample = min(target_stratum_samples, stratum_n_total)\n",
        "                    total_sampled_count += stratum_n_to_sample\n",
        "                    if stratum_n_to_sample > 0:\n",
        "                        stratum_sampled_orig_indices = self.rng.choice(group.index.values, size=stratum_n_to_sample, replace=replace)\n",
        "                        sampled_indices_list.extend(stratum_sampled_orig_indices)\n",
        "                    elif target_stratum_samples > 0:\n",
        "                         self._log(f\"Stratum '{stratum_val}' has {stratum_n_total}, needed {target_stratum_samples}, sampling {stratum_n_to_sample}.\", level=\"debug\")\n",
        "\n",
        "                sampled_indices = np.array(list(set(sampled_indices_list)), dtype=df.index.dtype)\n",
        "\n",
        "                if len(sampled_indices) != n_samples:\n",
        "                    self._log(f\"Stratified sampling resulted in {len(sampled_indices)} unique samples (target {n_samples}). Adjusting randomly.\", level=\"warning\")\n",
        "                    if len(sampled_indices) > n_samples:\n",
        "                        sampled_indices = self.rng.choice(sampled_indices, size=n_samples, replace=False)\n",
        "                    elif len(sampled_indices) < n_samples:\n",
        "                        n_needed_more = n_samples - len(sampled_indices)\n",
        "                        remaining_indices = np.setdiff1d(df.index.values, sampled_indices)\n",
        "                        if len(remaining_indices) >= n_needed_more:\n",
        "                            extra_indices = self.rng.choice(remaining_indices, size=n_needed_more, replace=False)\n",
        "                            sampled_indices = np.concatenate([sampled_indices, extra_indices])\n",
        "                        else: # Not enough remaining, take all that are left\n",
        "                            sampled_indices = np.concatenate([sampled_indices, remaining_indices])\n",
        "                            self._log(f\"Could only add {len(remaining_indices)} more samples. Final count: {len(sampled_indices)}.\", level=\"warning\")\n",
        "            except Exception as e:\n",
        "                self._log(f\"Stratified sampling failed: {e}. Falling back to random.\", level=\"error\")\n",
        "                sampled_indices = self.rng.choice(df.index.values, size=n_samples, replace=replace)\n",
        "        else:\n",
        "            if stratify_col: self._log(f\"Stratify col '{stratify_col}' invalid or has only one unique value. Using random downsampling.\")\n",
        "            self._log(f\"Random downsampling to {n_samples} samples.\")\n",
        "            sampled_indices = self.rng.choice(df.index.values, size=n_samples, replace=replace)\n",
        "\n",
        "        if replace and len(np.unique(sampled_indices)) < len(sampled_indices):\n",
        "             self._log(\"Sampling with replacement generated duplicates. Taking unique.\", level=\"debug\")\n",
        "             sampled_indices = np.unique(sampled_indices)\n",
        "             if len(sampled_indices) < n_samples: self._log(f\"Unique samples ({len(sampled_indices)}) < target after unique().\", level=\"warning\")\n",
        "\n",
        "        if len(sampled_indices) > n_samples: # Ensure correct final size if unique() reduced count or other adjustments\n",
        "             sampled_indices = sampled_indices[:n_samples]\n",
        "        elif len(sampled_indices) < n_samples and not replace: # Try to fill if possible (only if not sampling with replacement)\n",
        "            self._log(f\"Final sample count {len(sampled_indices)} is less than target {n_samples} after downsampling. This can happen if unique strata are very small.\", level=\"warning\")\n",
        "\n",
        "\n",
        "        df_sampled = df.loc[sampled_indices].copy()\n",
        "        self._log(f\"Downsampled from {n_total} to {len(df_sampled)} samples.\")\n",
        "        return df_sampled, sampled_indices\n",
        "\n",
        "\n",
        "    def rebalance_price_bands(self,\n",
        "                              df: pd.DataFrame,\n",
        "                              price_band_col: str = 'price_band',\n",
        "                              log_price_col: Optional[str] = None, # Column with log-transformed prices (e.g., log_sale_price)\n",
        "                              price_col: Optional[str] = None, # Column with original scale prices (e.g., sale_price)\n",
        "                              target_balance: Union[str, Dict[str, int]] = 'median',\n",
        "                              upsample_method: str = 'kde', # 'kde' or 'bootstrap'\n",
        "                              downsample_overrepresented: bool = True,\n",
        "                              return_weights: bool = True\n",
        "                             ) -> Union[pd.DataFrame, Tuple[pd.DataFrame, pd.Series]]:\n",
        "        \"\"\"\n",
        "        Rebalances DataFrame based on price bands (specified in `price_band_col`),\n",
        "        optionally returning sample weights.\n",
        "        Weights are calculated for upsampled bands. Weight = 1.0 otherwise.\n",
        "\n",
        "        The log_price_col (e.g., 'log_sale_price') and price_col (e.g., 'sale_price')\n",
        "        are used specifically for KDE upsampling if that method is chosen.\n",
        "        \"\"\"\n",
        "        # Ensure the input DataFrame has a simple numeric index for internal operations\n",
        "        # This helps prevent TypeErrors if the original df had a non-numeric index\n",
        "        # and any internal logic (even if not currently present) were to rely on numeric index properties.\n",
        "        df_original_indexed = df.copy() # Keep a copy with original index if needed later (not currently used)\n",
        "        df = df.reset_index(drop=True)\n",
        "\n",
        "        if price_band_col not in df.columns:\n",
        "            self.logger.error(f\"Price band column '{price_band_col}' not found in DataFrame.\")\n",
        "            return df_original_indexed if not return_weights else (df_original_indexed, pd.Series(1.0, index=df_original_indexed.index))\n",
        "\n",
        "        log_price_col_for_kde = log_price_col\n",
        "        price_col_for_kde = price_col\n",
        "\n",
        "        if upsample_method == 'kde':\n",
        "            if not log_price_col_for_kde or not price_col_for_kde:\n",
        "                self.logger.warning(f\"KDE upsampling requires both 'log_price_col' (e.g., 'log_sale_price') and 'price_col' (e.g., 'sale_price') to be specified. \"\n",
        "                                   f\"Provided: log_price_col='{log_price_col_for_kde}', price_col='{price_col_for_kde}'. Defaulting to 'bootstrap'.\")\n",
        "                upsample_method = 'bootstrap'\n",
        "            elif log_price_col_for_kde not in df.columns or price_col_for_kde not in df.columns:\n",
        "                self.logger.warning(f\"KDE upsampling requires columns '{log_price_col_for_kde}' and '{price_col_for_kde}' to be present in the DataFrame. \"\n",
        "                                   f\"Missing: \"\n",
        "                                   f\"{[c for c in [log_price_col_for_kde, price_col_for_kde] if c not in df.columns]}. \"\n",
        "                                   f\"Defaulting to 'bootstrap'.\")\n",
        "                upsample_method = 'bootstrap'\n",
        "\n",
        "        self._log(f\"--- Rebalancing by '{price_band_col}' (Upsample method: '{upsample_method}', \"\n",
        "                  f\"Log price col for KDE: '{log_price_col_for_kde if upsample_method == 'kde' else 'N/A'}', \"\n",
        "                  f\"Original price col for KDE: '{price_col_for_kde if upsample_method == 'kde' else 'N/A'}', \"\n",
        "                  f\"Downsample overrepresented: {downsample_overrepresented}, Return weights: {return_weights}) ---\", level=\"info\")\n",
        "\n",
        "        counts = df[price_band_col].value_counts().to_dict()\n",
        "        self._log(f\"Original counts for bands in '{price_band_col}': {counts}\", level=\"info\")\n",
        "\n",
        "        final_target_counts: Dict[Union[str, int], int] = {} # Allow for numeric band labels\n",
        "        target_val = 0\n",
        "        if isinstance(target_balance, dict):\n",
        "            final_target_counts = target_balance.copy()\n",
        "            for band in counts:\n",
        "                if band not in final_target_counts: final_target_counts[band] = counts[band]\n",
        "        elif isinstance(target_balance, str):\n",
        "            valid_counts = [c for c in counts.values() if c > 0]\n",
        "            if not valid_counts:\n",
        "                self.logger.error(f\"No bands with samples found in column '{price_band_col}'. Cannot determine target value for rebalancing.\");\n",
        "                return df_original_indexed if not return_weights else (df_original_indexed, pd.Series(1.0, index=df_original_indexed.index))\n",
        "\n",
        "            if target_balance == 'median': target_val = int(np.median(valid_counts))\n",
        "            elif target_balance == 'mean': target_val = int(np.mean(valid_counts))\n",
        "            elif target_balance == 'max': target_val = int(np.max(valid_counts))\n",
        "            else:\n",
        "                self.logger.error(f\"Invalid target_balance string: '{target_balance}'. Using median.\");\n",
        "                target_val = int(np.median(valid_counts))\n",
        "            target_val = max(1, target_val) # Ensure target is at least 1\n",
        "            self._log(f\"Base target value for each band: {target_val} (derived from '{target_balance}' of counts: {valid_counts})\", level=\"info\")\n",
        "            for band in counts: final_target_counts[band] = target_val\n",
        "        else:\n",
        "            self.logger.error(f\"Invalid target_balance type: {type(target_balance)}. Must be 'str' or 'dict'.\");\n",
        "            return df_original_indexed if not return_weights else (df_original_indexed, pd.Series(1.0, index=df_original_indexed.index))\n",
        "\n",
        "        self._log(f\"Effective target counts for '{price_band_col}': {final_target_counts}\", level=\"info\")\n",
        "\n",
        "        resampled_dfs: List[pd.DataFrame] = []\n",
        "\n",
        "        for band_label, current_count in counts.items():\n",
        "            target_count_for_band = final_target_counts.get(band_label, current_count) # Default to current if somehow missing\n",
        "            band_df = df[df[price_band_col] == band_label].copy() # Operate on copy with simple index\n",
        "\n",
        "            if current_count == 0:\n",
        "                self._log(f\"Band '{band_label}' in '{price_band_col}' is empty. Skipping.\", level=\"debug\");\n",
        "                continue\n",
        "\n",
        "            if current_count < target_count_for_band: # Upsample\n",
        "                n_needed = target_count_for_band - current_count\n",
        "                self._log(f\"Upsampling band '{band_label}' from {current_count} to {target_count_for_band} (+{n_needed}) using '{upsample_method}'. \"\n",
        "                          f\"(Log price col: '{log_price_col_for_kde}', Orig price col: '{price_col_for_kde}')\", level=\"info\")\n",
        "                resampled_dfs.append(band_df.copy()) # Add original samples first\n",
        "\n",
        "                new_samples_df = pd.DataFrame()\n",
        "                current_band_upsample_method = upsample_method # Allow fallback per band\n",
        "\n",
        "                if current_band_upsample_method == 'kde' and current_count >= 5 and log_price_col_for_kde and price_col_for_kde:\n",
        "                    try:\n",
        "                        kde_prices = band_df[log_price_col_for_kde].dropna().values.reshape(-1, 1)\n",
        "                        if len(kde_prices) < 2: raise ValueError(f\"Not enough finite log prices in '{log_price_col_for_kde}' for KDE bandwidth (found {len(kde_prices)}).\")\n",
        "\n",
        "                        # Scott's rule for bandwidth, ensure it's positive\n",
        "                        bandwidth = 1.06 * np.std(kde_prices) * (len(kde_prices) ** (-1./5))\n",
        "                        bandwidth = max(bandwidth, 1e-4) # Ensure positive bandwidth\n",
        "\n",
        "                        kde = KernelDensity(kernel='gaussian', bandwidth=bandwidth).fit(kde_prices)\n",
        "                        new_log_prices_synthetic = kde.sample(n_needed, random_state=self.rng).flatten()\n",
        "                        new_actual_prices_synthetic = np.expm1(new_log_prices_synthetic) # Transform back from log\n",
        "\n",
        "                        # Bootstrap other features based on original samples in the band\n",
        "                        # Original indices of band_df are 0 to len(band_df)-1 due to earlier reset_index\n",
        "                        new_sample_indices_for_bootstrap = self.rng.choice(band_df.index, size=n_needed, replace=True)\n",
        "                        new_samples_df = band_df.loc[new_sample_indices_for_bootstrap].copy()\n",
        "\n",
        "                        new_samples_df[log_price_col_for_kde] = new_log_prices_synthetic\n",
        "                        new_samples_df[price_col_for_kde] = new_actual_prices_synthetic\n",
        "                        new_samples_df[price_band_col] = band_label # Ensure band label is correct\n",
        "                        new_samples_df.reset_index(drop=True, inplace=True) # Ensure 0-based index for new samples\n",
        "\n",
        "                    except Exception as e:\n",
        "                        self.logger.warning(f\"KDE upsampling failed for band '{band_label}' (using log_price_col='{log_price_col_for_kde}', price_col='{price_col_for_kde}'): {e}. Falling back to bootstrap.\", exc_info=False)\n",
        "                        current_band_upsample_method = 'bootstrap'\n",
        "                        new_samples_df = pd.DataFrame() # Reset df if KDE failed partway\n",
        "\n",
        "                if current_band_upsample_method == 'bootstrap':\n",
        "                    if upsample_method == 'kde': # Log if this is a fallback\n",
        "                         self._log(f\"Using bootstrap for band '{band_label}' due to KDE failure or low count ({current_count} < 5).\", level=\"debug\")\n",
        "                    # Resample from the original band_df (which has original data for this band)\n",
        "                    new_samples_df = resample(band_df, replace=True, n_samples=n_needed, random_state=self.rng)\n",
        "                    new_samples_df.reset_index(drop=True, inplace=True) # Ensure 0-based index\n",
        "\n",
        "                if not new_samples_df.empty:\n",
        "                    resampled_dfs.append(new_samples_df)\n",
        "                elif n_needed > 0 : # Only warn if we actually needed samples\n",
        "                    self.logger.warning(f\"Upsampling for band '{band_label}' resulted in an empty DataFrame, though {n_needed} samples were needed.\")\n",
        "\n",
        "            elif current_count > target_count_for_band and downsample_overrepresented: # Downsample\n",
        "                self._log(f\"Downsampling band '{band_label}' from {current_count} to {target_count_for_band}.\", level=\"info\")\n",
        "                # Use internal downsample method. band_df already has a simple 0-based index here.\n",
        "                downsampled_band_df, _ = self.downsample(band_df, n_samples=target_count_for_band, replace=False)\n",
        "                resampled_dfs.append(downsampled_band_df)\n",
        "            else: # Keep as is (count matches target, or downsampling disabled)\n",
        "                self._log(f\"Keeping band '{band_label}' as is (count: {current_count}, target: {target_count_for_band}).\", level=\"debug\")\n",
        "                resampled_dfs.append(band_df.copy())\n",
        "\n",
        "        if not resampled_dfs:\n",
        "            self.logger.warning(\"Rebalancing resulted in no data. Returning original DataFrame (with original index).\")\n",
        "            return df_original_indexed if not return_weights else (df_original_indexed, pd.Series(1.0, index=df_original_indexed.index))\n",
        "\n",
        "        # Concatenate results - this creates a new default integer index (0 to N-1)\n",
        "        df_rebalanced = pd.concat(resampled_dfs, ignore_index=True)\n",
        "\n",
        "        final_weights_list = []\n",
        "        current_pos = 0\n",
        "        for temp_df_part in resampled_dfs: # Iterate through the same list of DataFrames used for concat\n",
        "            num_samples_in_part = len(temp_df_part)\n",
        "            if num_samples_in_part == 0: continue\n",
        "\n",
        "            # Infer band label from the first row of this part (all rows in temp_df_part should have same band)\n",
        "            # Use .mode()[0] for safety with categorical types or potential mixed types if data is messy.\n",
        "            band_label_for_part = temp_df_part[price_band_col].mode()[0]\n",
        "            original_count_for_part = counts.get(band_label_for_part, 0)\n",
        "            target_count_for_part = final_target_counts.get(band_label_for_part, original_count_for_part)\n",
        "\n",
        "            weight_for_part = 1.0\n",
        "            if original_count_for_part > 0 and original_count_for_part < target_count_for_part : # Was upsampled\n",
        "                weight_for_part = float(original_count_for_part) / target_count_for_part\n",
        "\n",
        "            final_weights_list.extend([weight_for_part] * num_samples_in_part)\n",
        "            current_pos += num_samples_in_part\n",
        "\n",
        "        weights_series = pd.Series(final_weights_list, index=df_rebalanced.index, name='sample_weight')\n",
        "\n",
        "        self._log(f\"Rebalanced counts in '{price_band_col}': {df_rebalanced[price_band_col].value_counts().to_dict()}\", level=\"info\")\n",
        "        self._log(f\"Total samples after rebalancing: {len(df_rebalanced)}\", level=\"info\")\n",
        "\n",
        "        if return_weights:\n",
        "            if len(weights_series) != len(df_rebalanced):\n",
        "                self.logger.error(f\"Weight series length ({len(weights_series)}) mismatch with rebalanced df ({len(df_rebalanced)}). \"\n",
        "                                  \"This indicates an issue in weight calculation logic. Returning default weights of 1.0.\")\n",
        "                weights_series = pd.Series(1.0, index=df_rebalanced.index, name='sample_weight')\n",
        "            else:\n",
        "                 self._log(f\"Sample weights generated for rebalanced data (Min: {weights_series.min():.3f}, Max: {weights_series.max():.3f}, Mean: {weights_series.mean():.3f}).\", level=\"info\")\n",
        "            return df_rebalanced, weights_series\n",
        "        else:\n",
        "            return df_rebalanced"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kVriLX2PyIMI"
      },
      "outputs": [],
      "source": [
        "# File: dimensionality_reduction.py\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import warnings\n",
        "from typing import Tuple, Optional, Dict, Any, NamedTuple, List, Union\n",
        "import logging\n",
        "\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.decomposition import FastICA\n",
        "from sklearn.neighbors import NearestNeighbors\n",
        "\n",
        "\n",
        "# --- Dependency Availability Check Placeholders ---\n",
        "# Assume these might be defined in a previous cell. If not, define them here.\n",
        "if 'FAISS_AVAILABLE' not in globals():\n",
        "    FAISS_AVAILABLE = False\n",
        "    try:\n",
        "        import faiss\n",
        "        FAISS_AVAILABLE = True\n",
        "    except ImportError:\n",
        "        pass\n",
        "\n",
        "if 'SKDIM_AVAILABLE' not in globals():\n",
        "    SKDIM_AVAILABLE = False\n",
        "    try:\n",
        "        from skdim.id import TwoNN # type: ignore\n",
        "        SKDIM_AVAILABLE = True\n",
        "    except ImportError:\n",
        "        pass\n",
        "\n",
        "if 'PICARD_AVAILABLE' not in globals():\n",
        "    PICARD_AVAILABLE = False\n",
        "    try:\n",
        "        from picard import picard # type: ignore\n",
        "        PICARD_AVAILABLE = True\n",
        "    except ImportError:\n",
        "        pass\n",
        "\n",
        "\n",
        "logger = get_logger(__name__) # Module-level logger if needed outside class\n",
        "\n",
        "\n",
        "class DimensionalityReducer:\n",
        "    \"\"\"\n",
        "    Handles intrinsic dimension estimation and Independent Component Analysis (ICA).\n",
        "    Uses n_jobs for compatible operations like k-NN.\n",
        "    \"\"\"\n",
        "    def __init__(self,\n",
        "                 random_state: Optional[int] = None,\n",
        "                 verbose: bool = True,\n",
        "                 n_jobs: int = -1, # For parallelizable operations (e.g., kNN)\n",
        "                 use_advanced_methods: bool = True, # Use FAISS, skdim, Picard if available\n",
        "                 prefer_gpu_for_faiss: bool = True, # Attempt GPU for FAISS kNN\n",
        "                 max_ica_iter: int = 3000):\n",
        "        self.random_state = random_state\n",
        "        self.rng = np.random.default_rng(random_state)\n",
        "        self.verbose = verbose\n",
        "        self.n_jobs = n_jobs # Store n_jobs for use where applicable\n",
        "        self.use_advanced_methods = use_advanced_methods\n",
        "        self.prefer_gpu_for_faiss = prefer_gpu_for_faiss\n",
        "        self.max_ica_iter = max_ica_iter\n",
        "        self.logger = get_logger(self.__class__.__name__, verbose=self.verbose)\n",
        "\n",
        "    def _log(self, message: str, level: str = \"info\", exc_info: bool = False):\n",
        "        \"\"\"Helper for conditional logging via the instance's logger.\"\"\"\n",
        "        # Renamed from logger to self.logger to avoid conflict with module logger\n",
        "        if self.verbose:\n",
        "            if level == \"info\":    self.logger.info(message)\n",
        "            elif level == \"warning\": self.logger.warning(message)\n",
        "            elif level == \"error\":   self.logger.error(message, exc_info=exc_info)\n",
        "            else:                    self.logger.debug(message)\n",
        "\n",
        "    def _preprocess_X(self, X: Union[pd.DataFrame, np.ndarray]) -> np.ndarray:\n",
        "        \"\"\"Converts input to numpy array, cleans NaN/Inf, and scales.\"\"\"\n",
        "        if isinstance(X, pd.DataFrame):\n",
        "            X_numeric = X.select_dtypes(include=np.number)\n",
        "            if X_numeric.shape[1] < X.shape[1]:\n",
        "                self._log(f\"Non-numeric columns found. Using only {X_numeric.shape[1]} numeric columns for DR.\", level=\"warning\")\n",
        "            if X_numeric.empty: raise ValueError(\"No numeric data for DR.\")\n",
        "            X_np = X_numeric.values.astype(np.float64)\n",
        "        elif isinstance(X, np.ndarray):\n",
        "            if not np.issubdtype(X.dtype, np.number): raise ValueError(\"Input NumPy array must be numeric.\")\n",
        "            X_np = X.astype(np.float64)\n",
        "        else: raise TypeError(\"Input X must be pandas DataFrame or NumPy array.\")\n",
        "\n",
        "        if not np.all(np.isfinite(X_np)):\n",
        "            self._log(\"NaN/Inf values detected before DR scaling. Imputing with median.\", level=\"warning\")\n",
        "            for j in range(X_np.shape[1]):\n",
        "                col_data = X_np[:, j]\n",
        "                if np.any(~np.isfinite(col_data)):\n",
        "                    finite_vals = col_data[np.isfinite(col_data)]\n",
        "                    fill_val = np.median(finite_vals) if len(finite_vals) > 0 else 0.0\n",
        "                    # Use nan_to_num for comprehensive cleaning\n",
        "                    X_np[:, j] = np.nan_to_num(col_data, nan=fill_val, posinf=fill_val, neginf=fill_val) # Fill infs also\n",
        "\n",
        "        scaler = StandardScaler()\n",
        "        X_scaled = scaler.fit_transform(X_np)\n",
        "\n",
        "        if np.any(~np.isfinite(X_scaled)):\n",
        "            self._log(\"NaNs/Infs present AFTER scaling in _preprocess_X. Likely constant columns. Imputing with 0.\", level=\"error\")\n",
        "            X_scaled = np.nan_to_num(X_scaled, nan=0.0, posinf=0.0, neginf=0.0) # Impute post-scaling as fallback\n",
        "\n",
        "        return X_scaled\n",
        "\n",
        "    def estimate_intrinsic_dimension_twonn(self, X_processed: np.ndarray) -> Optional[Dict[str, Any]]:\n",
        "        \"\"\"Estimates intrinsic dimension using TwoNN if available.\"\"\"\n",
        "        if SKDIM_AVAILABLE and self.use_advanced_methods:\n",
        "            self._log(\"Attempting TwoNN for intrinsic dimension estimation.\", level=\"info\")\n",
        "            if not np.all(np.isfinite(X_processed)):\n",
        "                self._log(\"Input data for TwoNN contains NaN/Inf. Cannot proceed.\", level=\"error\")\n",
        "                return None\n",
        "            try:\n",
        "                # Ensure TwoNN is imported if SKDIM_AVAILABLE is True\n",
        "                from skdim.id import TwoNN # type: ignore\n",
        "                estimator = TwoNN().fit(X_processed) # type: ignore\n",
        "                dim_estimate = estimator.dimension_\n",
        "                if not np.isfinite(dim_estimate) or dim_estimate <= 0:\n",
        "                    self._log(f\"TwoNN returned invalid dimension: {dim_estimate}. Fallback.\", level=\"warning\")\n",
        "                    return None\n",
        "\n",
        "                median_dim = int(round(max(1, dim_estimate)))\n",
        "                # Use a simple heuristic for std/quantiles based on median\n",
        "                std_dim = max(1.0, median_dim * 0.15) # Arbitrary std deviation guess\n",
        "                q25 = max(1, int(round(median_dim - std_dim)))\n",
        "                q75 = int(round(median_dim + std_dim))\n",
        "\n",
        "                result = {\n",
        "                    'median': median_dim, 'mean': float(dim_estimate), 'std': std_dim,\n",
        "                    'q25': q25, 'q75': q75,\n",
        "                    'raw_estimates': np.array([dim_estimate]), 'method': 'TwoNN'\n",
        "                }\n",
        "                self._log(f\"TwoNN Intrinsic Dimension: Median={median_dim} (Mean={dim_estimate:.2f}, Est.Std={std_dim:.2f})\", level=\"info\")\n",
        "                return result\n",
        "            except Exception as e:\n",
        "                self._log(f\"TwoNN failed: {e}. Check input data (NaNs?) or library compatibility.\", level=\"warning\", exc_info=True)\n",
        "        else:\n",
        "            self._log(\"TwoNN not available or not used.\", level=\"debug\")\n",
        "        return None\n",
        "\n",
        "    def estimate_intrinsic_dimension_levina_bickel(self, X_processed: np.ndarray, k_neighbors: int = 20) -> Dict[str, Any]:\n",
        "        \"\"\"Estimates intrinsic dimension using Levina-Bickel MLE.\"\"\"\n",
        "        self._log(f\"Using Levina-Bickel MLE (k_neighbors={k_neighbors}).\", level=\"info\")\n",
        "        n_samples, n_features = X_processed.shape\n",
        "\n",
        "        actual_k = min(k_neighbors, n_samples - 1)\n",
        "        if actual_k <= 1:\n",
        "            self._log(f\"Not enough samples/k for Levina-Bickel (need k>1, have {actual_k}). Defaulting dim=1.\", level=\"warning\")\n",
        "            return {'median': 1, 'mean': 1.0, 'std': 0.0, 'q25': 1, 'q75': 1, 'raw_estimates': np.array([1.0]), 'method': 'Levina-Bickel (Fallback)'}\n",
        "\n",
        "        if not np.all(np.isfinite(X_processed)):\n",
        "            self._log(\"Input data for Levina-Bickel kNN contains NaN/Inf. Cannot compute kNN.\", level=\"error\")\n",
        "            return {'median': 1, 'mean': 1.0, 'std': 0.0, 'q25': 1, 'q75': 1, 'raw_estimates': np.array([1.0]), 'method': 'Levina-Bickel (Error)'}\n",
        "\n",
        "        dists: Optional[np.ndarray] = None\n",
        "        if FAISS_AVAILABLE and self.use_advanced_methods and n_samples > 1000 :\n",
        "            self._log(\"Using FAISS for k-NN in Levina-Bickel.\", level=\"info\")\n",
        "            try:\n",
        "                import faiss # Import here again just in case\n",
        "                X_32 = X_processed.astype('float32')\n",
        "                d = X_32.shape[1]\n",
        "                index_cpu = faiss.IndexFlatL2(d)\n",
        "                index_to_use = index_cpu\n",
        "                gpu_res = None\n",
        "\n",
        "                if self.prefer_gpu_for_faiss and hasattr(faiss, 'StandardGpuResources'):\n",
        "                    try:\n",
        "                        gpu_res = faiss.StandardGpuResources()\n",
        "                        index_gpu = faiss.index_cpu_to_gpu(gpu_res, 0, index_cpu)\n",
        "                        index_to_use = index_gpu\n",
        "                        self._log(\"FAISS using GPU for k-NN.\", level=\"info\")\n",
        "                    except Exception as gpu_e:\n",
        "                        self._log(f\"FAISS GPU failed: {gpu_e}. Using CPU.\", level=\"warning\")\n",
        "                        # index_to_use remains index_cpu\n",
        "\n",
        "                index_to_use.add(X_32)\n",
        "                dists, _ = index_to_use.search(X_32, actual_k + 1) # Include self\n",
        "\n",
        "                # Clean up GPU resources explicitly if they were created\n",
        "                if gpu_res is not None: del gpu_res; del index_to_use # Allow GC, index_gpu uses gpu_res\n",
        "\n",
        "            except Exception as faiss_e:\n",
        "                self._log(f\"FAISS k-NN failed: {faiss_e}. Falling back to sklearn.\", level=\"warning\", exc_info=True)\n",
        "                dists = None\n",
        "\n",
        "        if dists is None:\n",
        "            self._log(f\"Using sklearn NearestNeighbors for k-NN (n_jobs={self.n_jobs}).\", level=\"info\")\n",
        "            try:\n",
        "                nn = NearestNeighbors(n_neighbors=actual_k + 1, n_jobs=self.n_jobs, algorithm='auto').fit(X_processed)\n",
        "                dists, _ = nn.kneighbors(X_processed)\n",
        "            except Exception as nn_e:\n",
        "                self._log(f\"NearestNeighbors failed: {nn_e}\", level=\"error\", exc_info=True)\n",
        "                return {'median': 1, 'mean': 1.0, 'std': 0.0, 'q25': 1, 'q75': 1, 'raw_estimates': np.array([1.0]), 'method': 'Levina-Bickel (kNN Error)'}\n",
        "\n",
        "        # Ensure distances are finite (should be if input was finite, but safety check)\n",
        "        if not np.all(np.isfinite(dists)):\n",
        "             self._log(\"Non-finite distances found AFTER kNN. Cleaning before Levina-Bickel calc.\", level=\"warning\")\n",
        "             dists = np.nan_to_num(dists, nan=np.inf) # Treat NaN dist as Inf\n",
        "\n",
        "        dists_k = dists[:, 1:actual_k+1] # Exclude self (0th neighbor)\n",
        "        dists_k = np.maximum(dists_k, 1e-12) # Avoid log(0)\n",
        "        T_k_i = dists_k[:, -1] # Distance to the actual_k th neighbor (k-th element, index k-1)\n",
        "\n",
        "        # Check for zero distances to k-th neighbor which cause issues\n",
        "        if np.any(T_k_i <= 1e-12):\n",
        "             zero_dist_indices = np.where(T_k_i <= 1e-12)[0]\n",
        "             self._log(f\"Found {len(zero_dist_indices)} points with zero/near-zero distance to k-th neighbor ({actual_k}). This implies duplicate points or insufficient k. Estimates for these points may be unstable.\", level=\"warning\")\n",
        "\n",
        "        log_ratios_sum = np.sum(np.log(T_k_i[:, np.newaxis] / dists_k[:, :-1]), axis=1) # Sum over j=1 to k-1\n",
        "\n",
        "        with np.errstate(divide='ignore', invalid='ignore'):\n",
        "            # Note: Levina-Bickel formula uses sum j=1 to k-1, so there are k-1 terms.\n",
        "            # Formula: d = ( (1 / n_samples) * sum_i [ (k-1) / sum_{j=1}^{k-1} log( T_k(i) / T_j(i) ) ] )^-1\n",
        "            # Or calculate per point: d_i = ( (k-1) / sum_log_ratios_i )\n",
        "            dim_estimates_i = (actual_k - 1) / log_ratios_sum\n",
        "            dim_estimates_i[~np.isfinite(dim_estimates_i)] = 1 # Handle inf/nan (e.g., if log_ratios_sum is 0)\n",
        "\n",
        "        dim_estimates_i = np.maximum(1, dim_estimates_i)\n",
        "        dim_estimates_i = np.minimum(dim_estimates_i, n_features) # Cap at original features\n",
        "\n",
        "        raw_estimates = dim_estimates_i[np.isfinite(dim_estimates_i)]\n",
        "        if len(raw_estimates) == 0:\n",
        "             self._log(\"No finite dimension estimates from Levina-Bickel. Returning default.\", level=\"error\")\n",
        "             raw_estimates = np.array([1.0])\n",
        "\n",
        "        median_dim = int(round(max(1, np.median(raw_estimates))))\n",
        "        mean_dim = float(np.mean(raw_estimates))\n",
        "        std_dim = float(np.std(raw_estimates))\n",
        "        q25, q75 = np.percentile(raw_estimates, [25, 75])\n",
        "        q25 = int(round(max(1, q25))); q75 = int(round(max(1, q75)))\n",
        "\n",
        "        result = {'median': median_dim, 'mean': mean_dim, 'std': std_dim, 'q25': q25, 'q75': q75,\n",
        "                  'raw_estimates': raw_estimates, 'method': 'Levina-Bickel'}\n",
        "        self._log(f\"Levina-Bickel ID: Median={median_dim} (Mean={mean_dim:.2f}, Std={std_dim:.2f}) from {len(raw_estimates)} estimates.\", level=\"info\")\n",
        "        return result\n",
        "\n",
        "    def estimate_intrinsic_dimension(self, X: Union[pd.DataFrame, np.ndarray],\n",
        "                                     methods: List[str] = ['twonn', 'levina'],\n",
        "                                     k_neighbors_lb: int = 20) -> Dict[str, Any]:\n",
        "        \"\"\"Estimates intrinsic dimension using specified methods, preferring first successful.\"\"\"\n",
        "        self._log(f\"\\n--- Estimating Intrinsic Dimension (Methods: {methods}) ---\", level=\"info\")\n",
        "        X_processed = self._preprocess_X(X) # Handles scaling and cleaning\n",
        "        n_samples, n_features = X_processed.shape\n",
        "\n",
        "        if n_samples < k_neighbors_lb + 1 or n_samples < 5 : # Check if enough samples for methods\n",
        "            self._log(f\"Low samples ({n_samples}). Defaulting dim=min(5, n_features).\", level=\"warning\")\n",
        "            default_dim = max(1, min(5, n_features))\n",
        "            return {'median': default_dim, 'mean': float(default_dim), 'std': 0.0, 'q25': default_dim, 'q75': default_dim,\n",
        "                    'raw_estimates': np.array([float(default_dim)]), 'method': 'Fallback (Low Samples)'}\n",
        "\n",
        "        result = None\n",
        "        for method in methods:\n",
        "            if method.lower() == 'twonn':\n",
        "                result = self.estimate_intrinsic_dimension_twonn(X_processed)\n",
        "            elif method.lower() == 'levina':\n",
        "                result = self.estimate_intrinsic_dimension_levina_bickel(X_processed, k_neighbors=k_neighbors_lb)\n",
        "\n",
        "            if result and result.get('median', 0) > 0:\n",
        "                self._log(f\"Using result from '{result.get('method', method)}' method.\", level=\"info\")\n",
        "                return result\n",
        "\n",
        "        self._log(\"All specified intrinsic dimension estimation methods failed. Defaulting.\", level=\"error\")\n",
        "        default_dim = max(1, min(5, n_features))\n",
        "        return {'median': default_dim, 'mean': float(default_dim), 'std': 0.0, 'q25': default_dim, 'q75': default_dim,\n",
        "                'raw_estimates': np.array([float(default_dim)]), 'method': 'Fallback (All Methods Failed)'}\n",
        "\n",
        "    def run_ica_picard(self, X_processed: np.ndarray, n_components: int) -> Optional[Tuple[np.ndarray, PicardResult]]:\n",
        "        \"\"\"Runs Picard ICA if available.\"\"\"\n",
        "        if PICARD_AVAILABLE and self.use_advanced_methods:\n",
        "            self._log(f\"Attempting Picard ICA with {n_components} components.\", level=\"info\")\n",
        "            if n_components > X_processed.shape[1]: n_components = X_processed.shape[1]\n",
        "            if n_components <=0: self._log(\"n_components <= 0 for Picard.\", level=\"error\"); return None\n",
        "            try:\n",
        "                # Import here again in case scope changed\n",
        "                from picard import picard # type: ignore\n",
        "\n",
        "                # Picard documentation usually expects (features, samples)\n",
        "                # Let's try that first.\n",
        "                X_T = X_processed.T\n",
        "                K, W, S_transformed_T = picard(X_T, n_components=n_components, ortho=False, max_iter=self.max_ica_iter, tol=1e-5, random_state=self.random_state)\n",
        "\n",
        "                # Picard returns sources S as (n_components, n_samples)\n",
        "                # We want (n_samples, n_components) for consistency with sklearn\n",
        "                S_transformed = S_transformed_T.T\n",
        "\n",
        "                # Verify output shapes\n",
        "                if S_transformed.shape != (X_processed.shape[0], n_components):\n",
        "                    raise ValueError(f\"Picard output S shape mismatch: expected {(X_processed.shape[0], n_components)}, got {S_transformed.shape}\")\n",
        "                # K is the unmixing matrix (components, features)\n",
        "                if K.shape != (n_components, X_processed.shape[1]):\n",
        "                    raise ValueError(f\"Picard output K (unmixing) shape mismatch: expected {(n_components, X_processed.shape[1])}, got {K.shape}\")\n",
        "                # W is the mixing matrix (features, components)\n",
        "                if W.shape != (X_processed.shape[1], n_components):\n",
        "                    raise ValueError(f\"Picard output W (mixing) shape mismatch: expected {(X_processed.shape[1], n_components)}, got {W.shape}\")\n",
        "\n",
        "                ica_result_obj = PicardResult(components_=K, mixing_=W, n_iter_=self.max_ica_iter) # Assuming max_iter is best guess for n_iter\n",
        "                self._log(f\"Picard ICA completed. Sources shape: {S_transformed.shape}\", level=\"info\")\n",
        "                return S_transformed, ica_result_obj\n",
        "            except Exception as e:\n",
        "                self._log(f\"Picard ICA failed: {e}. Trying FastICA.\", level=\"warning\", exc_info=True)\n",
        "        else:\n",
        "             self._log(\"Picard not available or not used.\", level=\"debug\")\n",
        "        return None\n",
        "\n",
        "    def run_ica_fastica(self, X_processed: np.ndarray, n_components: int) -> Optional[Tuple[np.ndarray, FastICA]]:\n",
        "        \"\"\"Runs FastICA, iterating max_iter budget if needed.\"\"\"\n",
        "        self._log(f\"Using FastICA with {n_components} components.\", level=\"info\")\n",
        "        if n_components > X_processed.shape[1]: n_components = X_processed.shape[1]\n",
        "        if n_components <=0: self._log(\"n_components <= 0 for FastICA.\", level=\"error\"); return None\n",
        "\n",
        "        iter_budgets = [500, 1500, self.max_ica_iter]\n",
        "        ica_model = None; S_transformed = None\n",
        "\n",
        "        for max_iter_budget in iter_budgets:\n",
        "            self._log(f\"Attempting FastICA with max_iter={max_iter_budget}...\", level=\"debug\")\n",
        "            with warnings.catch_warnings():\n",
        "                warnings.filterwarnings('ignore', category=ConvergenceWarning)\n",
        "                ica_model_current = FastICA(\n",
        "                    n_components=n_components, algorithm='parallel', whiten='unit-variance',\n",
        "                    max_iter=max_iter_budget, tol=1e-4, random_state=self.random_state\n",
        "                )\n",
        "                try:\n",
        "                    S_transformed = ica_model_current.fit_transform(X_processed)\n",
        "                    ica_model = ica_model_current\n",
        "                    if hasattr(ica_model, 'n_iter_') and ica_model.n_iter_ < max_iter_budget - 1 :\n",
        "                        self._log(f\"FastICA converged in {ica_model.n_iter_} iterations.\", level=\"info\")\n",
        "                        break # Converged early\n",
        "                    self._log(f\"FastICA reached max_iter {max_iter_budget}.\", level=\"info\")\n",
        "                    # Check if this is the last budget; if so, we accept this result even if it didn't converge early.\n",
        "                    if max_iter_budget == iter_budgets[-1]:\n",
        "                        self._log(\"FastICA completed using final max_iter budget.\", level=\"info\")\n",
        "                        break\n",
        "                except Exception as e:\n",
        "                    self._log(f\"FastICA failed (max_iter={max_iter_budget}): {e}\", level=\"warning\", exc_info=False) # Less verbose exc_info here\n",
        "                    if max_iter_budget == iter_budgets[-1]:\n",
        "                        self._log(\"FastICA failed on all attempts.\", level=\"error\")\n",
        "                        return None # Failed entirely after trying all budgets\n",
        "                    # continue to next budget if not the last one\n",
        "\n",
        "        if S_transformed is not None and ica_model is not None:\n",
        "            n_iter_final = getattr(ica_model, 'n_iter_', 'Unknown')\n",
        "            self._log(f\"FastICA completed. Sources shape: {S_transformed.shape}, Iterations: {n_iter_final}\", level=\"info\")\n",
        "            return S_transformed, ica_model\n",
        "\n",
        "        self._log(\"FastICA did not produce a result.\", level=\"error\")\n",
        "        return None\n",
        "\n",
        "    def run_ica(self, X: Union[pd.DataFrame, np.ndarray], n_components: int) -> Optional[Tuple[np.ndarray, Any]]:\n",
        "        \"\"\"Runs ICA, trying Picard then FastICA.\"\"\"\n",
        "        self._log(f\"\\n--- Running ICA for {n_components} Components ---\", level=\"info\")\n",
        "        X_processed = self._preprocess_X(X) # Handles scaling and cleaning\n",
        "\n",
        "        if n_components <= 0 : self._log(\"n_components must be > 0 for ICA.\", level=\"error\"); return None\n",
        "        if X_processed.shape[0] < n_components : self._log(f\"Samples ({X_processed.shape[0]}) < n_components ({n_components}). ICA may fail or produce unreliable results.\", level=\"warning\")\n",
        "        if X_processed.shape[1] < n_components :\n",
        "            self._log(f\"Features ({X_processed.shape[1]}) < n_components ({n_components}). Reducing n_components to {X_processed.shape[1]}.\", level=\"warning\")\n",
        "            n_components = X_processed.shape[1]\n",
        "        if n_components <= 0: self._log(\"Cannot run ICA with <= 0 components after adjustment.\", level=\"error\"); return None\n",
        "\n",
        "        # Try Picard first if available and preferred\n",
        "        picard_output = None\n",
        "        if PICARD_AVAILABLE and self.use_advanced_methods:\n",
        "             picard_output = self.run_ica_picard(X_processed, n_components)\n",
        "        if picard_output:\n",
        "             self._log(\"ICA successful using Picard.\", level=\"info\")\n",
        "             return picard_output\n",
        "\n",
        "        # Fallback to FastICA\n",
        "        self._log(\"Picard failed or not used. Falling back to FastICA.\", level=\"info\")\n",
        "        fastica_output = self.run_ica_fastica(X_processed, n_components)\n",
        "        if fastica_output:\n",
        "             self._log(\"ICA successful using FastICA.\", level=\"info\")\n",
        "             return fastica_output\n",
        "\n",
        "        self._log(\"ICA failed with all available methods.\", level=\"error\")\n",
        "        return None\n",
        "\n",
        "    def interpret_ica_components(self, ika_model: Any, feature_names: List[str], top_n: int = 5):\n",
        "        \"\"\"Interprets ICA components by showing top contributing features.\"\"\"\n",
        "        self._log(\"\\n--- Interpreting ICA Components ---\", level=\"info\")\n",
        "\n",
        "        components_matrix = None\n",
        "        if isinstance(ika_model, FastICA) and hasattr(ika_model, 'components_'):\n",
        "            components_matrix = ika_model.components_ # Unmixing matrix (n_components, n_features)\n",
        "            model_type = \"FastICA\"\n",
        "        elif isinstance(ika_model, PicardResult) and hasattr(ika_model, 'components_'):\n",
        "             components_matrix = ika_model.components_ # Unmixing matrix K (n_components, n_features)\n",
        "             model_type = \"Picard\"\n",
        "        else:\n",
        "             self._log(\"ICA model type not recognized or missing 'components_'. Cannot interpret.\", level=\"warning\")\n",
        "             return\n",
        "\n",
        "        if components_matrix is None:\n",
        "            self._log(\"Could not retrieve components matrix from ICA model.\", level=\"error\")\n",
        "            return\n",
        "\n",
        "        n_ica_components, n_features_in_ica = components_matrix.shape\n",
        "\n",
        "        if len(feature_names) != n_features_in_ica:\n",
        "            self._log(f\"Mismatch: provided feature names ({len(feature_names)}) vs ICA model features ({n_features_in_ica}). Cannot interpret accurately.\", level=\"error\")\n",
        "            # Optionally try to proceed with limited names, or return\n",
        "            # feature_names = [f\"Feature_{i}\" for i in range(n_features_in_ica)] # Example fallback\n",
        "            return\n",
        "\n",
        "        self._log(f\"Interpreting {n_ica_components} components from {model_type} (Shape: {components_matrix.shape})\", level=\"info\")\n",
        "        for i in range(n_ica_components):\n",
        "            comp_vector = components_matrix[i, :]\n",
        "            abs_weights = np.abs(comp_vector)\n",
        "            # Use argsort for indices, then slice for top_n\n",
        "            sorted_indices = np.argsort(abs_weights)[::-1][:min(top_n, n_features_in_ica)]\n",
        "\n",
        "            interpretation = []\n",
        "            for idx in sorted_indices:\n",
        "                weight = comp_vector[idx]\n",
        "                sign = '+' if weight >= 0 else '-'\n",
        "                # Format nicely\n",
        "                interpretation.append(f\"{sign}{feature_names[idx]} ({abs(weight):.3f})\")\n",
        "\n",
        "            self._log(f\"   Component {i+1}: {', '.join(interpretation)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "B1HVGumUyNpg"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import warnings\n",
        "import time\n",
        "import logging\n",
        "from typing import Tuple, Optional, Dict, Any, List, Union, Sequence\n",
        "import itertools\n",
        "\n",
        "# Scikit-learn imports\n",
        "from sklearn.mixture import BayesianGaussianMixture\n",
        "from sklearn.cluster import KMeans, AgglomerativeClustering, DBSCAN, MiniBatchKMeans, SpectralClustering\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.exceptions import ConvergenceWarning\n",
        "from sklearn.metrics import silhouette_score, davies_bouldin_score, calinski_harabasz_score\n",
        "from sklearn.utils import resample as sk_resample # For subsampling in k_search\n",
        "\n",
        "\n",
        "CLUSTERING_SUITE_AVAILABLE = True # Assuming this class itself means the suite is available\n",
        "\n",
        "class ClusteringSuite:\n",
        "    \"\"\"\n",
        "    Provides a suite of clustering algorithms and utilities, including DP-GMM,\n",
        "    ensemble clustering, and methods for finding an optimal number of clusters (K).\n",
        "    Handles preprocessing, optional dependencies, and performance considerations.\n",
        "    \"\"\"\n",
        "    def __init__(self,\n",
        "                 random_state: Optional[int] = None,\n",
        "                 verbose: bool = True,\n",
        "                 n_jobs: int = -1,\n",
        "                 use_hdbscan_if_available: bool = True,\n",
        "                 default_sample_size_for_k_search: int = 5000,\n",
        "                 ensemble_agglo_sample_threshold: int = 3000,\n",
        "                 consensus_spectral_threshold: int = 2000\n",
        "                 ):\n",
        "        self.random_state = random_state\n",
        "        self.verbose = verbose\n",
        "        self.n_jobs = n_jobs\n",
        "        self.use_hdbscan_if_available = use_hdbscan_if_available and HDBSCAN_AVAILABLE\n",
        "        self.default_sample_size_for_k_search = default_sample_size_for_k_search\n",
        "        self.ensemble_agglo_sample_threshold = ensemble_agglo_sample_threshold\n",
        "        self.consensus_spectral_threshold = consensus_spectral_threshold\n",
        "\n",
        "        self.last_dp_gmm_model: Optional[BayesianGaussianMixture] = None\n",
        "        self.last_dp_gmm_labels: Optional[np.ndarray] = None\n",
        "\n",
        "        self.class_logger = get_logger(f\"{self.__class__.__name__}\", verbose=self.verbose)\n",
        "        self.class_logger.info(f\"ClusteringSuite initialized. HDBSCAN Available: {HDBSCAN_AVAILABLE}, Will Use: {self.use_hdbscan_if_available}\")\n",
        "\n",
        "    def _log(self, message: str, level: str = \"info\", exc_info: bool = False):\n",
        "        if self.verbose:\n",
        "            if level == \"info\":    self.class_logger.info(message)\n",
        "            elif level == \"warning\": self.class_logger.warning(message)\n",
        "            elif level == \"error\":   self.class_logger.error(message, exc_info=exc_info)\n",
        "            else:                    self.class_logger.debug(message)\n",
        "\n",
        "    def _preprocess_X(self, X: Union[pd.DataFrame, np.ndarray], scale_data: bool = True) -> np.ndarray:\n",
        "        if isinstance(X, pd.DataFrame):\n",
        "            X_numeric = X.select_dtypes(include=np.number)\n",
        "            if X_numeric.shape[1] < X.shape[1]:\n",
        "                self._log(f\"Non-numeric columns detected. Using {X_numeric.shape[1]} numeric columns.\", level=\"debug\")\n",
        "            if X_numeric.empty: raise ValueError(\"No numeric data found in DataFrame for clustering.\")\n",
        "            X_np = X_numeric.values.astype(np.float64)\n",
        "        elif isinstance(X, np.ndarray):\n",
        "            if not np.issubdtype(X.dtype, np.number): raise ValueError(\"Input NumPy array must be numeric.\")\n",
        "            X_np = X.astype(np.float64)\n",
        "        else: raise TypeError(\"Input X must be a pandas DataFrame or a NumPy ndarray.\")\n",
        "\n",
        "        if X_np.size == 0:\n",
        "            raise ValueError(\"Input data resulted in an empty array after numeric selection.\")\n",
        "\n",
        "        if not np.all(np.isfinite(X_np)):\n",
        "            self._log(\"NaN/Inf values detected before scaling. Cleaning column-wise.\", level=\"warning\")\n",
        "            for i in range(X_np.shape[1]):\n",
        "                col_data = X_np[:, i]\n",
        "                if np.any(~np.isfinite(col_data)):\n",
        "                    finite_vals = col_data[np.isfinite(col_data)]\n",
        "                    if len(finite_vals) > 0:\n",
        "                        fill_value = np.median(finite_vals)\n",
        "                        posinf_fill = np.max(finite_vals) if len(finite_vals) > 1 else fill_value\n",
        "                        neginf_fill = np.min(finite_vals) if len(finite_vals) > 1 else fill_value\n",
        "                        X_np[:, i] = np.nan_to_num(col_data, nan=fill_value, posinf=posinf_fill, neginf=neginf_fill)\n",
        "                    else:\n",
        "                        self._log(f\"Column {i} is entirely non-finite. Filling with 0.0.\", level=\"warning\")\n",
        "                        X_np[:, i] = 0.0\n",
        "\n",
        "        if scale_data:\n",
        "            self._log(\"Applying StandardScaler.\", level=\"debug\")\n",
        "            scaler = StandardScaler()\n",
        "            X_scaled = scaler.fit_transform(X_np)\n",
        "            if not np.all(np.isfinite(X_scaled)):\n",
        "                self._log(\"Non-finite values detected AFTER scaling. Replacing with 0.0 (fallback).\", level=\"warning\")\n",
        "                X_scaled = np.nan_to_num(X_scaled, nan=0.0, posinf=0.0, neginf=0.0)\n",
        "            return X_scaled\n",
        "        return X_np\n",
        "\n",
        "    def run_kmeans(self, X_processed: np.ndarray, n_clusters: int, n_init: Union[str, int] = 'auto') -> Optional[np.ndarray]:\n",
        "        if n_clusters <= 0: self._log(f\"KMeans: n_clusters must be > 0, got {n_clusters}.\", level=\"error\"); return None\n",
        "        if X_processed.shape[0] == 0: self._log(\"KMeans: empty data.\", level=\"error\"); return np.array([], dtype=int)\n",
        "        actual_n_clusters = max(1, min(n_clusters, X_processed.shape[0]))\n",
        "        if actual_n_clusters != n_clusters:\n",
        "            self._log(f\"KMeans: Adjusted n_clusters from {n_clusters} to {actual_n_clusters} (samples={X_processed.shape[0]}).\", level=\"warning\")\n",
        "        self._log(f\"Running KMeans with n_clusters={actual_n_clusters}.\", level=\"debug\")\n",
        "        try:\n",
        "            kmeans = KMeans(n_clusters=actual_n_clusters, random_state=self.random_state, n_init=n_init, init='k-means++')\n",
        "            return kmeans.fit_predict(X_processed)\n",
        "        except Exception as e:\n",
        "            self._log(f\"KMeans failed (k={actual_n_clusters}): {e}\", level=\"error\", exc_info=True)\n",
        "            return None\n",
        "\n",
        "    def run_minibatch_kmeans(self, X_processed: np.ndarray, n_clusters: int,\n",
        "                             batch_size_ratio: float = 0.1, min_batch_size: int = 256,\n",
        "                             max_batch_size: int = 4096, n_init: int = 3) -> Optional[np.ndarray]:\n",
        "        if n_clusters <= 0: self._log(f\"MiniBatchKMeans: n_clusters must be > 0, got {n_clusters}.\", level=\"error\"); return None\n",
        "        if X_processed.shape[0] == 0: self._log(\"MiniBatchKMeans: empty data.\", level=\"error\"); return np.array([], dtype=int)\n",
        "        actual_n_clusters = max(1, min(n_clusters, X_processed.shape[0]))\n",
        "        if actual_n_clusters != n_clusters:\n",
        "            self._log(f\"MiniBatchKMeans: Adjusted n_clusters from {n_clusters} to {actual_n_clusters} (samples={X_processed.shape[0]}).\", level=\"warning\")\n",
        "        n_samples = X_processed.shape[0]\n",
        "        calculated_batch_size = int(n_samples * batch_size_ratio)\n",
        "        effective_batch_size = min(max_batch_size, max(min_batch_size, calculated_batch_size))\n",
        "        effective_batch_size = min(effective_batch_size, n_samples)\n",
        "        if effective_batch_size <= 0 and n_samples > 0: effective_batch_size = n_samples\n",
        "        self._log(f\"Running MiniBatchKMeans with n_clusters={actual_n_clusters}, batch_size={effective_batch_size}.\", level=\"debug\")\n",
        "        try:\n",
        "            mbk = MiniBatchKMeans(\n",
        "                n_clusters=actual_n_clusters, random_state=self.random_state,\n",
        "                batch_size=effective_batch_size, n_init=n_init,\n",
        "                max_iter=100,\n",
        "            )\n",
        "            return mbk.fit_predict(X_processed)\n",
        "        except Exception as e:\n",
        "            self._log(f\"MiniBatchKMeans failed (k={actual_n_clusters}): {e}\", level=\"error\", exc_info=True)\n",
        "            return None\n",
        "\n",
        "    def run_agglomerative(self, X_processed: np.ndarray, n_clusters: int, linkage: str = 'ward') -> Optional[np.ndarray]:\n",
        "        if n_clusters <= 0: self._log(f\"Agglomerative: n_clusters must be > 0, got {n_clusters}.\", level=\"error\"); return None\n",
        "        if X_processed.shape[0] == 0: self._log(\"Agglomerative: empty data.\", level=\"error\"); return np.array([], dtype=int)\n",
        "        actual_n_clusters = max(1, min(n_clusters, X_processed.shape[0]))\n",
        "        if actual_n_clusters != n_clusters:\n",
        "            self._log(f\"Agglomerative: Adjusted n_clusters from {n_clusters} to {actual_n_clusters} (samples={X_processed.shape[0]}).\", level=\"warning\")\n",
        "        if linkage == 'ward' and X_processed.shape[0] < 2:\n",
        "            self._log(f\"Ward linkage requires >= 2 samples, got {X_processed.shape[0]}. Cannot run.\", level=\"warning\")\n",
        "            return np.zeros(X_processed.shape[0], dtype=int) if actual_n_clusters == 1 else None\n",
        "        self._log(f\"Running Agglomerative Clustering: k={actual_n_clusters}, linkage='{linkage}'.\", level=\"debug\")\n",
        "        try:\n",
        "            metric = 'euclidean'\n",
        "            agglo = AgglomerativeClustering(n_clusters=actual_n_clusters, metric=metric, linkage=linkage)\n",
        "            return agglo.fit_predict(X_processed)\n",
        "        except Exception as e:\n",
        "            self._log(f\"Agglomerative Clustering failed (k={actual_n_clusters}, linkage={linkage}): {e}\", level=\"error\", exc_info=True)\n",
        "            return None\n",
        "\n",
        "    def run_dbscan(self, X_processed: np.ndarray, eps: float = 0.5, min_samples: int = 5) -> Optional[np.ndarray]:\n",
        "        if X_processed.shape[0] == 0: self._log(\"DBSCAN: empty data.\", level=\"error\"); return np.array([], dtype=int)\n",
        "        self._log(f\"Running DBSCAN: eps={eps}, min_samples={min_samples}.\", level=\"debug\")\n",
        "        try:\n",
        "            dbscan_n_jobs = self.n_jobs if self.n_jobs != 0 else None\n",
        "            dbscan = DBSCAN(eps=eps, min_samples=min_samples, n_jobs=dbscan_n_jobs)\n",
        "            labels = dbscan.fit_predict(X_processed)\n",
        "            n_clusters_found = len(set(labels)) - (1 if -1 in labels else 0)\n",
        "            n_noise = np.sum(labels == -1)\n",
        "            self._log(f\"DBSCAN found {n_clusters_found} clusters, {n_noise} noise points ({n_noise*100.0/max(1,X_processed.shape[0]):.1f}%).\", level=\"info\")\n",
        "            return labels\n",
        "        except Exception as e:\n",
        "            self._log(f\"DBSCAN failed: {e}\", level=\"error\", exc_info=True)\n",
        "            return None\n",
        "\n",
        "    def run_hdbscan(self, X_processed: np.ndarray, min_cluster_size: int = 15, min_samples: Optional[int] = None,\n",
        "                      allow_single_cluster: bool = False) -> Optional[np.ndarray]:\n",
        "        if not self.use_hdbscan_if_available:\n",
        "            self._log(\"HDBSCAN usage disabled by configuration.\", level=\"info\"); return None\n",
        "        if not HDBSCAN_AVAILABLE or hdbscan is None:\n",
        "            self._log(\"HDBSCAN library not available.\", level=\"warning\"); return None\n",
        "        if X_processed.shape[0] == 0: self._log(\"HDBSCAN: empty data.\", level=\"error\"); return np.array([], dtype=int)\n",
        "        actual_min_cluster_size = max(2, min_cluster_size)\n",
        "        actual_min_samples = min_samples if min_samples is not None else max(1, int(actual_min_cluster_size * 0.5))\n",
        "        actual_min_samples = max(1, min(actual_min_samples, actual_min_cluster_size))\n",
        "        self._log(f\"Running HDBSCAN: min_cluster_size={actual_min_cluster_size}, min_samples={actual_min_samples}.\", level=\"debug\")\n",
        "        if X_processed.shape[0] < actual_min_cluster_size:\n",
        "            self._log(f\"HDBSCAN: samples ({X_processed.shape[0]}) < min_cluster_size ({actual_min_cluster_size}). Expect all noise.\", level=\"warning\")\n",
        "        try:\n",
        "            clusterer = hdbscan.HDBSCAN(\n",
        "                min_cluster_size=actual_min_cluster_size, min_samples=actual_min_samples,\n",
        "                allow_single_cluster=allow_single_cluster,\n",
        "                core_dist_n_jobs=1\n",
        "            )\n",
        "            labels = clusterer.fit_predict(X_processed)\n",
        "            n_clusters_found = len(set(labels)) - (1 if -1 in labels else 0)\n",
        "            n_noise = np.sum(labels == -1)\n",
        "            self._log(f\"HDBSCAN found {n_clusters_found} clusters, {n_noise} noise points ({n_noise*100.0/max(1,X_processed.shape[0]):.1f}%).\", level=\"info\")\n",
        "            return labels\n",
        "        except Exception as e:\n",
        "            self._log(f\"HDBSCAN failed: {e}\", level=\"error\", exc_info=True)\n",
        "            return np.full(X_processed.shape[0], -1, dtype=int) if X_processed.shape[0] > 0 else np.array([], dtype=int)\n",
        "\n",
        "    def run_dp_gmm(self, X_processed: np.ndarray,\n",
        "                   target_n_components: int = 15, # K* from find_optimal_k can be passed here\n",
        "                   weight_concentration_prior: float = 0.1,\n",
        "                   reg_covar: float = 1e-5,\n",
        "                   mean_precision_prior: float = 0.01,\n",
        "                   n_init_bgm: int = 1\n",
        "                   ) -> Tuple[Optional[BayesianGaussianMixture], Dict[str, Any]]:\n",
        "        \"\"\"\n",
        "        Fits a Bayesian Gaussian Mixture model (DP-GMM).\n",
        "        target_n_components sets the upper bound for n_components in BGM.\n",
        "        \"\"\"\n",
        "        self._log(f\"Fitting DP-GMM (BayesianGaussianMixture, target_n_components={target_n_components}, reg_covar={reg_covar}).\", level=\"info\")\n",
        "        n_samples, n_features = X_processed.shape\n",
        "        if n_samples == 0:\n",
        "            self._log(\"DP-GMM: empty data.\", level=\"error\")\n",
        "            return None, {'error': 'Empty input data', 'effective_components': 0, 'labels': np.array([], dtype=int)}\n",
        "\n",
        "        # Use target_n_components as the n_components for BGM initialization\n",
        "        # BGM will attempt to fit up to this many components.\n",
        "        actual_n_components_for_bgm = min(target_n_components, n_samples)\n",
        "        actual_n_components_for_bgm = max(1, actual_n_components_for_bgm) # Must be >= 1\n",
        "        self._log(f\"DP-GMM effective n_components for BGM init: {actual_n_components_for_bgm}.\", level=\"debug\")\n",
        "\n",
        "        try:\n",
        "            bgm = BayesianGaussianMixture(\n",
        "                n_components=actual_n_components_for_bgm, covariance_type='diag',\n",
        "                weight_concentration_prior_type='dirichlet_process',\n",
        "                weight_concentration_prior=weight_concentration_prior,\n",
        "                mean_precision_prior=mean_precision_prior,\n",
        "                reg_covar=reg_covar,\n",
        "                max_iter=300, n_init=n_init_bgm,\n",
        "                random_state=self.random_state,\n",
        "            )\n",
        "            with warnings.catch_warnings():\n",
        "                warnings.filterwarnings('ignore', category=ConvergenceWarning, module='sklearn')\n",
        "                bgm.fit(X_processed)\n",
        "\n",
        "            weights = bgm.weights_\n",
        "            # Determine effective components based on weight threshold\n",
        "            # Use actual_n_components_for_bgm for calculating threshold\n",
        "            significant_threshold = 1.0 / (actual_n_components_for_bgm * 10.0)\n",
        "            significant_threshold = min(0.01, max(1e-4, significant_threshold))\n",
        "            effective_mask = weights > significant_threshold\n",
        "            effective_n = int(np.sum(effective_mask))\n",
        "\n",
        "            if effective_n == 0 and len(weights) > 0:\n",
        "                self._log(\"DP-GMM: All components below threshold. Taking component with largest weight.\", level=\"warning\")\n",
        "                effective_n = 1\n",
        "                effective_mask = np.zeros_like(weights, dtype=bool)\n",
        "                effective_mask[np.argmax(weights)] = True\n",
        "\n",
        "            labels = bgm.predict(X_processed) if n_samples > 0 else np.array([], dtype=int)\n",
        "            self.last_dp_gmm_model = bgm\n",
        "            self.last_dp_gmm_labels = labels\n",
        "\n",
        "            info = {\n",
        "                'effective_components': effective_n, 'labels': labels,\n",
        "                'model_converged': bgm.converged_, 'n_iter': bgm.n_iter_,\n",
        "                'weights': weights[effective_mask].tolist() if effective_n > 0 else [],\n",
        "                'means': bgm.means_[effective_mask].tolist() if effective_n > 0 else [],\n",
        "                'covariances': bgm.covariances_[effective_mask].tolist() if effective_n > 0 else [],\n",
        "                'bic_score': bgm.bic(X_processed) if hasattr(bgm, 'bic') else None,\n",
        "                'aic_score': bgm.aic(X_processed) if hasattr(bgm, 'aic') else None,\n",
        "                'lower_bound': bgm.lower_bound_\n",
        "            }\n",
        "            self._log(f\"DP-GMM converged: {bgm.converged_} in {bgm.n_iter_} iters. Effective components found: {effective_n} (from target K={target_n_components}).\", level=\"info\")\n",
        "            return bgm, info\n",
        "        except Exception as e:\n",
        "            self._log(f\"Error during DP-GMM fitting: {e}\", level=\"error\", exc_info=True)\n",
        "            return None, {'error': str(e), 'effective_components': 0, 'labels': np.zeros(n_samples, dtype=int) if n_samples > 0 else np.array([], dtype=int)}\n",
        "\n",
        "    def find_optimal_k(self,\n",
        "                       X_processed: np.ndarray,\n",
        "                       k_range: Union[Sequence[int], range] = range(2, 11),\n",
        "                       metric: str = 'silhouette',\n",
        "                       base_clustering_algo: str = 'minibatch_kmeans',\n",
        "                       sample_size_for_k_search: Optional[int] = None\n",
        "                       ) -> int:\n",
        "        actual_sample_size = sample_size_for_k_search if sample_size_for_k_search is not None else self.default_sample_size_for_k_search\n",
        "        self._log(f\"Finding optimal K using '{metric}' (Range: {min(k_range)}-{max(k_range)}, Algo: '{base_clustering_algo}', SampleSize: {actual_sample_size}).\", level=\"info\")\n",
        "        n_samples_orig = X_processed.shape[0]\n",
        "\n",
        "        X_eval = X_processed\n",
        "        if n_samples_orig > actual_sample_size:\n",
        "            self._log(f\"Subsampling data from {n_samples_orig} to {actual_sample_size} for optimal K search.\", level=\"debug\")\n",
        "            X_eval = sk_resample(X_processed, n_samples=actual_sample_size, random_state=self.random_state)\n",
        "\n",
        "        n_eval_samples = X_eval.shape[0]\n",
        "        valid_k_range = [k for k in k_range if 1 < k < n_eval_samples]\n",
        "\n",
        "        if not valid_k_range:\n",
        "            fallback_k = 2 if n_eval_samples >= 2 else 1\n",
        "            self._log(f\"k_range {list(k_range)} invalid or no K > 1 possible for sample size {n_eval_samples}. Defaulting K to {fallback_k}.\", level=\"warning\")\n",
        "            return fallback_k\n",
        "\n",
        "        scores = {}\n",
        "        best_score = -np.inf if metric in ['silhouette', 'calinski_harabasz'] else np.inf\n",
        "        optimal_k = min(valid_k_range)\n",
        "\n",
        "        self._log(f\"Evaluating K values: {valid_k_range}\", level=\"debug\")\n",
        "        for k_val in valid_k_range:\n",
        "            labels = None\n",
        "            try:\n",
        "                if base_clustering_algo == 'kmeans': labels = self.run_kmeans(X_eval, k_val)\n",
        "                elif base_clustering_algo == 'minibatch_kmeans': labels = self.run_minibatch_kmeans(X_eval, k_val)\n",
        "                else: self._log(f\"Unsupported base_clustering_algo '{base_clustering_algo}'.\", level=\"warning\"); break\n",
        "\n",
        "                if labels is None:\n",
        "                    self._log(f\"Base clustering '{base_clustering_algo}' failed for K={k_val}. Skipping.\", level=\"warning\")\n",
        "                    continue\n",
        "\n",
        "                unique_labels = np.unique(labels)\n",
        "                n_unique_clusters = len(unique_labels) - (1 if -1 in unique_labels else 0)\n",
        "\n",
        "                if n_unique_clusters < 2:\n",
        "                    self._log(f\"Skipping metric evaluation for K={k_val}, resulted in {n_unique_clusters} valid cluster(s).\", level=\"debug\")\n",
        "                    continue\n",
        "\n",
        "                current_score: Optional[float] = None\n",
        "                if metric == 'silhouette': current_score = silhouette_score(X_eval, labels)\n",
        "                elif metric == 'davies_bouldin': current_score = davies_bouldin_score(X_eval, labels)\n",
        "                elif metric == 'calinski_harabasz': current_score = calinski_harabasz_score(X_eval, labels)\n",
        "                else: self._log(f\"Unsupported metric '{metric}'. Stopping K search.\", level=\"error\"); break\n",
        "\n",
        "                if current_score is not None and np.isfinite(current_score):\n",
        "                    scores[k_val] = current_score\n",
        "                    self._log(f\"   K={k_val}, {metric.capitalize()} Score: {current_score:.4f}\", level=\"debug\")\n",
        "                    higher_is_better = metric in ['silhouette', 'calinski_harabasz']\n",
        "                    if higher_is_better and current_score > best_score:\n",
        "                        best_score, optimal_k = current_score, k_val\n",
        "                    elif not higher_is_better and current_score < best_score:\n",
        "                        best_score, optimal_k = current_score, k_val\n",
        "                else:\n",
        "                    self._log(f\"Metric calculation failed or resulted in non-finite score for K={k_val}.\", level=\"warning\")\n",
        "\n",
        "            except Exception as e:\n",
        "                self._log(f\"Error evaluating K={k_val} with metric '{metric}': {e}\", level=\"warning\", exc_info=False)\n",
        "\n",
        "        if not scores:\n",
        "            self._log(f\"Could not evaluate any K in range {valid_k_range}. Defaulting to {optimal_k}.\", level=\"warning\")\n",
        "        else:\n",
        "            self._log(f\"Optimal K determined as {optimal_k} (Best Score for '{metric}': {best_score:.4f})\", level=\"info\")\n",
        "        return optimal_k\n",
        "\n",
        "    def run_ensemble_clustering(self,\n",
        "                                X: Union[pd.DataFrame, np.ndarray],\n",
        "                                n_clusters: int # Target K* determined externally\n",
        "                               ) -> Tuple[Dict[str, Optional[np.ndarray]], Dict[str, Any]]:\n",
        "        self._log(f\"Running Ensemble Clustering (Target K={n_clusters}).\", level=\"info\")\n",
        "        X_processed = self._preprocess_X(X, scale_data=True)\n",
        "        n_samples = X_processed.shape[0]\n",
        "\n",
        "        if n_samples == 0:\n",
        "            self._log(\"Ensemble: empty data.\", level=\"error\")\n",
        "            return {}, {'error': 'Empty input data', 'labels': np.array([], dtype=int), 'n_clusters_found': 0}\n",
        "\n",
        "        final_n_clusters = max(1, min(n_clusters, n_samples))\n",
        "        if final_n_clusters != n_clusters:\n",
        "            self._log(f\"Adjusted ensemble K from {n_clusters} to {final_n_clusters} (samples: {n_samples}).\", level=\"warning\")\n",
        "\n",
        "        if final_n_clusters == 1 and n_samples > 0:\n",
        "            self._log(\"Ensemble target K=1. Assigning all points to cluster 0.\", level=\"info\")\n",
        "            labels = np.zeros(n_samples, dtype=int)\n",
        "            return {'single_cluster': labels}, {'final_labels': labels, 'n_clusters_found': 1, 'method': 'Single Cluster'}\n",
        "\n",
        "        labels_dict: Dict[str, Optional[np.ndarray]] = {}\n",
        "        labels_dict['kmeans'] = self.run_kmeans(X_processed, final_n_clusters)\n",
        "        labels_dict['minibatch_kmeans'] = self.run_minibatch_kmeans(X_processed, final_n_clusters)\n",
        "\n",
        "        if n_samples < self.ensemble_agglo_sample_threshold:\n",
        "            if final_n_clusters >= 2:\n",
        "                for linkage_type in ['ward', 'average', 'complete']:\n",
        "                    if linkage_type == 'ward' and n_samples < 2: continue\n",
        "                    labels_dict[f'agglo_{linkage_type}'] = self.run_agglomerative(X_processed, final_n_clusters, linkage=linkage_type)\n",
        "        else:\n",
        "            self._log(f\"Skipping Agglomerative base clusterers (N={n_samples} >= threshold {self.ensemble_agglo_sample_threshold}).\", level=\"info\")\n",
        "\n",
        "        if self.use_hdbscan_if_available:\n",
        "            hdb_min_size = max(5, n_samples // (final_n_clusters * 5 if final_n_clusters > 1 else 20))\n",
        "            hdb_min_size = max(2, min(hdb_min_size, n_samples // 2 if n_samples > 1 else 1))\n",
        "            labels_dict['hdbscan'] = self.run_hdbscan(X_processed, min_cluster_size=hdb_min_size, allow_single_cluster=(final_n_clusters == 1))\n",
        "\n",
        "        if self.last_dp_gmm_labels is not None and len(self.last_dp_gmm_labels) == n_samples:\n",
        "            labels_dict['dp_gmm'] = self.last_dp_gmm_labels\n",
        "\n",
        "        valid_partitions = {name: lbls for name, lbls in labels_dict.items()\n",
        "                            if lbls is not None and len(lbls) == n_samples}\n",
        "\n",
        "        if not valid_partitions:\n",
        "            self._log(\"No successful base clusterings for ensemble. Cannot proceed.\", level=\"error\")\n",
        "            return {}, {'error': 'All base clusterers failed', 'final_labels': np.zeros(n_samples, dtype=int), 'n_clusters_found': 1, 'method':'Fallback'}\n",
        "\n",
        "        co_association_matrix = np.zeros((n_samples, n_samples), dtype=np.float32)\n",
        "        num_partitions_used = 0\n",
        "        self._log(f\"Building co-association matrix from {len(valid_partitions)} base partitions.\", level=\"debug\")\n",
        "        for algo_name, labels in valid_partitions.items():\n",
        "            num_partitions_used += 1\n",
        "            for i in range(n_samples):\n",
        "                if labels[i] != -1:\n",
        "                    same_cluster_mask = (labels == labels[i]) & (labels != -1)\n",
        "                    co_association_matrix[i, same_cluster_mask] += 1.0\n",
        "\n",
        "        if num_partitions_used == 0:\n",
        "            self._log(\"No non-trivial partitions found for co-association. Fallback needed.\", level=\"warning\")\n",
        "            fallback_labels = labels_dict.get('kmeans', labels_dict.get('minibatch_kmeans'))\n",
        "            if fallback_labels is None: fallback_labels = np.zeros(n_samples, dtype=int)\n",
        "            return valid_partitions, {'error':'No valid partitions for consensus', 'final_labels': fallback_labels, 'n_clusters_found': len(np.unique(fallback_labels)), 'method': 'Fallback'}\n",
        "\n",
        "        co_association_matrix /= num_partitions_used\n",
        "\n",
        "        self._log(f\"Performing consensus clustering (Target K={final_n_clusters}) from co-association matrix (N={n_samples}).\", level=\"info\")\n",
        "        consensus_labels: Optional[np.ndarray] = None\n",
        "        consensus_method: str = \"N/A\"\n",
        "\n",
        "        try:\n",
        "            if n_samples >= self.consensus_spectral_threshold and final_n_clusters >= 2:\n",
        "                self._log(f\"Using SpectralClustering for consensus (N={n_samples} >= threshold {self.consensus_spectral_threshold}).\", level=\"info\")\n",
        "                affinity_matrix = np.nan_to_num(co_association_matrix, nan=0.0, posinf=1.0, neginf=0.0)\n",
        "                affinity_matrix = np.maximum(affinity_matrix, 0)\n",
        "                spectral_k = min(final_n_clusters, n_samples - 1 if n_samples > 1 else 1)\n",
        "                if spectral_k < 2 and n_samples >=2 : # If n_samples = 1, spectral_k would be 1.\n",
        "                     self._log(f\"SpectralClustering needs K>=2, effective K is {spectral_k}. Using Agglomerative for consensus.\", level=\"warning\")\n",
        "                     # Fallthrough to Agglomerative\n",
        "                elif spectral_k >=2 : # Only if K>=2 proceed with spectral\n",
        "                    consensus_spectral = SpectralClustering(\n",
        "                        n_clusters=spectral_k, affinity='precomputed',\n",
        "                        assign_labels='kmeans',\n",
        "                        random_state=self.random_state, n_jobs=self.n_jobs\n",
        "                    )\n",
        "                    consensus_labels = consensus_spectral.fit_predict(affinity_matrix)\n",
        "                    consensus_method = f\"SpectralClustering (Target K={spectral_k})\"\n",
        "                # If spectral_k < 2, consensus_labels remains None, will try agglomerative\n",
        "\n",
        "            # If Spectral was not used (either due to size or spectral_k < 2) or failed implicitly\n",
        "            if consensus_labels is None:\n",
        "                self._log(f\"Using AgglomerativeClustering for consensus (N={n_samples}).\", level=\"info\")\n",
        "                distance_matrix = np.maximum(1.0 - co_association_matrix, 0)\n",
        "                np.fill_diagonal(distance_matrix, 0)\n",
        "\n",
        "                # Ensure K for Agglomerative is valid\n",
        "                agglo_k = final_n_clusters\n",
        "                if agglo_k >= n_samples and n_samples > 0 : agglo_k = n_samples -1\n",
        "                if agglo_k < 1 and n_samples > 0 : agglo_k = 1\n",
        "                if agglo_k == 0 and n_samples == 0: agglo_k = 0 # No clusters for no samples\n",
        "                elif agglo_k <= 0 : # If n_samples > 0 but agglo_k is still <=0\n",
        "                     self._log(f\"Agglomerative consensus K became invalid ({agglo_k}) for N={n_samples}. Defaulting to 1.\", \"warning\")\n",
        "                     agglo_k = 1\n",
        "\n",
        "\n",
        "                if agglo_k > 0 or (agglo_k==0 and n_samples==0): # Proceed if K is valid\n",
        "                    consensus_agglo = AgglomerativeClustering(\n",
        "                        n_clusters=agglo_k, metric='precomputed', linkage='average'\n",
        "                    )\n",
        "                    consensus_labels = consensus_agglo.fit_predict(distance_matrix)\n",
        "                    consensus_method = f\"Agglomerative (Target K={agglo_k})\"\n",
        "                else: # Should not happen with above logic\n",
        "                    raise ValueError(\"Agglomerative K calculation resulted in invalid K for consensus.\")\n",
        "\n",
        "\n",
        "            if consensus_labels is None: raise ValueError(\"Consensus clustering algorithm returned None.\")\n",
        "\n",
        "            actual_consensus_k = len(np.unique(consensus_labels)) - (1 if -1 in consensus_labels else 0)\n",
        "            self._log(f\"Consensus clustering ({consensus_method}) resulted in {actual_consensus_k} clusters.\", level=\"info\")\n",
        "            sil_score, db_score, ch_score = None, None, None\n",
        "            if actual_consensus_k > 1 and n_samples > actual_consensus_k:\n",
        "                try:\n",
        "                    sil_score = silhouette_score(X_processed, consensus_labels)\n",
        "                    db_score = davies_bouldin_score(X_processed, consensus_labels)\n",
        "                    ch_score = calinski_harabasz_score(X_processed, consensus_labels)\n",
        "                    self._log(f\"Consensus scores: Sil={sil_score:.3f}, DB={db_score:.3f}, CH={ch_score:.1f}\", level=\"info\")\n",
        "                except ValueError as ve_metric:\n",
        "                    self._log(f\"Could not compute metrics for consensus labels: {ve_metric}\", level=\"warning\")\n",
        "\n",
        "            consensus_info = {\n",
        "                'final_labels': consensus_labels,\n",
        "                'n_clusters_found': actual_consensus_k, 'method': consensus_method,\n",
        "                'silhouette_score': sil_score, 'davies_bouldin_score': db_score,\n",
        "                'calinski_harabasz_score': ch_score,\n",
        "            }\n",
        "            return valid_partitions, consensus_info\n",
        "\n",
        "        except Exception as e_consensus:\n",
        "            self._log(f\"Consensus clustering failed: {e_consensus}\", level=\"error\", exc_info=True)\n",
        "            fallback_labels = valid_partitions.get('kmeans', valid_partitions.get('minibatch_kmeans'))\n",
        "            if fallback_labels is None: fallback_labels = np.zeros(n_samples, dtype=int)\n",
        "            return valid_partitions, {'error': str(e_consensus), 'final_labels': fallback_labels, 'n_clusters_found': len(np.unique(fallback_labels)), 'method': 'Fallback'}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "y69ulek-ySIR"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from typing import Tuple, Optional, Dict, Any, Union\n",
        "\n",
        "# Attempt to import geopandas, but make its usage conditional\n",
        "try:\n",
        "    import geopandas as gpd\n",
        "    from shapely.geometry import Point, Polygon\n",
        "    GEOPANDAS_AVAILABLE = True\n",
        "except ImportError:\n",
        "    GEOPANDAS_AVAILABLE = False\n",
        "\n",
        "# Assuming utils.py is in the same directory or accessible via PYTHONPATH\n",
        "# from .utils import get_logger\n",
        "\n",
        "logger = get_logger(__name__)\n",
        "\n",
        "class SpatialAnalyzer:\n",
        "    \"\"\"\n",
        "    Handles spatial analysis tasks, such as grid assignment and\n",
        "    spatial pattern analysis.\n",
        "    Requires geopandas for most functionalities.\n",
        "    \"\"\"\n",
        "    def __init__(self, random_state: Optional[int] = None, verbose: bool = True):\n",
        "        self.random_state = random_state\n",
        "        self.verbose = verbose\n",
        "        self.grid_gdf: Optional['gpd.GeoDataFrame'] = None\n",
        "        self.grid_params: Optional[Dict[str, Any]] = None\n",
        "        self.cell_to_rows_lookup: Optional[Dict[str, list]] = None\n",
        "\n",
        "        if not GEOPANDAS_AVAILABLE:\n",
        "            logger.warning(\"Geopandas not available. Spatial analysis features will be limited.\")\n",
        "\n",
        "    def _log(self, message: str, level: str = \"info\"):\n",
        "        if self.verbose:\n",
        "            if level == \"info\":\n",
        "                logger.info(message)\n",
        "            elif level == \"warning\":\n",
        "                logger.warning(message)\n",
        "\n",
        "    def assign_grid_indices(self,\n",
        "                            df: pd.DataFrame,\n",
        "                            x_coord_col: str,\n",
        "                            y_coord_col: str,\n",
        "                            grid_size: Union[float, int], # e.g., 250 meters or degrees\n",
        "                            crs: Optional[str] = None # Optional CRS for GeoDataFrame\n",
        "                           ) -> pd.DataFrame:\n",
        "        \"\"\"\n",
        "        Assigns spatial grid indices to properties based on their coordinates.\n",
        "\n",
        "        Args:\n",
        "            df: DataFrame with property data including coordinate columns.\n",
        "            x_coord_col: Name of the column with x-coordinates (e.g., 'longitude', 'xcoord').\n",
        "            y_coord_col: Name of the column with y-coordinates (e.g., 'latitude', 'ycoord').\n",
        "            grid_size: The size of each grid cell in the units of the coordinates.\n",
        "            crs: Coordinate Reference System string (e.g., 'EPSG:4326' for WGS84).\n",
        "                 Required if creating a GeoDataFrame for self.grid_gdf.\n",
        "\n",
        "        Returns:\n",
        "            DataFrame with added 'grid_i', 'grid_j', and 'grid_cell_id' columns.\n",
        "        \"\"\"\n",
        "        self._log(f\"\\n--- Assigning Spatial Grid Indices (Size: {grid_size}) ---\")\n",
        "        if not GEOPANDAS_AVAILABLE:\n",
        "            self._log(\"Geopandas not available, cannot perform advanced grid assignment or create GeoDataFrame.\", level=\"warning\")\n",
        "\n",
        "        if x_coord_col not in df.columns or y_coord_col not in df.columns:\n",
        "            logger.error(f\"Coordinate columns '{x_coord_col}' or '{y_coord_col}' not found.\")\n",
        "            return df.copy()\n",
        "\n",
        "        _df = df.copy()\n",
        "        _df[x_coord_col] = pd.to_numeric(_df[x_coord_col], errors='coerce')\n",
        "        _df[y_coord_col] = pd.to_numeric(_df[y_coord_col], errors='coerce')\n",
        "\n",
        "        n_initial_coords = len(_df)\n",
        "        _df.dropna(subset=[x_coord_col, y_coord_col], inplace=True)\n",
        "        n_valid_coords = len(_df)\n",
        "        if n_initial_coords > n_valid_coords:\n",
        "            self._log(f\"Dropped {n_initial_coords - n_valid_coords} rows with NaN coordinates.\")\n",
        "\n",
        "        if n_valid_coords == 0:\n",
        "            self._log(\"No valid coordinates remaining for grid assignment.\", level=\"warning\")\n",
        "            _df['grid_i'] = pd.NA\n",
        "            _df['grid_j'] = pd.NA\n",
        "            _df['grid_cell_id'] = pd.NA\n",
        "            return _df\n",
        "\n",
        "        x_coords = _df[x_coord_col].values\n",
        "        y_coords = _df[y_coord_col].values\n",
        "\n",
        "        q_low, q_high = 0.001, 0.999\n",
        "        finite_x_coords = x_coords[np.isfinite(x_coords)]\n",
        "        finite_y_coords = y_coords[np.isfinite(y_coords)]\n",
        "\n",
        "        if len(finite_x_coords) < 2 or len(finite_y_coords) < 2:\n",
        "            self._log(\"Too few finite coordinate points to define robust grid boundaries using quantiles. Using min/max.\", level=\"warning\")\n",
        "            x_min, x_max = np.min(finite_x_coords), np.max(finite_x_coords)\n",
        "            y_min, y_max = np.min(finite_y_coords), np.max(finite_y_coords)\n",
        "        else:\n",
        "            x_min = np.quantile(finite_x_coords, q_low)\n",
        "            x_max = np.quantile(finite_x_coords, q_high)\n",
        "            y_min = np.quantile(finite_y_coords, q_low)\n",
        "            y_max = np.quantile(finite_y_coords, q_high)\n",
        "\n",
        "        x_min -= grid_size / 2\n",
        "        x_max += grid_size / 2\n",
        "        y_min -= grid_size / 2\n",
        "        y_max += grid_size / 2\n",
        "\n",
        "        grid_i = np.floor((x_coords - x_min) / grid_size).astype(int)\n",
        "        grid_j = np.floor((y_coords - y_min) / grid_size).astype(int)\n",
        "\n",
        "        n_cols = int(np.ceil((x_max - x_min) / grid_size))\n",
        "        n_rows = int(np.ceil((y_max - y_min) / grid_size))\n",
        "\n",
        "        grid_i = np.clip(grid_i, 0, n_cols -1 if n_cols > 0 else 0)\n",
        "        grid_j = np.clip(grid_j, 0, n_rows -1 if n_rows > 0 else 0)\n",
        "\n",
        "        _df['grid_i'] = grid_i\n",
        "        _df['grid_j'] = grid_j\n",
        "        _df['grid_cell_id'] = _df['grid_i'].astype(str) + \"_\" + _df['grid_j'].astype(str)\n",
        "\n",
        "        self._log(f\"Assigned grid indices. Grid dimensions: {n_cols} cols x {n_rows} rows.\")\n",
        "        self._log(f\"Unique grid cells populated: {_df['grid_cell_id'].nunique()}\")\n",
        "\n",
        "        self.grid_params = {\n",
        "            'x_min': x_min, 'y_min': y_min, 'x_max': x_max, 'y_max': y_max,\n",
        "            'n_cols': n_cols, 'n_rows': n_rows, 'grid_size': grid_size, 'crs': crs\n",
        "        }\n",
        "\n",
        "        self.cell_to_rows_lookup = _df.groupby('grid_cell_id').groups\n",
        "\n",
        "        if GEOPANDAS_AVAILABLE and crs:\n",
        "            polygons = []\n",
        "            cell_ids_for_gdf = []\n",
        "            for i_idx in range(n_cols):\n",
        "                for j_idx in range(n_rows):\n",
        "                    cell_id = f\"{i_idx}_{j_idx}\"\n",
        "                    if cell_id in self.cell_to_rows_lookup:\n",
        "                        cell_x_min = x_min + i_idx * grid_size\n",
        "                        cell_y_min = y_min + j_idx * grid_size\n",
        "                        cell_x_max = cell_x_min + grid_size\n",
        "                        cell_y_max = cell_y_min + grid_size\n",
        "                        polygons.append(Polygon([\n",
        "                            (cell_x_min, cell_y_min), (cell_x_max, cell_y_min),\n",
        "                            (cell_x_max, cell_y_max), (cell_x_min, cell_y_max)\n",
        "                        ]))\n",
        "                        cell_ids_for_gdf.append(cell_id)\n",
        "\n",
        "            if polygons:\n",
        "                self.grid_gdf = gpd.GeoDataFrame({'cell_id': cell_ids_for_gdf, 'geometry': polygons}, crs=crs)  # type: ignore\n",
        "                self._log(f\"Created GeoDataFrame for {len(self.grid_gdf)} populated grid cells.\")\n",
        "            else:\n",
        "                self._log(\"No populated grid cells to create a GeoDataFrame.\", level=\"warning\")\n",
        "\n",
        "        return _df\n",
        "\n",
        "    # Future methods:\n",
        "    # - analyze_spatial_autocorrelation (e.g., Moran's I on prices/residuals)\n",
        "    # - create_spatial_lag_features\n",
        "    # - visualize_spatial_patterns (choropleth maps etc.)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zcIl9sLuyTd3"
      },
      "outputs": [],
      "source": [
        "# File: analysis_orchestrator.py\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import time\n",
        "import logging\n",
        "from typing import List, Dict, Optional, Any, Tuple, Union, Sequence\n",
        "from collections import defaultdict\n",
        "from joblib import Parallel, delayed, cpu_count\n",
        "import pickle\n",
        "import os\n",
        "\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from scipy.stats import skew, kurtosis\n",
        "\n",
        "\n",
        "# --- Analysis Orchestrator Class ---\n",
        "\n",
        "class AnalysisOrchestrator:\n",
        "    \"\"\"\n",
        "    Orchestrates the robust real estate analysis pipeline.\n",
        "    Uses ClusteringSuite for dynamic K selection and adapts clustering methods.\n",
        "    \"\"\"\n",
        "    def __init__(self,\n",
        "                 random_state: Optional[int] = None,\n",
        "                 verbose: bool = True,\n",
        "                 n_jobs: int = -1,\n",
        "                 max_total_time_seconds: Optional[int] = 3600,\n",
        "                 price_col: str = 'sale_price',\n",
        "                 log_price_col_name: str = 'log_sale_price',\n",
        "                 apply_spatial_analysis: bool = False,\n",
        "                 # Configs passed to DataPreprocessor.fit_transform\n",
        "                 min_price_percentile: float = 0.005,\n",
        "                 max_price_percentile: float = 0.995,\n",
        "                 default_transform_type: Optional[str] = 'quantile',\n",
        "                 transform_skew_threshold: float = 1.0,\n",
        "                 # Orchestrator execution controls\n",
        "                 max_samples_limit: int = 50000,\n",
        "                 max_bootstraps: int = 30,\n",
        "                 # Informational targets (not directly used for control)\n",
        "                 target_dim_ci: float = 2.0,\n",
        "                 target_k_ci: float = 1.0,\n",
        "                 target_ari_ci: float = 0.05\n",
        "                 ):\n",
        "\n",
        "        self.random_state = random_state\n",
        "        self.verbose = verbose\n",
        "        self.n_jobs = cpu_count() if n_jobs <= 0 else n_jobs\n",
        "        self.max_total_time_seconds = max_total_time_seconds\n",
        "        self.logger = get_logger(self.__class__.__name__, verbose=self.verbose)\n",
        "\n",
        "        self.price_col = price_col\n",
        "        self.log_price_col = log_price_col_name\n",
        "\n",
        "        # Store config params for potential use or passing down\n",
        "        self.min_price_percentile = min_price_percentile\n",
        "        self.max_price_percentile = max_price_percentile\n",
        "        self.default_transform_type = default_transform_type\n",
        "        self.transform_skew_threshold = abs(transform_skew_threshold)\n",
        "\n",
        "        self.max_samples_limit = max_samples_limit\n",
        "        self.max_bootstraps = max_bootstraps\n",
        "        # Store informational targets\n",
        "        self.target_dim_ci = target_dim_ci\n",
        "        self.target_k_ci = target_k_ci\n",
        "        self.target_ari_ci = target_ari_ci\n",
        "\n",
        "        # Initialize components\n",
        "        try:\n",
        "            # Pass relevant general configs to components\n",
        "            self.preprocessor = DataPreprocessor(\n",
        "                price_col=self.price_col,\n",
        "                log_price_col_name=self.log_price_col,\n",
        "                random_state=self.random_state,\n",
        "                verbose=self.verbose\n",
        "                # Other DataPreprocessor configs are passed via fit_transform\n",
        "            )\n",
        "            self.sampler = SamplingUtils(random_state=self.random_state, verbose=self.verbose)\n",
        "            self.dim_reducer = DimensionalityReducer(random_state=self.random_state, verbose=self.verbose, n_jobs=self.n_jobs)\n",
        "            self.clusterer = ClusteringSuite(random_state=self.random_state, verbose=self.verbose, n_jobs=self.n_jobs)\n",
        "\n",
        "            self.ica_module = ICAModule(random_state=self.random_state, verbose=self.verbose)\n",
        "            self.logger.info(\"ICAModule initialized.\")\n",
        "\n",
        "\n",
        "        except NameError as ne:\n",
        "            self.logger.error(f\"Failed to initialize a component (NameError): {ne}. Ensure all dependent classes are defined/imported.\", exc_info=True)\n",
        "            raise RuntimeError(f\"Orchestrator component initialization failed: {ne}\")\n",
        "        except Exception as e:\n",
        "            self.logger.error(f\"Unexpected error initializing components: {e}\", exc_info=True)\n",
        "            raise RuntimeError(f\"Orchestrator component initialization failed: {e}\")\n",
        "\n",
        "        # Initialize Spatial Analyzer if requested and available\n",
        "        self.apply_spatial_analysis = apply_spatial_analysis\n",
        "        self.spatial_analyzer = None\n",
        "        if self.apply_spatial_analysis:\n",
        "            if GEOPANDAS_AVAILABLE and 'SpatialAnalyzer' in globals() and SpatialAnalyzer is not None:\n",
        "                try:\n",
        "                    self.spatial_analyzer = SpatialAnalyzer(random_state=self.random_state, verbose=self.verbose)\n",
        "                    self.logger.info(\"SpatialAnalyzer initialized.\")\n",
        "                except NameError:\n",
        "                     self.logger.warning(\"SpatialAnalyzer class not found despite GEOPANDAS_AVAILABLE=True.\")\n",
        "                     self.apply_spatial_analysis = False\n",
        "            else:\n",
        "                self.logger.warning(\"Spatial analysis requested but SpatialAnalyzer class or GeoPandas not available. Spatial steps will be skipped.\")\n",
        "                self.apply_spatial_analysis = False\n",
        "\n",
        "        # State / Results storage\n",
        "        self.results_history: Dict[str, Any] = defaultdict(dict)\n",
        "        self.start_time_pipeline: Optional[float] = None\n",
        "        self.fitted_price_model: Optional[Any] = None\n",
        "        self.price_thresholds: Optional[List[float]] = None\n",
        "        self._numeric_features_for_analysis: Optional[List[str]] = None\n",
        "        self._max_samples_for_core_analysis: int = max_samples_limit\n",
        "        self._bootstrap_params: Dict = {'run': False, 'n_samples': 0}\n",
        "\n",
        "        self.logger.info(\"AnalysisOrchestrator initialized.\")\n",
        "        if self.verbose: check_available_methods(verbose=self.verbose) # Call utility if available\n",
        "\n",
        "    def _check_timeout(self) -> bool:\n",
        "        \"\"\"Checks if the pipeline execution time has exceeded the maximum allowed.\"\"\"\n",
        "        if self.max_total_time_seconds and self.start_time_pipeline:\n",
        "            elapsed = time.time() - self.start_time_pipeline\n",
        "            if elapsed > self.max_total_time_seconds:\n",
        "                self.logger.warning(f\"Pipeline timeout ({self.max_total_time_seconds}s) reached (Elapsed: {elapsed:.1f}s). Stopping current stage.\")\n",
        "                return True\n",
        "        return False\n",
        "\n",
        "    def _get_numeric_data_and_features(self,\n",
        "                                       df: pd.DataFrame,\n",
        "                                       numeric_features_list: Optional[List[str]],\n",
        "                                       context: str) -> Tuple[Optional[np.ndarray], Optional[List[str]]]:\n",
        "        \"\"\"Extracts and cleans numeric data based on provided list or defaults.\"\"\"\n",
        "        self.logger.info(f\"Preparing numeric data for {context}...\")\n",
        "        target_cols = numeric_features_list\n",
        "        using_default_selection = False\n",
        "\n",
        "        if not target_cols:\n",
        "            self.logger.debug(f\"No specific numeric features for {context}. Selecting defaults (all numeric excluding price/common IDs).\")\n",
        "            using_default_selection = True\n",
        "            price_related = [self.price_col, self.log_price_col, getattr(self.preprocessor, 'price_outlier_col', 'is_price_outlier')]\n",
        "            # Expanded list of potential ID-like column name fragments\n",
        "            potential_id_fragments = [\n",
        "                'borough', 'block', 'lot', 'zip', 'cd', 'council', 'dist', 'health', 'police', 'precinct',\n",
        "                'year', 'date', 'bbl', 'coord', 'lat', 'lon', 'index', 'rowid', 'id', 'gid', 'fips', 'tract', 'bldg', 'unit',\n",
        "                'census', 'nta', 'community', 'xcoord', 'ycoord', # Added more common ones\n",
        "            ]\n",
        "            target_cols = df.select_dtypes(include=np.number).columns.tolist()\n",
        "            target_cols = [\n",
        "                c for c in target_cols if c not in price_related and\n",
        "                not any(pid_frag in c.lower() for pid_frag in potential_id_fragments)\n",
        "            ]\n",
        "            self.logger.debug(f\"Default numeric features for {context}: {target_cols}\")\n",
        "\n",
        "        # Filter target_cols to those existing and numeric in df\n",
        "        existing_cols = [col for col in target_cols if col in df.columns]\n",
        "        final_numeric_features = []\n",
        "        for col in existing_cols:\n",
        "            if pd.api.types.is_numeric_dtype(df[col]):\n",
        "                final_numeric_features.append(col)\n",
        "            elif not using_default_selection: # Warn only if a specific requested col is non-numeric\n",
        "                self.logger.warning(f\"Requested column '{col}' for {context} is not numeric. Skipping.\")\n",
        "        if len(existing_cols) < len(target_cols or []):\n",
        "             missing = set(target_cols or []) - set(existing_cols)\n",
        "             self.logger.warning(f\"Requested columns missing from DataFrame for {context}: {missing}\")\n",
        "\n",
        "\n",
        "        if not final_numeric_features:\n",
        "            self.logger.error(f\"No valid numeric features found for {context}.\")\n",
        "            return None, None\n",
        "\n",
        "        X_numeric = df[final_numeric_features].copy().astype(np.float64)\n",
        "\n",
        "        # Robust NaN/Inf Cleaning\n",
        "        if not np.all(np.isfinite(X_numeric.values)):\n",
        "             cols_with_issues_before = X_numeric.columns[~np.all(np.isfinite(X_numeric.values), axis=0)].tolist()\n",
        "             self.logger.debug(f\"NaN/Inf detected in {len(cols_with_issues_before)} features for {context} before imputation: {cols_with_issues_before}\")\n",
        "             for col in final_numeric_features: # Iterate over the list of names\n",
        "                 col_data_series = X_numeric[col]\n",
        "                 if not np.all(np.isfinite(col_data_series.values)): # Check if this specific column needs cleaning\n",
        "                     finite_vals = col_data_series[np.isfinite(col_data_series.values)]\n",
        "                     if not finite_vals.empty:\n",
        "                         median_val = finite_vals.median()\n",
        "                         posinf_fill = finite_vals.max() if len(finite_vals) > 1 else median_val\n",
        "                         neginf_fill = finite_vals.min() if len(finite_vals) > 1 else median_val\n",
        "                         fill_val = median_val # For NaNs\n",
        "                     else:\n",
        "                         fill_val, posinf_fill, neginf_fill = 0.0, 0.0, 0.0\n",
        "                         self.logger.warning(f\"Column '{col}' for {context} is all NaN/Inf. Imputing with 0.\")\n",
        "                     X_numeric[col] = np.nan_to_num(col_data_series.values, nan=fill_val, posinf=posinf_fill, neginf=neginf_fill)\n",
        "\n",
        "        if not np.all(np.isfinite(X_numeric.values)): # Final check\n",
        "            cols_still_bad = X_numeric.columns[~np.all(np.isfinite(X_numeric.values), axis=0)].tolist()\n",
        "            self.logger.error(f\"Non-finite values persist after cleaning in {context} for cols: {cols_still_bad}. This may cause downstream errors.\")\n",
        "            return None, None\n",
        "\n",
        "        if X_numeric.empty:\n",
        "            self.logger.error(f\"No numeric data remaining after processing for {context}.\")\n",
        "            return None, None\n",
        "\n",
        "        self.logger.info(f\"Numeric data for {context} prepared. Shape: {X_numeric.shape}, Features: {final_numeric_features}\")\n",
        "        return X_numeric.values, final_numeric_features\n",
        "\n",
        "    def analyze_sample(self,\n",
        "                       df_main_sample: pd.DataFrame,\n",
        "                       numeric_features_for_analysis: Optional[List[str]],\n",
        "                       full_analysis: bool = True,\n",
        "                       # Dynamic K parameters for ensemble\n",
        "                       ensemble_k_range: Sequence[int] = range(2, 12),\n",
        "                       ensemble_k_metric: str = 'silhouette',\n",
        "                       # DP-GMM config (now uses K* from find_optimal_k)\n",
        "                       # dp_gmm_max_components_init_heuristic_factor: float = 2.0, # No longer needed\n",
        "                       # dp_gmm_max_components_init_cap: int = 15, # No longer needed\n",
        "                       dp_gmm_weight_prior: float = 0.1, # Keep other BGM params\n",
        "                       # ICA config\n",
        "                       use_ica_for_clustering: bool = False,\n",
        "                       # Flag for bootstrap context\n",
        "                       run_ensemble_k_selection_in_bootstrap: bool = False,\n",
        "                       bootstrap_iter: Optional[int] = None) -> Dict[str, Any]:\n",
        "        \"\"\"Performs core analysis (Dim Est, Clustering) on a sample DataFrame.\"\"\"\n",
        "        sample_label = f\"Iter {bootstrap_iter}\" if bootstrap_iter is not None else \"Main\"\n",
        "        self.logger.info(f\"\\n--- Analyzing Sample (Shape: {df_main_sample.shape}, Label: {sample_label}, FullAnalysis: {full_analysis}) ---\")\n",
        "        sample_artifacts: Dict[str, Any] = {'status': 'started', 'error': None, 'params': locals()}\n",
        "\n",
        "        if self.clusterer is None or self.dim_reducer is None:\n",
        "            msg = \"ClusteringSuite or DimensionalityReducer not initialized.\"\n",
        "            self.logger.error(msg); sample_artifacts.update({'status': 'error', 'error': msg}); return sample_artifacts\n",
        "\n",
        "        try:\n",
        "            X_numeric_values, actual_features = self._get_numeric_data_and_features(\n",
        "                df_main_sample, numeric_features_for_analysis, f\"sample_analysis_{sample_label}\"\n",
        "            )\n",
        "            if X_numeric_values is None or not actual_features: raise ValueError(\"No valid numeric data.\")\n",
        "\n",
        "            # Remove constant columns before scaling\n",
        "            variances = np.var(X_numeric_values, axis=0)\n",
        "            constant_cols_mask = variances < 1e-12\n",
        "            if np.any(constant_cols_mask):\n",
        "                constant_feature_names = [name for i, name in enumerate(actual_features) if constant_cols_mask[i]]\n",
        "                self.logger.warning(f\"{sample_label}: Removing constant columns before scaling: {constant_feature_names}\")\n",
        "                X_numeric_values = X_numeric_values[:, ~constant_cols_mask]\n",
        "                actual_features = [name for i, name in enumerate(actual_features) if not constant_cols_mask[i]]\n",
        "                if X_numeric_values.shape[1] == 0: raise ValueError(\"All features became constant.\")\n",
        "\n",
        "            scaler = StandardScaler()\n",
        "            X_scaled = scaler.fit_transform(X_numeric_values)\n",
        "            if np.any(~np.isfinite(X_scaled)): # Safeguard\n",
        "                self.logger.error(f\"{sample_label}: NaNs/Infs DETECTED in X_scaled AFTER scaling. Imputing with 0.\");\n",
        "                X_scaled = np.nan_to_num(X_scaled, nan=0.0, posinf=0.0, neginf=0.0)\n",
        "            sample_artifacts.update({'core_features_used': actual_features, 'scaler_for_core_features': scaler, 'X_scaled_shape': X_scaled.shape})\n",
        "\n",
        "            # --- Intrinsic Dimension ---\n",
        "            if self._check_timeout(): raise TimeoutError(\"Timeout before Intrinsic Dimension\")\n",
        "            id_results = self.dim_reducer.estimate_intrinsic_dimension(X_scaled)\n",
        "            sample_artifacts['intrinsic_dimension_results'] = id_results\n",
        "            latent_dim_suggestion = id_results.get('median', max(1, X_scaled.shape[1] // 2))\n",
        "            latent_dim_suggestion = max(1, min(int(latent_dim_suggestion), X_scaled.shape[1])) # Ensure valid dim\n",
        "            sample_artifacts['latent_dim_suggestion'] = latent_dim_suggestion\n",
        "            self.logger.info(f\"{sample_label}: Suggested latent dim = {latent_dim_suggestion} (Method: {id_results.get('method', 'N/A')})\")\n",
        "\n",
        "            # --- Optional ICA ---\n",
        "            data_for_clustering = X_scaled\n",
        "            sample_artifacts['clustering_input_type'] = 'original_scaled_features'\n",
        "            if self.ica_module and use_ica_for_clustering and latent_dim_suggestion > 0 and X_scaled.shape[1] >= latent_dim_suggestion:\n",
        "                if self._check_timeout(): raise TimeoutError(\"Timeout during ICA\")\n",
        "                ica_n_components = min(latent_dim_suggestion, X_scaled.shape[1])\n",
        "                # Assuming ica_module.run_ica returns a dict now\n",
        "                ica_run_results = self.ica_module.run_ica(X_scaled, n_components=ica_n_components, feature_names=actual_features)\n",
        "                sample_artifacts.update(ica_run_results) # Merge results\n",
        "                ica_sources = ica_run_results.get('ica_sources')\n",
        "                if ica_sources is not None and ica_sources.shape[1] > 0:\n",
        "                    ica_scaler = StandardScaler(); data_for_clustering = ica_scaler.fit_transform(ica_sources)\n",
        "                    sample_artifacts.update({'scaler_for_ica_sources': ica_scaler, 'clustering_input_type': 'scaled_ica_sources'})\n",
        "                    self.logger.info(f\"{sample_label}: Using {data_for_clustering.shape[1]} scaled ICA sources for clustering.\")\n",
        "                else: self.logger.warning(f\"{sample_label}: ICA sources not generated or invalid. Using original scaled features.\")\n",
        "\n",
        "            if data_for_clustering.shape[0] <= 1: raise ValueError(f\"Insufficient samples ({data_for_clustering.shape[0]}) for clustering.\")\n",
        "\n",
        "            # --- Dynamic K Selection (Common for DP-GMM target K and Ensemble) ---\n",
        "            optimal_k_dynamic = 0\n",
        "            # Run K-selection if it's the main analysis OR if specifically requested for bootstrap\n",
        "            if full_analysis or run_ensemble_k_selection_in_bootstrap:\n",
        "                if self._check_timeout(): raise TimeoutError(\"Timeout during Dynamic K Selection\")\n",
        "                self.logger.info(f\"{sample_label}: Determining optimal K (Metric: {ensemble_k_metric})...\")\n",
        "                n_samples_for_k_search = data_for_clustering.shape[0]\n",
        "                # Make k_range more robust against large N\n",
        "                k_list = list(ensemble_k_range)\n",
        "                max_k_possible = n_samples_for_k_search - 1 if n_samples_for_k_search > 1 else 1\n",
        "                valid_k_range = [k for k in k_list if 1 < k <= max_k_possible]\n",
        "\n",
        "                if not valid_k_range:\n",
        "                    self.logger.warning(f\"{sample_label}: Dynamic K range {k_list} invalid for sample size {n_samples_for_k_search}. Using fallback K.\")\n",
        "                    # Simple fallback: latent dim or 2\n",
        "                    optimal_k_dynamic = min(max(2, latent_dim_suggestion), max_k_possible) if max_k_possible >=2 else 1\n",
        "                else:\n",
        "                    optimal_k_dynamic = self.clusterer.find_optimal_k(\n",
        "                        X_processed=data_for_clustering, k_range=valid_k_range, metric=ensemble_k_metric\n",
        "                    )\n",
        "                sample_artifacts['optimal_k_dynamic'] = optimal_k_dynamic # Store the determined K*\n",
        "                self.logger.info(f\"{sample_label}: Optimal K dynamically determined as {optimal_k_dynamic}\")\n",
        "            else:\n",
        "                # If not running K selection (e.g., bootstrap without the flag), use a reasonable default or placeholder\n",
        "                 optimal_k_dynamic = min(max(2, latent_dim_suggestion), data_for_clustering.shape[0] - 1 if data_for_clustering.shape[0] > 1 else 1)\n",
        "                 optimal_k_dynamic = max(1, optimal_k_dynamic) # Ensure >= 1\n",
        "                 sample_artifacts['optimal_k_dynamic'] = optimal_k_dynamic # Store fallback K used\n",
        "                 self.logger.info(f\"{sample_label}: Using default/latent K = {optimal_k_dynamic} (K-selection skipped).\")\n",
        "\n",
        "\n",
        "            # --- DP-GMM Clustering (Using optimal_k_dynamic as target) ---\n",
        "            if self._check_timeout(): raise TimeoutError(\"Timeout before DP-GMM\")\n",
        "            # Use the dynamically determined K as the target/upper bound for DP-GMM\n",
        "            target_k_for_dp = optimal_k_dynamic\n",
        "\n",
        "            dp_gmm_model, dp_gmm_info = self.clusterer.run_dp_gmm(\n",
        "                data_for_clustering,\n",
        "                target_n_components=target_k_for_dp,\n",
        "                weight_concentration_prior=dp_gmm_weight_prior # Pass other relevant params\n",
        "            )\n",
        "            sample_artifacts.update({'dp_gmm_model_info_dict': dp_gmm_info})\n",
        "            for key in ['effective_components', 'labels', 'model_converged', 'bic_score', 'aic_score']:\n",
        "                artifact_key = f\"dp_gmm_{key}\" if key != 'effective_components' else 'dp_gmm_n_components'\n",
        "                sample_artifacts[artifact_key] = dp_gmm_info.get(key)\n",
        "            self.logger.info(f\"{sample_label}: DP-GMM effective components = {sample_artifacts.get('dp_gmm_n_components', 'N/A')} (Target K={target_k_for_dp})\")\n",
        "\n",
        "\n",
        "            # --- Full Ensemble Clustering (only if full_analysis is True) ---\n",
        "            # Use the *same* optimal_k_dynamic determined earlier\n",
        "            if full_analysis and optimal_k_dynamic > 0:\n",
        "                if self._check_timeout(): raise TimeoutError(\"Timeout during Ensemble Clustering\")\n",
        "                self.logger.info(f\"{sample_label}: Running Ensemble Clustering with K={optimal_k_dynamic}...\")\n",
        "                # Pass the *data* used for clustering (could be original or ICA)\n",
        "                ensemble_partitions, ensemble_consensus_info = self.clusterer.run_ensemble_clustering(\n",
        "                    X=data_for_clustering, # Use the potentially ICA-transformed data\n",
        "                    n_clusters=optimal_k_dynamic\n",
        "                )\n",
        "                sample_artifacts['ensemble_partitions_shapes'] = {k:v.shape for k,v in ensemble_partitions.items() if hasattr(v, 'shape')}\n",
        "                sample_artifacts['ensemble_consensus_summary'] = {k:v for k,v in ensemble_consensus_info.items() if k not in ['co_association_matrix', 'final_labels']}\n",
        "                sample_artifacts['ensemble_labels'] = ensemble_consensus_info.get('final_labels')\n",
        "                sample_artifacts['ensemble_n_clusters'] = ensemble_consensus_info.get('n_clusters_found', optimal_k_dynamic)\n",
        "                sample_artifacts['ensemble_silhouette_score'] = ensemble_consensus_info.get('silhouette_score')\n",
        "            elif full_analysis:\n",
        "                self.logger.warning(f\"{sample_label}: Skipping ensemble clustering as optimal K was {optimal_k_dynamic}.\")\n",
        "                sample_artifacts['ensemble_clustering_status'] = f'skipped_k_{optimal_k_dynamic}'\n",
        "\n",
        "\n",
        "            sample_artifacts['status'] = 'completed'\n",
        "        except TimeoutError as te:\n",
        "            sample_artifacts.update({'status': 'timeout', 'error': f'Timeout: {str(te)}'})\n",
        "        except Exception as e:\n",
        "            self.logger.error(f\"Error during analyze_sample for {sample_label}: {e}\", exc_info=True)\n",
        "            sample_artifacts.update({'status': 'error', 'error': str(e)})\n",
        "\n",
        "        self.logger.info(f\"--- Sample Analysis ({sample_label}) Completed: {sample_artifacts['status']} ---\")\n",
        "        return sample_artifacts\n",
        "\n",
        "    def full_analysis_pipeline(self,\n",
        "                               df_input: pd.DataFrame,\n",
        "                               numeric_features_for_analysis: Optional[List[str]] = None,\n",
        "                               transform_numeric_features: Optional[List[str]] = None, # Which features to consider for skew transform\n",
        "                               # Configs passed to DataPreprocessor.fit_transform:\n",
        "                               transform_type: Optional[str] = None, # Specific transform to use if auto_transform_skewed=False\n",
        "                               auto_transform_skewed: bool = True,   # Auto-detect and transform skewed features?\n",
        "                               run_bootstrap: bool = True,\n",
        "                               n_bootstrap_samples: int = 30,\n",
        "                               max_samples_for_core_analysis: int = 20000,\n",
        "                               # Configs passed to analyze_sample -> find_optimal_k:\n",
        "                               ensemble_k_range: Sequence[int] = range(2, 12),\n",
        "                               ensemble_k_metric: str = 'silhouette',\n",
        "                               # Spatial params:\n",
        "                               spatial_x_coord: Optional[str] = None,\n",
        "                               spatial_y_coord: Optional[str] = None,\n",
        "                               spatial_grid_size: Union[int, float] = 250,\n",
        "                               spatial_crs: Optional[str] = None\n",
        "                               ) -> Dict[str, Any]:\n",
        "        \"\"\"Runs the full pipeline: Preprocessing, Sampling, Analysis, Bootstrap, Summary.\"\"\"\n",
        "        self.start_time_pipeline = time.time()\n",
        "        self.logger.info(f\"====== Starting Full Analysis Pipeline (Bootstrap: {run_bootstrap}, MaxSamplesCore: {max_samples_for_core_analysis}) ======\")\n",
        "        # Store key parameters used for this run\n",
        "        self._numeric_features_for_analysis = numeric_features_for_analysis\n",
        "        self._max_samples_for_core_analysis = max_samples_for_core_analysis\n",
        "        self._bootstrap_params = {'run': run_bootstrap, 'n_samples': n_bootstrap_samples}\n",
        "        self.results_history = defaultdict(dict) # Reset history\n",
        "\n",
        "        overall_results: Dict[str, Any] = {\n",
        "            \"summary_report\": {}, \"main_artifacts\": {}, \"bootstrap_summary\": {},\n",
        "            \"status\": \"started\", \"error\": None\n",
        "        }\n",
        "        main_artifacts: Dict[str, Any] = {} # To store results of main analysis run\n",
        "\n",
        "        try:\n",
        "            # --- Stage 1: Preprocessing ---\n",
        "            self.logger.info(\"--- Stage 1: Preprocessing ---\")\n",
        "            # Determine which features DataPreprocessor should attempt to transform based on skew\n",
        "            features_to_consider_for_transform = transform_numeric_features if transform_numeric_features is not None else numeric_features_for_analysis\n",
        "\n",
        "            # Use orchestrator's stored config for price percentiles and thresholds\n",
        "            df_processed = self.preprocessor.fit_transform(\n",
        "                df=df_input.copy(),\n",
        "                numeric_features_to_process=features_to_consider_for_transform, # Features DP should potentially transform/scale\n",
        "                auto_transform_skewed=auto_transform_skewed,\n",
        "                # Pass specific overrides relevant to DP's fit_transform method\n",
        "                min_price_percentile=self.min_price_percentile,\n",
        "                max_price_percentile=self.max_price_percentile,\n",
        "                # Pass transform config only if auto_transform is enabled or a specific type is given\n",
        "                default_transform_type_override=(self.default_transform_type if auto_transform_skewed else transform_type),\n",
        "                transform_skew_threshold_override=(self.transform_skew_threshold if auto_transform_skewed else None),\n",
        "                scale_numeric=True, # Assume we always scale numeric features after potential transform\n",
        "                create_bbl=True   # Assume BBL creation is desired\n",
        "            )\n",
        "            if df_processed is None or df_processed.empty: raise ValueError(\"Preprocessing returned empty DataFrame.\")\n",
        "            self.results_history['preprocessing']['shape_after'] = df_processed.shape\n",
        "            self.results_history['preprocessing']['price_stats'] = getattr(self.preprocessor, 'price_analysis_stats', {})\n",
        "            overall_results['price_analysis_stats'] = getattr(self.preprocessor, 'price_analysis_stats', {})\n",
        "\n",
        "\n",
        "            # --- Stage 2: Price Modeling and Banding ---\n",
        "            self.logger.info(\"--- Stage 2: Price Modeling and Banding ---\")\n",
        "            self.fitted_price_model = None; self.price_thresholds = None\n",
        "            if STUDENT_T_AVAILABLE and 'StudentTMixture' in globals() and StudentTMixture and self.log_price_col in df_processed.columns:\n",
        "                price_data = df_processed[self.log_price_col].dropna().values.reshape(-1, 1)\n",
        "                if len(price_data) > 10: # Only fit if enough data\n",
        "                    try:\n",
        "                        # Use a reasonable n_components, e.g., 3 for low/medium/high\n",
        "                        price_model = StudentTMixture(n_components=3, random_state=self.random_state)\n",
        "                        if price_data.shape[0] > price_model.n_components: # type: ignore\n",
        "                            price_model.fit(price_data); self.fitted_price_model = price_model\n",
        "                            if price_model.converged_ and price_model.means_ is not None and len(price_model.means_.flatten()) > 1:\n",
        "                                sorted_means = sorted(price_model.means_.flatten())\n",
        "                                self.price_thresholds = [(sorted_means[i] + sorted_means[i+1])/2.0 for i in range(len(sorted_means)-1)]\n",
        "                            self.results_history['price_model_fit'] = {'converged': price_model.converged_, 'n_iter': price_model.n_iter_}\n",
        "                    except Exception as e: self.logger.error(f\"Price modeling (StudentT) failed: {e}\", exc_info=False)\n",
        "\n",
        "            if self.price_thresholds is None: # Fallback to quantiles\n",
        "                self.logger.warning(\"Using quantile thresholds for price banding.\")\n",
        "                if self.log_price_col in df_processed.columns:\n",
        "                     log_price_data_clean = df_processed[self.log_price_col].dropna()\n",
        "                     if len(log_price_data_clean) > 0 :\n",
        "                        self.price_thresholds = [log_price_data_clean.quantile(q) for q in [0.33, 0.67]]\n",
        "                        self.price_thresholds = [t for t in self.price_thresholds if pd.notna(t)] # Remove potential NaNs\n",
        "                     else: self.price_thresholds = []\n",
        "                else: self.price_thresholds = []\n",
        "\n",
        "            # Apply banding based on determined thresholds\n",
        "            if self.price_thresholds and self.log_price_col in df_processed.columns:\n",
        "                # Ensure bins are unique and sorted, including -inf and +inf\n",
        "                bins = sorted(list(set([-np.inf] + self.price_thresholds + [np.inf])));\n",
        "                if len(bins) >= 2 :\n",
        "                    # Adjust labels based on number of bins\n",
        "                    num_bands = len(bins) - 1\n",
        "                    if num_bands == 1: labels = ['medium']\n",
        "                    elif num_bands == 2: labels = ['low', 'high']\n",
        "                    elif num_bands == 3: labels = ['low','medium','high']\n",
        "                    else: labels = [f'band_{i}' for i in range(num_bands)]\n",
        "\n",
        "                    try:\n",
        "                        df_processed['price_band'] = pd.cut(df_processed[self.log_price_col], bins=bins, labels=labels, right=False, duplicates='drop')\n",
        "                        # Handle potential NaNs created by cut if values fall outside explicit bins (shouldn't happen with inf)\n",
        "                        # or if log_price_col itself was NaN\n",
        "                        df_processed['price_band'] = df_processed['price_band'].cat.add_categories('unknown').fillna('unknown')\n",
        "\n",
        "                    except ValueError as cut_err:\n",
        "                        self.logger.error(f\"Error pd.cut price_band: {cut_err}. Assigning 'unknown'.\");\n",
        "                        df_processed['price_band'] = 'unknown'\n",
        "                    self.results_history['price_banding'] = {'bands_used': labels, 'thresholds_log': self.price_thresholds, 'bins': bins}\n",
        "                else: # Only one bin possible (-inf, inf)\n",
        "                     df_processed['price_band'] = 'medium'\n",
        "                     self.results_history['price_banding'] = {'bands_used': ['medium'], 'thresholds_log': self.price_thresholds, 'bins': bins}\n",
        "            else: # No thresholds determined or no log price col\n",
        "                df_processed['price_band'] = 'unknown' # Use 'unknown' if banding fails\n",
        "                self.results_history['price_banding'] = {'status': 'skipped_no_thresholds'}\n",
        "            self.logger.info(f\"Price banding applied. Bands found: {df_processed['price_band'].unique()}\")\n",
        "\n",
        "\n",
        "            # --- Stage 3: Rebalancing Bands ---\n",
        "            self.logger.info(\"--- Stage 3: Rebalancing Bands ---\")\n",
        "            # If you need weights:\n",
        "            df_rebalanced, sample_weights = self.sampler.rebalance_price_bands(\n",
        "                df_processed,\n",
        "                price_band_col='price_band',\n",
        "                log_price_col=self.log_price_col,\n",
        "                price_col=self.price_col,\n",
        "                target_balance='median',\n",
        "                downsample_overrepresented=True,\n",
        "                return_weights=True\n",
        "            )\n",
        "            self.results_history['rebalancing']['shape_after'] = df_rebalanced.shape\n",
        "            self.logger.info(f\"Shape after rebalancing: {df_rebalanced.shape}\")\n",
        "\n",
        "            # --- Stage 4: Spatial Gridding (Optional) ---\n",
        "            self.logger.info(\"--- Stage 4: Spatial Gridding (Optional) ---\")\n",
        "            df_final_for_sampling = df_rebalanced\n",
        "            if self.apply_spatial_analysis and self.spatial_analyzer:\n",
        "                if spatial_x_coord and spatial_y_coord and spatial_x_coord in df_final_for_sampling.columns and spatial_y_coord in df_final_for_sampling.columns:\n",
        "                    if self._check_timeout(): raise TimeoutError(\"Timeout during Spatial Gridding\")\n",
        "                    df_final_for_sampling = self.spatial_analyzer.assign_grid_indices(df_rebalanced, spatial_x_coord, spatial_y_coord, spatial_grid_size, spatial_crs)\n",
        "                    self.results_history['spatial_gridding'] = getattr(self.spatial_analyzer, 'grid_params', {'status':'completed'})\n",
        "                else:\n",
        "                    self.logger.warning(\"Spatial analysis requested but coordinate columns invalid/missing. Skipping gridding.\")\n",
        "                    self.results_history['spatial_gridding'] = {'status':'skipped_missing_coords'}\n",
        "            else: self.results_history['spatial_gridding'] = {'status':'skipped_disabled'}\n",
        "            self.results_history['df_fully_processed_before_sampling_shape'] = df_final_for_sampling.shape\n",
        "\n",
        "\n",
        "            # --- Stage 5: Downsampling for Core Analysis ---\n",
        "            self.logger.info(f\"--- Stage 5: Downsampling for Core Analysis (Target: {max_samples_for_core_analysis}) ---\")\n",
        "            actual_core_sample_size = min(max_samples_for_core_analysis, len(df_final_for_sampling))\n",
        "            if actual_core_sample_size <= 0: raise ValueError(\"Dataset empty before core downsampling.\")\n",
        "            # Use stratified sampling if price_band exists and has >1 category, else random\n",
        "            stratify_col_core = 'price_band' if 'price_band' in df_final_for_sampling.columns and df_final_for_sampling['price_band'].nunique() > 1 else None\n",
        "            self.logger.info(f\"Downsampling to {actual_core_sample_size} for core analysis. Stratify by: {stratify_col_core}\")\n",
        "            df_main_sample, _ = self.sampler.downsample(\n",
        "                df_final_for_sampling, n_samples=actual_core_sample_size,\n",
        "                stratify_col=stratify_col_core,\n",
        "                replace=False\n",
        "            )\n",
        "            if df_main_sample.empty: raise ValueError(\"Main sample is empty after downsampling.\")\n",
        "            self.results_history['main_sampling'] = {'sample_shape': df_main_sample.shape, 'stratified_by': stratify_col_core}\n",
        "\n",
        "\n",
        "            # --- Stage 6: Main Sample Analysis ---\n",
        "            self.logger.info(\"--- Stage 6: Main Sample Analysis ---\")\n",
        "            if self._check_timeout(): raise TimeoutError(\"Timeout before Main Analysis\")\n",
        "            main_artifacts = self.analyze_sample(\n",
        "                df_main_sample=df_main_sample,\n",
        "                numeric_features_for_analysis=self._numeric_features_for_analysis,\n",
        "                full_analysis=True, # Run full ensemble clustering on main sample\n",
        "                ensemble_k_range=ensemble_k_range,\n",
        "                ensemble_k_metric=ensemble_k_metric,\n",
        "                # Don't need run_ensemble_k_selection_in_bootstrap=True here, it's not bootstrap\n",
        "            )\n",
        "            # Add price stats from preprocessing to main artifacts for context\n",
        "            main_artifacts['price_analysis_stats'] = self.results_history.get('preprocessing',{}).get('price_stats',{})\n",
        "            overall_results[\"main_artifacts\"] = main_artifacts\n",
        "            if main_artifacts.get(\"status\") != 'completed': raise ValueError(f\"Main analysis failed: {main_artifacts.get('error', 'Unknown error')}\")\n",
        "\n",
        "\n",
        "            # --- Stage 7: Bootstrap Analysis ---\n",
        "            bootstrap_aggregated_stats = {}\n",
        "            if run_bootstrap and not self._check_timeout():\n",
        "                n_boot = min(n_bootstrap_samples, self.max_bootstraps) # Use the smaller value\n",
        "                self.logger.info(f\"--- Stage 7: Bootstrap Analysis ({n_boot} samples) ---\")\n",
        "                # Bootstrap sample size should match the main analysis sample size\n",
        "                bootstrap_sample_size_for_iter = actual_core_sample_size\n",
        "                if bootstrap_sample_size_for_iter <= 0:\n",
        "                     self.logger.warning(\"Main sample size 0, cannot run bootstrap.\")\n",
        "                     self.results_history['bootstrap_analysis_status'] = 'skipped_zero_main_sample'\n",
        "                else:\n",
        "                    # Stratify bootstrap sampling if possible, using the same column as main sampling\n",
        "                    stratify_col_boot = stratify_col_core\n",
        "\n",
        "                    # Define the function to run in parallel for one bootstrap iteration\n",
        "                    def run_one_bootstrap_iteration(iter_num: int):\n",
        "                        # Check timeout at start of worker task\n",
        "                        if self._check_timeout(): return {'status': 'timeout', 'error': 'timeout in bootstrap worker', 'iter': iter_num}\n",
        "                        try:\n",
        "                            # Sample WITH REPLACEMENT from the *final_for_sampling* pool\n",
        "                            boot_df, _ = self.sampler.downsample(\n",
        "                                df_final_for_sampling,\n",
        "                                n_samples=bootstrap_sample_size_for_iter,\n",
        "                                stratify_col=stratify_col_boot,\n",
        "                                replace=True # <<< Bootstrap uses replacement\n",
        "                            )\n",
        "                            if boot_df.empty: return {'status': 'error', 'error': 'empty_bootstrap_sample', 'iter': iter_num}\n",
        "\n",
        "                            # Analyze the bootstrap sample:\n",
        "                            # full_analysis=False (don't need full ensemble results, just K)\n",
        "                            # run_ensemble_k_selection_in_bootstrap=True (need optimal K metric)\n",
        "                            return self.analyze_sample(\n",
        "                                df_main_sample=boot_df,\n",
        "                                numeric_features_for_analysis=self._numeric_features_for_analysis,\n",
        "                                full_analysis=False, # Don't need full ensemble clustering results\n",
        "                                run_ensemble_k_selection_in_bootstrap=True, # DO run K-selection\n",
        "                                ensemble_k_range=ensemble_k_range, # Use same range/metric\n",
        "                                ensemble_k_metric=ensemble_k_metric,\n",
        "                                bootstrap_iter=iter_num\n",
        "                            )\n",
        "                        except Exception as boot_e_inner:\n",
        "                            # Log error from worker if possible\n",
        "                            worker_logger = get_logger(f\"{self.__class__.__name__}.BootstrapWorker\", verbose=self.verbose)\n",
        "                            worker_logger.error(f\"Error in bootstrap worker {iter_num}: {boot_e_inner}\", exc_info=False)\n",
        "                            return {'status': 'error', 'error': f'Exception in worker {iter_num}: {str(boot_e_inner)}', 'iter': iter_num}\n",
        "\n",
        "                    # Run bootstrap iterations in parallel\n",
        "                    try:\n",
        "                        self.logger.info(f\"Running {n_boot} bootstrap iterations using {self.n_jobs} jobs...\")\n",
        "                        parallel_verbose_level = 5 if self.verbose else 0\n",
        "                        bootstrap_results_list = Parallel(n_jobs=self.n_jobs, backend='loky', verbose=parallel_verbose_level)(\n",
        "                            delayed(run_one_bootstrap_iteration)(i) for i in range(n_boot)\n",
        "                        )\n",
        "\n",
        "                        # Process results\n",
        "                        valid_bootstrap_results = [r for r in bootstrap_results_list if isinstance(r, dict) and r.get('status') == 'completed']\n",
        "                        failed_runs = [r for r in bootstrap_results_list if not (isinstance(r, dict) and r.get('status') == 'completed')]\n",
        "\n",
        "                        self.results_history['bootstrap_runs_raw'] = bootstrap_results_list # Store raw results for debugging\n",
        "\n",
        "                        if valid_bootstrap_results:\n",
        "                            bootstrap_aggregated_stats = self._aggregate_bootstrap_results(valid_bootstrap_results)\n",
        "                            self.results_history['bootstrap_summary'] = bootstrap_aggregated_stats\n",
        "                            # Refine main artifacts using bootstrap stats (e.g., update suggested K)\n",
        "                            self._refine_artifacts_with_bootstrap(main_artifacts, bootstrap_aggregated_stats)\n",
        "                            overall_results[\"main_artifacts\"] = main_artifacts # Update with refined versions\n",
        "                            self.logger.info(f\"Completed {len(valid_bootstrap_results)} successful bootstrap runs.\")\n",
        "                        else:\n",
        "                            self.logger.warning(\"No successful bootstrap runs completed.\")\n",
        "\n",
        "                        if failed_runs:\n",
        "                            self.logger.warning(f\"{len(failed_runs)} bootstrap runs failed or timed out.\")\n",
        "                            # Log details of first few failures\n",
        "                            for i, failed_run in enumerate(failed_runs[:min(3, len(failed_runs))]):\n",
        "                                self.logger.warning(f\"  Failure Detail {i+1}: {failed_run}\")\n",
        "\n",
        "                        self.results_history['bootstrap_analysis_status'] = f'completed_{len(valid_bootstrap_results)}_ok_{len(failed_runs)}_failed'\n",
        "\n",
        "                    except Exception as e_par:\n",
        "                        self.logger.error(f\"Bootstrap parallel execution failed: {e_par}\", exc_info=True)\n",
        "                        self.results_history['bootstrap_analysis_status'] = f'error_{str(e_par)}'\n",
        "\n",
        "            else: # Bootstrap skipped\n",
        "                 self.results_history['bootstrap_analysis_status'] = 'skipped_due_to_config_or_timeout'\n",
        "\n",
        "\n",
        "            # --- Stage 8: Final Summary ---\n",
        "            self.logger.info(\"--- Stage 8: Generating Final Summary ---\")\n",
        "            final_summary = self._generate_pipeline_summary(main_artifacts, bootstrap_aggregated_stats)\n",
        "            overall_results.update({\n",
        "                \"summary_report\": final_summary,\n",
        "                \"status\": \"completed\",\n",
        "                # expose the fully-processed, rebalanced DataFrame for downstream steps\n",
        "                \"df_processed_and_rebalanced\": df_final_for_sampling\n",
        "            })\n",
        "\n",
        "        except TimeoutError as te_pipe:\n",
        "             self.logger.error(f\"Pipeline stopped due to timeout: {te_pipe}\", exc_info=False)\n",
        "             overall_results.update({\"error\": f\"Timeout Error: {te_pipe}\", \"status\": \"timeout\"})\n",
        "             overall_results[\"main_artifacts\"] = main_artifacts # Include whatever main artifacts were generated\n",
        "\n",
        "        except Exception as e_pipe:\n",
        "            self.logger.error(f\"FATAL Error in full analysis pipeline: {e_pipe}\", exc_info=True)\n",
        "            overall_results.update({\"error\": str(e_pipe), \"status\": \"error\"})\n",
        "            overall_results[\"main_artifacts\"] = main_artifacts # Include partial artifacts if available\n",
        "\n",
        "        total_time_taken = time.time() - self.start_time_pipeline\n",
        "        self.logger.info(f\"====== Pipeline Completed in {total_time_taken:.2f}s (Status: {overall_results['status']}) ======\")\n",
        "        overall_results['total_time_seconds'] = total_time_taken\n",
        "        overall_results['detailed_results_history'] = dict(self.results_history) # Convert defaultdict for saving\n",
        "\n",
        "        return overall_results\n",
        "\n",
        "    def _aggregate_bootstrap_results(self, bootstrap_results: List[Dict]) -> Dict:\n",
        "        \"\"\"Aggregates statistics from multiple bootstrap runs.\"\"\"\n",
        "        if not bootstrap_results: return {}\n",
        "        self.logger.info(f\"Aggregating results from {len(bootstrap_results)} bootstrap runs...\")\n",
        "        agg_stats = defaultdict(list) # Use defaultdict to easily append\n",
        "\n",
        "        # Keys to aggregate and their artifact key name\n",
        "        keys_to_aggregate = {\n",
        "            'latent_dim_suggestion': 'intrinsic_dim',\n",
        "            'dp_gmm_n_components': 'dp_gmm_components',\n",
        "            'optimal_k_dynamic': 'ensemble_optimal_k', # K chosen by find_optimal_k\n",
        "            # Add other numeric metrics if needed, e.g., 'ensemble_silhouette_score' if calculated\n",
        "        }\n",
        "\n",
        "        for run_result in bootstrap_results:\n",
        "            for artifact_key, base_key in keys_to_aggregate.items():\n",
        "                 value = run_result.get(artifact_key)\n",
        "                 if value is not None and isinstance(value, (int, float)) and np.isfinite(value):\n",
        "                     agg_stats[base_key].append(value)\n",
        "\n",
        "        final_agg_stats = {}\n",
        "        for base_key, values in agg_stats.items():\n",
        "             if values:\n",
        "                 final_agg_stats[f'{base_key}_mean'] = float(np.mean(values))\n",
        "                 final_agg_stats[f'{base_key}_median'] = float(np.median(values))\n",
        "                 final_agg_stats[f'{base_key}_std'] = float(np.std(values))\n",
        "                 q25, q75 = np.percentile(values, [25, 75])\n",
        "                 final_agg_stats[f'{base_key}_q25'] = float(q25)\n",
        "                 final_agg_stats[f'{base_key}_q75'] = float(q75)\n",
        "                 final_agg_stats[f'{base_key}_count'] = len(values)\n",
        "\n",
        "        self.logger.info(f\"Aggregated bootstrap stats: {final_agg_stats}\")\n",
        "        return final_agg_stats\n",
        "\n",
        "    def _refine_artifacts_with_bootstrap(self, main_artifacts: Dict, bootstrap_stats: Dict):\n",
        "        \"\"\"Refines estimates in main_artifacts using stable results from bootstrap.\"\"\"\n",
        "        if not bootstrap_stats or not main_artifacts: return\n",
        "        self.logger.info(\"Refining main artifacts using bootstrap statistics...\")\n",
        "\n",
        "        # Refine Intrinsic Dimension\n",
        "        if 'intrinsic_dim_median' in bootstrap_stats:\n",
        "            new_dim = int(round(bootstrap_stats['intrinsic_dim_median'])); new_dim = max(1, new_dim)\n",
        "            old_dim = main_artifacts.get('latent_dim_suggestion')\n",
        "            if old_dim is None or new_dim != old_dim:\n",
        "                self.logger.info(f\"Refining latent_dim_suggestion from {old_dim} to {new_dim} based on bootstrap median.\")\n",
        "                main_artifacts['latent_dim_suggestion'] = new_dim\n",
        "                if 'intrinsic_dimension_results' in main_artifacts:\n",
        "                    main_artifacts['intrinsic_dimension_results']['median_refined_bootstrap'] = new_dim\n",
        "\n",
        "        # Refine DP-GMM K suggestion (although DP-GMM finds effective K anyway)\n",
        "        if 'dp_gmm_components_median' in bootstrap_stats:\n",
        "            new_k = int(round(bootstrap_stats['dp_gmm_components_median'])); new_k = max(1, new_k)\n",
        "            old_k = main_artifacts.get('dp_gmm_n_components') # This was the *effective* K from main run\n",
        "            self.logger.info(f\"Bootstrap median for DP-GMM effective components is {new_k} (main run found {old_k}). Storing for reference.\")\n",
        "            main_artifacts['dp_gmm_n_components_bootstrap_median'] = new_k\n",
        "            # We might choose NOT to override the main run's effective K, as DP-GMM adapts.\n",
        "\n",
        "        # Refine Optimal K for Ensemble\n",
        "        if 'ensemble_optimal_k_median' in bootstrap_stats:\n",
        "            new_ensemble_k = int(round(bootstrap_stats['ensemble_optimal_k_median'])); new_ensemble_k = max(1, new_ensemble_k)\n",
        "            old_ensemble_k = main_artifacts.get('optimal_k_dynamic') # K chosen in main run\n",
        "            if old_ensemble_k is None or new_ensemble_k != old_ensemble_k:\n",
        "                self.logger.info(f\"Refining optimal_k_dynamic from {old_ensemble_k} to {new_ensemble_k} based on bootstrap median.\")\n",
        "                main_artifacts['optimal_k_dynamic'] = new_ensemble_k # Override the dynamically chosen K\n",
        "            main_artifacts['optimal_k_ensemble_bootstrap_median'] = new_ensemble_k # Store for reference regardless\n",
        "\n",
        "    def _generate_pipeline_summary(self, main_artifacts: Dict, bootstrap_stats: Dict) -> Dict:\n",
        "        \"\"\"Generates a concise summary report from main and bootstrap results.\"\"\"\n",
        "        summary = {}\n",
        "        if not main_artifacts: return {\"status\": \"error - no main artifacts\"}\n",
        "\n",
        "        # Intrinsic Dimension Summary\n",
        "        id_res = main_artifacts.get('intrinsic_dimension_results', {})\n",
        "        # Use the refined dimension if available\n",
        "        dim_est = main_artifacts.get('latent_dim_suggestion', id_res.get('median', 'N/A'))\n",
        "        dim_method = id_res.get('method', 'N/A')\n",
        "        dim_std_bs = bootstrap_stats.get('intrinsic_dim_std')\n",
        "        summary['intrinsic_dimension'] = {\n",
        "            'point_estimate': dim_est, 'method': dim_method,\n",
        "            'bootstrap_median': bootstrap_stats.get('intrinsic_dim_median'),\n",
        "            'bootstrap_std': f\"{dim_std_bs:.2f}\" if dim_std_bs is not None else 'N/A',\n",
        "            'bootstrap_q25': bootstrap_stats.get('intrinsic_dim_q25'),\n",
        "            'bootstrap_q75': bootstrap_stats.get('intrinsic_dim_q75'),\n",
        "        }\n",
        "\n",
        "        # Clustering Summary\n",
        "        dp_k_main = main_artifacts.get('dp_gmm_n_components', 'N/A') # Effective K from main run\n",
        "        dp_k_bs_median = bootstrap_stats.get('dp_gmm_components_median') # Median effective K from bootstrap\n",
        "        dp_k_bs_std = bootstrap_stats.get('dp_gmm_components_std')\n",
        "\n",
        "        ens_k_chosen_main = main_artifacts.get('optimal_k_dynamic', 'N/A') # K chosen by find_optimal_k (possibly refined)\n",
        "        ens_k_final_run = main_artifacts.get('ensemble_n_clusters') # Actual K after ensemble ran (if different)\n",
        "        ens_k_bs_median = bootstrap_stats.get('ensemble_optimal_k_median') # Median K chosen by find_optimal_k in bootstrap\n",
        "        ens_k_bs_std = bootstrap_stats.get('ensemble_optimal_k_std')\n",
        "\n",
        "        summary['clustering'] = {\n",
        "            'dp_gmm_effective_k_main': dp_k_main,\n",
        "            'dp_gmm_bootstrap_median_k': f\"{dp_k_bs_median:.1f}\" if dp_k_bs_median is not None else 'N/A',\n",
        "            'dp_gmm_bootstrap_std_k': f\"{dp_k_bs_std:.2f}\" if dp_k_bs_std is not None else 'N/A',\n",
        "            'ensemble_k_chosen': ens_k_chosen_main,\n",
        "            'ensemble_k_final_clusters': ens_k_final_run if ens_k_final_run != ens_k_chosen_main else None, # Only show if different\n",
        "            'ensemble_k_bootstrap_median': f\"{ens_k_bs_median:.1f}\" if ens_k_bs_median is not None else 'N/A',\n",
        "            'ensemble_k_bootstrap_std': f\"{ens_k_bs_std:.2f}\" if ens_k_bs_std is not None else 'N/A',\n",
        "            'ensemble_silhouette_main_run': f\"{main_artifacts.get('ensemble_silhouette_score'):.3f}\" if isinstance(main_artifacts.get('ensemble_silhouette_score'), float) else 'N/A',\n",
        "        }\n",
        "\n",
        "        summary['vae_recommendations'] = self._generate_vae_recommendations(main_artifacts) # Uses refined K\n",
        "        summary['price_analysis'] = main_artifacts.get('price_analysis_stats', {})\n",
        "        summary['core_features_used'] = main_artifacts.get('core_features_used', [])[:15] # Show first few\n",
        "        summary['clustering_input_type'] = main_artifacts.get('clustering_input_type', 'unknown')\n",
        "\n",
        "        return summary\n",
        "\n",
        "    def _generate_vae_recommendations(self, main_artifacts: Dict) -> List[str]:\n",
        "        \"\"\"Generates VAE architecture and prior recommendations based on analysis.\"\"\"\n",
        "        recs = []\n",
        "        # Use the potentially refined latent dimension suggestion\n",
        "        latent_dim = main_artifacts.get('latent_dim_suggestion', 2)\n",
        "        latent_dim = max(1, int(latent_dim)) # Ensure positive integer\n",
        "\n",
        "        # Use the potentially refined ensemble K as the primary indicator for mixtures\n",
        "        n_mixtures = main_artifacts.get('optimal_k_dynamic', # Refined ensemble K\n",
        "                                     main_artifacts.get('dp_gmm_n_components', 1)) # Fallback to DP-GMM effective K\n",
        "        n_mixtures = int(round(max(1, n_mixtures if isinstance(n_mixtures, (int, float)) else 1)))\n",
        "\n",
        "        # Estimate input dim for VAE (based on features used for clustering + potentially price)\n",
        "        core_features = main_artifacts.get('core_features_used', [])\n",
        "        input_dim_vae_placeholder = len(core_features)\n",
        "        # Assume log_price might be added as an input feature to VAE if not already in core_features\n",
        "        if self.log_price_col not in core_features:\n",
        "             input_dim_vae_placeholder += 1\n",
        "        input_dim_vae_placeholder = max(1, input_dim_vae_placeholder)\n",
        "\n",
        "\n",
        "        # Simple heuristic for hidden layers\n",
        "        h1_dim = max(latent_dim * 2, input_dim_vae_placeholder // 3, latent_dim + 5) # Ensure hidden > latent\n",
        "        h1_dim = min(h1_dim, input_dim_vae_placeholder * 2) # Avoid excessive expansion\n",
        "        h1_dim = min(h1_dim, 512) # Cap max hidden size\n",
        "        h1_dim = max(h1_dim, latent_dim + 1 if input_dim_vae_placeholder > latent_dim else latent_dim) # Must be >= latent_dim\n",
        "\n",
        "        h2_dim = max(int(h1_dim * 0.5), latent_dim + 1 if h1_dim > latent_dim + 1 else latent_dim) # Second layer smaller\n",
        "\n",
        "        hidden_layers_enc = []\n",
        "        if h1_dim > latent_dim : hidden_layers_enc.append(h1_dim)\n",
        "        if h2_dim > latent_dim and h2_dim < h1_dim: hidden_layers_enc.append(h2_dim)\n",
        "\n",
        "        arch_str_enc = str(hidden_layers_enc) if hidden_layers_enc else \"[direct]\"\n",
        "        hidden_layers_dec = hidden_layers_enc[::-1]\n",
        "        arch_str_dec = str(hidden_layers_dec) if hidden_layers_dec else \"[direct]\"\n",
        "\n",
        "        recs.append(f\"Suggest VAE Arch: Input({input_dim_vae_placeholder}) -> Enc{arch_str_enc} -> Latent({latent_dim}) -> Dec{arch_str_dec} -> Output({input_dim_vae_placeholder}).\")\n",
        "\n",
        "        # Prior Type Recommendation\n",
        "        prior_type_rec = \"Gaussian\"\n",
        "        price_heavy_tailed = main_artifacts.get('price_analysis_stats', {}).get('log_heavy_tailed', False)\n",
        "        # Add check for latent kurtosis if ICA stats are collected and stored under 'latent_dim_stats'\n",
        "        latent_heavy_tailed = False\n",
        "        if 'latent_dim_stats' in main_artifacts:\n",
        "             kurtosis_values = [v.get('kurtosis',0) for k,v in main_artifacts.get('latent_dim_stats', {}).items() if isinstance(v, dict)]\n",
        "             if kurtosis_values: latent_heavy_tailed = any(k > 1.0 for k in kurtosis_values)\n",
        "\n",
        "        if price_heavy_tailed or latent_heavy_tailed: prior_type_rec = \"Student-T\"\n",
        "\n",
        "        final_prior_rec = prior_type_rec + (\" Mixture\" if n_mixtures > 1 else \"\")\n",
        "        recs.append(f\"Use {final_prior_rec} prior (K={n_mixtures}, df~4 if Student-T) based on data/latent distributions.\")\n",
        "\n",
        "        # General VAE training recommendations\n",
        "        recs.append(\"Consider learnable per-feature reconstruction variance (Gaussian NLL) or Huber loss for robustness.\")\n",
        "        recs.append(\"Use KL annealing (e.g., ~20-50 epochs or 25-50% of total training duration).\")\n",
        "        recs.append(\"Employ gradient clipping (e.g., max_norm=1.0) and LR scheduling (e.g., ReduceLROnPlateau).\")\n",
        "        if input_dim_vae_placeholder > 50 : recs.append(\"For high input dims, consider more aggressive bottleneck or deeper encoder/decoder.\")\n",
        "\n",
        "        return recs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eTHWqChXyVf-"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import warnings\n",
        "import time\n",
        "import logging\n",
        "from typing import List, Optional, Dict, Tuple, Union, Any\n",
        "from collections import defaultdict\n",
        "\n",
        "from sklearn.preprocessing import StandardScaler, KBinsDiscretizer\n",
        "from sklearn.neighbors import kneighbors_graph\n",
        "from sklearn.feature_selection import mutual_info_classif, f_classif, VarianceThreshold\n",
        "from scipy.sparse import diags as sparse_diags\n",
        "from scipy.sparse import csgraph, csr_matrix # csgraph is used for graph laplacian\n",
        "from sklearn.utils import resample # For bootstrapping\n",
        "from sklearn.cluster import KMeans\n",
        "\n",
        "# --- Global FAISS availability check ---\n",
        "FAISS_AVAILABLE = False\n",
        "faiss = None # Define faiss as None initially\n",
        "try:\n",
        "    import faiss\n",
        "    FAISS_AVAILABLE = True\n",
        "except ImportError:\n",
        "    pass\n",
        "\n",
        "\n",
        "class FeatureSelectorConfig:\n",
        "    \"\"\"Configuration class for FeatureSelector.\"\"\"\n",
        "    def __init__(self,\n",
        "                 # General thresholds\n",
        "                 variance_threshold: float = 1e-4,\n",
        "                 correlation_threshold: Optional[float] = 0.9, # Set to None or >= 1.0 to disable\n",
        "                 methods_to_run: List[str] = ['variance', 'laplacian', 'pseudo_label'],\n",
        "                 # Coverage-based k selection (New/Updated as per spec)\n",
        "                 coverage_threshold: float = 0.9, # Target cumulative importance\n",
        "                 # Bootstrap stability selection (New/Updated as per spec)\n",
        "                 bootstrap_B: int = 30, # Number of bootstrap iterations\n",
        "                 bootstrap_k_per_iter: int = 15, # k for the .select() call within each bootstrap\n",
        "                 bootstrap_selection_freq_threshold: Optional[float] = 0.7, # e.g., 0.7 (70% of bootstraps)\n",
        "                 # Laplacian score parameters\n",
        "                 laplacian_n_neighbors: int = 15,\n",
        "                 laplacian_sample_size: Optional[int] = 50000,\n",
        "                 laplacian_weight_mode: str = 'heat', # 'heat' or 'connectivity'\n",
        "                 laplacian_gamma: float = 1.0, # For 'heat' mode (sigma for Gaussian kernel)\n",
        "                 laplacian_use_faiss: bool = True, # Attempt to use FAISS if available\n",
        "                 laplacian_prefer_gpu: bool = False, # If using FAISS, attempt to use GPU\n",
        "                 # Pseudo-label ensemble parameters\n",
        "                 pseudo_label_n_clusters_list: List[int] = [3, 5, 8],\n",
        "                 pseudo_label_methods: List[str] = ['kmeans'], # e.g. ['kmeans', 'agglomerative'] from ClusteringSuite\n",
        "                 pseudo_label_scoring: str = 'f_classif', # 'f_classif' or 'mi' (mutual information)\n",
        "                 pseudo_label_sample_size: Optional[int] = 50000,\n",
        "                 use_clustering_suite_for_pseudo: bool = True # Attempt to use ClusteringSuite if available\n",
        "                 ):\n",
        "        self.variance_threshold = variance_threshold\n",
        "        self.correlation_threshold = correlation_threshold\n",
        "        self.methods_to_run = methods_to_run\n",
        "        self.coverage_threshold = coverage_threshold\n",
        "        self.bootstrap_B = bootstrap_B\n",
        "        self.bootstrap_k_per_iter = bootstrap_k_per_iter\n",
        "        self.bootstrap_selection_freq_threshold = bootstrap_selection_freq_threshold\n",
        "        self.laplacian_n_neighbors = laplacian_n_neighbors\n",
        "        self.laplacian_sample_size = laplacian_sample_size\n",
        "        self.laplacian_weight_mode = laplacian_weight_mode\n",
        "        self.laplacian_gamma = laplacian_gamma\n",
        "        self.laplacian_use_faiss = laplacian_use_faiss\n",
        "        self.laplacian_prefer_gpu = laplacian_prefer_gpu\n",
        "        self.pseudo_label_n_clusters_list = pseudo_label_n_clusters_list\n",
        "        self.pseudo_label_methods = pseudo_label_methods\n",
        "        self.pseudo_label_scoring = pseudo_label_scoring\n",
        "        self.pseudo_label_sample_size = pseudo_label_sample_size\n",
        "        self.use_clustering_suite_for_pseudo = use_clustering_suite_for_pseudo\n",
        "\n",
        "        # Basic validation\n",
        "        if not (0 <= self.coverage_threshold <= 1):\n",
        "            raise ValueError(\"coverage_threshold must be between 0 and 1.\")\n",
        "        if self.correlation_threshold is not None and \\\n",
        "           not (0 <= self.correlation_threshold <= 1) and \\\n",
        "           self.correlation_threshold < 0: # Note: if threshold is >1 it's disabled, so only check <0 here\n",
        "            raise ValueError(\"correlation_threshold must be non-negative or None.\")\n",
        "        if self.bootstrap_selection_freq_threshold is not None and \\\n",
        "           not (0 <= self.bootstrap_selection_freq_threshold <= 1):\n",
        "            raise ValueError(\"bootstrap_selection_freq_threshold must be between 0 and 1.\")\n",
        "        if self.bootstrap_k_per_iter <= 0:\n",
        "            raise ValueError(\"bootstrap_k_per_iter must be positive.\")\n",
        "        if self.bootstrap_B <= 0:\n",
        "            raise ValueError(\"bootstrap_B must be positive.\")\n",
        "\n",
        "\n",
        "class FeatureSelector:\n",
        "    \"\"\"\n",
        "    Selects relevant numeric features using a combination of methods.\n",
        "    Supports fixed-k, coverage-based, or stability-based selection.\n",
        "    \"\"\"\n",
        "    def __init__(self,\n",
        "                 df: pd.DataFrame,\n",
        "                 id_cols: Optional[List[str]] = None,\n",
        "                 target_col: Optional[str] = None,\n",
        "                 random_state: Optional[int] = None,\n",
        "                 verbose: bool = True,\n",
        "                 n_jobs: int = -1,\n",
        "                 config: Optional[FeatureSelectorConfig] = None):\n",
        "\n",
        "        if df.empty:\n",
        "            raise ValueError(\"Input DataFrame `df` cannot be empty.\")\n",
        "\n",
        "        self.df = df.copy() # Internal working copy\n",
        "        self.original_df_for_bootstrap = df # Keep original for bootstrap, DO NOT MODIFY\n",
        "        self.id_cols = id_cols if id_cols is not None else []\n",
        "        self.target_col = target_col\n",
        "        self.random_state = random_state\n",
        "        self.verbose = verbose # Verbosity of this specific FeatureSelector instance\n",
        "        self.n_jobs = n_jobs\n",
        "        self.logger = get_logger(self.__class__.__name__, verbose)\n",
        "        self.config = config if config is not None else FeatureSelectorConfig()\n",
        "\n",
        "        self.feature_scores: Dict[str, Dict[str, float]] = defaultdict(dict)\n",
        "        self.numeric_cols: List[str] = []\n",
        "        self.categorical_cols: List[str] = []\n",
        "        self.datetime_cols: List[str] = []\n",
        "\n",
        "        self._identify_column_types()\n",
        "        self._initial_clean() # Clean self.df\n",
        "\n",
        "        self.logger.info(\n",
        "            f\"FeatureSelector initialized. Numeric: {len(self.numeric_cols)}, \"\n",
        "            f\"Categorical: {len(self.categorical_cols)}, Datetime: {len(self.datetime_cols)}.\"\n",
        "        )\n",
        "        self.logger.debug(f\"Config: {vars(self.config)}\")\n",
        "\n",
        "    def _identify_column_types(self):\n",
        "        \"\"\"Identifies numeric, categorical, and datetime columns, excluding ID/target.\"\"\"\n",
        "        self.logger.info(\"Identifying column types...\")\n",
        "        potential_feature_cols = [\n",
        "            col for col in self.df.columns\n",
        "            if col not in self.id_cols and col != self.target_col\n",
        "        ]\n",
        "        potential_feature_cols = [col for col in potential_feature_cols if col in self.df.columns]\n",
        "\n",
        "        self.numeric_cols = self.df[potential_feature_cols].select_dtypes(include=np.number).columns.tolist()\n",
        "        self.categorical_cols = self.df[potential_feature_cols].select_dtypes(include=['category', 'object', 'string']).columns.tolist()\n",
        "        self.datetime_cols = self.df[potential_feature_cols].select_dtypes(include=['datetime', 'datetimetz', 'timedelta']).columns.tolist()\n",
        "\n",
        "        self.logger.info(\n",
        "            f\"Identified - Numeric: {len(self.numeric_cols)}, \"\n",
        "            f\"Categorical: {len(self.categorical_cols)}, Datetime: {len(self.datetime_cols)}\"\n",
        "        )\n",
        "        self.logger.debug(f\"Numeric cols found: {self.numeric_cols}\")\n",
        "\n",
        "    def _initial_clean(self):\n",
        "        \"\"\"Initial NaN/Inf imputation on the internal DataFrame copy for numeric columns.\"\"\"\n",
        "        self.logger.info(\"Performing initial NaN/Inf cleaning for numeric columns on internal df copy...\")\n",
        "        num_nan_imputed = 0\n",
        "        num_inf_imputed = 0\n",
        "\n",
        "        if self.numeric_cols:\n",
        "            for col in self.numeric_cols:\n",
        "                col_data = self.df[col]\n",
        "                if col_data.isnull().any():\n",
        "                    median_val = col_data.median()\n",
        "                    fill_nan_val = median_val if pd.notna(median_val) else 0.0\n",
        "                    self.df[col].fillna(fill_nan_val, inplace=True)\n",
        "                    num_nan_imputed += 1\n",
        "\n",
        "                if np.isinf(self.df[col].values).any():\n",
        "                    finite_vals = self.df[col][np.isfinite(self.df[col])]\n",
        "                    fill_inf_val = finite_vals.median() if not finite_vals.empty else 0.0\n",
        "                    self.df[col].replace([np.inf, -np.inf], fill_inf_val, inplace=True)\n",
        "                    num_inf_imputed += 1\n",
        "\n",
        "            if num_nan_imputed > 0:\n",
        "                self.logger.info(f\"Imputed NaNs in {num_nan_imputed} numeric columns using median.\")\n",
        "            if num_inf_imputed > 0:\n",
        "                self.logger.info(f\"Replaced Infs in {num_inf_imputed} numeric columns using median of finite values.\")\n",
        "        else:\n",
        "            self.logger.info(\"No numeric columns to clean initially.\")\n",
        "\n",
        "    def filter_by_variance(self) -> List[str]:\n",
        "        \"\"\"Filters features based on variance threshold.\"\"\"\n",
        "        threshold = self.config.variance_threshold\n",
        "        self.logger.info(f\"Filtering features with variance < {threshold}...\")\n",
        "\n",
        "        if not self.numeric_cols:\n",
        "            self.logger.warning(\"No numeric columns for variance filtering.\")\n",
        "            return []\n",
        "\n",
        "        df_numeric = self.df[self.numeric_cols].copy()\n",
        "        cleaned_cols_count = 0\n",
        "        for col in df_numeric.columns:\n",
        "            if not np.all(np.isfinite(df_numeric[col])):\n",
        "                finite_vals = df_numeric[col][np.isfinite(df_numeric[col])]\n",
        "                fill_val = finite_vals.median() if not finite_vals.empty else 0.0\n",
        "                df_numeric[col] = np.nan_to_num(df_numeric[col], nan=fill_val, posinf=fill_val, neginf=fill_val)\n",
        "                cleaned_cols_count += 1\n",
        "        if cleaned_cols_count > 0:\n",
        "            self.logger.debug(f\"Ensured finite values in {cleaned_cols_count} columns before variance check.\")\n",
        "\n",
        "        selector = VarianceThreshold(threshold=threshold)\n",
        "        try:\n",
        "            selector.fit(df_numeric)\n",
        "            selected_mask = selector.get_support()\n",
        "            selected_cols = df_numeric.columns[selected_mask].tolist()\n",
        "            removed_cols = df_numeric.columns[~selected_mask].tolist()\n",
        "\n",
        "            removed_log_msg = f\"Removed {len(removed_cols)} low-variance columns\"\n",
        "            if removed_cols:\n",
        "                removed_log_msg += f\": {removed_cols[:10]}{'...' if len(removed_cols) > 10 else ''}\"\n",
        "            self.logger.info(removed_log_msg)\n",
        "            self.logger.info(f\"{len(selected_cols)} numeric columns remain after variance filtering.\")\n",
        "\n",
        "            # Update internal state AND store scores for selected features\n",
        "            # self.numeric_cols = selected_cols # This should not update self.numeric_cols but return the list\n",
        "                                             # The caller (.select) will manage current_numeric_features\n",
        "            valid_variances = selector.variances_[selected_mask]\n",
        "            for i, col_name in enumerate(selected_cols):\n",
        "                self.feature_scores[col_name]['variance'] = valid_variances[i]\n",
        "            return selected_cols\n",
        "\n",
        "        except Exception as e:\n",
        "            self.logger.error(f\"Error during variance filtering: {e}. Returning pre-filter numeric columns.\", exc_info=True)\n",
        "            # Fallback to numeric columns that were present before this filter attempt\n",
        "            return [col for col in self.df.columns if col in self.numeric_cols]\n",
        "\n",
        "\n",
        "    def _build_knn_graph(self,\n",
        "                         X_scaled: np.ndarray,\n",
        "                         n_neighbors: int,\n",
        "                         use_faiss_cfg: bool,\n",
        "                         prefer_gpu_cfg: bool) -> Optional[csr_matrix]:\n",
        "        \"\"\"Builds k-NN graph using FAISS or sklearn.\"\"\"\n",
        "        n_samples = X_scaled.shape[0]\n",
        "\n",
        "        if n_samples <= n_neighbors:\n",
        "            n_neighbors = max(1, n_samples - 1)\n",
        "            self.logger.warning(f\"Reduced k-NN k to {n_neighbors} because n_samples ({n_samples}) <= k.\")\n",
        "        if n_neighbors == 0 and n_samples > 0:\n",
        "            n_neighbors = 1\n",
        "        if n_neighbors <= 0 or n_samples == 0:\n",
        "            self.logger.error(\"Cannot build k-NN graph with n_samples=0 or k<=0.\")\n",
        "            return None\n",
        "\n",
        "        t_start = time.time()\n",
        "\n",
        "        if use_faiss_cfg and FAISS_AVAILABLE and faiss and X_scaled.ndim == 2 and X_scaled.shape[1] > 0:\n",
        "            self.logger.info(f\"Building k-NN graph using FAISS (k={n_neighbors}).\")\n",
        "            index_cpu = None\n",
        "            index_gpu = None\n",
        "            gpu_resources = None\n",
        "            index_to_use = None\n",
        "            try:\n",
        "                d = X_scaled.shape[1]\n",
        "                X_scaled_32 = X_scaled.astype('float32')\n",
        "                index_cpu = faiss.IndexFlatL2(d)\n",
        "                index_to_use = index_cpu\n",
        "\n",
        "                if prefer_gpu_cfg and hasattr(faiss, 'StandardGpuResources'):\n",
        "                    try:\n",
        "                        gpu_resources = faiss.StandardGpuResources()\n",
        "                        index_gpu = faiss.index_cpu_to_gpu(gpu_resources, 0, index_cpu)\n",
        "                        index_to_use = index_gpu\n",
        "                        self.logger.info(\"FAISS is using GPU.\")\n",
        "                    except Exception as gpu_e:\n",
        "                        self.logger.warning(f\"FAISS GPU initialization failed: {gpu_e}. Falling back to CPU.\")\n",
        "                        index_to_use = index_cpu\n",
        "                elif prefer_gpu_cfg:\n",
        "                    self.logger.info(\"FAISS GPU preferred but StandardGpuResources not available. Using CPU.\")\n",
        "\n",
        "                index_to_use.add(X_scaled_32)\n",
        "                distances, indices = index_to_use.search(X_scaled_32, n_neighbors + 1)\n",
        "\n",
        "                rows = np.arange(n_samples).repeat(n_neighbors)\n",
        "                cols = indices[:, 1:].flatten()\n",
        "                data = distances[:, 1:].flatten()\n",
        "\n",
        "                valid_mask = (cols >= 0) & (cols < n_samples)\n",
        "                rows, cols, data = rows[valid_mask], cols[valid_mask], data[valid_mask]\n",
        "\n",
        "                if data.dtype.kind == 'f':\n",
        "                    data[~np.isfinite(data)] = np.finfo(data.dtype).max\n",
        "                else:\n",
        "                    data[~np.isfinite(data)] = 0\n",
        "\n",
        "                graph = csr_matrix((data, (rows, cols)), shape=(n_samples, n_samples))\n",
        "                self.logger.info(f\"FAISS k-NN graph built in {time.time() - t_start:.2f}s.\")\n",
        "                return graph\n",
        "            except Exception as faiss_e:\n",
        "                self.logger.warning(f\"FAISS k-NN graph construction failed: {faiss_e}. Falling back to sklearn.\", exc_info=True)\n",
        "            finally:\n",
        "                del index_cpu, index_gpu, gpu_resources, index_to_use\n",
        "\n",
        "        self.logger.info(f\"Building k-NN graph using sklearn (k={n_neighbors}).\")\n",
        "        try:\n",
        "            graph = kneighbors_graph(X_scaled,\n",
        "                                     n_neighbors=n_neighbors,\n",
        "                                     mode='distance',\n",
        "                                     include_self=False,\n",
        "                                     n_jobs=self.n_jobs,\n",
        "                                     algorithm='auto')\n",
        "            self.logger.info(f\"Sklearn k-NN graph built in {time.time() - t_start:.2f}s.\")\n",
        "            return graph\n",
        "        except Exception as sk_e:\n",
        "            self.logger.error(f\"Sklearn kneighbors_graph failed: {sk_e}\", exc_info=True)\n",
        "            return None\n",
        "\n",
        "    def calculate_laplacian_score(self, features: List[str]) -> None:\n",
        "        \"\"\"Calculates Laplacian score for unsupervised feature selection.\"\"\"\n",
        "        cfg = self.config\n",
        "        self.logger.info(\n",
        "            f\"Calculating Laplacian Scores (k={cfg.laplacian_n_neighbors}, \"\n",
        "            f\"sample_size={cfg.laplacian_sample_size}, weight_mode='{cfg.laplacian_weight_mode}')...\"\n",
        "        )\n",
        "\n",
        "        if not features:\n",
        "            self.logger.warning(\"No features provided for Laplacian Score calculation.\")\n",
        "            return\n",
        "\n",
        "        df_subset = self.df # Use the cleaned self.df\n",
        "        if cfg.laplacian_sample_size is not None and \\\n",
        "           0 < cfg.laplacian_sample_size < len(df_subset):\n",
        "            actual_sample_size = min(cfg.laplacian_sample_size, len(df_subset))\n",
        "            self.logger.info(f\"Subsampling to {actual_sample_size} rows for Laplacian Score.\")\n",
        "            if actual_sample_size <= cfg.laplacian_n_neighbors:\n",
        "                self.logger.warning(\n",
        "                    f\"Sample size ({actual_sample_size}) is less than or equal to \"\n",
        "                    f\"k_neighbors ({cfg.laplacian_n_neighbors}). This might lead to issues.\"\n",
        "                )\n",
        "            sampler_rng = np.random.default_rng(self.random_state)\n",
        "            indices_used = sampler_rng.choice(df_subset.index, size=actual_sample_size, replace=False)\n",
        "            df_subset = df_subset.loc[indices_used]\n",
        "\n",
        "        X = df_subset[features].values\n",
        "\n",
        "        if X.shape[0] <= 1 or X.shape[1] == 0:\n",
        "            self.logger.warning(f\"Not enough data (Shape: {X.shape}) for Laplacian score calculation. Skipping.\")\n",
        "            for col_name in features: self.feature_scores[col_name]['laplacian_score'] = np.inf\n",
        "            return\n",
        "\n",
        "        try:\n",
        "            scaler = StandardScaler()\n",
        "            X_scaled = scaler.fit_transform(X)\n",
        "            X_scaled = np.nan_to_num(X_scaled, nan=0.0)\n",
        "\n",
        "            distance_graph = self._build_knn_graph(\n",
        "                X_scaled,\n",
        "                cfg.laplacian_n_neighbors,\n",
        "                cfg.laplacian_use_faiss,\n",
        "                cfg.laplacian_prefer_gpu\n",
        "            )\n",
        "            if distance_graph is None:\n",
        "                self.logger.error(\"Failed to build k-NN graph for Laplacian Score. Scores will be set to inf.\")\n",
        "                for col_name in features: self.feature_scores[col_name]['laplacian_score'] = np.inf\n",
        "                return\n",
        "\n",
        "            if cfg.laplacian_weight_mode == 'heat':\n",
        "                W = distance_graph.copy()\n",
        "                W.data = np.exp(- (W.data**2) / (2 * cfg.laplacian_gamma**2))\n",
        "            else: # 'connectivity' mode\n",
        "                W = distance_graph.copy()\n",
        "                W.data[:] = 1.0\n",
        "\n",
        "            W = 0.5 * (W + W.T)\n",
        "\n",
        "            D_values = np.array(W.sum(axis=1)).flatten()\n",
        "            if np.all(np.abs(D_values) < 1e-9):\n",
        "                self.logger.error(\"Graph appears to be disconnected (all D_values are near zero). Laplacian scores will be inf.\")\n",
        "                for col_name in features: self.feature_scores[col_name]['laplacian_score'] = np.inf\n",
        "                return\n",
        "            D_sparse = sparse_diags(D_values)\n",
        "\n",
        "            L_graph = D_sparse - W\n",
        "\n",
        "            for i, col_name in enumerate(features):\n",
        "                f_r = X[:, i].astype(float)\n",
        "                if np.isnan(f_r).any(): # Should be handled by _initial_clean, but safeguard\n",
        "                    median_val_col = self.df[col_name].median()\n",
        "                    f_r = np.nan_to_num(f_r, nan=median_val_col if pd.notna(median_val_col) else 0.0)\n",
        "\n",
        "                sum_D = D_values.sum()\n",
        "                if abs(sum_D) < 1e-9:\n",
        "                    f_r_mean = np.mean(f_r)\n",
        "                else:\n",
        "                    f_r_mean = np.dot(f_r, D_values) / sum_D\n",
        "\n",
        "                f_r_tilde = f_r - f_r_mean\n",
        "\n",
        "                numerator = f_r_tilde.T @ L_graph @ f_r_tilde\n",
        "                denominator = f_r_tilde.T @ D_sparse @ f_r_tilde\n",
        "\n",
        "                if np.abs(denominator) < 1e-9:\n",
        "                    score = np.inf\n",
        "                else:\n",
        "                    score = numerator / denominator\n",
        "                self.feature_scores[col_name]['laplacian_score'] = score\n",
        "\n",
        "            self.logger.info(\"Laplacian Score calculation complete.\")\n",
        "\n",
        "        except Exception as e:\n",
        "            self.logger.error(f\"Error during Laplacian Score calculation: {e}\", exc_info=True)\n",
        "            for col_name in features:\n",
        "                self.feature_scores[col_name]['laplacian_score'] = np.inf\n",
        "\n",
        "\n",
        "    def calculate_ensemble_pseudo_label_scores(self, features: List[str]) -> None:\n",
        "        \"\"\"Calculates feature scores based on pseudo-labels from multiple clustering runs.\"\"\"\n",
        "        cfg = self.config\n",
        "        self.logger.info(\n",
        "            f\"Calculating Ensemble Pseudo-Label Scores (K-values: {cfg.pseudo_label_n_clusters_list}, \"\n",
        "            f\"Methods: {cfg.pseudo_label_methods}, Scoring: '{cfg.pseudo_label_scoring}', \"\n",
        "            f\"Sample: {cfg.pseudo_label_sample_size})...\"\n",
        "        )\n",
        "\n",
        "        if not features:\n",
        "            self.logger.warning(\"No features provided for pseudo-label score calculation.\")\n",
        "            return\n",
        "\n",
        "        df_subset = self.df # Use the cleaned self.df\n",
        "        if cfg.pseudo_label_sample_size is not None and \\\n",
        "           0 < cfg.pseudo_label_sample_size < len(df_subset):\n",
        "            actual_sample_size = min(cfg.pseudo_label_sample_size, len(df_subset))\n",
        "            self.logger.info(f\"Subsampling to {actual_sample_size} rows for pseudo-label generation.\")\n",
        "\n",
        "            max_k_val = max(cfg.pseudo_label_n_clusters_list if cfg.pseudo_label_n_clusters_list else [2])\n",
        "            if actual_sample_size <= max_k_val:\n",
        "                self.logger.warning(\n",
        "                    f\"Sample size ({actual_sample_size}) is less than or equal to max K ({max_k_val}). \"\n",
        "                    \"This may not be ideal for clustering. Consider increasing sample_size or decreasing K.\"\n",
        "                )\n",
        "            sampler_rng = np.random.default_rng(self.random_state)\n",
        "            indices_used = sampler_rng.choice(df_subset.index, size=actual_sample_size, replace=False)\n",
        "            df_subset = df_subset.loc[indices_used]\n",
        "\n",
        "        if df_subset.empty:\n",
        "            self.logger.error(\"Subsampled DataFrame for pseudo-labels is empty. Skipping.\")\n",
        "            return\n",
        "        X_for_scoring = df_subset[features].values\n",
        "        if X_for_scoring.shape[0] == 0:\n",
        "            self.logger.error(\"No data rows available for pseudo-label scoring after selecting features. Skipping.\")\n",
        "            return\n",
        "\n",
        "        max_k_val_check = max(cfg.pseudo_label_n_clusters_list if cfg.pseudo_label_n_clusters_list else [2])\n",
        "        if X_for_scoring.shape[0] <= max_k_val_check:\n",
        "            self.logger.warning(\n",
        "                f\"Number of samples ({X_for_scoring.shape[0]}) is less than or equal to the \"\n",
        "                f\"maximum k for clustering ({max_k_val_check}). Skipping pseudo-label scoring.\"\n",
        "            )\n",
        "            return\n",
        "\n",
        "        scaler = StandardScaler()\n",
        "        X_scaled = scaler.fit_transform(X_for_scoring)\n",
        "        X_scaled = np.nan_to_num(X_scaled, nan=0.0)\n",
        "\n",
        "        feature_scores_aggregate = defaultdict(list)\n",
        "\n",
        "        _use_suite = cfg.use_clustering_suite_for_pseudo and CLUSTERING_SUITE_AVAILABLE and ClusteringSuite is not None\n",
        "        clusterer_instance = None\n",
        "        if _use_suite:\n",
        "            try:\n",
        "                clusterer_instance = ClusteringSuite(\n",
        "                    random_state=self.random_state,\n",
        "                    verbose=False,\n",
        "                    n_jobs=self.n_jobs\n",
        "                )\n",
        "                self.logger.info(\"Using ClusteringSuite for pseudo-label generation.\")\n",
        "            except Exception as e_cs:\n",
        "                self.logger.error(f\"Failed to initialize ClusteringSuite: {e_cs}. Falling back to basic KMeans.\", exc_info=False)\n",
        "                _use_suite = False\n",
        "\n",
        "        for k_clusters in cfg.pseudo_label_n_clusters_list:\n",
        "            if k_clusters <= 1 or k_clusters >= X_scaled.shape[0]:\n",
        "                self.logger.warning(f\"Skipping k={k_clusters} for pseudo-labels (invalid for {X_scaled.shape[0]} samples).\")\n",
        "                continue\n",
        "\n",
        "            for method_name in cfg.pseudo_label_methods:\n",
        "                labels = None\n",
        "                self.logger.debug(f\"Generating pseudo-labels: Method='{method_name}', k={k_clusters}\")\n",
        "                try:\n",
        "                    if _use_suite and clusterer_instance:\n",
        "                        if method_name == 'kmeans' and hasattr(clusterer_instance, 'run_kmeans'):\n",
        "                            labels = clusterer_instance.run_kmeans(X_scaled, k_clusters)\n",
        "                        elif method_name == 'agglomerative' and hasattr(clusterer_instance, 'run_agglomerative'):\n",
        "                            labels = clusterer_instance.run_agglomerative(X_scaled, k_clusters)\n",
        "                        else:\n",
        "                            self.logger.warning(f\"Method '{method_name}' not available in configured ClusteringSuite. Skipping for k={k_clusters}.\")\n",
        "                            continue\n",
        "                    elif method_name == 'kmeans':\n",
        "                        kmeans = KMeans(n_clusters=k_clusters, random_state=self.random_state, n_init='auto')\n",
        "                        labels = kmeans.fit_predict(X_scaled)\n",
        "                    else:\n",
        "                        self.logger.warning(f\"Unsupported pseudo-label method '{method_name}' without ClusteringSuite. Skipping for k={k_clusters}.\")\n",
        "                        continue\n",
        "                except Exception as e_clust:\n",
        "                    self.logger.warning(f\"Clustering method '{method_name}' with k={k_clusters} failed: {e_clust}\", exc_info=False)\n",
        "                    continue\n",
        "\n",
        "                if labels is not None and len(np.unique(labels)) > 1:\n",
        "                    try:\n",
        "                        current_run_scores = []\n",
        "                        if cfg.pseudo_label_scoring == 'f_classif':\n",
        "                            f_values, _ = f_classif(X_scaled, labels)\n",
        "                            current_run_scores = np.nan_to_num(f_values, nan=0.0)\n",
        "                        elif cfg.pseudo_label_scoring == 'mi':\n",
        "                            n_bins = max(3, min(10, int(np.sqrt(X_scaled.shape[0]) / 5)))\n",
        "                            discretizer = KBinsDiscretizer(n_bins=n_bins, encode='ordinal', strategy='uniform', subsample=None)\n",
        "                            try:\n",
        "                                X_discretized = discretizer.fit_transform(X_scaled)\n",
        "                                current_run_scores = mutual_info_classif(X_discretized, labels, discrete_features=True, random_state=self.random_state)\n",
        "                            except ValueError as ve:\n",
        "                                self.logger.warning(f\"Discretization or MI calculation failed for k={k_clusters} (scoring: {cfg.pseudo_label_scoring}): {ve}. Setting scores to 0 for this run.\")\n",
        "                                current_run_scores = np.zeros(X_scaled.shape[1])\n",
        "                        else:\n",
        "                            self.logger.warning(f\"Unsupported pseudo_label_scoring '{cfg.pseudo_label_scoring}'. Skipping scoring for this run.\")\n",
        "                            continue\n",
        "\n",
        "                        for i, col_name in enumerate(features):\n",
        "                            feature_scores_aggregate[col_name].append(current_run_scores[i])\n",
        "                    except Exception as e_score:\n",
        "                        self.logger.warning(f\"Score calculation (type: {cfg.pseudo_label_scoring}) failed for k={k_clusters}, method={method_name}: {e_score}\", exc_info=False)\n",
        "                elif labels is not None:\n",
        "                    self.logger.warning(f\"Clustering with method '{method_name}', k={k_clusters} resulted in < 2 unique labels. Skipping scoring for this run.\")\n",
        "\n",
        "        score_key = f'ensemble_pseudo_label_{cfg.pseudo_label_scoring}'\n",
        "        for col_name in features:\n",
        "            scores_list = feature_scores_aggregate[col_name]\n",
        "            if scores_list:\n",
        "                avg_score = np.nanmean(scores_list)\n",
        "                self.feature_scores[col_name][score_key] = avg_score if np.isfinite(avg_score) else 0.0\n",
        "            else:\n",
        "                self.feature_scores[col_name][score_key] = 0.0\n",
        "                self.logger.debug(f\"No pseudo-label scores collected for feature '{col_name}'. Setting to 0.\")\n",
        "\n",
        "        self.logger.info(f\"Ensemble Pseudo-Label Score (using '{cfg.pseudo_label_scoring}') calculation complete.\")\n",
        "\n",
        "    def aggregate_scores(self) -> pd.DataFrame:\n",
        "        \"\"\"Aggregates scores from different methods into a single importance score and rank.\"\"\"\n",
        "        self.logger.info(\"Aggregating feature scores...\")\n",
        "\n",
        "        if not self.feature_scores:\n",
        "            self.logger.warning(\"No feature scores available to aggregate.\")\n",
        "            return pd.DataFrame()\n",
        "\n",
        "        all_features_with_scores = list(self.feature_scores.keys())\n",
        "        if not all_features_with_scores:\n",
        "             self.logger.warning(\"Feature scores dictionary is populated but has no feature keys.\")\n",
        "             return pd.DataFrame()\n",
        "\n",
        "        try:\n",
        "            score_df = pd.DataFrame.from_dict(self.feature_scores, orient='index')\n",
        "        except Exception as e:\n",
        "            self.logger.error(f\"Failed to create DataFrame from feature_scores: {e}. Feature scores: {self.feature_scores}\")\n",
        "            return pd.DataFrame()\n",
        "\n",
        "        if score_df.empty:\n",
        "            self.logger.warning(\"Created score_df is empty. No scores to aggregate.\")\n",
        "            return pd.DataFrame()\n",
        "\n",
        "        ascending_score_types = ['laplacian_score']\n",
        "        ranks_df = pd.DataFrame(index=score_df.index)\n",
        "        ranked_any_method = False\n",
        "\n",
        "        for score_name in score_df.columns:\n",
        "            if score_df[score_name].notna().any() and \\\n",
        "               np.isfinite(score_df[score_name].replace([np.inf, -np.inf], np.nan)).any():\n",
        "\n",
        "                series_to_rank = score_df[score_name].copy()\n",
        "                is_ascending = score_name in ascending_score_types\n",
        "\n",
        "                if is_ascending:\n",
        "                    series_to_rank.replace(np.inf, np.nan, inplace=True)\n",
        "                    # Smallest finite value or a large negative if all are inf/nan\n",
        "                    fill_neg_inf_val = (series_to_rank[np.isfinite(series_to_rank)].min() - 1\n",
        "                                        if np.isfinite(series_to_rank).any() else -1e12)\n",
        "                    series_to_rank.replace(-np.inf, fill_neg_inf_val, inplace=True)\n",
        "                else: # Descending rank\n",
        "                    series_to_rank.replace(-np.inf, np.nan, inplace=True)\n",
        "                    # Largest finite value or a large positive if all are inf/nan\n",
        "                    fill_pos_inf_val = (series_to_rank[np.isfinite(series_to_rank)].max() + 1\n",
        "                                         if np.isfinite(series_to_rank).any() else 1e12)\n",
        "                    series_to_rank.replace(np.inf, fill_pos_inf_val, inplace=True)\n",
        "\n",
        "                ranks_df[f'{score_name}_rank'] = series_to_rank.rank(\n",
        "                    ascending=is_ascending,\n",
        "                    na_option='bottom',\n",
        "                    method='average'\n",
        "                )\n",
        "                ranked_any_method = True\n",
        "            else:\n",
        "                self.logger.debug(f\"Score type '{score_name}' has no valid finite values to rank. It will not contribute to aggregated rank.\")\n",
        "                ranks_df[f'{score_name}_rank'] = np.nan\n",
        "\n",
        "        if not ranked_any_method:\n",
        "            self.logger.warning(\"No scores could be ranked from any method. Returning empty aggregated DataFrame.\")\n",
        "            return pd.DataFrame()\n",
        "\n",
        "        max_rank_across_methods = ranks_df.max().max() if not ranks_df.isnull().all().all() else len(ranks_df)\n",
        "        ranks_df.fillna(max_rank_across_methods + 1, inplace=True)\n",
        "\n",
        "        score_df['aggregated_rank'] = ranks_df.mean(axis=1)\n",
        "\n",
        "        if score_df['aggregated_rank'].isnull().any():\n",
        "             self.logger.warning(\"Aggregated rank contains NaNs, which is unexpected. Filling with max possible rank + 1.\")\n",
        "             fill_val = (score_df['aggregated_rank'].max() + 1\n",
        "                         if score_df['aggregated_rank'].notna().any() else len(score_df) + 1)\n",
        "             score_df['aggregated_rank'].fillna(fill_val, inplace=True)\n",
        "\n",
        "        max_aggregated_rank = score_df['aggregated_rank'].max()\n",
        "        score_df['importance_score'] = (max_aggregated_rank - score_df['aggregated_rank']) + 1\n",
        "\n",
        "        score_df.sort_values('aggregated_rank', ascending=True, inplace=True)\n",
        "        score_df.reset_index(inplace=True)\n",
        "        score_df.rename(columns={'index': 'feature'}, inplace=True)\n",
        "\n",
        "        self.logger.info(\"Score aggregation complete.\")\n",
        "        if not score_df.empty:\n",
        "             self.logger.info(f\"Top 5 features by aggregated rank:\\n{score_df[['feature', 'aggregated_rank', 'importance_score']].head()}\")\n",
        "        else:\n",
        "             self.logger.warning(\"Aggregated score DataFrame is empty after processing.\")\n",
        "\n",
        "        cols_to_return = ['feature', 'aggregated_rank', 'importance_score']\n",
        "        if all_features_with_scores:\n",
        "            first_feature_key = all_features_with_scores[0]\n",
        "            original_score_keys = list(self.feature_scores.get(first_feature_key, {}).keys())\n",
        "            cols_to_return.extend([col for col in original_score_keys if col in score_df.columns and col not in cols_to_return])\n",
        "        cols_to_return.extend([col for col in ranks_df.columns if col in score_df.columns and col not in cols_to_return])\n",
        "\n",
        "        final_cols = []\n",
        "        for col in cols_to_return:\n",
        "            if col not in final_cols:\n",
        "                final_cols.append(col)\n",
        "        for col in score_df.columns:\n",
        "            if col not in final_cols:\n",
        "                 final_cols.append(col)\n",
        "\n",
        "        return score_df[final_cols]\n",
        "\n",
        "    def _prune_highly_correlated_features(self,\n",
        "                                          features_df: pd.DataFrame,\n",
        "                                          data_for_correlation: pd.DataFrame) -> pd.DataFrame:\n",
        "        \"\"\"\n",
        "        Prunes highly correlated features. Keeps the one with better (lower) aggregated_rank.\n",
        "        If ranks are tied, keeps the one with lower mean absolute correlation with other features.\n",
        "        `features_df` must contain 'feature' and 'aggregated_rank'.\n",
        "        `data_for_correlation` is the DataFrame to compute correlations from (e.g., self.df or a bootstrap sample).\n",
        "        \"\"\"\n",
        "        threshold = self.config.correlation_threshold\n",
        "        if threshold is None or threshold >= 1.0:\n",
        "            self.logger.info(\"Correlation pruning skipped as threshold is None or >= 1.0.\")\n",
        "            return features_df.copy()\n",
        "\n",
        "        if features_df.empty or 'feature' not in features_df.columns or 'aggregated_rank' not in features_df.columns:\n",
        "            self.logger.warning(\"Cannot prune correlated features: input features_df is empty or missing required columns ('feature', 'aggregated_rank').\")\n",
        "            return features_df.copy() # Return copy to avoid side effects\n",
        "\n",
        "        self.logger.info(f\"Pruning features with absolute correlation > {threshold}...\")\n",
        "\n",
        "        ranked_features = features_df['feature'].tolist()\n",
        "        features_in_data = [f for f in ranked_features if f in data_for_correlation.columns]\n",
        "\n",
        "        if len(features_in_data) < 2:\n",
        "            self.logger.info(\"Not enough features present in data_for_correlation to compute correlation matrix. Skipping pruning.\")\n",
        "            return features_df.copy()\n",
        "\n",
        "        df_clean_for_corr = data_for_correlation[features_in_data].copy()\n",
        "        cols_to_exclude_from_corr = set()\n",
        "\n",
        "        for col in features_in_data: # Use features_in_data here\n",
        "            if not np.all(np.isfinite(df_clean_for_corr[col])):\n",
        "                finite_vals = df_clean_for_corr[col][np.isfinite(df_clean_for_corr[col])]\n",
        "                fill_val = finite_vals.median() if not finite_vals.empty else 0.0\n",
        "                df_clean_for_corr[col] = np.nan_to_num(df_clean_for_corr[col], nan=fill_val, posinf=fill_val, neginf=fill_val)\n",
        "                if not np.all(np.isfinite(df_clean_for_corr[col])):\n",
        "                    cols_to_exclude_from_corr.add(col)\n",
        "                    self.logger.warning(f\"Excluding column '{col}' from correlation calculation as it could not be made finite.\")\n",
        "\n",
        "        final_features_for_corr = [f for f in features_in_data if f not in cols_to_exclude_from_corr]\n",
        "\n",
        "        if len(final_features_for_corr) < 2:\n",
        "            self.logger.info(\"Not enough features remain after cleaning for correlation matrix computation. Skipping pruning.\")\n",
        "            return features_df.copy()\n",
        "\n",
        "        corrmat = df_clean_for_corr[final_features_for_corr].corr().abs()\n",
        "        upper_tri = corrmat.where(np.triu(np.ones(corrmat.shape), k=1).astype(bool))\n",
        "        features_to_drop = set()\n",
        "\n",
        "        # Iterate based on features_df's rank order to ensure consistent choices when ranks are different\n",
        "        # The provided features_df is already sorted by 'aggregated_rank'\n",
        "        sorted_features_for_pruning = features_df[features_df['feature'].isin(final_features_for_corr)]['feature'].tolist()\n",
        "\n",
        "        for i in range(len(sorted_features_for_pruning)):\n",
        "            f1_name = sorted_features_for_pruning[i]\n",
        "            if f1_name in features_to_drop: # If already decided to drop f1, skip\n",
        "                continue\n",
        "            for j in range(i + 1, len(sorted_features_for_pruning)):\n",
        "                f2_name = sorted_features_for_pruning[j]\n",
        "                if f2_name in features_to_drop: # If already decided to drop f2, skip\n",
        "                    continue\n",
        "\n",
        "                # Check if f1_name and f2_name are in upper_tri (which they should be if in final_features_for_corr)\n",
        "                if f1_name not in upper_tri.index or f2_name not in upper_tri.columns:\n",
        "                    # This might happen if one of them was excluded due to all NaNs, etc.\n",
        "                    # Or if matrix construction had an issue.\n",
        "                    self.logger.debug(f\"Skipping correlation check between {f1_name} and {f2_name} due to absence in correlation matrix parts.\")\n",
        "                    continue\n",
        "\n",
        "\n",
        "                # Determine which way to access from upper_tri (it's symmetric in value after .abs())\n",
        "                # Ensure f1_name is row index and f2_name is column index, and f1_name < f2_name (lexicographically for consistency if not triangular)\n",
        "                # However, upper_tri is already set up to have values only in the upper triangle.\n",
        "                # We need to ensure we query it correctly (row_idx < col_idx generally)\n",
        "                # The easiest is to get the value regardless of which is row/col from the full corrmat\n",
        "                correlation_value = corrmat.loc[f1_name, f2_name]\n",
        "\n",
        "                if pd.isna(correlation_value) or correlation_value <= threshold:\n",
        "                    continue\n",
        "\n",
        "                # Both features are highly correlated, decide which one to drop\n",
        "                try:\n",
        "                    # features_df is indexed by numbers, use .loc with boolean condition\n",
        "                    rank_f1 = features_df.loc[features_df['feature'] == f1_name, 'aggregated_rank'].iloc[0]\n",
        "                    rank_f2 = features_df.loc[features_df['feature'] == f2_name, 'aggregated_rank'].iloc[0]\n",
        "                except IndexError:\n",
        "                    self.logger.warning(f\"Could not find rank for '{f1_name}' or '{f2_name}' in features_df. Skipping this pair for pruning.\")\n",
        "                    continue\n",
        "\n",
        "                feature_to_drop_this_pair = None\n",
        "                tie_break_info = \"\"\n",
        "\n",
        "                # Lower rank is better. Drop the one with the WORSE (higher) rank.\n",
        "                if rank_f1 > rank_f2:\n",
        "                    feature_to_drop_this_pair = f1_name\n",
        "                elif rank_f2 > rank_f1:\n",
        "                    feature_to_drop_this_pair = f2_name\n",
        "                else: # Ranks are tied, use mean absolute correlation as tie-breaker\n",
        "                    # Ensure columns used for mean calculation are present in corrmat\n",
        "                    valid_cols_for_f1 = [col for col in corrmat.columns if col != f1_name and col in corrmat.index] # Check col in corrmat.index too\n",
        "                    valid_cols_for_f2 = [col for col in corrmat.columns if col != f2_name and col in corrmat.index]\n",
        "\n",
        "                    mean_corr_f1 = corrmat.loc[f1_name, valid_cols_for_f1].mean() if valid_cols_for_f1 else np.inf\n",
        "                    mean_corr_f2 = corrmat.loc[f2_name, valid_cols_for_f2].mean() if valid_cols_for_f2 else np.inf\n",
        "\n",
        "                    tie_break_info = (f\"Ranks tied ({rank_f1:.2f}). Mean Abs Corr: \"\n",
        "                                      f\"{f1_name}({mean_corr_f1:.3f}), {f2_name}({mean_corr_f2:.3f}).\")\n",
        "\n",
        "                    # Drop the one with HIGHER mean absolute correlation (more redundant)\n",
        "                    if mean_corr_f1 >= mean_corr_f2: # If tied here, f1 is dropped (consistent due to outer loop order)\n",
        "                        feature_to_drop_this_pair = f1_name\n",
        "                    else:\n",
        "                        feature_to_drop_this_pair = f2_name\n",
        "\n",
        "                if feature_to_drop_this_pair:\n",
        "                    features_to_drop.add(feature_to_drop_this_pair)\n",
        "                    self.logger.debug(\n",
        "                        f\"Correlation > {threshold}: {f1_name} vs {f2_name} ({correlation_value:.2f}). \"\n",
        "                        f\"{tie_break_info} Dropping '{feature_to_drop_this_pair}'.\"\n",
        "                    )\n",
        "                    # If f1_name was dropped, break inner loop and continue outer,\n",
        "                    # because f1_name (the one with better rank or chosen by tie-break if ranks equal) is kept.\n",
        "                    # No, if feature_to_drop_this_pair is f1_name, we add f1_name to features_to_drop.\n",
        "                    # The outer loop already handles `if f1_name in features_to_drop: continue`\n",
        "\n",
        "        if features_to_drop:\n",
        "            self.logger.info(f\"Removing {len(features_to_drop)} features due to high correlation: {sorted(list(features_to_drop))}\")\n",
        "            return features_df[~features_df['feature'].isin(features_to_drop)].copy()\n",
        "        else:\n",
        "            self.logger.info(\"No features pruned by correlation.\")\n",
        "            return features_df.copy()\n",
        "\n",
        "\n",
        "    def select(self,\n",
        "               k: Optional[int] = None,\n",
        "               methods_to_run_override: Optional[List[str]] = None,\n",
        "               config_overrides: Optional[Dict[str, Any]] = None,\n",
        "               auto_k_selection_method: Optional[str] = None) -> List[str]:\n",
        "        \"\"\"\n",
        "        Main method to run the feature selection pipeline.\n",
        "        Selects features based on fixed-k, coverage, or stability.\n",
        "        \"\"\"\n",
        "        original_config_obj = self.config\n",
        "        temp_config_dict = vars(original_config_obj).copy()\n",
        "\n",
        "        if config_overrides:\n",
        "            temp_config_dict.update(config_overrides)\n",
        "        if methods_to_run_override is not None:\n",
        "            temp_config_dict['methods_to_run'] = methods_to_run_override\n",
        "\n",
        "        # Temporarily use the potentially modified config for this run\n",
        "        # Create a new config object to avoid modifying the instance's config if this is called multiple times\n",
        "        # or from within a bootstrap loop that shouldn't affect the parent's config.\n",
        "        current_run_config = FeatureSelectorConfig(**temp_config_dict)\n",
        "\n",
        "        # Store the current config and replace it with the temporary one for the duration of this method\n",
        "        # This is tricky if self.config is used by other methods called herein, they need to see the temp one.\n",
        "        # A better way is to pass current_run_config to sub-methods if they need it, or just use current_run_config locally.\n",
        "        # For simplicity, and as self.config is used by helper methods, we'll swap it.\n",
        "        _original_instance_config = self.config\n",
        "        self.config = current_run_config\n",
        "\n",
        "        self.logger.info(f\"Running select() with effective config: {vars(self.config)}\")\n",
        "\n",
        "        selected_features: List[str] = []\n",
        "\n",
        "        try:\n",
        "            if auto_k_selection_method:\n",
        "                method_lower = auto_k_selection_method.lower()\n",
        "                self.logger.info(f\"--- Automatic Feature Selection by: {method_lower} ---\")\n",
        "                if method_lower == \"coverage\":\n",
        "                    selected_features = self.select_k_by_coverage()\n",
        "                elif method_lower == \"stability\":\n",
        "                    stable_features_list, _ = self.select_by_bootstrap_stability() # stability_df is ignored here\n",
        "                    selected_features = stable_features_list\n",
        "                else:\n",
        "                    self.logger.error(f\"Unsupported auto_k_selection_method: '{auto_k_selection_method}'. \"\n",
        "                                      \"Choose 'coverage' or 'stability'.\")\n",
        "                    return []\n",
        "            elif k is not None and k > 0:\n",
        "                self.logger.info(f\"--- Fixed-k Feature Selection (Target k={k}) ---\")\n",
        "                self.feature_scores.clear()\n",
        "\n",
        "                current_numeric_features = self.numeric_cols[:]\n",
        "                if 'variance' in self.config.methods_to_run or self.config.variance_threshold > 0:\n",
        "                    current_numeric_features = self.filter_by_variance() # Uses self.config\n",
        "\n",
        "                if not current_numeric_features:\n",
        "                    self.logger.warning(\"No numeric features remaining after variance filtering (or none to begin with).\")\n",
        "                    return []\n",
        "\n",
        "                if 'laplacian' in self.config.methods_to_run:\n",
        "                    self.calculate_laplacian_score(features=current_numeric_features) # Uses self.config\n",
        "                if 'pseudo_label' in self.config.methods_to_run:\n",
        "                    self.calculate_ensemble_pseudo_label_scores(features=current_numeric_features) # Uses self.config\n",
        "\n",
        "                features_that_got_scored = [\n",
        "                    f for f in current_numeric_features\n",
        "                    if f in self.feature_scores and (\n",
        "                        self.feature_scores[f] or\n",
        "                        ('variance' in self.feature_scores.get(f,{}) and 'variance' in self.config.methods_to_run)\n",
        "                    )\n",
        "                ]\n",
        "\n",
        "                if not features_that_got_scored:\n",
        "                    self.logger.warning(\"No features were scored by the enabled methods. \"\n",
        "                                        \"Returning the top k from variance-filtered list if available, or empty.\")\n",
        "                    return current_numeric_features[:k]\n",
        "\n",
        "                # Scope feature_scores for aggregation to only include currently relevant features\n",
        "                # This is important if select() is called multiple times with different feature subsets\n",
        "                scoped_feature_scores = defaultdict(dict, {\n",
        "                    f: self.feature_scores.get(f, {}) for f in features_that_got_scored\n",
        "                })\n",
        "                original_scores_backup = self.feature_scores # Backup the main scores\n",
        "                self.feature_scores = scoped_feature_scores # Use scoped for aggregation\n",
        "\n",
        "                aggregated_df = self.aggregate_scores() # Uses self.config implicitly via score types\n",
        "\n",
        "                self.feature_scores = original_scores_backup # Restore main scores\n",
        "\n",
        "                if aggregated_df.empty:\n",
        "                    self.logger.warning(\"Score aggregation resulted in an empty DataFrame. \"\n",
        "                                        \"Returning the top k from variance-filtered list if available.\")\n",
        "                    return current_numeric_features[:k]\n",
        "\n",
        "                pruned_df = aggregated_df\n",
        "                if self.config.correlation_threshold is not None and self.config.correlation_threshold < 1.0:\n",
        "                    # Use self.df (cleaned main df) for correlation data in fixed-k selection\n",
        "                    pruned_df = self._prune_highly_correlated_features(aggregated_df, self.df) # Uses self.config\n",
        "\n",
        "                if pruned_df.empty:\n",
        "                    self.logger.warning(\"No features remaining after correlation pruning.\")\n",
        "                    return []\n",
        "\n",
        "                final_k_to_select = min(k, len(pruned_df))\n",
        "                selected_features = pruned_df['feature'].head(final_k_to_select).tolist()\n",
        "                self.logger.info(\n",
        "                    f\"--- Feature Selection Completed (fixed-k). Selected {len(selected_features)} features: {selected_features[:20]}{'...' if len(selected_features)>20 else ''} ---\"\n",
        "                )\n",
        "            else:\n",
        "                self.logger.error(\"Invalid selection mode: Provide a positive integer 'k' or a \"\n",
        "                                  \"valid 'auto_k_selection_method' ('coverage', 'stability').\")\n",
        "                return []\n",
        "\n",
        "        finally:\n",
        "            self.config = _original_instance_config # Restore original config\n",
        "\n",
        "        return selected_features\n",
        "\n",
        "\n",
        "    def select_k_by_coverage(self) -> List[str]:\n",
        "        \"\"\"Selects features based on achieving a cumulative importance coverage.\"\"\"\n",
        "        # This method uses self.config directly, which should be the (potentially temporarily overridden) config.\n",
        "        self.logger.info(\n",
        "            f\"--- Automatic Feature Selection by Coverage (Target: {self.config.coverage_threshold*100:.1f}%) ---\"\n",
        "        )\n",
        "        self.feature_scores.clear()\n",
        "\n",
        "        current_numeric_features = self.numeric_cols[:]\n",
        "        if 'variance' in self.config.methods_to_run or self.config.variance_threshold > 0:\n",
        "            current_numeric_features = self.filter_by_variance()\n",
        "\n",
        "        if not current_numeric_features:\n",
        "            self.logger.warning(\"No numeric features remaining after variance filtering for coverage selection.\")\n",
        "            return []\n",
        "\n",
        "        if 'laplacian' in self.config.methods_to_run:\n",
        "            self.calculate_laplacian_score(features=current_numeric_features)\n",
        "        if 'pseudo_label' in self.config.methods_to_run:\n",
        "            self.calculate_ensemble_pseudo_label_scores(features=current_numeric_features)\n",
        "\n",
        "        features_that_got_scored = [\n",
        "            f for f in current_numeric_features\n",
        "            if f in self.feature_scores and (\n",
        "                self.feature_scores[f] or\n",
        "                ('variance' in self.feature_scores.get(f,{}) and 'variance' in self.config.methods_to_run)\n",
        "            )\n",
        "        ]\n",
        "        if not features_that_got_scored:\n",
        "            self.logger.warning(\"No features were scored. Cannot select by coverage. Returning all variance-filtered features.\")\n",
        "            return current_numeric_features\n",
        "\n",
        "        scoped_feature_scores = defaultdict(dict, {f: self.feature_scores.get(f, {}) for f in features_that_got_scored})\n",
        "        original_scores_backup = self.feature_scores\n",
        "        self.feature_scores = scoped_feature_scores\n",
        "\n",
        "        aggregated_df = self.aggregate_scores()\n",
        "        self.feature_scores = original_scores_backup\n",
        "\n",
        "        if aggregated_df.empty or 'importance_score' not in aggregated_df.columns:\n",
        "            self.logger.warning(\"Score aggregation failed or 'importance_score' missing. \"\n",
        "                                \"Returning all variance-filtered features.\")\n",
        "            return current_numeric_features\n",
        "\n",
        "        pruned_df = aggregated_df\n",
        "        if self.config.correlation_threshold is not None and self.config.correlation_threshold < 1.0:\n",
        "            pruned_df = self._prune_highly_correlated_features(aggregated_df, self.df) # Use self.df for correlation data\n",
        "\n",
        "        if pruned_df.empty or 'importance_score' not in pruned_df.columns:\n",
        "            self.logger.warning(\"No features remaining after correlation pruning or 'importance_score' missing.\")\n",
        "            return []\n",
        "\n",
        "        features_sorted_by_importance = pruned_df.sort_values('importance_score', ascending=False)\n",
        "        features_sorted_by_importance.dropna(subset=['importance_score'], inplace=True)\n",
        "        features_sorted_by_importance = features_sorted_by_importance[np.isfinite(features_sorted_by_importance['importance_score'])]\n",
        "\n",
        "        if features_sorted_by_importance.empty:\n",
        "            self.logger.warning(\"No features with finite importance scores available for coverage selection.\")\n",
        "            return []\n",
        "\n",
        "        importance_scores = features_sorted_by_importance['importance_score'].values\n",
        "        if np.any(importance_scores < 0):\n",
        "            self.logger.debug(\"Normalizing importance scores to be non-negative for coverage calculation.\")\n",
        "            importance_scores = importance_scores - np.min(importance_scores)\n",
        "\n",
        "        total_importance = np.sum(importance_scores)\n",
        "        if total_importance <= 1e-9:\n",
        "            self.logger.warning(\"Total importance of features is near zero. \"\n",
        "                                \"Returning all features sorted by importance (up to original number).\")\n",
        "            return features_sorted_by_importance['feature'].tolist()\n",
        "\n",
        "        cumulative_importance_ratio = np.cumsum(importance_scores) / total_importance\n",
        "        k_auto_coverage = np.searchsorted(cumulative_importance_ratio, self.config.coverage_threshold, side='left') + 1\n",
        "        k_auto_coverage = min(k_auto_coverage, len(features_sorted_by_importance))\n",
        "\n",
        "        selected_features = features_sorted_by_importance['feature'].head(k_auto_coverage).tolist()\n",
        "        final_coverage_achieved = cumulative_importance_ratio[k_auto_coverage-1] if k_auto_coverage > 0 else 0.0\n",
        "        self.logger.info(\n",
        "            f\"Selected {len(selected_features)} features by coverage \"\n",
        "            f\"(achieved ~{final_coverage_achieved*100:.1f}%): {selected_features[:20]}{'...' if len(selected_features)>20 else ''}\"\n",
        "        )\n",
        "        return selected_features\n",
        "\n",
        "\n",
        "    def select_by_bootstrap_stability(self) -> Tuple[List[str], pd.DataFrame]:\n",
        "        \"\"\"\n",
        "        Selects features based on their stability (frequency of selection)\n",
        "        across multiple bootstrap samples of the original DataFrame.\n",
        "        Each bootstrap sample undergoes a fixed-k selection process using this instance's current config.\n",
        "        \"\"\"\n",
        "        # This method uses self.config directly.\n",
        "        cfg = self.config\n",
        "        self.logger.info(\n",
        "            f\"--- Automatic Feature Selection by Bootstrap Stability (B={cfg.bootstrap_B}, k_per_iter={cfg.bootstrap_k_per_iter}) ---\"\n",
        "        )\n",
        "\n",
        "        if self.original_df_for_bootstrap.empty: # Use the pristine original df\n",
        "            self.logger.error(\"Original DataFrame for bootstrapping is empty. Cannot perform stability selection.\")\n",
        "            return [], pd.DataFrame()\n",
        "\n",
        "        feature_selection_counts = defaultdict(int)\n",
        "        all_features_selected_across_bootstraps = set()\n",
        "\n",
        "        for i in range(cfg.bootstrap_B):\n",
        "            self.logger.info(f\"Bootstrap iteration {i + 1}/{cfg.bootstrap_B}\")\n",
        "\n",
        "            bootstrap_rs = (self.random_state + i) if self.random_state is not None else None\n",
        "            bootstrap_df = resample(\n",
        "                self.original_df_for_bootstrap, # Sample from the original unmodified DataFrame\n",
        "                n_samples=len(self.original_df_for_bootstrap),\n",
        "                replace=True,\n",
        "                random_state=bootstrap_rs\n",
        "            )\n",
        "\n",
        "            # Create a temporary FeatureSelector for this bootstrap sample.\n",
        "            # It inherits the current main config (self.config, which might be overridden for the main select call)\n",
        "            # but with verbose=False to keep logs clean.\n",
        "            temp_selector_config_dict = vars(self.config).copy()\n",
        "            # temp_selector_config_dict['verbose'] = False # This would affect the logger level, not just info messages\n",
        "\n",
        "            temp_selector = FeatureSelector(\n",
        "                df=bootstrap_df,\n",
        "                id_cols=self.id_cols,\n",
        "                target_col=self.target_col,\n",
        "                random_state=bootstrap_rs,\n",
        "                verbose=False, # Make the temporary selector non-verbose for its own logger\n",
        "                n_jobs=self.n_jobs,\n",
        "                config=FeatureSelectorConfig(**temp_selector_config_dict)\n",
        "            )\n",
        "\n",
        "            try:\n",
        "                # Perform fixed-k selection on the bootstrap sample using the temp_selector's config\n",
        "                bootstrap_selected_features = temp_selector.select(\n",
        "                    k=cfg.bootstrap_k_per_iter, # Use k_per_iter from the main config\n",
        "                    auto_k_selection_method=None # Crucial: force fixed-k selection\n",
        "                )\n",
        "                for feature in bootstrap_selected_features:\n",
        "                    feature_selection_counts[feature] += 1\n",
        "                    all_features_selected_across_bootstraps.add(feature)\n",
        "            except Exception as e_boot:\n",
        "                self.logger.warning(f\"Bootstrap iteration {i + 1} failed during feature selection: {e_boot}.\", exc_info=False)\n",
        "\n",
        "        if not feature_selection_counts:\n",
        "            self.logger.warning(\"No features were selected in any bootstrap iteration.\")\n",
        "            return [], pd.DataFrame()\n",
        "\n",
        "        frequency_df = pd.DataFrame({\n",
        "            'feature': list(all_features_selected_across_bootstraps),\n",
        "            'frequency': [feature_selection_counts.get(f, 0) for f in all_features_selected_across_bootstraps]\n",
        "        })\n",
        "        frequency_df['selection_percentage'] = (frequency_df['frequency'] / cfg.bootstrap_B) * 100.0\n",
        "        frequency_df.sort_values('frequency', ascending=False, inplace=True)\n",
        "        frequency_df.reset_index(drop=True, inplace=True)\n",
        "\n",
        "        self.logger.info(f\"Bootstrap stability results (Top 20 features or all if fewer):\\n{frequency_df.head(20)}\")\n",
        "\n",
        "        stable_features_list: List[str]\n",
        "        if cfg.bootstrap_selection_freq_threshold is not None:\n",
        "            min_frequency_value = cfg.bootstrap_selection_freq_threshold * cfg.bootstrap_B\n",
        "            stable_features_df = frequency_df[frequency_df['frequency'] >= min_frequency_value]\n",
        "            stable_features_list = stable_features_df['feature'].tolist()\n",
        "            self.logger.info(\n",
        "                f\"Selected {len(stable_features_list)} features appearing in at least \"\n",
        "                f\"{cfg.bootstrap_selection_freq_threshold*100:.1f}% of bootstrap iterations: \"\n",
        "                f\"{stable_features_list[:20]}{'...' if len(stable_features_list)>20 else ''}\"\n",
        "            )\n",
        "        else:\n",
        "            self.logger.info(\"No bootstrap_selection_freq_threshold set. Returning all features ranked by selection frequency.\")\n",
        "            stable_features_list = frequency_df['feature'].tolist()\n",
        "\n",
        "        return stable_features_list, frequency_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cQEv_1sTyXSK"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from scipy.optimize import minimize\n",
        "from scipy.special import gammaln, digamma # For direct use of log gamma and digamma\n",
        "from scipy.stats import t as scipy_t # For comparison or alternative PDF calculation\n",
        "import warnings\n",
        "\n",
        "# Assuming utils.py is in the same directory or accessible via PYTHONPATH\n",
        "# from .utils import get_logger\n",
        "\n",
        "logger = get_logger(__name__)\n",
        "\n",
        "STUDENT_T_AVAILABLE = True\n",
        "\n",
        "class StudentTMixture:\n",
        "    \"\"\"\n",
        "    Mixture of Student's t-distributions fitted using the EM algorithm.\n",
        "    Assumes diagonal covariance matrices (independent features for each component).\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, n_components=3, max_iter=150, tol=1e-4,\n",
        "                 min_df=2.0, max_df=150.0, random_state=None, reg_covar=1e-6):\n",
        "        if n_components < 1:\n",
        "            raise ValueError(\"Number of components must be at least 1.\")\n",
        "        self.n_components = n_components\n",
        "        self.max_iter = max_iter\n",
        "        self.tol = tol\n",
        "        self.min_df = min_df # Minimum degrees of freedom (Fix for Issue 2)\n",
        "        self.max_df = max_df # Maximum degrees of freedom\n",
        "        self.random_state = random_state\n",
        "        self.rng = np.random.default_rng(random_state)\n",
        "        self.reg_covar = reg_covar # Regularization for scale/covariance (Fix for Issue 2)\n",
        "\n",
        "        self.weights_ = None\n",
        "        self.means_ = None\n",
        "        self.scales_ = None # Standard deviations (sqrt of variances)\n",
        "        self.dfs_ = None\n",
        "        self.converged_ = False\n",
        "        self.n_iter_ = 0\n",
        "        self.log_likelihood_ = -np.inf\n",
        "\n",
        "    def _init_params(self, X: np.ndarray):\n",
        "        \"\"\"Initialize parameters using k-means++ for means.\"\"\"\n",
        "        n_samples, n_features = X.shape\n",
        "        if n_samples < self.n_components:\n",
        "            # Fallback: reduce n_components if not enough samples\n",
        "            logger.warning(f\"Number of samples ({n_samples}) is less than n_components ({self.n_components}). \"\n",
        "                           f\"Reducing n_components to {n_samples}.\")\n",
        "            self.n_components = n_samples\n",
        "            if self.n_components == 0: # Should not happen if X is not empty\n",
        "                 raise ValueError(\"Cannot initialize parameters with 0 components from 0 samples.\")\n",
        "\n",
        "\n",
        "        # K-means++ initialization for means\n",
        "        centers = np.zeros((self.n_components, n_features))\n",
        "        if n_samples > 0:\n",
        "            centers[0] = X[self.rng.choice(n_samples)]\n",
        "            distances = np.full(n_samples, np.inf)\n",
        "\n",
        "            for i in range(1, self.n_components):\n",
        "                dist_sq = np.sum((X - centers[i-1])**2, axis=1)\n",
        "                distances = np.minimum(distances, dist_sq)\n",
        "\n",
        "                # Handle cases where all distances are zero (e.g. all points are identical)\n",
        "                sum_distances = np.sum(distances)\n",
        "                if sum_distances == 0 : # if all points are the same, or remaining points are same\n",
        "                    probs = np.ones(n_samples) / n_samples\n",
        "                else:\n",
        "                    probs = distances / sum_distances\n",
        "\n",
        "                cumulative_probs = np.cumsum(probs)\n",
        "                # Ensure probabilities sum to 1, handle potential floating point issues\n",
        "                if cumulative_probs[-1] == 0 : # if all probs are 0, this can happen if distances are all 0\n",
        "                    center_idx = self.rng.choice(n_samples) # pick randomly\n",
        "                else:\n",
        "                    cumulative_probs /= cumulative_probs[-1]\n",
        "                    r = self.rng.random()\n",
        "                    center_idx = np.searchsorted(cumulative_probs, r, side='right')\n",
        "\n",
        "                center_idx = min(center_idx, n_samples - 1) # Ensure index is within bounds\n",
        "                centers[i] = X[center_idx]\n",
        "        else: # Should not be reached if n_samples < self.n_components raised error correctly\n",
        "            # Fallback if n_samples is 0 somehow\n",
        "            centers = np.zeros((self.n_components, n_features))\n",
        "\n",
        "\n",
        "        self.means_ = centers\n",
        "        self.weights_ = np.ones(self.n_components) / self.n_components\n",
        "\n",
        "        # Initialize scales (std devs)\n",
        "        # Use overall std dev for initial scales, add reg_covar for stability\n",
        "        # Ensure std_X is not zero\n",
        "        std_X = np.std(X, axis=0)\n",
        "        if np.any(std_X == 0): # If any feature has zero variance\n",
        "            logger.warning(\"One or more features have zero variance. Initializing scales with small default.\")\n",
        "            # Create scales with a small default value where std_X is 0\n",
        "            feature_scales = np.where(std_X == 0, np.sqrt(self.reg_covar), std_X)\n",
        "        else:\n",
        "            feature_scales = std_X\n",
        "\n",
        "        self.scales_ = np.ones((self.n_components, n_features)) * np.maximum(feature_scales, np.sqrt(self.reg_covar))\n",
        "\n",
        "        # Initialize DoF (Fix for Issue 2 - ensure reasonable start)\n",
        "        self.dfs_ = np.ones(self.n_components) * np.clip(5.0, self.min_df, self.max_df)\n",
        "\n",
        "\n",
        "    def _log_t_pdf_component(self, X: np.ndarray, mean_k: np.ndarray, scale_k: np.ndarray, df_k: float) -> np.ndarray:\n",
        "        \"\"\"\n",
        "        Calculate log PDF of a single multivariate Student's t-distribution component\n",
        "        (assuming diagonal covariance).\n",
        "        X: shape (n_samples, n_features)\n",
        "        mean_k: shape (n_features,)\n",
        "        scale_k: shape (n_features,) - standard deviations\n",
        "        df_k: scalar\n",
        "        Returns: log_pdf_val of shape (n_samples,)\n",
        "        \"\"\"\n",
        "        n_samples, n_features = X.shape\n",
        "\n",
        "        # Ensure parameters are valid and add regularization to scale\n",
        "        df_safe = np.maximum(df_k, self.min_df)\n",
        "        scale_safe = np.maximum(scale_k, np.sqrt(self.reg_covar)) # scale is std_dev\n",
        "\n",
        "        # Log PDF using scipy.stats.t for simplicity and robustness per dimension, then sum\n",
        "        # This assumes diagonal covariance (features are independent within a component)\n",
        "        log_pdf_features = np.zeros_like(X)\n",
        "        for j in range(n_features):\n",
        "            log_pdf_features[:, j] = scipy_t.logpdf(X[:, j], df=df_safe, loc=mean_k[j], scale=scale_safe[j])\n",
        "\n",
        "        return np.sum(log_pdf_features, axis=1)\n",
        "\n",
        "    def _e_step(self, X: np.ndarray) -> Tuple[np.ndarray, float]:\n",
        "        \"\"\"Expectation step: Calculate responsibilities and log-likelihood.\"\"\"\n",
        "        n_samples = X.shape[0]\n",
        "        log_prob_norm = np.zeros((n_samples, self.n_components))\n",
        "\n",
        "        for k in range(self.n_components):\n",
        "            # Add small epsilon to weights to prevent log(0)\n",
        "            weight_k = np.maximum(self.weights_[k], 1e-9)\n",
        "            log_prob_norm[:, k] = np.log(weight_k) + \\\n",
        "                                  self._log_t_pdf_component(X, self.means_[k], self.scales_[k], self.dfs_[k])\n",
        "\n",
        "        # Log-sum-exp trick for numerical stability\n",
        "        log_likelihood_per_sample = np.logaddexp.reduce(log_prob_norm, axis=1) # More stable\n",
        "\n",
        "        with np.errstate(divide='ignore', invalid='ignore'): # Handle potential division by zero if all log_prob_norm are -inf\n",
        "            log_responsibilities = log_prob_norm - log_likelihood_per_sample[:, np.newaxis]\n",
        "            responsibilities = np.exp(log_responsibilities)\n",
        "\n",
        "        # Normalize responsibilities to sum to 1, handle cases where sum is 0 (all components had 0 prob)\n",
        "        resp_sum = responsibilities.sum(axis=1, keepdims=True)\n",
        "        responsibilities = np.divide(responsibilities, resp_sum, out=np.zeros_like(responsibilities), where=resp_sum!=0)\n",
        "\n",
        "        # If any sample has all-zero responsibilities (e.g., numerical underflow for all components),\n",
        "        # assign it equally among components to prevent NaN issues in M-step.\n",
        "        all_zero_resp_mask = (resp_sum == 0).flatten()\n",
        "        if np.any(all_zero_resp_mask):\n",
        "            logger.warning(f\"{np.sum(all_zero_resp_mask)} samples had zero responsibility for all components. Re-distributing.\")\n",
        "            responsibilities[all_zero_resp_mask, :] = 1.0 / self.n_components\n",
        "\n",
        "\n",
        "        total_log_likelihood = np.sum(log_likelihood_per_sample[np.isfinite(log_likelihood_per_sample)])\n",
        "        if not np.isfinite(total_log_likelihood): # Fallback if sum is not finite\n",
        "            total_log_likelihood = -np.inf if self.log_likelihood_ == -np.inf else self.log_likelihood_\n",
        "\n",
        "\n",
        "        return responsibilities, total_log_likelihood\n",
        "\n",
        "    def _m_step(self, X: np.ndarray, responsibilities: np.ndarray):\n",
        "        \"\"\"Maximization step: Update model parameters.\"\"\"\n",
        "        n_samples, n_features = X.shape\n",
        "\n",
        "        # Add epsilon to prevent division by zero\n",
        "        nk = np.sum(responsibilities, axis=0) + 1e-9 # Sum of responsibilities for each component\n",
        "\n",
        "        # Update weights\n",
        "        self.weights_ = nk / n_samples\n",
        "        self.weights_ = np.maximum(self.weights_, 1e-9) # Ensure weights are not zero\n",
        "        self.weights_ /= np.sum(self.weights_) # Normalize\n",
        "\n",
        "        # Update means\n",
        "        self.means_ = np.dot(responsibilities.T, X) / nk[:, np.newaxis]\n",
        "\n",
        "        # Update scales (std devs) and dfs\n",
        "        for k in range(self.n_components):\n",
        "            if nk[k] < 1e-6 : # Skip update for collapsed components\n",
        "                logger.warning(f\"Component {k} has negligible responsibility ({nk[k]:.2e}). Skipping its M-step update.\")\n",
        "                self.scales_[k] = np.maximum(self.scales_[k], np.sqrt(self.reg_covar)) # Keep old scale but ensure regularized\n",
        "                self.dfs_[k] = np.clip(self.dfs_[k], self.min_df, self.max_df) # Keep old df but ensure bounded\n",
        "                continue\n",
        "\n",
        "            diff = X - self.means_[k] # (n_samples, n_features)\n",
        "\n",
        "            # Calculate u_ik weights for M-step update (Peel & McLachlan, 2000)\n",
        "            # u_ik = (v_k + d) / (v_k + delta_ik^2)\n",
        "            # where delta_ik^2 is Mahalanobis distance. For diagonal cov:\n",
        "            # delta_ik_j^2 = (x_ij - mu_kj)^2 / sigma_kj^2\n",
        "            # sum_j (delta_ik_j^2)\n",
        "\n",
        "            # Ensure scale_k is not zero and includes regularization\n",
        "            current_scale_k = np.maximum(self.scales_[k], np.sqrt(self.reg_covar))\n",
        "            mahal_sq_terms = (diff / current_scale_k)**2 # (n_samples, n_features)\n",
        "            mahal_sq_sum = np.sum(mahal_sq_terms, axis=1) # (n_samples,)\n",
        "\n",
        "            current_df_k = np.clip(self.dfs_[k], self.min_df, self.max_df)\n",
        "            u_ik = (current_df_k + n_features) / (current_df_k + mahal_sq_sum + 1e-9) # (n_samples,)\n",
        "\n",
        "            # Check for NaNs or Infs in u_ik\n",
        "            if np.any(~np.isfinite(u_ik)):\n",
        "                logger.warning(f\"Warning: Non-finite u_ik encountered for component {k}. Using mean of valid u_ik or 1.0.\")\n",
        "                u_ik[~np.isfinite(u_ik)] = np.nanmean(u_ik[np.isfinite(u_ik)]) if np.any(np.isfinite(u_ik)) else 1.0\n",
        "\n",
        "\n",
        "            # Update scales (variances first, then sqrt)\n",
        "            weighted_u_ik_resp = responsibilities[:, k] * u_ik # (n_samples,)\n",
        "            sum_weighted_u_ik_resp = np.sum(weighted_u_ik_resp) + 1e-9\n",
        "\n",
        "            if sum_weighted_u_ik_resp <= 1e-9: # If sum is effectively zero\n",
        "                 logger.warning(f\"Sum of weighted_u_ik_resp is non-positive for component {k}. Scale update might be unstable.\")\n",
        "                 # Keep previous scale or reset to a regularized version of global std\n",
        "                 self.scales_[k] = np.maximum(self.scales_[k], np.sqrt(self.reg_covar))\n",
        "\n",
        "            else:\n",
        "                new_variance_k = np.sum(weighted_u_ik_resp[:, np.newaxis] * (diff ** 2), axis=0) / sum_weighted_u_ik_resp\n",
        "                # Apply regularization to variance before sqrt\n",
        "                self.scales_[k] = np.sqrt(np.maximum(new_variance_k, self.reg_covar))\n",
        "\n",
        "\n",
        "            # Update degrees of freedom using optimization (Equation 20, Peel & McLachlan 2000)\n",
        "            # Requires solving: -digamma(nu/2) + log(nu/2) + 1 - E_q[log u_ik] + E_q[u_ik] + digamma((nu+d)/2) - log((nu+d)/2) = 0\n",
        "            # where E_q[log u_ik] = sum(r_ik * log u_ik) / sum(r_ik)\n",
        "            # and   E_q[u_ik] = sum(r_ik * u_ik) / sum(r_ik)\n",
        "\n",
        "            resp_k = responsibilities[:, k] # (n_samples,)\n",
        "            # valid_idx = resp_k > 1e-9 # Use samples with some responsibility for this component\n",
        "            # if not np.any(valid_idx):\n",
        "            #     logger.warning(f\"No samples with significant responsibility for component {k}. Skipping df update.\")\n",
        "            #     self.dfs_[k] = np.clip(self.dfs_[k], self.min_df, self.max_df)\n",
        "            #     continue\n",
        "\n",
        "            # resp_k_valid = resp_k[valid_idx]\n",
        "            # u_ik_valid = u_ik[valid_idx]\n",
        "            # log_u_ik_valid = np.log(np.maximum(u_ik_valid, 1e-9)) # Avoid log(0)\n",
        "\n",
        "            # eq_log_u_ik = np.sum(resp_k_valid * log_u_ik_valid) / np.sum(resp_k_valid)\n",
        "            # eq_u_ik = np.sum(resp_k_valid * u_ik_valid) / np.sum(resp_k_valid)\n",
        "\n",
        "            # Simplified from scikit-learn's GaussianMixture for t-distribution M-step for df\n",
        "            # (often df is fixed or updated with simpler heuristics due to complexity)\n",
        "            # Here, we use the optimization approach as in the original code but with safeguards.\n",
        "\n",
        "            # Define optimization function for degrees of freedom\n",
        "            def neg_df_objective(nu_scalar):\n",
        "                if nu_scalar <= self.min_df / 2: # Search in log space or transform to ensure positivity\n",
        "                    return np.inf\n",
        "\n",
        "                # Calculate E_q[log u_ik] and E_q[u_ik] for current nu_scalar via u_ik definition\n",
        "                # This is tricky because u_ik itself depends on nu.\n",
        "                # The original paper's objective function is simpler if E_q terms are treated as constants from E-step\n",
        "                # Let's use the structure from the provided code which is common.\n",
        "\n",
        "                # Recalculate u_ik terms based on nu_scalar for the objective function\n",
        "                # This is what makes the optimization tricky.\n",
        "                # We need E_rik_uik and E_rik_log_uik where uik depends on nu\n",
        "                # For this component k:\n",
        "                mahal_sq_sum_k = np.sum(( (X - self.means_[k]) / np.maximum(self.scales_[k], np.sqrt(self.reg_covar)) )**2, axis=1)\n",
        "                u_ik_for_nu_opt = (nu_scalar + n_features) / (nu_scalar + mahal_sq_sum_k + 1e-9)\n",
        "\n",
        "                # Ensure u_ik_for_nu_opt is positive for log\n",
        "                log_u_ik_for_nu_opt = np.log(np.maximum(u_ik_for_nu_opt, 1e-9))\n",
        "\n",
        "                # Weighted averages by responsibilities\n",
        "                avg_log_u_ik = np.sum(responsibilities[:, k] * log_u_ik_for_nu_opt) / nk[k]\n",
        "                avg_u_ik = np.sum(responsibilities[:, k] * u_ik_for_nu_opt) / nk[k]\n",
        "\n",
        "\n",
        "                # Objective function based on Q2 w.r.t nu_k (Peel and McLachlan, 2000, eq. between (19) and (20))\n",
        "                # Maximize: -digamma(nu/2) + log(nu/2) - avg_log_u_ik + avg_u_ik -1 + digamma((nu+d)/2) - log((nu+d)/2)\n",
        "                # (Note: the constant +1 in the original text's neg_q2_df seems to be absorbed)\n",
        "\n",
        "                val = (\n",
        "                    -digamma(nu_scalar / 2.0)\n",
        "                    + np.log(nu_scalar / 2.0)\n",
        "                    - avg_log_u_ik\n",
        "                    + avg_u_ik\n",
        "                    - 1.0 # This -1.0 helps align with typical forms.\n",
        "                    + digamma((nu_scalar + n_features) / 2.0)\n",
        "                    - np.log((nu_scalar + n_features) / 2.0)\n",
        "                )\n",
        "                return -val # We minimize the negative of the objective\n",
        "\n",
        "            initial_df_k = np.clip(self.dfs_[k], self.min_df, self.max_df)\n",
        "            try:\n",
        "                # Bounds for df, ensuring df > 0 (or min_df) and not excessively large\n",
        "                opt_result = minimize(neg_df_objective, initial_df_k, method='L-BFGS-B',\n",
        "                                      bounds=[(self.min_df, self.max_df)])\n",
        "                if opt_result.success and np.isfinite(opt_result.x[0]):\n",
        "                    self.dfs_[k] = np.clip(opt_result.x[0], self.min_df, self.max_df)\n",
        "                else:\n",
        "                    logger.warning(f\"DF optimization failed for component {k}. Status: {opt_result.message}. Keeping previous df {initial_df_k:.2f}.\")\n",
        "                    self.dfs_[k] = initial_df_k # Keep old value if optimization fails\n",
        "            except Exception as e:\n",
        "                logger.error(f\"Error in DF optimization for component {k}: {e}. Keeping previous df {initial_df_k:.2f}.\")\n",
        "                self.dfs_[k] = initial_df_k\n",
        "\n",
        "        # Final parameter checks after M-step\n",
        "        self.scales_ = np.maximum(self.scales_, np.sqrt(self.reg_covar)) # Ensure scales are positive and regularized\n",
        "        self.dfs_ = np.clip(self.dfs_, self.min_df, self.max_df) # Ensure DoF are within bounds\n",
        "\n",
        "    def fit(self, X: np.ndarray):\n",
        "        \"\"\"Fit the mixture model to the data X.\"\"\"\n",
        "        if X.ndim == 1:\n",
        "            X = X.reshape(-1, 1)\n",
        "\n",
        "        if X.shape[0] == 0:\n",
        "            logger.warning(\"Fitting on empty data. Model will not be fitted.\")\n",
        "            self.converged_ = False # Or True, depending on desired state for an empty fit\n",
        "            # Set attributes to indicate no fit or default values\n",
        "            self.weights_ = np.ones(self.n_components) / self.n_components if self.n_components > 0 else np.array([])\n",
        "            self.means_ = np.zeros((self.n_components, X.shape[1])) if X.shape[1] > 0 and self.n_components > 0 else np.array([[]])\n",
        "            self.scales_ = np.ones((self.n_components, X.shape[1])) if X.shape[1] > 0 and self.n_components > 0 else np.array([[]])\n",
        "            self.dfs_ = np.ones(self.n_components) * self.min_df if self.n_components > 0 else np.array([])\n",
        "            return self\n",
        "\n",
        "        # Robustness: Check for NaNs or Infs in input X (Issue 1 related)\n",
        "        if not np.all(np.isfinite(X)):\n",
        "            logger.error(\"Input data X contains NaN or Inf values. Cannot fit StudentTMixture.\")\n",
        "            # Optionally, clean X here or raise an error\n",
        "            # X = X[np.all(np.isfinite(X), axis=1)]\n",
        "            # if X.shape[0] == 0:\n",
        "            #     logger.error(\"All rows removed due to NaN/Inf. Cannot fit.\")\n",
        "            #     return self\n",
        "            raise ValueError(\"Input data X for StudentTMixture contains NaN or Inf values.\")\n",
        "\n",
        "\n",
        "        try:\n",
        "            self._init_params(X)\n",
        "        except ValueError as e:\n",
        "            logger.error(f\"Error during initialization: {e}. Model cannot be fitted.\")\n",
        "            self.converged_ = False\n",
        "            return self\n",
        "\n",
        "        prev_ll = -np.inf\n",
        "        self.converged_ = False\n",
        "\n",
        "        for iteration in range(self.max_iter):\n",
        "            self.n_iter_ = iteration + 1\n",
        "            try:\n",
        "                responsibilities, current_ll = self._e_step(X)\n",
        "\n",
        "                if not np.isfinite(current_ll) or np.any(~np.isfinite(responsibilities)):\n",
        "                    logger.warning(f\"Warning: Non-finite values in E-step (Iter {self.n_iter_}, LL: {current_ll}). Stopping.\")\n",
        "                    # Attempt to recover or break\n",
        "                    if not np.isfinite(current_ll) and prev_ll != -np.inf :\n",
        "                        current_ll = prev_ll # Keep last good log-likelihood\n",
        "                    else: # Cannot recover, break\n",
        "                        break\n",
        "\n",
        "\n",
        "                self._m_step(X, responsibilities)\n",
        "\n",
        "                # Parameter sanity check after M-step\n",
        "                if (np.any(~np.isfinite(self.weights_)) or\n",
        "                    np.any(~np.isfinite(self.means_)) or\n",
        "                    np.any(~np.isfinite(self.scales_)) or\n",
        "                    np.any(~np.isfinite(self.dfs_))):\n",
        "                    logger.warning(f\"Warning: Non-finite parameters after M-step (Iter {self.n_iter_}). Stopping.\")\n",
        "                    break # Stop if parameters become non-finite\n",
        "\n",
        "                delta_ll = current_ll - prev_ll\n",
        "                # Check convergence, allow for slight decreases if tol is relative\n",
        "                # and prev_ll is very small.\n",
        "                abs_delta_ll = abs(delta_ll)\n",
        "\n",
        "                converged_abs = abs_delta_ll < self.tol\n",
        "                converged_rel = False\n",
        "                if np.isfinite(prev_ll) and abs(prev_ll) > 1e-6 and np.isfinite(delta_ll): # prev_ll not zero or -inf\n",
        "                    converged_rel = abs(delta_ll / prev_ll) < self.tol\n",
        "\n",
        "                # If LL decreases significantly, it might indicate instability\n",
        "                if delta_ll < -self.tol * 10 and iteration > 5 : # Allow some initial fluctuation\n",
        "                    logger.warning(f\"Log-likelihood decreased significantly from {prev_ll:.2f} to {current_ll:.2f} at iter {self.n_iter_}. Stopping.\")\n",
        "                    break\n",
        "\n",
        "\n",
        "                if converged_abs or converged_rel:\n",
        "                    self.converged_ = True\n",
        "                    logger.info(f\"Converged in {self.n_iter_} iterations. Log-likelihood: {current_ll:.4f}\")\n",
        "                    break\n",
        "\n",
        "                prev_ll = current_ll\n",
        "                self.log_likelihood_ = current_ll\n",
        "\n",
        "            except (np.linalg.LinAlgError, ValueError, FloatingPointError) as e:\n",
        "                logger.error(f\"Numerical error during EM iteration {self.n_iter_}: {e}. Stopping.\")\n",
        "                # Consider re-initializing or breaking more gracefully\n",
        "                break\n",
        "            except Exception as e: # Catch any other unexpected error\n",
        "                logger.error(f\"Unexpected error in EM iteration {self.n_iter_}: {e}. Stopping.\")\n",
        "                break\n",
        "\n",
        "\n",
        "        if not self.converged_ and self.n_iter_ == self.max_iter:\n",
        "            logger.warning(f\"EM algorithm did not converge within {self.max_iter} iterations. Final LL: {self.log_likelihood_:.4f}\")\n",
        "        elif not self.converged_ :\n",
        "             logger.warning(f\"EM algorithm stopped prematurely after {self.n_iter_} iterations due to issues. Final LL: {self.log_likelihood_:.4f}\")\n",
        "\n",
        "\n",
        "        # Ensure final parameters are valid, even if not converged\n",
        "        if self.weights_ is not None:\n",
        "            self.weights_ = np.maximum(self.weights_, 1e-9)\n",
        "            self.weights_ /= np.sum(self.weights_)\n",
        "        if self.scales_ is not None:\n",
        "            self.scales_ = np.maximum(self.scales_, np.sqrt(self.reg_covar))\n",
        "        if self.dfs_ is not None:\n",
        "            self.dfs_ = np.clip(self.dfs_, self.min_df, self.max_df)\n",
        "\n",
        "        # If any component weight is near zero, its parameters might be unreliable\n",
        "        if self.weights_ is not None and np.any(self.weights_ < 1e-5):\n",
        "            logger.info(\"Some components have very small weights. Their parameters might be less reliable.\")\n",
        "\n",
        "        return self\n",
        "\n",
        "    def score_samples(self, X: np.ndarray) -> np.ndarray:\n",
        "        \"\"\"Calculate the log probability density for each sample under the mixture.\"\"\"\n",
        "        if self.weights_ is None or self.means_ is None or self.scales_ is None or self.dfs_ is None:\n",
        "            raise ValueError(\"Model has not been fitted yet or fitting failed.\")\n",
        "        if X.ndim == 1:\n",
        "            X = X.reshape(-1, 1)\n",
        "\n",
        "        if X.shape[0] == 0:\n",
        "            return np.array([])\n",
        "\n",
        "        # Ensure input X is finite\n",
        "        if not np.all(np.isfinite(X)):\n",
        "            logger.warning(\"Input X for score_samples contains NaN/Inf. Results may be unpredictable.\")\n",
        "            # Decide on a strategy: error, clean, or proceed with caution.\n",
        "            # For now, proceed, but PDF might be NaN/Inf for those rows.\n",
        "\n",
        "        log_prob_norm = np.zeros((X.shape[0], self.n_components))\n",
        "        for k in range(self.n_components):\n",
        "            weight_k = np.maximum(self.weights_[k], 1e-9) # Smallest possible weight\n",
        "            log_pdf_k = self._log_t_pdf_component(X, self.means_[k], self.scales_[k], self.dfs_[k])\n",
        "\n",
        "            # Handle non-finite log_pdf_k (e.g., from extreme values in X)\n",
        "            log_pdf_k[~np.isfinite(log_pdf_k)] = -np.inf # Treat non-finite pdf as zero probability\n",
        "            log_prob_norm[:, k] = np.log(weight_k) + log_pdf_k\n",
        "\n",
        "        # Log-sum-exp for stability\n",
        "        max_log_prob = np.max(log_prob_norm, axis=1, keepdims=True)\n",
        "        # Handle cases where all log_prob_norm are -inf for a sample\n",
        "        finite_max_log_prob = np.where(np.isinf(max_log_prob), 0, max_log_prob)\n",
        "\n",
        "        log_density = finite_max_log_prob.flatten() + \\\n",
        "                      np.log(np.sum(np.exp(log_prob_norm - finite_max_log_prob), axis=1) + 1e-40) # Add epsilon for log(0)\n",
        "\n",
        "        # If original max_log_prob was -inf, the log_density should also be -inf\n",
        "        log_density[np.isinf(max_log_prob.flatten())] = -np.inf\n",
        "\n",
        "        return log_density\n",
        "\n",
        "    def predict_proba(self, X: np.ndarray) -> np.ndarray:\n",
        "        \"\"\"Predict posterior probability of each component given the data.\"\"\"\n",
        "        if self.weights_ is None:\n",
        "            raise ValueError(\"Model has not been fitted yet.\")\n",
        "        if X.ndim == 1:\n",
        "            X = X.reshape(-1, 1)\n",
        "\n",
        "        responsibilities, _ = self._e_step(X) # _e_step already handles normalization\n",
        "        return responsibilities\n",
        "\n",
        "    def predict(self, X: np.ndarray) -> np.ndarray:\n",
        "        \"\"\"Predict the hardest assignment to the mixture components for each sample.\"\"\"\n",
        "        return np.argmax(self.predict_proba(X), axis=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ULtML6nE4uo4"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import DataLoader, TensorDataset, Subset\n",
        "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
        "from torch.distributions import (\n",
        "    Normal, StudentT, Categorical, MixtureSameFamily, Independent,\n",
        "    MultivariateNormal, Distribution,utils as dist_utils # For broadcast_all\n",
        ")\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
        "from sklearn.model_selection import train_test_split # Not used in this file directly, but common\n",
        "from typing import List, Tuple, Optional, Dict, Any, Union, Sequence, NamedTuple\n",
        "import warnings\n",
        "import time\n",
        "import os\n",
        "import logging\n",
        "import math\n",
        "from collections import defaultdict\n",
        "\n",
        "\n",
        "# --- Custom Distributions (Potentially with enhancements) ---\n",
        "class StudentTDistribution(StudentT):\n",
        "    def __init__(self, df, loc, scale, validate_args=None, epsilon=1e-6):\n",
        "        df = torch.clamp(df, min=epsilon)\n",
        "        scale = torch.clamp(scale, min=epsilon)\n",
        "        # Use torch.distributions.utils.broadcast_all for robust broadcasting\n",
        "        try:\n",
        "            self.df_broadcast, self.loc_broadcast, self.scale_broadcast = dist_utils.broadcast_all(df, loc, scale)\n",
        "        except ValueError as e:\n",
        "            raise ValueError(f\"Broadcasting failed for StudentT params: df={df.shape}, loc={loc.shape}, scale={scale.shape}. Error: {e}\") from e\n",
        "        super().__init__(self.df_broadcast, self.loc_broadcast, self.scale_broadcast, validate_args=validate_args)\n",
        "        self._epsilon = epsilon\n",
        "\n",
        "    def rsample(self, sample_shape=torch.Size()):\n",
        "        # Relying on parent's rsample which is correctly implemented\n",
        "        if not hasattr(super(), 'rsample'):\n",
        "            raise NotImplementedError(\"Parent StudentT class does not have rsample method.\")\n",
        "        return super().rsample(sample_shape)\n",
        "\n",
        "class MixtureDistribution(MixtureSameFamily):\n",
        "    def __init__(self, mixture_distribution: Categorical,\n",
        "                 component_distribution: Distribution, validate_args=None):\n",
        "        num_components = mixture_distribution.logits.shape[-1]\n",
        "        current_logger_mix = get_logger(f\"{self.__class__.__name__}\", verbose=True) # Ensure logger is verbose for warnings\n",
        "\n",
        "        if num_components == 0:\n",
        "            current_logger_mix.warning(\"MixtureDistribution received 0 components. Defaulting to a single Normal component for stability.\")\n",
        "            # Create a dummy single component setup\n",
        "            # Infer batch_shape and event_shape from the (empty) component_distribution\n",
        "            dummy_event_shape = component_distribution.event_shape\n",
        "            if isinstance(component_distribution, Independent): # For Independent distributions\n",
        "                dummy_batch_shape = component_distribution.base_dist.batch_shape[:-1] # Exclude component dim\n",
        "            else: # For distributions where last batch dim is component dim\n",
        "                dummy_batch_shape = component_distribution.batch_shape[:-1] if len(component_distribution.batch_shape) > 0 else torch.Size([])\n",
        "\n",
        "            device = mixture_distribution.logits.device\n",
        "            dummy_mix_logits = torch.ones(dummy_batch_shape + torch.Size([1]), device=device) # Single component, prob 1\n",
        "            mixture_distribution = Categorical(logits=dummy_mix_logits, validate_args=validate_args)\n",
        "\n",
        "            # Create a default Normal component\n",
        "            comp_loc = torch.zeros(dummy_batch_shape + torch.Size([1]) + dummy_event_shape, device=device)\n",
        "            comp_scale = torch.ones(dummy_batch_shape + torch.Size([1]) + dummy_event_shape, device=device)\n",
        "            base_dist = Normal(loc=comp_loc, scale=comp_scale, validate_args=validate_args)\n",
        "\n",
        "            # If original component_distribution was Independent, wrap the new base_dist similarly\n",
        "            if isinstance(component_distribution, Independent):\n",
        "                 component_distribution = Independent(base_dist, len(dummy_event_shape)) # Reinterpret_batch_ndims should match original\n",
        "            else:\n",
        "                 component_distribution = base_dist # If original wasn't Independent\n",
        "\n",
        "        super().__init__(mixture_distribution, component_distribution, validate_args=validate_args)\n",
        "\n",
        "\n",
        "# --- Base VAE Architecture ---\n",
        "class VariationalAutoencoderBase(nn.Module):\n",
        "    def __init__(self,\n",
        "                 input_dim: int,\n",
        "                 encoder_layer_sizes: List[int],\n",
        "                 latent_dim: int,\n",
        "                 decoder_layer_sizes: List[int],\n",
        "                 activation_fn: nn.Module = nn.ReLU(),\n",
        "                 use_batch_norm: bool = True,\n",
        "                 prior_type: str = 'standard_gaussian',\n",
        "                 n_prior_components: int = 1,\n",
        "                 prior_params: Optional[Dict[str, Any]] = None,\n",
        "                 learn_prior: bool = False,\n",
        "                 device: Optional[torch.device] = None):\n",
        "        super().__init__()\n",
        "        self.input_dim = input_dim\n",
        "        self.latent_dim = latent_dim\n",
        "        self.activation_fn = activation_fn\n",
        "        self.use_batch_norm = use_batch_norm\n",
        "        self.prior_type = prior_type\n",
        "        self.n_prior_components = max(1, n_prior_components) # Ensure at least 1 component\n",
        "        self.learn_prior = learn_prior\n",
        "        self.device_ = device if device else torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "        self.logger = get_logger(f\"{self.__class__.__name__}(In:{input_dim},Lat:{latent_dim})\", verbose=True)\n",
        "\n",
        "        # Encoder\n",
        "        encoder_modules = []\n",
        "        current_dim_enc = input_dim\n",
        "        for i, layer_size in enumerate(encoder_layer_sizes):\n",
        "            encoder_modules.append(nn.Linear(current_dim_enc, layer_size))\n",
        "            if self.use_batch_norm:\n",
        "                encoder_modules.append(nn.BatchNorm1d(layer_size))\n",
        "            encoder_modules.append(activation_fn)\n",
        "            current_dim_enc = layer_size\n",
        "        self.encoder_net = nn.Sequential(*encoder_modules)\n",
        "        self.fc_mu = nn.Linear(current_dim_enc, latent_dim)\n",
        "        self.fc_logvar = nn.Linear(current_dim_enc, latent_dim)\n",
        "\n",
        "        # Decoder (Corrected logic from original prompt for clarity)\n",
        "        decoder_main_modules = []\n",
        "        current_dim_dec = latent_dim\n",
        "        for i, layer_size in enumerate(decoder_layer_sizes):\n",
        "            decoder_main_modules.append(nn.Linear(current_dim_dec, layer_size))\n",
        "            if self.use_batch_norm:\n",
        "                decoder_main_modules.append(nn.BatchNorm1d(layer_size))\n",
        "            decoder_main_modules.append(activation_fn)\n",
        "            current_dim_dec = layer_size\n",
        "\n",
        "        # Final output layer for X reconstruction\n",
        "        self.decoder_output_x = nn.Linear(current_dim_dec, input_dim)\n",
        "        decoder_main_modules.append(self.decoder_output_x)\n",
        "        self.decoder_net_x = nn.Sequential(*decoder_main_modules)\n",
        "\n",
        "        self._setup_prior(prior_params if prior_params else {})\n",
        "        self.to(self.device_)\n",
        "        self.logger.debug(f\"VAEBase: InputDim={input_dim}, Enc={encoder_layer_sizes}, Latent={latent_dim}, Dec={decoder_layer_sizes}, Prior={prior_type}(K={n_prior_components}) on {self.device_}\")\n",
        "\n",
        "\n",
        "    def _init_param(self, data: Optional[Any], default_shape: Tuple[int, ...], default_val: Any) -> Union[torch.Tensor, nn.Parameter]:\n",
        "        param_wrapper = nn.Parameter if self.learn_prior else lambda x: x # Apply nn.Parameter if prior is learnable\n",
        "        val_to_wrap = None\n",
        "        data_tensor: Optional[torch.Tensor] = None\n",
        "\n",
        "        if data is not None:\n",
        "            if not isinstance(data, torch.Tensor):\n",
        "                try:\n",
        "                    data_tensor = torch.tensor(data, dtype=torch.float32) # Device set later\n",
        "                except Exception as e:\n",
        "                    self.logger.warning(f\"Could not convert prior param data to tensor: {e}. Defaulting.\")\n",
        "                    data_tensor = None\n",
        "            else:\n",
        "                data_tensor = data.clone().detach() # Use clone of input tensor\n",
        "\n",
        "            if data_tensor is not None:\n",
        "                if not torch.isfinite(data_tensor).all():\n",
        "                    self.logger.warning(f\"Non-finite values in provided prior param data. Defaulting. Data: {data_tensor}\")\n",
        "                    data_tensor = None\n",
        "                elif data_tensor.shape == default_shape:\n",
        "                    val_to_wrap = data_tensor\n",
        "                elif data_tensor.numel() == 1 and default_shape != torch.Size([]): # Scalar to expand\n",
        "                    val_to_wrap = data_tensor.expand(default_shape)\n",
        "                else:\n",
        "                    self.logger.warning(f\"Prior param shape mismatch (Provided: {data_tensor.shape}, Expected: {default_shape}). Defaulting.\")\n",
        "                    data_tensor = None # Signal to use default\n",
        "\n",
        "        if val_to_wrap is None: # Use default_val\n",
        "            if isinstance(default_val, torch.Tensor):\n",
        "                if default_val.shape == default_shape:\n",
        "                    val_to_wrap = default_val.clone().detach()\n",
        "                elif default_val.numel() == 1 and default_shape != torch.Size([]):\n",
        "                    val_to_wrap = default_val.expand(default_shape)\n",
        "                else: # Should not happen if defaults are well-defined\n",
        "                    self.logger.error(f\"Default tensor shape {default_val.shape} vs expected {default_shape}. Using zeros.\")\n",
        "                    val_to_wrap = torch.zeros(default_shape, dtype=torch.float32)\n",
        "            else: # default_val is a scalar (e.g. float, int)\n",
        "                val_to_wrap = torch.full(default_shape, float(default_val), dtype=torch.float32)\n",
        "\n",
        "        return param_wrapper(val_to_wrap.to(self.device_))\n",
        "\n",
        "\n",
        "    def _setup_prior(self, prior_params_dict: Dict[str, Any]):\n",
        "        # Default parameters are defined on the target device directly\n",
        "        default_mean = torch.zeros(self.n_prior_components, self.latent_dim, device=self.device_)\n",
        "        default_cov_chol = torch.eye(self.latent_dim, device=self.device_).unsqueeze(0).expand(self.n_prior_components, self.latent_dim, self.latent_dim)\n",
        "        default_weights_logits = torch.zeros(self.n_prior_components, device=self.device_)\n",
        "        default_df_student_t = torch.full((self.n_prior_components, self.latent_dim), 4.0, device=self.device_) # Default DoF for Student-t\n",
        "\n",
        "        self.prior_weights_logits = self._init_param(prior_params_dict.get('weights_logits'), (self.n_prior_components,), default_weights_logits)\n",
        "        self.prior_means = self._init_param(prior_params_dict.get('means'), (self.n_prior_components, self.latent_dim), default_mean)\n",
        "        self.prior_cov_cholesky = self._init_param(prior_params_dict.get('cov_cholesky'), (self.n_prior_components, self.latent_dim, self.latent_dim), default_cov_chol)\n",
        "\n",
        "        self.prior_student_t_df = None # Initialize\n",
        "        if 'student_t' in self.prior_type.lower():\n",
        "            self.prior_student_t_df = self._init_param(prior_params_dict.get('df'), (self.n_prior_components, self.latent_dim), default_df_student_t)\n",
        "        self.logger.debug(f\"Prior setup: Type='{self.prior_type}', K={self.n_prior_components}, Learnable={self.learn_prior}\")\n",
        "\n",
        "    def get_prior_distribution(self) -> Distribution:\n",
        "        # Ensure parameters are on the correct device, though _init_param should handle this.\n",
        "        # Accessing them directly here, e.g., self.prior_means.to(self.device_) is redundant if _init_param is correct.\n",
        "        try:\n",
        "            if self.n_prior_components == 1 and 'mixture' not in self.prior_type.lower(): # Single component prior\n",
        "                loc = self.prior_means.squeeze(0) # Shape: (latent_dim)\n",
        "                chol = self.prior_cov_cholesky.squeeze(0) # Shape: (latent_dim, latent_dim)\n",
        "\n",
        "                if 'student_t' in self.prior_type.lower():\n",
        "                    if self.prior_student_t_df is None: raise ValueError(\"Student-T prior_student_t_df is None.\")\n",
        "                    df = self.prior_student_t_df.squeeze(0).clamp(min=1e-6) # Shape: (latent_dim)\n",
        "\n",
        "                    # For Independent StudentT, scale is per dimension\n",
        "                    cov_matrix_diag = torch.sum(chol * chol, dim=-1) # Diagonal elements of covariance if chol is diagonal\n",
        "                                                                    # If chol is full, (chol @ chol.T).diag()\n",
        "                    if chol.shape == (self.latent_dim, self.latent_dim) and torch.allclose(chol, torch.diag(torch.diag(chol))): # Is diagonal\n",
        "                        scales = torch.diag(chol).clamp(min=1e-6)\n",
        "                    else: # Full Cholesky, get diagonal variances\n",
        "                         scales = torch.sqrt((chol @ chol.transpose(-1, -2)).diag()).clamp(min=1e-6)\n",
        "\n",
        "                    if loc.ndim == 0: loc = loc.unsqueeze(0) # Ensure 1D for component dist\n",
        "                    if scales.ndim == 0: scales = scales.unsqueeze(0)\n",
        "                    if df.ndim == 0: df = df.unsqueeze(0)\n",
        "\n",
        "                    base_dist = StudentTDistribution(df=df, loc=loc, scale=scales) # Batch shape (latent_dim), event_shape ()\n",
        "                    return Independent(base_dist, 1) # Makes event_shape (latent_dim)\n",
        "                else: # Single Multivariate Gaussian\n",
        "                    return MultivariateNormal(loc=loc, scale_tril=chol)\n",
        "            else: # Mixture prior\n",
        "                mix = Categorical(logits=self.prior_weights_logits) # Mixture weights, shape (K)\n",
        "\n",
        "                if 'student_t' in self.prior_type.lower():\n",
        "                    if self.prior_student_t_df is None: raise ValueError(\"Student-T mixture prior_student_t_df is None.\")\n",
        "                    df_mix = self.prior_student_t_df.clamp(min=1e-6) # Shape: (K, latent_dim)\n",
        "                    loc_mix = self.prior_means # Shape: (K, latent_dim)\n",
        "\n",
        "                    # For Independent StudentT components in a mixture\n",
        "                    # Assuming self.prior_cov_cholesky are Cholesky factors of *diagonal* covariances for each component\n",
        "                    chol_mix = self.prior_cov_cholesky # Shape: (K, latent_dim, latent_dim)\n",
        "                    if torch.allclose(chol_mix, torch.diag_embed(torch.diagonal(chol_mix, dim1=-2, dim2=-1))): # Check if all components have diagonal chols\n",
        "                        scales_mix = torch.diagonal(chol_mix, dim1=-2, dim2=-1).clamp(min=1e-6) # Shape: (K, latent_dim)\n",
        "                    else: # If full cholesky per component\n",
        "                         scales_mix = torch.sqrt(torch.diagonal(chol_mix @ chol_mix.transpose(-1,-2), dim1=-2, dim2=-1)).clamp(min=1e-6)\n",
        "\n",
        "                    base_dist_mix = StudentTDistribution(df=df_mix, loc=loc_mix, scale=scales_mix) # Batch_shape (K, latent_dim), event_shape ()\n",
        "                    comp = Independent(base_dist_mix, 1) # Makes component event_shape (latent_dim), batch_shape (K)\n",
        "                else: # Gaussian Mixture\n",
        "                    comp = MultivariateNormal(loc=self.prior_means, scale_tril=self.prior_cov_cholesky) # Batch_shape (K), event_shape (latent_dim)\n",
        "\n",
        "                return MixtureDistribution(mix, comp)\n",
        "        except Exception as e:\n",
        "            self.logger.error(f\"Error creating prior distribution (type: {self.prior_type}, K: {self.n_prior_components}): {e}. Defaulting to Standard Normal.\", exc_info=True)\n",
        "            return Independent(Normal(torch.zeros(self.latent_dim, device=self.device_),\n",
        "                                      torch.ones(self.latent_dim, device=self.device_)), 1)\n",
        "\n",
        "    def encode(self, x: torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor]:\n",
        "        h = self.encoder_net(x)\n",
        "        mu = self.fc_mu(h)\n",
        "        logvar = self.fc_logvar(h)\n",
        "        return mu, logvar\n",
        "\n",
        "    def reparameterize(self, mu: torch.Tensor, logvar: torch.Tensor) -> torch.Tensor:\n",
        "        std = torch.exp(0.5 * logvar)\n",
        "        eps = torch.randn_like(std)\n",
        "        return mu + eps * std\n",
        "\n",
        "    def decode_x(self, z: torch.Tensor) -> torch.Tensor:\n",
        "        \"\"\"Decodes latent variable z to reconstruct X (input features).\"\"\"\n",
        "        return self.decoder_net_x(z)\n",
        "\n",
        "\n",
        "# --- Semi-Supervised MIWAE Model (Includes Price Heads) ---\n",
        "class SemiSupMIWAE(VariationalAutoencoderBase):\n",
        "    def __init__(self,\n",
        "                 input_dim: int, # Dimension of X features\n",
        "                 y_dim: int,     # Dimension of Y target (e.g., price, typically 1)\n",
        "                 encoder_layer_sizes: List[int],\n",
        "                 latent_dim: int,\n",
        "                 decoder_layer_sizes: List[int], # For X reconstruction\n",
        "                 price_head_layer_sizes: List[int], # For Y prediction\n",
        "                 activation_fn: nn.Module = nn.ReLU(),\n",
        "                 use_batch_norm: bool = True,\n",
        "                 prior_type: str = 'standard_gaussian',\n",
        "                 n_prior_components: int = 1,\n",
        "                 prior_params: Optional[Dict[str, Any]] = None,\n",
        "                 learn_prior: bool = False,\n",
        "                 alpha_price_loss: float = 1.0, # Weight for the price prediction loss\n",
        "                 device: Optional[torch.device] = None):\n",
        "        super().__init__(input_dim=input_dim, encoder_layer_sizes=encoder_layer_sizes,\n",
        "                         latent_dim=latent_dim, decoder_layer_sizes=decoder_layer_sizes,\n",
        "                         activation_fn=activation_fn, use_batch_norm=use_batch_norm,\n",
        "                         prior_type=prior_type, n_prior_components=n_prior_components,\n",
        "                         prior_params=prior_params, learn_prior=learn_prior, device=device)\n",
        "        self.y_dim = y_dim\n",
        "        self.alpha_price_loss = alpha_price_loss\n",
        "        # Update logger name for clarity\n",
        "        self.logger = get_logger(f\"{self.__class__.__name__}(X:{input_dim},Y:{y_dim},Lat:{latent_dim})\", verbose=True)\n",
        "\n",
        "\n",
        "        # Price Prediction Heads (operate on latent mu)\n",
        "        price_mean_modules = []\n",
        "        price_logvar_modules = [] # Separate head for uncertainty (logvariance)\n",
        "\n",
        "        current_dim_price_head = latent_dim # Input to price head is the latent mean (mu)\n",
        "        for i, layer_size in enumerate(price_head_layer_sizes):\n",
        "            price_mean_modules.append(nn.Linear(current_dim_price_head, layer_size))\n",
        "            price_logvar_modules.append(nn.Linear(current_dim_price_head, layer_size))\n",
        "            if self.use_batch_norm: # Can use BN in price head too\n",
        "                price_mean_modules.append(nn.BatchNorm1d(layer_size))\n",
        "                price_logvar_modules.append(nn.BatchNorm1d(layer_size))\n",
        "            price_mean_modules.append(activation_fn)\n",
        "            price_logvar_modules.append(activation_fn)\n",
        "            current_dim_price_head = layer_size\n",
        "\n",
        "        self.price_mean_output = nn.Linear(current_dim_price_head, y_dim)\n",
        "        self.price_logvar_output = nn.Linear(current_dim_price_head, y_dim) # Predicts log-variance of y\n",
        "\n",
        "        price_mean_modules.append(self.price_mean_output)\n",
        "        price_logvar_modules.append(self.price_logvar_output)\n",
        "\n",
        "        self.price_mean_head = nn.Sequential(*price_mean_modules)\n",
        "        self.price_logvar_head = nn.Sequential(*price_logvar_modules)\n",
        "\n",
        "        self.logger.debug(f\"Price Mean Head (for Y mean): {self.price_mean_head}\")\n",
        "        self.logger.debug(f\"Price LogVar Head (for Y logvar): {self.price_logvar_head}\")\n",
        "        self.to(self.device_) # Ensure sub-modules are also moved\n",
        "        self.logger.info(f\"SemiSupMIWAE Initialized: InputXDim={input_dim}, TargetYDim={y_dim}, \"\n",
        "                         f\"LatentDim={latent_dim}, AlphaPriceLoss={self.alpha_price_loss}, Device={self.device_}\")\n",
        "\n",
        "\n",
        "    def forward(self, x: torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor, torch.Tensor, torch.Tensor]:\n",
        "        \"\"\"\n",
        "        Forward pass for training.\n",
        "        Returns: reconstruction_x, mu_latent, logvar_latent, y_pred_mu, y_pred_logvar\n",
        "        \"\"\"\n",
        "        mu_latent, logvar_latent = self.encode(x)\n",
        "        z = self.reparameterize(mu_latent, logvar_latent)\n",
        "        reconstruction_x = self.decode_x(z) # Reconstruct X features\n",
        "\n",
        "        # Price prediction heads take latent mu as input\n",
        "        y_pred_mu = self.price_mean_head(mu_latent)\n",
        "        y_pred_logvar = self.price_logvar_head(mu_latent)\n",
        "\n",
        "        return reconstruction_x, mu_latent, logvar_latent, y_pred_mu, y_pred_logvar\n",
        "\n",
        "    def predict_price(self, x: torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor]:\n",
        "        \"\"\"\n",
        "        Predicts price (y_pred_mu) and its log-variance (y_pred_logvar) from input X.\n",
        "        For inference/prediction time.\n",
        "        \"\"\"\n",
        "        self.eval() # Ensure model is in evaluation mode\n",
        "        with torch.no_grad():\n",
        "            mu_latent, _ = self.encode(x) # Only need mu_latent for price prediction\n",
        "            y_pred_mu = self.price_mean_head(mu_latent)\n",
        "            y_pred_logvar = self.price_logvar_head(mu_latent)\n",
        "        return y_pred_mu, y_pred_logvar\n",
        "\n",
        "# (VAETrainer, prepare_vae_input, create_vae_from_artifacts, and helper functions will follow)\n",
        "# ... (Rest of the VAE Pipeline file starting from VAETrainer)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dUqeCQLJu_at"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Assuming necessary imports are available from other modules or defined above:\n",
        "# import torch, torch.nn as nn, torch.optim as optim, etc.\n",
        "# from vae_base_module import VariationalAutoencoderBase, SemiSupMIWAE # Example import\n",
        "# from logger_setup import get_logger # Example import\n",
        "# from distributions import StudentTDistribution, MixtureDistribution # Example import\n",
        "\n",
        "import math\n",
        "import os\n",
        "import pickle\n",
        "import time\n",
        "import logging\n",
        "from typing import List, Optional, Dict, Tuple, Union, Any, NamedTuple\n",
        "from collections import defaultdict\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import DataLoader, TensorDataset, Subset\n",
        "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
        "# Import necessary distributions\n",
        "from torch.distributions import (\n",
        "    Normal, StudentT, Categorical, MixtureSameFamily, Independent,\n",
        "    MultivariateNormal, Distribution, utils as dist_utils\n",
        ")\n",
        "# Example Scaler/Model imports (adjust paths as needed)\n",
        "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
        "\n",
        "# --- VAE Trainer Class (Refactored for Semi-Supervised MIWAE) ---\n",
        "class VAETrainer:\n",
        "    \"\"\"\n",
        "    Handles training, evaluation, imputation, saving/loading of VAE/SemiSupMIWAE models.\n",
        "    Manages loss calculation including reconstruction for X, KLD, and optional supervised loss for Y (target).\n",
        "    \"\"\"\n",
        "    def __init__(self,\n",
        "                 model: Union[VariationalAutoencoderBase, SemiSupMIWAE],\n",
        "                 learning_rate: float = 1e-3,\n",
        "                 kld_weight: float = 1.0, # Base KLD weight (beta), can be annealed\n",
        "                 alpha_price_loss: Optional[float] = None, # Weight for supervised Y loss, trainer can override model's alpha\n",
        "                 optimizer_str: str = 'adam',\n",
        "                 reconstruction_loss_type_x: str = 'mse', # Loss type for X features: 'mse' or 'huber'\n",
        "                 price_loss_type_y: str = 'gaussian_nll', # Loss type for Y target: 'gaussian_nll' or 'mse'\n",
        "                 huber_delta: float = 1.0, # Delta for Huber loss (if used for X)\n",
        "                 device: Optional[torch.device] = None,\n",
        "                 results_dir: str = \"vae_results\",\n",
        "                 verbose: bool = True):\n",
        "\n",
        "        self.model = model\n",
        "        self.is_semi_supervised = isinstance(model, SemiSupMIWAE)\n",
        "        self.device = device if device else torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "        self.results_dir = results_dir\n",
        "        self.verbose = verbose\n",
        "        self.kld_weight = kld_weight\n",
        "        self.logger = get_logger(self.__class__.__name__, verbose)\n",
        "\n",
        "        # Determine alpha_price_loss for the Y target\n",
        "        if self.is_semi_supervised:\n",
        "            model_alpha = getattr(model, 'alpha_price_loss', 1.0) # Default from model if exists\n",
        "            if alpha_price_loss is not None: # Trainer override\n",
        "                self.alpha_price_loss = alpha_price_loss\n",
        "                if abs(self.alpha_price_loss - model_alpha) > 1e-6: # Log if different\n",
        "                     self.logger.info(f\"Trainer overriding model's alpha_price_loss ({model_alpha}) with: {self.alpha_price_loss}\")\n",
        "            else: # Use model's alpha if trainer doesn't override\n",
        "                self.alpha_price_loss = model_alpha\n",
        "        else: # Unsupervised case\n",
        "            self.alpha_price_loss = 0.0\n",
        "\n",
        "        self.reconstruction_loss_type_x = reconstruction_loss_type_x.lower()\n",
        "        self.price_loss_type_y = price_loss_type_y.lower() if self.is_semi_supervised else 'none'\n",
        "        self.huber_delta = huber_delta\n",
        "\n",
        "        os.makedirs(self.results_dir, exist_ok=True)\n",
        "        self.model.to(self.device)\n",
        "\n",
        "        # Setup Optimizer\n",
        "        optimizer_map = {'adam': optim.Adam, 'adamw': optim.AdamW, 'rmsprop': optim.RMSprop}\n",
        "        optimizer_class = optimizer_map.get(optimizer_str.lower(), optim.Adam)\n",
        "        if optimizer_str.lower() not in optimizer_map:\n",
        "            self.logger.warning(f\"Unknown optimizer '{optimizer_str}'. Defaulting to Adam.\")\n",
        "        self.optimizer = optimizer_class(self.model.parameters(), lr=learning_rate)\n",
        "\n",
        "        self.history: Dict[str, List[float]] = defaultdict(list) # Stores training history\n",
        "\n",
        "        model_type_str = self.model.__class__.__name__\n",
        "        self._log(f\"VAETrainer initialized for {model_type_str}. Device: {self.device}. \"\n",
        "                  f\"Optimizer: {optimizer_str}. LR: {learning_rate}. \"\n",
        "                  f\"ReconLoss(X): '{self.reconstruction_loss_type_x}'. \"\n",
        "                  f\"Alpha(Y Loss): {self.alpha_price_loss if self.is_semi_supervised else 'N/A'}. \"\n",
        "                  f\"LossType(Y): '{self.price_loss_type_y if self.is_semi_supervised else 'N/A'}'\",\n",
        "                  level=\"info\")\n",
        "\n",
        "    def _log(self, message: str, level: str = \"info\", exc_info: bool = False):\n",
        "        \"\"\"Helper for conditional logging based on self.verbose\"\"\"\n",
        "        current_log_level_val = getattr(logging, level.upper(), logging.INFO)\n",
        "        if self.verbose or current_log_level_val >= logging.WARNING:\n",
        "            log_func = getattr(self.logger, level, self.logger.info)\n",
        "            log_func(message, exc_info=exc_info)\n",
        "\n",
        "    def _calculate_y_loss(self, y_true: torch.Tensor, y_pred_mu: torch.Tensor, y_pred_logvar: torch.Tensor, y_mask: torch.Tensor) -> torch.Tensor:\n",
        "        \"\"\"\n",
        "        Calculates the loss for the target variable Y, considering only observed values specified by y_mask.\n",
        "        Supports 'gaussian_nll' or 'mse'.\n",
        "        \"\"\"\n",
        "        # Ensure y_true, y_pred_mu, y_pred_logvar, y_mask have compatible shapes\n",
        "        # y_true, y_pred_mu, y_pred_logvar likely (batch_size, y_dim)\n",
        "        # y_mask likely (batch_size, y_dim)\n",
        "\n",
        "        num_observed_y = y_mask.sum().clamp(min=1) # Avoid division by zero\n",
        "\n",
        "        if self.price_loss_type_y == 'gaussian_nll':\n",
        "            y_pred_logvar_clamped = torch.clamp(y_pred_logvar, min=-15.0, max=15.0) # Stability\n",
        "            variance = torch.exp(y_pred_logvar_clamped) + 1e-9 # Add epsilon\n",
        "            # Gaussian NLL per element: 0.5 * (log(2*pi) + log_variance + ((y_true - y_pred_mu)^2 / variance))\n",
        "            elementwise_loss = 0.5 * (math.log(2 * math.pi) + y_pred_logvar_clamped +\n",
        "                                     F.mse_loss(y_pred_mu, y_true, reduction='none') / variance)\n",
        "        elif self.price_loss_type_y == 'mse':\n",
        "            # Simple MSE loss for Y, does not use y_pred_logvar\n",
        "            elementwise_loss = F.mse_loss(y_pred_mu, y_true, reduction='none')\n",
        "        else:\n",
        "            self.logger.error(f\"Unknown price_loss_type_y: '{self.price_loss_type_y}'. Add support or use 'gaussian_nll'/'mse'. Defaulting to MSE.\")\n",
        "            elementwise_loss = F.mse_loss(y_pred_mu, y_true, reduction='none')\n",
        "\n",
        "        # Apply mask (element-wise multiply) and average over observed elements\n",
        "        masked_loss = elementwise_loss * y_mask\n",
        "        final_y_loss = masked_loss.sum() / num_observed_y\n",
        "\n",
        "        # Safety check for NaN/Inf in final Y loss\n",
        "        if torch.isnan(final_y_loss) or torch.isinf(final_y_loss):\n",
        "            self.logger.error(f\"Loss for Y (target) became NaN/Inf using type '{self.price_loss_type_y}'. \"\n",
        "                              f\"Check inputs: y_true range [{y_true.min():.2f},{y_true.max():.2f}], \"\n",
        "                              f\"y_pred_mu range [{y_pred_mu.min():.2f},{y_pred_mu.max():.2f}], \"\n",
        "                              f\"y_pred_logvar range [{y_pred_logvar.min():.2f},{y_pred_logvar.max():.2f}]. \"\n",
        "                              f\"Observed Y count: {num_observed_y.item()}. Setting Y loss to 0 for this batch.\")\n",
        "            return torch.tensor(0.0, device=self.device)\n",
        "\n",
        "        return final_y_loss\n",
        "\n",
        "    def _calculate_loss(self,\n",
        "                        x_batch: torch.Tensor,       # Input features X, (batch, x_dim)\n",
        "                        x_mask_batch: torch.Tensor,  # Mask for X, (batch, x_dim)\n",
        "                        y_batch: Optional[torch.Tensor], # Target Y, (batch, y_dim)\n",
        "                        y_mask_batch: Optional[torch.Tensor],# Mask for Y, (batch, y_dim)\n",
        "                        model_outputs: Tuple,        # Outputs from model.forward()\n",
        "                        current_beta_kld: float = 1.0  # Annealing factor for KLD\n",
        "                       ) -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor, torch.Tensor]:\n",
        "        \"\"\"Calculates the total loss (ELBO negative) and its components.\"\"\"\n",
        "\n",
        "        # Unpack model outputs\n",
        "        if self.is_semi_supervised:\n",
        "            recon_x, mu_latent, logvar_latent, y_pred_mu, y_pred_logvar = model_outputs\n",
        "        else: # Unsupervised\n",
        "            recon_x, mu_latent, logvar_latent = model_outputs\n",
        "            y_pred_mu, y_pred_logvar, y_batch, y_mask_batch = None, None, None, None # Ensure Y related vars are None\n",
        "\n",
        "        # --- 1. Reconstruction Loss for X (MIWAE style) ---\n",
        "        num_observed_x = x_mask_batch.sum().clamp(min=1)\n",
        "        if self.reconstruction_loss_type_x == 'huber':\n",
        "            elementwise_loss_x = F.huber_loss(recon_x, x_batch, reduction='none', delta=self.huber_delta)\n",
        "        else: # Default 'mse'\n",
        "            elementwise_loss_x = F.mse_loss(recon_x, x_batch, reduction='none')\n",
        "        recon_loss_x = (elementwise_loss_x * x_mask_batch).sum() / num_observed_x\n",
        "\n",
        "        # --- 2. KL Divergence ---\n",
        "        kld_loss = -0.5 * torch.sum(1 + logvar_latent - mu_latent.pow(2) - logvar_latent.exp(), dim=1).mean()\n",
        "        if torch.isnan(kld_loss) or torch.isinf(kld_loss):\n",
        "            self.logger.error(f\"KLD loss is {kld_loss.item()}. Inputs: mu_mean={mu_latent.mean():.2f}, logvar_mean={logvar_latent.mean():.2f}. Setting KLD to 0.\")\n",
        "            kld_loss = torch.tensor(0.0, device=self.device)\n",
        "\n",
        "        # --- 3. Supervised Prediction Loss for Y ---\n",
        "        loss_y = torch.tensor(0.0, device=self.device)\n",
        "        if self.is_semi_supervised and y_batch is not None and y_mask_batch is not None:\n",
        "             # Ensure all required tensors for Y loss are present\n",
        "             if y_pred_mu is not None and y_pred_logvar is not None:\n",
        "                 if y_mask_batch.sum() > 0: # Calculate only if observed Y exist\n",
        "                     loss_y = self._calculate_y_loss(y_batch, y_pred_mu, y_pred_logvar, y_mask_batch)\n",
        "             else: # Should not happen if model is SemiSupMIWAE, but safeguard\n",
        "                 self.logger.error(\"Semi-supervised mode, but Y predictions (mu/logvar) are missing from model output.\")\n",
        "        # else: loss_y remains 0 for unsupervised or if Y data/predictions are missing\n",
        "\n",
        "        # --- Total Loss (Negative ELBO + Supervised Loss) ---\n",
        "        # Loss = ReconLoss_X + beta * KLD + alpha * Loss_Y\n",
        "        total_loss = recon_loss_x + current_beta_kld * kld_loss + self.alpha_price_loss * loss_y\n",
        "\n",
        "        if torch.isnan(total_loss):\n",
        "             self.logger.error(f\"Total loss is NaN! Components -> ReconX: {recon_loss_x.item()}, KLD: {kld_loss.item()}, LossY: {loss_y.item()}. Skipping update.\")\n",
        "             # Potentially return sentinel or raise error depending on desired behavior\n",
        "             # For now, let it propagate and potentially get caught in train loop\n",
        "\n",
        "        return total_loss, recon_loss_x, kld_loss, loss_y # Return components for logging\n",
        "\n",
        "    def train(self,\n",
        "              train_dataset: TensorDataset,\n",
        "              val_dataset: Optional[TensorDataset] = None,\n",
        "              epochs: int = 50,\n",
        "              batch_size: int = 64,\n",
        "              kld_anneal_epochs: int = 0,\n",
        "              early_stopping_patience: Optional[int] = None,\n",
        "              print_every_n_epochs: int = 1,\n",
        "              scheduler_patience: int = 5,\n",
        "              scheduler_factor: float = 0.2\n",
        "             ) -> Dict[str, List[float]]:\n",
        "        \"\"\"Trains the VAE model.\"\"\"\n",
        "        self._log(f\"Starting training: Epochs={epochs}, BatchSize={batch_size}, \"\n",
        "                  f\"KLD_weight={self.kld_weight}, KLD_AnnealEpochs={kld_anneal_epochs}, \"\n",
        "                  f\"Alpha(Y_Loss)={self.alpha_price_loss if self.is_semi_supervised else 'N/A'}.\",\n",
        "                  level=\"info\")\n",
        "\n",
        "        # Setup DataLoaders\n",
        "        use_pin_memory = self.device.type == 'cuda'\n",
        "        num_workers = min(4, os.cpu_count() or 1) # Sensible default\n",
        "        should_drop_last = (len(train_dataset) % batch_size == 1) and getattr(self.model, 'use_batch_norm', False)\n",
        "        train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True,\n",
        "                                  drop_last=should_drop_last, num_workers=num_workers, pin_memory=use_pin_memory)\n",
        "        self._log(f\"Train loader: {len(train_dataset)} samples, {len(train_loader)} batches. Drop last: {should_drop_last}\")\n",
        "\n",
        "        val_loader = None; best_val_loss = float('inf'); epochs_no_improve = 0\n",
        "        if val_dataset:\n",
        "            val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False,\n",
        "                                    num_workers=num_workers, pin_memory=use_pin_memory)\n",
        "            self._log(f\"Validation loader: {len(val_dataset)} samples, {len(val_loader)} batches.\")\n",
        "        else:\n",
        "            self._log(\"No validation dataset. Early stopping/best model saving disabled.\")\n",
        "            early_stopping_patience = None # Disable if no val data\n",
        "\n",
        "        # Setup Scheduler\n",
        "        scheduler = ReduceLROnPlateau(self.optimizer, mode='min', factor=scheduler_factor,\n",
        "                                      patience=scheduler_patience)\n",
        "\n",
        "        # --- Training Loop ---\n",
        "        for epoch in range(epochs):\n",
        "            self.model.train() # Set model to training mode\n",
        "            epoch_losses = defaultdict(float) # Accumulate losses for the epoch average\n",
        "            num_batches_processed = 0\n",
        "            start_time_epoch = time.time()\n",
        "\n",
        "            # KLD annealing factor (beta) for this epoch\n",
        "            current_beta_kld = self.kld_weight * min(1.0, (epoch + 1) / kld_anneal_epochs) if kld_anneal_epochs > 0 else self.kld_weight\n",
        "\n",
        "            for batch_idx, batch_data in enumerate(train_loader):\n",
        "                # --- Batch Processing ---\n",
        "                try:\n",
        "                    # Unpack data and move to device\n",
        "                    expected_tensors = 4 if self.is_semi_supervised else 2\n",
        "                    if not isinstance(batch_data, (list, tuple)) or len(batch_data) != expected_tensors:\n",
        "                        raise TypeError(f\"DataLoader returned {len(batch_data)} tensors, expected {expected_tensors}\")\n",
        "\n",
        "                    if self.is_semi_supervised:\n",
        "                        x_b, x_m_b, y_b, y_m_b = [t.to(self.device) for t in batch_data]\n",
        "                    else:\n",
        "                        x_b, x_m_b = [t.to(self.device) for t in batch_data]\n",
        "                        y_b, y_m_b = None, None\n",
        "\n",
        "                    # Skip batches potentially causing BN issues (safeguard)\n",
        "                    if x_b.shape[0] <= 1 and getattr(self.model, 'use_batch_norm', False): continue\n",
        "\n",
        "                    # Forward pass, loss calculation, backward pass, optimizer step\n",
        "                    self.optimizer.zero_grad()\n",
        "                    model_outputs = self.model(x_b)\n",
        "                    loss, recon_lx, kld_l, loss_ly = self._calculate_loss(\n",
        "                        x_b, x_m_b, y_b, y_m_b, model_outputs, current_beta_kld\n",
        "                    )\n",
        "\n",
        "                    if torch.isnan(loss) or torch.isinf(loss):\n",
        "                        self.logger.warning(f\"NaN/Inf loss in Epoch {epoch+1}/Batch {batch_idx}. Skipping update. \"\n",
        "                                            f\"Loss: {loss.item()}, ReconX: {recon_lx.item()}, KLD: {kld_l.item()}, LossY: {loss_ly.item()}\")\n",
        "                        continue # Skip optimizer step if loss is invalid\n",
        "\n",
        "                    loss.backward()\n",
        "                    self.optimizer.step()\n",
        "\n",
        "                    # Accumulate losses for epoch average\n",
        "                    epoch_losses['total_loss'] += loss.item()\n",
        "                    epoch_losses['recon_loss_x'] += recon_lx.item()\n",
        "                    epoch_losses['kld_loss'] += kld_l.item()\n",
        "                    epoch_losses['loss_y'] += loss_ly.item() # loss_ly is 0 if not applicable\n",
        "                    num_batches_processed += 1\n",
        "\n",
        "                except Exception as batch_err:\n",
        "                    self.logger.error(f\"Error in training batch {epoch+1}/{batch_idx}: {batch_err}\", exc_info=False)\n",
        "                    continue # Skip to next batch on error\n",
        "            # --- End of Batch Loop ---\n",
        "\n",
        "            if num_batches_processed == 0:\n",
        "                self.logger.warning(f\"Epoch {epoch+1}: No batches processed successfully. Skipping epoch summary.\")\n",
        "                continue\n",
        "\n",
        "            # Calculate average losses for the epoch\n",
        "            avg_train_total_loss = epoch_losses['total_loss'] / num_batches_processed\n",
        "            avg_train_recon_loss_x = epoch_losses['recon_loss_x'] / num_batches_processed\n",
        "            avg_train_kld_loss = epoch_losses['kld_loss'] / num_batches_processed\n",
        "            avg_train_loss_y = epoch_losses['loss_y'] / num_batches_processed\n",
        "\n",
        "            # Record training history\n",
        "            self.history['train_loss'].append(avg_train_total_loss)\n",
        "            self.history['train_recon_loss_x'].append(avg_train_recon_loss_x)\n",
        "            self.history['train_kld_loss'].append(avg_train_kld_loss)\n",
        "            if self.is_semi_supervised: self.history['train_loss_y'].append(avg_train_loss_y)\n",
        "\n",
        "            epoch_duration = time.time() - start_time_epoch\n",
        "            # Construct train log message\n",
        "            log_msg_parts = [\n",
        "                f\"Epoch [{epoch+1}/{epochs}] Train TotalLoss: {avg_train_total_loss:.4f}\",\n",
        "                f\"(ReconX: {avg_train_recon_loss_x:.4f}\",\n",
        "                f\"KLD: {avg_train_kld_loss:.4f}*{current_beta_kld:.2f}\"\n",
        "            ]\n",
        "            if self.is_semi_supervised: log_msg_parts.append(f\"LossY: {avg_train_loss_y:.4f}*{self.alpha_price_loss:.2f}\")\n",
        "            log_msg_parts.append(f\"), Time: {epoch_duration:.1f}s\")\n",
        "            train_log_message = \", \".join(log_msg_parts[:-1]) + log_msg_parts[-1] # Combine nicely\n",
        "\n",
        "            # --- Validation Step ---\n",
        "            current_val_loss_for_scheduler = avg_train_total_loss # Use train loss if no validation\n",
        "            if val_loader:\n",
        "                self.model.eval() # Set to evaluation mode\n",
        "                val_losses = defaultdict(float)\n",
        "                val_batches_processed = 0\n",
        "                with torch.no_grad():\n",
        "                    for val_batch_data in val_loader:\n",
        "                        try:\n",
        "                            expected_tensors = 4 if self.is_semi_supervised else 2\n",
        "                            if not isinstance(val_batch_data, (list, tuple)) or len(val_batch_data) != expected_tensors: continue # Skip malformed batch\n",
        "\n",
        "                            if self.is_semi_supervised:\n",
        "                                x_v, x_m_v, y_v, y_m_v = [t.to(self.device) for t in val_batch_data]\n",
        "                            else:\n",
        "                                x_v, x_m_v = [t.to(self.device) for t in val_batch_data]\n",
        "                                y_v, y_m_v = None, None\n",
        "                            if x_v.shape[0] <= 1 and getattr(self.model, 'use_batch_norm', False): continue\n",
        "\n",
        "                            val_model_outputs = self.model(x_v)\n",
        "                            v_loss, v_recon_x, v_kld, v_loss_y = self._calculate_loss(\n",
        "                                x_v, x_m_v, y_v, y_m_v, val_model_outputs, self.kld_weight # Use full beta for val loss\n",
        "                            )\n",
        "                            if not (torch.isnan(v_loss) or torch.isinf(v_loss)):\n",
        "                                val_losses['total_loss'] += v_loss.item()\n",
        "                                val_losses['recon_loss_x'] += v_recon_x.item()\n",
        "                                val_losses['kld_loss'] += v_kld.item()\n",
        "                                val_losses['loss_y'] += v_loss_y.item()\n",
        "                                val_batches_processed += 1\n",
        "                        except Exception as val_err:\n",
        "                            self.logger.warning(f\"Error in validation batch: {val_err}\", exc_info=False)\n",
        "                            continue\n",
        "\n",
        "                if val_batches_processed > 0:\n",
        "                    avg_val_total_loss = val_losses['total_loss'] / val_batches_processed\n",
        "                    avg_val_recon_loss_x = val_losses['recon_loss_x'] / val_batches_processed\n",
        "                    avg_val_kld_loss = val_losses['kld_loss'] / val_batches_processed\n",
        "                    avg_val_loss_y = val_losses['loss_y'] / val_batches_processed\n",
        "                    current_val_loss_for_scheduler = avg_val_total_loss # Use this for scheduler/early stopping\n",
        "\n",
        "                    # Record validation history\n",
        "                    self.history['val_loss'].append(avg_val_total_loss)\n",
        "                    self.history['val_recon_loss_x'].append(avg_val_recon_loss_x)\n",
        "                    self.history['val_kld_loss'].append(avg_val_kld_loss)\n",
        "                    if self.is_semi_supervised: self.history['val_loss_y'].append(avg_val_loss_y)\n",
        "\n",
        "                    # Append validation info to log message\n",
        "                    val_log_parts = [\n",
        "                         f\"| Val TotalLoss: {avg_val_total_loss:.4f}\",\n",
        "                         f\"(ReconX: {avg_val_recon_loss_x:.4f}\",\n",
        "                         f\"KLD: {avg_val_kld_loss:.4f}\"\n",
        "                    ]\n",
        "                    if self.is_semi_supervised: val_log_parts.append(f\"LossY: {avg_val_loss_y:.4f}\")\n",
        "                    val_log_parts.append(\")\")\n",
        "                    train_log_message += \" \" + \", \".join(val_log_parts[:-1]) + val_log_parts[-1]\n",
        "                else: # No valid validation batches processed\n",
        "                    self.history['val_loss'].append(float('nan')) # Log NaN if val failed\n",
        "                    self.history['val_recon_loss_x'].append(float('nan'))\n",
        "                    self.history['val_kld_loss'].append(float('nan'))\n",
        "                    if self.is_semi_supervised: self.history['val_loss_y'].append(float('nan'))\n",
        "                    train_log_message += \" | Val Loss: N/A\"\n",
        "                    current_val_loss_for_scheduler = float('nan') # Cannot use for scheduler/early stop\n",
        "\n",
        "            # --- Logging, Scheduler, Early Stopping ---\n",
        "            if (epoch + 1) % print_every_n_epochs == 0 or epoch == epochs - 1 or \\\n",
        "               (early_stopping_patience and epochs_no_improve > 0): # Log if patience counting\n",
        "                self._log(train_log_message, level=\"info\")\n",
        "\n",
        "            if not np.isnan(current_val_loss_for_scheduler): # Only step if val loss is valid\n",
        "                scheduler.step(current_val_loss_for_scheduler)\n",
        "                if early_stopping_patience is not None:\n",
        "                    if current_val_loss_for_scheduler < best_val_loss:\n",
        "                        best_val_loss = current_val_loss_for_scheduler; epochs_no_improve = 0\n",
        "                        self.save_model(filename=\"best_model.pth\") # Save best model state\n",
        "                        self._log(f\"Epoch {epoch+1}: New best val_loss: {best_val_loss:.4f}. Model saved.\", level=\"debug\")\n",
        "                    else:\n",
        "                        epochs_no_improve += 1\n",
        "                        if epochs_no_improve >= early_stopping_patience:\n",
        "                            self._log(f\"Early stopping triggered at epoch {epoch+1}. Best val_loss: {best_val_loss:.4f}\")\n",
        "                            break # Exit training loop\n",
        "        # --- End of Epoch Loop ---\n",
        "\n",
        "        self._log(\"Training loop finished.\", level=\"info\")\n",
        "        # Load best model if early stopping was active and saved a model\n",
        "        best_model_path = os.path.join(self.results_dir, \"best_model.pth\")\n",
        "        if early_stopping_patience is not None and os.path.exists(best_model_path):\n",
        "            self._log(f\"Loading best model state from '{best_model_path}' (Val Loss: {best_val_loss:.4f}).\")\n",
        "            try:\n",
        "                 self.load_model(filepath=best_model_path, device=self.device) # Load the best weights back into self.model\n",
        "            except Exception as e_load:\n",
        "                 self._log(f\"Failed to load best model: {e_load}. Continuing with final epoch model.\", level=\"error\")\n",
        "                 self.save_model(filename=\"final_model_epoch_end.pth\") # Save final weights instead\n",
        "        else: # No early stopping or no best model saved\n",
        "            self.save_model(filename=\"final_model_epoch_end.pth\") # Save the model from the last epoch\n",
        "\n",
        "        return dict(self.history) # Return training history\n",
        "\n",
        "    def predict_price_and_uncertainty(self, X_filled_tensor: torch.Tensor, batch_size: int = 1024) -> Tuple[Optional[np.ndarray], Optional[np.ndarray]]:\n",
        "        \"\"\"Predicts target Y mean and log-variance using a trained SemiSupMIWAE model.\"\"\"\n",
        "        if not self.is_semi_supervised or not isinstance(self.model, SemiSupMIWAE):\n",
        "            self.logger.error(\"Price prediction requires a trained SemiSupMIWAE model.\")\n",
        "            return None, None\n",
        "\n",
        "        self._log(f\"Predicting target (Y) mean and log-variance for {X_filled_tensor.shape[0]} samples...\", level=\"info\")\n",
        "        self.model.eval() # Set model to evaluation mode\n",
        "\n",
        "        # Create a dataset/loader for efficient batch processing\n",
        "        pred_dataset = TensorDataset(X_filled_tensor)\n",
        "        pred_loader = DataLoader(pred_dataset, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "        all_y_pred_mu, all_y_pred_logvar = [], []\n",
        "        with torch.no_grad():\n",
        "            for (x_batch,) in pred_loader: # DataLoader wraps tensor in a tuple\n",
        "                x_batch = x_batch.to(self.device)\n",
        "                try:\n",
        "                    # Use the model's dedicated prediction method if available\n",
        "                    if hasattr(self.model, 'predict_price') and callable(getattr(self.model, 'predict_price')):\n",
        "                        y_mu_batch, y_logvar_batch = self.model.predict_price(x_batch)\n",
        "                    else: # Fallback (should not be needed for SemiSupMIWAE)\n",
        "                         mu_latent, _ = self.model.encode(x_batch)\n",
        "                         y_mu_batch = self.model.price_mean_head(mu_latent)\n",
        "                         y_logvar_batch = self.model.price_logvar_head(mu_latent)\n",
        "\n",
        "                    all_y_pred_mu.append(y_mu_batch.cpu().numpy())\n",
        "                    all_y_pred_logvar.append(y_logvar_batch.cpu().numpy())\n",
        "                except Exception as e_pred:\n",
        "                    self.logger.error(f\"Error during batch Y prediction: {e_pred}\", exc_info=True)\n",
        "                    return None, None # Abort prediction\n",
        "\n",
        "        if not all_y_pred_mu: # Check if any predictions were made\n",
        "             self.logger.warning(\"Y prediction process generated no output.\")\n",
        "             return None, None\n",
        "\n",
        "        # Concatenate results from all batches\n",
        "        try:\n",
        "            y_mu_full = np.concatenate(all_y_pred_mu, axis=0)\n",
        "            y_logvar_full = np.concatenate(all_y_pred_logvar, axis=0)\n",
        "        except ValueError as e_concat:\n",
        "            self._log(f\"Error concatenating Y prediction results: {e_concat}. Check batch output shapes.\", level=\"error\")\n",
        "            return None, None\n",
        "\n",
        "        self._log(f\"Y prediction complete. Output shapes: y_mu={y_mu_full.shape}, y_logvar={y_logvar_full.shape}\", level=\"debug\")\n",
        "        return y_mu_full, y_logvar_full\n",
        "\n",
        "    def impute(self, X_missing_np: np.ndarray, n_imputations: int = 1) -> Union[np.ndarray, List[np.ndarray]]:\n",
        "        \"\"\"Imputes missing values (NaN) in X_missing_np using the trained VAE model.\"\"\"\n",
        "        if self.model is None:\n",
        "            raise RuntimeError(\"Model is not loaded or trained. Cannot impute.\")\n",
        "        self._log(f\"Starting imputation for X data ({X_missing_np.shape[0]} samples, {X_missing_np.shape[1]} features), n_imputations={n_imputations}...\", level=\"info\")\n",
        "        self.model.eval() # Set to evaluation mode\n",
        "\n",
        "        nan_mask = np.isnan(X_missing_np)\n",
        "        if not np.any(nan_mask):\n",
        "            self._log(\"No missing values (NaNs) found in input X data. Returning copy/copies of original.\", level=\"info\")\n",
        "            return [X_missing_np.copy() for _ in range(n_imputations)] if n_imputations > 1 else X_missing_np.copy()\n",
        "\n",
        "        # Fill NaNs with 0.0 for model input\n",
        "        X_filled_for_input_np = np.nan_to_num(X_missing_np, nan=0.0)\n",
        "        # Ensure input dimension matches model's expected input_dim for X\n",
        "        if X_filled_for_input_np.shape[1] != self.model.input_dim:\n",
        "             self._log(f\"Input data feature dimension for imputation ({X_filled_for_input_np.shape[1]}) \"\n",
        "                       f\"does not match model input dimension ({self.model.input_dim}). Cannot impute.\", level=\"error\")\n",
        "             raise ValueError(\"Input dimension mismatch during imputation.\")\n",
        "\n",
        "        X_filled_for_input_tensor = torch.from_numpy(X_filled_for_input_np).float().to(self.device)\n",
        "\n",
        "        imputed_datasets = []\n",
        "        with torch.no_grad():\n",
        "            for i in range(n_imputations):\n",
        "                # MIWAE imputation: Encode -> Reparameterize -> Decode X\n",
        "                mu, logvar = self.model.encode(X_filled_for_input_tensor)\n",
        "                z = self.model.reparameterize(mu, logvar)\n",
        "                reconstructed_x_tensor = self.model.decode_x(z)\n",
        "                reconstructed_x_np = reconstructed_x_tensor.cpu().numpy()\n",
        "\n",
        "                # Create imputed dataset: start with original, fill NaNs with reconstruction\n",
        "                current_imputed_dataset_np = X_missing_np.copy()\n",
        "                current_imputed_dataset_np[nan_mask] = reconstructed_x_np[nan_mask]\n",
        "                imputed_datasets.append(current_imputed_dataset_np)\n",
        "\n",
        "                if n_imputations > 1 and (i + 1) % max(1, n_imputations // 5) == 0: # Log progress\n",
        "                    self._log(f\"Completed imputation {i+1}/{n_imputations}\", level=\"debug\")\n",
        "\n",
        "        self._log(f\"Imputation complete. Returning {len(imputed_datasets)} imputed dataset(s).\", level=\"info\")\n",
        "        return imputed_datasets[0] if n_imputations == 1 else imputed_datasets\n",
        "\n",
        "    def save_model(self, filename: str = \"vae_model_final.pth\"):\n",
        "        \"\"\"Saves the model state, configuration, optimizer state, and history.\"\"\"\n",
        "        if self.model is None: self._log(\"No model instance to save.\", level=\"warning\"); return\n",
        "        filepath = os.path.join(self.results_dir, filename)\n",
        "        self._log(f\"Saving model and trainer state to {filepath}...\", level=\"info\")\n",
        "        try:\n",
        "            # --- Extract Model Configuration ---\n",
        "            # Basic architecture\n",
        "            encoder_layers = [l.out_features for l in getattr(self.model, 'encoder_net', []) if isinstance(l, nn.Linear)]\n",
        "            decoder_layers_all = [l for l in getattr(self.model, 'decoder_net_x', []) if isinstance(l, nn.Linear)]\n",
        "            decoder_layers = [l.out_features for l in decoder_layers_all[:-1]] if len(decoder_layers_all)>1 else [] # Exclude final output layer\n",
        "\n",
        "            model_config = {\n",
        "                'model_class': self.model.__class__.__name__,\n",
        "                'input_dim': getattr(self.model, 'input_dim', None), # X dim\n",
        "                'latent_dim': getattr(self.model, 'latent_dim', None),\n",
        "                'encoder_layer_sizes': encoder_layers,\n",
        "                'decoder_layer_sizes': decoder_layers,\n",
        "                'activation_fn_str': getattr(getattr(self.model, 'activation_fn', nn.ReLU()), '__class__', nn.Module).__name__.lower(),\n",
        "                'use_batch_norm': getattr(self.model, 'use_batch_norm', True),\n",
        "                'prior_type': getattr(self.model, 'prior_type', 'standard_gaussian'),\n",
        "                'n_prior_components': getattr(self.model, 'n_prior_components', 1),\n",
        "                'learn_prior': getattr(self.model, 'learn_prior', False),\n",
        "            }\n",
        "            # Add SemiSupMIWAE specific parts if applicable\n",
        "            if isinstance(self.model, SemiSupMIWAE):\n",
        "                model_config['y_dim'] = getattr(self.model, 'y_dim', 1)\n",
        "                price_head_layers_all = [l for l in getattr(self.model, 'price_mean_head', []) if isinstance(l, nn.Linear)]\n",
        "                model_config['price_head_layer_sizes'] = [l.out_features for l in price_head_layers_all[:-1]] if len(price_head_layers_all)>1 else []\n",
        "                model_config['alpha_price_loss'] = getattr(self.model, 'alpha_price_loss', 1.0) # Model's alpha\n",
        "\n",
        "            final_model_config = {k: v for k, v in model_config.items() if v is not None} # Remove None values\n",
        "\n",
        "            # --- Extract Trainer Configuration ---\n",
        "            trainer_config = {\n",
        "                'learning_rate': self.optimizer.defaults.get('lr'),\n",
        "                'kld_weight': self.kld_weight, # Base KLD weight used for training/val loss reporting\n",
        "                'alpha_price_loss_trainer': self.alpha_price_loss, # Trainer's effective alpha\n",
        "                'optimizer_str': self.optimizer.__class__.__name__.lower(),\n",
        "                'reconstruction_loss_type_x': self.reconstruction_loss_type_x,\n",
        "                'price_loss_type_y': self.price_loss_type_y,\n",
        "                'huber_delta': self.huber_delta,\n",
        "            }\n",
        "            final_trainer_config = {k: v for k,v in trainer_config.items() if v is not None}\n",
        "\n",
        "            # --- Prepare Save Content ---\n",
        "            save_content = {\n",
        "                'model_state_dict': self.model.state_dict(),\n",
        "                'config': final_model_config, # Model architecture config\n",
        "                'optimizer_state_dict': self.optimizer.state_dict(),\n",
        "                'history': dict(self.history), # Convert defaultdict for saving\n",
        "                'trainer_config': final_trainer_config # Trainer setup config\n",
        "            }\n",
        "            torch.save(save_content, filepath)\n",
        "            self._log(f\"Successfully saved model and trainer state to {filepath}\", level=\"info\")\n",
        "\n",
        "        except Exception as e:\n",
        "            self._log(f\"Error saving model state to {filepath}: {e}\", level=\"error\", exc_info=True)\n",
        "\n",
        "    @classmethod\n",
        "    def load_model(cls, filepath: str, device: Optional[torch.device] = None, verbose: bool = True) -> 'VAETrainer':\n",
        "        \"\"\"Loads a saved VAE model and trainer state.\"\"\"\n",
        "        logger_loader = get_logger(cls.__name__ + \"_Loader\", verbose)\n",
        "        effective_device = device if device else torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "        if not os.path.exists(filepath):\n",
        "            logger_loader.error(f\"Model checkpoint file not found: {filepath}\")\n",
        "            raise FileNotFoundError(f\"VAE model file not found: {filepath}\")\n",
        "\n",
        "        logger_loader.info(f\"Loading VAE model and trainer state from {filepath} to device {effective_device}...\")\n",
        "        try:\n",
        "            checkpoint = torch.load(filepath, map_location=effective_device)\n",
        "\n",
        "            # --- Reconstruct Model ---\n",
        "            config = checkpoint.get('config', {})\n",
        "            if not config: raise ValueError(\"Model configuration ('config') missing in checkpoint.\")\n",
        "            model_class_name = config.get('model_class', 'VariationalAutoencoderBase')\n",
        "\n",
        "            # Activation function\n",
        "            activation_map = {'relu': nn.ReLU(), 'leaky_relu': nn.LeakyReLU(), 'elu': nn.ELU(), 'selu': nn.SELU(), 'tanh': nn.Tanh(), 'sigmoid': nn.Sigmoid()}\n",
        "            activation_fn = activation_map.get(config.get('activation_fn_str','relu'), nn.ReLU())\n",
        "\n",
        "            # Basic VAE arguments from config\n",
        "            model_args = {\n",
        "                k: config.get(k) for k in ['input_dim', 'latent_dim', 'encoder_layer_sizes', 'decoder_layer_sizes']\n",
        "            }\n",
        "            if None in model_args.values(): raise ValueError(f\"Missing core VAE dimensions in config: {model_args}\")\n",
        "            model_args.update({\n",
        "                'activation_fn': activation_fn,\n",
        "                'use_batch_norm': config.get('use_batch_norm', True),\n",
        "                'prior_type': config.get('prior_type', 'standard_gaussian'),\n",
        "                'n_prior_components': config.get('n_prior_components', 1),\n",
        "                'learn_prior': config.get('learn_prior', False),\n",
        "                'device': effective_device, # Pass device for model instantiation\n",
        "            })\n",
        "\n",
        "            ModelClass: Union[type(VariationalAutoencoderBase), type(SemiSupMIWAE)]\n",
        "            if model_class_name == 'SemiSupMIWAE':\n",
        "                ModelClass = SemiSupMIWAE\n",
        "                model_args.update({ # Add SemiSupMIWAE specific args\n",
        "                    'y_dim': config.get('y_dim', 1),\n",
        "                    'price_head_layer_sizes': config.get('price_head_layer_sizes', []),\n",
        "                    'alpha_price_loss': config.get('alpha_price_loss', 1.0) # Use model's saved alpha\n",
        "                })\n",
        "            elif model_class_name == 'VariationalAutoencoderBase':\n",
        "                ModelClass = VariationalAutoencoderBase\n",
        "            else:\n",
        "                raise ValueError(f\"Unknown model_class '{model_class_name}' in checkpoint.\")\n",
        "\n",
        "            model_instance = ModelClass(**model_args)\n",
        "            model_instance.load_state_dict(checkpoint['model_state_dict'])\n",
        "            model_instance.to(effective_device) # Ensure model is on the correct device\n",
        "            logger_loader.info(f\"{ModelClass.__name__} model created and state loaded successfully.\")\n",
        "\n",
        "            # --- Reconstruct Trainer ---\n",
        "            trainer_config = checkpoint.get('trainer_config', {}) # Get trainer config saved previously\n",
        "            trainer_instance = cls(\n",
        "                model=model_instance,\n",
        "                learning_rate=trainer_config.get('learning_rate', 1e-3),\n",
        "                kld_weight=trainer_config.get('kld_weight', 1.0),\n",
        "                # Use alpha from trainer_config if saved, otherwise None (trainer init will handle)\n",
        "                alpha_price_loss=trainer_config.get('alpha_price_loss_trainer'),\n",
        "                optimizer_str=trainer_config.get('optimizer_str', 'adam'),\n",
        "                reconstruction_loss_type_x=trainer_config.get('reconstruction_loss_type_x', 'mse'),\n",
        "                price_loss_type_y=trainer_config.get('price_loss_type_y', 'gaussian_nll'),\n",
        "                huber_delta=trainer_config.get('huber_delta', 1.0),\n",
        "                device=effective_device,\n",
        "                results_dir=os.path.dirname(filepath) or \".\", # Use directory of loaded file\n",
        "                verbose=verbose\n",
        "            )\n",
        "\n",
        "            # Load optimizer state if available\n",
        "            if 'optimizer_state_dict' in checkpoint:\n",
        "                try:\n",
        "                    trainer_instance.optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
        "                    logger_loader.debug(\"Optimizer state loaded successfully.\")\n",
        "                except Exception as e_optim:\n",
        "                    logger_loader.warning(f\"Could not load optimizer state_dict: {e_optim}. Optimizer remains re-initialized.\")\n",
        "            else:\n",
        "                logger_loader.warning(\"Optimizer state_dict not found in checkpoint. Optimizer remains re-initialized.\")\n",
        "\n",
        "            # Restore history\n",
        "            trainer_instance.history = defaultdict(list, checkpoint.get('history', {}))\n",
        "            logger_loader.info(f\"VAETrainer instance created and state loaded for model from {filepath}.\")\n",
        "            return trainer_instance\n",
        "\n",
        "        except Exception as e:\n",
        "            logger_loader.error(f\"Critical error loading VAE model and trainer state from {filepath}: {e}\", exc_info=True)\n",
        "            raise RuntimeError(f\"Failed to load VAE model and trainer from {filepath}\") from e\n",
        "\n",
        "    def get_latent_representation(self, X_np: np.ndarray, batch_size: int = 256) -> Optional[Tuple[np.ndarray, np.ndarray]]:\n",
        "        \"\"\"Encodes input data X_np into the latent space (mu, logvar). Handles NaN imputation.\"\"\"\n",
        "        if self.model is None:\n",
        "             self._log(\"Model not available for latent representation.\", level=\"error\"); return None\n",
        "        self.model.eval()\n",
        "\n",
        "        # Impute NaNs for encoding purposes\n",
        "        if np.any(np.isnan(X_np)):\n",
        "            self._log(\"Input data for latent encoding contains NaNs. Filling with 0.0.\", level=\"debug\")\n",
        "            X_filled_np = np.nan_to_num(X_np, nan=0.0).astype(np.float32)\n",
        "        else:\n",
        "            X_filled_np = X_np.astype(np.float32)\n",
        "\n",
        "        # Check dimensions\n",
        "        if X_filled_np.shape[1] != self.model.input_dim:\n",
        "            self._log(f\"Input data dim ({X_filled_np.shape[1]}) != model input dim ({self.model.input_dim}).\", level=\"error\")\n",
        "            return None\n",
        "\n",
        "        # Batch processing for potentially large inputs\n",
        "        dataset = TensorDataset(torch.from_numpy(X_filled_np))\n",
        "        loader = DataLoader(dataset, batch_size=batch_size, shuffle=False)\n",
        "        all_mu, all_logvar = [], []\n",
        "\n",
        "        with torch.no_grad():\n",
        "            for (x_batch_tensor,) in loader:\n",
        "                try:\n",
        "                    mu_batch, logvar_batch = self.model.encode(x_batch_tensor.to(self.device))\n",
        "                    all_mu.append(mu_batch.cpu().numpy())\n",
        "                    all_logvar.append(logvar_batch.cpu().numpy())\n",
        "                except Exception as e_encode:\n",
        "                    self._log(f\"Error during batch encoding for latent representation: {e_encode}\", level=\"error\", exc_info=True)\n",
        "                    return None\n",
        "\n",
        "        if not all_mu: return None # No batches processed\n",
        "\n",
        "        # Concatenate results\n",
        "        try:\n",
        "            mu_full = np.concatenate(all_mu, axis=0)\n",
        "            logvar_full = np.concatenate(all_logvar, axis=0)\n",
        "        except ValueError as e_concat:\n",
        "            self._log(f\"Error concatenating latent representations: {e_concat}.\", level=\"error\"); return None\n",
        "\n",
        "        self._log(f\"Encoded data to latent space. Mu shape: {mu_full.shape}, LogVar shape: {logvar_full.shape}\", level=\"debug\")\n",
        "        return mu_full, logvar_full\n",
        "\n",
        "\n",
        "# --- Helper Function: Prepare Data for VAE (Refactored for Semi-Supervised MIWAE) ---\n",
        "def prepare_vae_input(\n",
        "    df: pd.DataFrame,\n",
        "    feature_columns: List[str],\n",
        "    target_col: Optional[str] = None, # *** ADDED for semi-supervised target ***\n",
        "    scaler_type: str = 'standard', # 'standard', 'minmax', or 'none'\n",
        ") -> Tuple[\n",
        "    Optional[np.ndarray], Optional[np.ndarray], # X_filled_processed, X_mask\n",
        "    Optional[np.ndarray], Optional[np.ndarray], # Y_filled_processed, Y_mask (if target_col is provided)\n",
        "    Optional[List[str]], Optional[Any], Optional[Dict[str, str]] # actual_feature_columns_x, scaler_object, feature_metadata\n",
        "]:\n",
        "    \"\"\"\n",
        "    Prepares DataFrame data for Semi-Supervised MIWAE input.\n",
        "    - Selects feature (X) and optionally target (y) columns.\n",
        "    - Scales features (X) if a scaler_type is specified. Target (y) is NOT scaled by this function.\n",
        "    - Fills NaNs with 0.0 (a common strategy for VAEs, especially MIWAE).\n",
        "    - Returns the NaN-filled processed data AND binary masks (1 for observed, 0 for missing) for both X and y.\n",
        "    \"\"\"\n",
        "    current_logger = get_logger(f\"{__name__}.prepare_vae_input\")\n",
        "    current_logger.info(f\"Preparing data and masks for VAE. Input features: {len(feature_columns)}, \"\n",
        "                        f\"Target column: {target_col if target_col else 'N/A'}, Scaler type for X: {scaler_type}\")\n",
        "\n",
        "    # --- Process Features (X) ---\n",
        "    # Ensure target_col is not accidentally in feature_columns for X\n",
        "    actual_cols_present_x = [f_col for f_col in feature_columns if f_col in df.columns and f_col != target_col]\n",
        "\n",
        "    if not actual_cols_present_x:\n",
        "        current_logger.error(\"No valid X feature columns found in DataFrame after excluding target or missing columns.\")\n",
        "        return None, None, None, None, None, None, None\n",
        "\n",
        "    # Warn if some requested feature columns (excluding target) were not found\n",
        "    requested_x_features = [f_col for f_col in feature_columns if f_col != target_col]\n",
        "    if len(actual_cols_present_x) < len(requested_x_features):\n",
        "        missing_x_cols = set(requested_x_features) - set(actual_cols_present_x)\n",
        "        current_logger.warning(f\"Specified X feature columns not found in DataFrame: {missing_x_cols}. \"\n",
        "                               f\"Using available X columns: {actual_cols_present_x}\")\n",
        "\n",
        "    df_selected_x = df[actual_cols_present_x].copy() # Use .copy() to avoid SettingWithCopyWarning\n",
        "\n",
        "    try:\n",
        "        data_np_x = df_selected_x.values.astype(np.float32)\n",
        "    except Exception as e:\n",
        "        current_logger.error(f\"Failed to convert selected X features to np.float32: {e}\", exc_info=True)\n",
        "        return None, None, None, None, None, None, None\n",
        "\n",
        "    # Create mask for X: 1 where data is present (not NaN), 0 where it's NaN\n",
        "    mask_np_x = (~np.isnan(data_np_x)).astype(np.float32)\n",
        "    num_missing_x = np.sum(mask_np_x == 0)\n",
        "    current_logger.info(f\"X data shape: {data_np_x.shape}. X mask shape: {mask_np_x.shape}. \"\n",
        "                        f\"Number of missing values in X: {num_missing_x} ({(num_missing_x / data_np_x.size) * 100:.2f}%).\")\n",
        "\n",
        "    scaler_object = None\n",
        "    scaled_data_nan_preserved_x = data_np_x # Start with original data (NaNs preserved)\n",
        "\n",
        "    if scaler_type and scaler_type.lower() != 'none':\n",
        "        if scaler_type.lower() == 'standard':\n",
        "            scaler_object = StandardScaler()\n",
        "        elif scaler_type.lower() == 'minmax':\n",
        "            scaler_object = MinMaxScaler()\n",
        "        # Add other scalers here if needed\n",
        "\n",
        "        if scaler_object:\n",
        "            current_logger.info(f\"Applying {scaler_type} scaling to X features (NaNs are typically ignored by fit and transform)...\")\n",
        "            try:\n",
        "                # Fit scaler only on non-NaN values for more robust scaling if possible,\n",
        "                # though sklearn scalers usually handle NaNs by ignoring them in fit\n",
        "                # and propagating them in transform.\n",
        "                # To be absolutely sure, one might fit on df_selected_x.dropna() if all rows have some NaNs.\n",
        "                # However, standard practice is to fit on the full data.\n",
        "                scaler_object.fit(data_np_x) # Fit on data possibly containing NaNs\n",
        "                scaled_data_nan_preserved_x = scaler_object.transform(data_np_x).astype(np.float32)\n",
        "            except Exception as e_scale:\n",
        "                current_logger.error(f\"Error during X scaling with {scaler_type}: {e_scale}. \"\n",
        "                                     \"Using unscaled X data instead.\", exc_info=True)\n",
        "                scaled_data_nan_preserved_x = data_np_x # Revert to unscaled\n",
        "                scaler_object = None # Nullify scaler if it failed\n",
        "        else:\n",
        "            current_logger.warning(f\"Unknown scaler_type '{scaler_type}' for X. No scaling will be applied to X.\")\n",
        "    else:\n",
        "        current_logger.info(\"No scaling will be applied to X features as per 'scaler_type'.\")\n",
        "\n",
        "\n",
        "    # Fill NaNs with 0.0 AFTER scaling (if any)\n",
        "    # `scaled_data_nan_preserved_x` still has NaNs where original data had them.\n",
        "    data_filled_processed_x = np.nan_to_num(scaled_data_nan_preserved_x, nan=0.0).astype(np.float32)\n",
        "\n",
        "    # Check for non-finite values (inf, -inf) that might arise from operations and replace them\n",
        "    if np.any(~np.isfinite(data_filled_processed_x)):\n",
        "        current_logger.warning(\"Non-finite values (inf/-inf) detected in processed X data after NaN filling. \"\n",
        "                               \"Replacing them with 0.0.\")\n",
        "        data_filled_processed_x = np.nan_to_num(data_filled_processed_x, nan=0.0, posinf=0.0, neginf=0.0)\n",
        "\n",
        "\n",
        "    feature_metadata = {\n",
        "        col_name: 'numeric_scaled' if scaler_object else 'numeric_unscaled'\n",
        "        for col_name in actual_cols_present_x\n",
        "    }\n",
        "\n",
        "    # --- Process Target (y) ---\n",
        "    data_filled_processed_y = None\n",
        "    mask_np_y = None\n",
        "\n",
        "    if target_col:\n",
        "        if target_col not in df.columns:\n",
        "            current_logger.error(f\"Specified target column '{target_col}' not found in DataFrame.\")\n",
        "            # Return processed X as it might still be useful for unsupervised VAE\n",
        "            return data_filled_processed_x, mask_np_x, None, None, actual_cols_present_x, scaler_object, feature_metadata\n",
        "\n",
        "        try:\n",
        "            # Ensure y is 2D (e.g., for consistency if y_dim > 1 in future) by using df[[target_col]]\n",
        "            data_np_y = df[[target_col]].values.astype(np.float32)\n",
        "            mask_np_y = (~np.isnan(data_np_y)).astype(np.float32)\n",
        "            data_filled_processed_y = np.nan_to_num(data_np_y, nan=0.0).astype(np.float32) # Fill NaNs with 0 for y\n",
        "\n",
        "            num_missing_y = np.sum(mask_np_y == 0)\n",
        "            current_logger.info(f\"Y target ('{target_col}') data shape: {data_np_y.shape}. Y mask shape: {mask_np_y.shape}. \"\n",
        "                                f\"Number of missing values in Y: {num_missing_y} ({(num_missing_y / data_np_y.size) * 100:.2f}%).\")\n",
        "\n",
        "            if np.any(~np.isfinite(data_filled_processed_y)):\n",
        "                current_logger.warning(f\"Non-finite values (inf/-inf) detected in processed Y target ('{target_col}') \"\n",
        "                                       \"after NaN filling. Replacing them with 0.0.\")\n",
        "                data_filled_processed_y = np.nan_to_num(data_filled_processed_y, nan=0.0, posinf=0.0, neginf=0.0)\n",
        "\n",
        "        except Exception as e_target:\n",
        "            current_logger.error(f\"Error processing target column '{target_col}': {e_target}\", exc_info=True)\n",
        "            # Return processed X, but Y will be None\n",
        "            return data_filled_processed_x, mask_np_x, None, None, actual_cols_present_x, scaler_object, feature_metadata\n",
        "    else:\n",
        "        current_logger.info(\"No target column specified. Preparing data for unsupervised VAE.\")\n",
        "\n",
        "\n",
        "    current_logger.info(f\"VAE input preparation complete. Shapes: \"\n",
        "                        f\"X_filled: {data_filled_processed_x.shape}, X_mask: {mask_np_x.shape}, \"\n",
        "                        f\"Y_filled: {data_filled_processed_y.shape if data_filled_processed_y is not None else 'N/A'}, \"\n",
        "                        f\"Y_mask: {mask_np_y.shape if mask_np_y is not None else 'N/A'}\")\n",
        "\n",
        "    return (data_filled_processed_x, mask_np_x,\n",
        "            data_filled_processed_y, mask_np_y,\n",
        "            actual_cols_present_x, scaler_object, feature_metadata)\n",
        "\n",
        "\n",
        "\n",
        "def create_vae_from_artifacts(\n",
        "    artifacts: Dict[str, Any],\n",
        "    feature_order: List[str], # Full list of features expected by the model, including target if applicable\n",
        "    target_col_name: Optional[str] = None, # Name of the target column for SemiSupMIWAE\n",
        "    alpha_price_loss: float = 1.0, # Weight for the price loss term in SemiSupMIWAE\n",
        "    price_head_layer_sizes: Optional[List[int]] = None, # Override for price head layers\n",
        "    device: Optional[torch.device] = None,\n",
        "    encoder_layers_override: Optional[List[int]] = None,\n",
        "    decoder_layers_override: Optional[List[int]] = None,\n",
        "    latent_dim_override: Optional[int] = None,\n",
        "    prior_type_override: Optional[str] = None, # e.g., 'standard_gaussian', 'gaussian_mixture', 'student_t', 'student_t_mixture'\n",
        "    n_components_override: Optional[int] = None, # Number of components for mixture priors\n",
        "    student_t_df_override: float = 4.0 # Default DoF for Student-t priors\n",
        ") -> Optional[Union[VariationalAutoencoderBase, SemiSupMIWAE]]:\n",
        "\n",
        "    current_logger = get_logger(f\"{__name__}.create_vae_from_artifacts\")\n",
        "    current_logger.info(f\"Creating VAE model. Target column for supervision: {target_col_name if target_col_name else 'None (Unsupervised VAE)'}.\")\n",
        "\n",
        "    if not artifacts: # artifacts can be an empty dict if no prior info is available\n",
        "        current_logger.warning(\"Artifacts dictionary is empty or None. Will use defaults or overrides for VAE parameters.\")\n",
        "        artifacts = {} # Ensure artifacts is a dict\n",
        "    if not feature_order:\n",
        "        current_logger.error(\"Feature_order list is missing or empty. Cannot determine input dimension for X.\")\n",
        "        return None\n",
        "\n",
        "    is_semi_supervised = target_col_name is not None\n",
        "    # Determine X features (input to the main VAE encoder/decoder)\n",
        "    # These are all features in feature_order *except* the target_col_name\n",
        "    x_feature_names = [f_name for f_name in feature_order if f_name != target_col_name]\n",
        "    input_dim_x = len(x_feature_names)\n",
        "\n",
        "    if input_dim_x == 0:\n",
        "        current_logger.error(\"No input features (X) are available after excluding the target column (if any). Cannot create VAE.\")\n",
        "        return None\n",
        "    current_logger.info(f\"Number of input features for VAE (X): {input_dim_x} (Features: {x_feature_names[:5]}...)\")\n",
        "\n",
        "    y_dim = 1 if is_semi_supervised else 0 # Assuming target is single-dimensional if present\n",
        "    if is_semi_supervised:\n",
        "        current_logger.info(f\"Semi-supervised mode: Target '{target_col_name}' (y_dim={y_dim}). Alpha for price loss: {alpha_price_loss}.\")\n",
        "\n",
        "\n",
        "    # Determine Latent Dimension\n",
        "    latent_dim: int\n",
        "    if latent_dim_override is not None:\n",
        "        latent_dim = latent_dim_override\n",
        "        current_logger.info(f\"Using overridden latent_dim: {latent_dim}\")\n",
        "    else:\n",
        "        # Suggestion from artifacts, or a heuristic\n",
        "        default_latent_suggestion = max(1, input_dim_x // 4, 2) # Heuristic: e.g., quarter of input, min 2\n",
        "        latent_dim = artifacts.get('latent_dim_suggestion', default_latent_suggestion)\n",
        "        current_logger.info(f\"Using latent_dim: {latent_dim} (from artifacts or default heuristic).\")\n",
        "    # Ensure latent_dim is at least 1 and not excessively large (e.g., larger than input_dim_x)\n",
        "    latent_dim = max(1, min(int(latent_dim), input_dim_x -1 if input_dim_x > 1 else 1))\n",
        "    current_logger.info(f\"Final effective Latent Dimension: {latent_dim}\")\n",
        "\n",
        "\n",
        "    # Determine Encoder/Decoder Layer Sizes\n",
        "    # Heuristic default: one hidden layer, size related to input/latent dim\n",
        "    default_hidden_layer_size = max(latent_dim * 2, input_dim_x // 2, latent_dim + 5, 10) # Ensure some reasonable size\n",
        "    default_hidden_layer_size = min(default_hidden_layer_size, 512) # Cap max default size\n",
        "\n",
        "    encoder_layer_sizes = encoder_layers_override if encoder_layers_override is not None else \\\n",
        "                          ([default_hidden_layer_size] if default_hidden_layer_size > latent_dim else [])\n",
        "    decoder_layer_sizes = decoder_layers_override if decoder_layers_override is not None else \\\n",
        "                          ([default_hidden_layer_size] if default_hidden_layer_size > latent_dim else [])\n",
        "    current_logger.info(f\"Encoder layers: {encoder_layer_sizes}, Decoder layers: {decoder_layer_sizes}\")\n",
        "\n",
        "\n",
        "    # Determine Prior Type and Number of Components\n",
        "    prior_type: str\n",
        "    n_prior_components: int\n",
        "    prior_params_init: Dict[str, Any] = {}\n",
        "\n",
        "    if prior_type_override is not None and n_components_override is not None:\n",
        "        prior_type = prior_type_override\n",
        "        n_prior_components = n_components_override\n",
        "        current_logger.info(f\"Using overridden prior: Type={prior_type}, K={n_prior_components}\")\n",
        "    else:\n",
        "        # Infer from artifacts if overrides are not fully provided\n",
        "        inferred_prior_type, inferred_k_components = _infer_prior_from_artifacts(artifacts)\n",
        "        prior_type = prior_type_override if prior_type_override is not None else inferred_prior_type\n",
        "        n_prior_components = n_components_override if n_components_override is not None else inferred_k_components\n",
        "        current_logger.info(f\"Using determined prior: Type={prior_type}, K={n_prior_components} (from overrides or artifact inference).\")\n",
        "\n",
        "    # Initialize parameters for mixture priors if applicable\n",
        "    if 'mixture' in prior_type.lower() and n_prior_components > 1:\n",
        "        current_logger.info(f\"Initializing parameters for {prior_type} with K={n_prior_components} components...\")\n",
        "        prior_params_init = _initialize_mixture_prior_params(artifacts, n_prior_components, latent_dim, current_logger)\n",
        "\n",
        "    # Add Student-t degrees of freedom if it's a Student-t prior and not already in params\n",
        "    if 'student_t' in prior_type.lower() and 'df' not in prior_params_init:\n",
        "        prior_params_init['df'] = student_t_df_override\n",
        "        current_logger.info(f\"Setting Student-t df to {student_t_df_override} for prior.\")\n",
        "\n",
        "\n",
        "    # Instantiate Model\n",
        "    try:\n",
        "        model_constructor_args = {\n",
        "            'encoder_layer_sizes': encoder_layer_sizes,\n",
        "            'latent_dim': latent_dim,\n",
        "            'decoder_layer_sizes': decoder_layer_sizes,\n",
        "            'prior_type': prior_type,\n",
        "            'n_prior_components': n_prior_components,\n",
        "            'prior_params': prior_params_init, # Pass initialized prior params\n",
        "            'device': device,\n",
        "            # Other VAEBase params like activation_fn, use_batch_norm use defaults unless overridden here\n",
        "        }\n",
        "\n",
        "        vae_model: Union[VariationalAutoencoderBase, SemiSupMIWAE]\n",
        "        if is_semi_supervised:\n",
        "            current_logger.info(\"Instantiating SemiSupMIWAE model...\")\n",
        "            # Default price head layers: simple 2-layer MLP\n",
        "            default_ph_l1 = max(latent_dim // 2, y_dim * 4, 8) # Sensible lower bound\n",
        "            default_ph_l2 = max(latent_dim // 4, y_dim * 2, 4)\n",
        "            default_price_head_ls = [default_ph_l1, default_ph_l2] if default_ph_l1 > y_dim and default_ph_l2 > y_dim else \\\n",
        "                                     [default_ph_l1] if default_ph_l1 > y_dim else []\n",
        "\n",
        "\n",
        "            final_price_head_layers = price_head_layer_sizes if price_head_layer_sizes is not None else default_price_head_ls\n",
        "            current_logger.info(f\"Price head layers for SemiSupMIWAE: {final_price_head_layers}\")\n",
        "\n",
        "            model_constructor_args.update({\n",
        "                'input_dim': input_dim_x, # X features dimension\n",
        "                'y_dim': y_dim,\n",
        "                'price_head_layer_sizes': final_price_head_layers,\n",
        "                'alpha_price_loss': alpha_price_loss,\n",
        "            })\n",
        "            ModelClass = SemiSupMIWAE\n",
        "        else:\n",
        "            current_logger.info(\"Instantiating VariationalAutoencoderBase (unsupervised) model...\")\n",
        "            model_constructor_args.update({\n",
        "                'input_dim': input_dim_x, # X features dimension\n",
        "            })\n",
        "            ModelClass = VariationalAutoencoderBase\n",
        "\n",
        "        vae_model = ModelClass(**model_constructor_args) # type: ignore\n",
        "        current_logger.info(f\"{vae_model.__class__.__name__} model created successfully on device: {vae_model.device_}.\")\n",
        "        return vae_model\n",
        "\n",
        "    except Exception as e:\n",
        "        current_logger.error(f\"Failed during VAE model instantiation: {e}\", exc_info=True)\n",
        "        return None\n",
        "\n",
        "\n",
        "# --- Helper Functions: _infer_prior_from_artifacts, _initialize_mixture_prior_params (Unchanged logic, reformatted) ---\n",
        "def _infer_prior_from_artifacts(artifacts: Dict[str, Any]) -> Tuple[str, int]:\n",
        "    \"\"\"Infers prior type and number of components from analysis artifacts.\"\"\"\n",
        "    current_logger = get_logger(f\"{__name__}._infer_prior\") # Shortened name\n",
        "\n",
        "    # Default prior: Standard Gaussian\n",
        "    inferred_type = 'standard_gaussian'\n",
        "    inferred_k = 1\n",
        "\n",
        "    # Check for signals of heavy-tailed distribution (e.g., from price analysis)\n",
        "    price_stats = artifacts.get('price_analysis_stats', {})\n",
        "    price_is_heavy_tailed = price_stats.get('log_heavy_tailed', False) or price_stats.get('kurtosis', 0) > 3.5 # Example threshold\n",
        "\n",
        "    # Check for suggestions of number of clusters (K)\n",
        "    # Order of preference for K: optimal_k_dynamic, ensemble_n_clusters, dp_gmm_n_components\n",
        "    k_options = [\n",
        "        artifacts.get('optimal_k_dynamic'),\n",
        "        artifacts.get('ensemble_n_clusters'),\n",
        "        artifacts.get('dp_gmm_n_components')\n",
        "    ]\n",
        "    for k_val in k_options:\n",
        "        if isinstance(k_val, (int, float)) and np.isfinite(k_val) and k_val > 1:\n",
        "            inferred_k = int(round(k_val))\n",
        "            current_logger.debug(f\"Inferred K={inferred_k} from artifacts (value: {k_val}).\")\n",
        "            break # Use the first valid K found\n",
        "\n",
        "    # Determine prior type based on K and heavy-tailedness\n",
        "    if inferred_k > 1: # Mixture model\n",
        "        if price_is_heavy_tailed:\n",
        "            inferred_type = 'student_t_mixture'\n",
        "            current_logger.debug(\"Suggesting Student-t Mixture prior due to K > 1 and heavy-tailed signal.\")\n",
        "        else:\n",
        "            inferred_type = 'gaussian_mixture'\n",
        "            current_logger.debug(\"Suggesting Gaussian Mixture prior due to K > 1.\")\n",
        "    elif price_is_heavy_tailed: # Single component, heavy-tailed\n",
        "        inferred_type = 'student_t'\n",
        "        current_logger.debug(\"Suggesting Student-t prior due to heavy-tailed signal (K=1).\")\n",
        "    else: # Single component, not necessarily heavy-tailed (or no info)\n",
        "        current_logger.debug(\"Suggesting Standard Gaussian prior (default or K=1, no heavy-tailed signal).\")\n",
        "        # inferred_type remains 'standard_gaussian'\n",
        "\n",
        "    return inferred_type, inferred_k\n",
        "\n",
        "\n",
        "def _initialize_mixture_prior_params(\n",
        "    artifacts: Dict[str, Any],\n",
        "    n_components: int, # Desired number of components for the VAE prior\n",
        "    latent_dim: int,   # Latent dimension of the VAE\n",
        "    logger_instance: logging.Logger # Pass logger for messages\n",
        ") -> Dict[str, torch.Tensor]:\n",
        "    \"\"\"\n",
        "    Initializes parameters (weights, means, covariances) for a mixture prior\n",
        "    using information from DP-GMM or other clustering artifacts if available.\n",
        "    \"\"\"\n",
        "    params: Dict[str, torch.Tensor] = {}\n",
        "    dp_gmm_info = artifacts.get('dp_gmm_model_info_dict', {})\n",
        "\n",
        "    # Extract DP-GMM results if available and somewhat conformant\n",
        "    fit_k_gmm = dp_gmm_info.get('effective_components', 0)\n",
        "    gmm_means_np = np.array(dp_gmm_info.get('means', []))         # Expected shape (fit_k_gmm, latent_dim)\n",
        "    gmm_weights_np = np.array(dp_gmm_info.get('weights', []))     # Expected shape (fit_k_gmm,)\n",
        "    # Assuming diagonal covariances from DP-GMM output for simplicity here\n",
        "    gmm_covs_diag_np = np.array(dp_gmm_info.get('covariances', [])) # Expected shape (fit_k_gmm, latent_dim)\n",
        "\n",
        "    # Check if GMM results are usable\n",
        "    gmm_usable = (\n",
        "        fit_k_gmm > 0 and\n",
        "        gmm_means_np.ndim == 2 and gmm_means_np.shape[0] == fit_k_gmm and gmm_means_np.shape[1] == latent_dim and\n",
        "        gmm_weights_np.ndim == 1 and len(gmm_weights_np) == fit_k_gmm\n",
        "    )\n",
        "\n",
        "    if gmm_usable:\n",
        "        logger_instance.info(f\"Attempting to initialize prior from DP-GMM artifacts: {fit_k_gmm} GMM components found with matching latent_dim {latent_dim}.\")\n",
        "\n",
        "        # Select top K components from GMM if GMM has more components than requested\n",
        "        if fit_k_gmm >= n_components:\n",
        "            top_indices = np.argsort(gmm_weights_np)[::-1][:n_components] # Sort by weight, take top n_components\n",
        "            selected_means_np = gmm_means_np[top_indices]\n",
        "            selected_weights_np = gmm_weights_np[top_indices]\n",
        "            if gmm_covs_diag_np.ndim == 2 and gmm_covs_diag_np.shape[0] == fit_k_gmm and gmm_covs_diag_np.shape[1] == latent_dim:\n",
        "                selected_covs_diag_np = gmm_covs_diag_np[top_indices]\n",
        "            else:\n",
        "                selected_covs_diag_np = None # Covariances not usable or mismatched\n",
        "                logger_instance.warning(\"GMM covariances from artifacts are missing, malformed, or do not match latent_dim. Will use default identity for prior covariances.\")\n",
        "            logger_instance.info(f\"Initialized prior parameters from the top {n_components} (by weight) GMM components.\")\n",
        "        else: # GMM has fewer components than requested, use all available GMM components\n",
        "            selected_means_np = gmm_means_np\n",
        "            selected_weights_np = gmm_weights_np\n",
        "            if gmm_covs_diag_np.ndim == 2 and gmm_covs_diag_np.shape[0] == fit_k_gmm and gmm_covs_diag_np.shape[1] == latent_dim:\n",
        "                 selected_covs_diag_np = gmm_covs_diag_np\n",
        "            else:\n",
        "                selected_covs_diag_np = None\n",
        "                logger_instance.warning(\"GMM covariances from artifacts are missing or malformed for the available components. Defaulting.\")\n",
        "            logger_instance.warning(f\"DP-GMM artifacts have {fit_k_gmm} components, which is less than \"\n",
        "                                   f\"the requested {n_components} for the prior. Initializing with {fit_k_gmm} components from GMM and will use defaults for the rest if needed, or adapt.\")\n",
        "            # Note: The VAE prior will still have n_components. This logic just initializes *some* of them.\n",
        "            # The VAE's _setup_prior will handle filling remaining components with defaults if these are partial.\n",
        "            # For simplicity here, we might just initialize the first fit_k_gmm and let others be default.\n",
        "            # However, the `VariationalAutoencoderBase._init_param` expects full size or scalar.\n",
        "            # So, it's better to provide full-sized defaults if GMM is insufficient.\n",
        "            # Current logic below will attempt to use selected_*, if they are not sufficient, defaults in _init_param take over.\n",
        "\n",
        "        # Convert to tensors for VAE\n",
        "        # Weights (logits)\n",
        "        selected_weights_np_normalized = selected_weights_np / selected_weights_np.sum() # Normalize\n",
        "        # Ensure no zero weights for log\n",
        "        weights_logits_tensor = torch.log(torch.tensor(selected_weights_np_normalized, dtype=torch.float32).clamp(min=1e-9))\n",
        "        params['weights_logits'] = weights_logits_tensor\n",
        "\n",
        "        # Means\n",
        "        params['means'] = torch.tensor(selected_means_np, dtype=torch.float32)\n",
        "\n",
        "        # Covariances (Cholesky decomposition of diagonal covariance matrices)\n",
        "        if selected_covs_diag_np is not None:\n",
        "            try:\n",
        "                # Ensure diagonal elements are positive before sqrt for Cholesky\n",
        "                covs_diag_positive_np = np.maximum(selected_covs_diag_np, 1e-6) # Floor to small positive\n",
        "                cholesky_diag_elements_tensor = torch.sqrt(torch.tensor(covs_diag_positive_np, dtype=torch.float32))\n",
        "                # Create batch of diagonal matrices for Cholesky factors\n",
        "                cov_cholesky_tensor = torch.diag_embed(cholesky_diag_elements_tensor)\n",
        "                params['cov_cholesky'] = cov_cholesky_tensor\n",
        "                logger_instance.info(\"Initialized prior cov_cholesky from GMM diagonal covariances.\")\n",
        "            except Exception as e_chol:\n",
        "                logger_instance.error(f\"Failed to compute Cholesky from GMM diagonal covariances: {e_chol}. \"\n",
        "                                     \"Default identity covariances will be used for prior.\", exc_info=False)\n",
        "                if 'cov_cholesky' in params: del params['cov_cholesky'] # Remove to allow default\n",
        "        else:\n",
        "            logger_instance.warning(\"No usable GMM diagonal covariances for prior initialization. Default identity will be used.\")\n",
        "\n",
        "    else: # GMM artifacts not usable or not found\n",
        "        logger_instance.warning(\"DP-GMM artifacts are unsuitable or insufficient for initializing mixture prior parameters. \"\n",
        "                                \"Default parameters (e.g., zeros for means/logits, identity for covariances) will be used by the VAE.\")\n",
        "\n",
        "    # The VAE's _init_param method will handle defaults if these keys are missing or shapes are wrong.\n",
        "    # This function just provides initial guesses if GMM data is good.\n",
        "    # If params returned here are partial (e.g. fewer than n_components), _init_param might have issues.\n",
        "    # It's safer to ensure that if we provide something, it's for the correct n_components.\n",
        "    # The current logic above for selection (top_indices) tries to match n_components.\n",
        "    # If fit_k_gmm < n_components, then selected_* will have fit_k_gmm items.\n",
        "    # _init_param needs to be robust to this or this function should pad with defaults.\n",
        "    # Given _init_param structure, it's better if this function doesn't return partial tensors.\n",
        "    # So, if GMM data is insufficient to populate all n_components, we just don't add that param key,\n",
        "    # letting _init_param use its full default.\n",
        "\n",
        "    # Revised logic: Only add to `params` if we have data for *all* `n_components` or for the number of `fit_k_gmm` if `fit_k_gmm < n_components`\n",
        "    # and the `_init_param` can handle broadcasting or partial initialization (which it currently doesn't gracefully for all params).\n",
        "    # For safety, if GMM provides fewer than n_components, we might avoid initializing from it to prevent shape mismatches, unless _init_param is updated.\n",
        "    # The original code's _init_param might default if shapes don't match, which is okay.\n",
        "\n",
        "    # Example: if `params['means']` is (fit_k_gmm, latent_dim) but default is (n_components, latent_dim), _init_param defaults.\n",
        "    # This is acceptable.\n",
        "\n",
        "    return params"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5nBjdxYsngXh"
      },
      "outputs": [],
      "source": [
        "# File: validation.py\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import TimeSeriesSplit, BaseCrossValidator\n",
        "from typing import Tuple, Optional, Iterator, Union\n",
        "import logging\n",
        "\n",
        "\n",
        "def get_time_series_splitter(\n",
        "    df: pd.DataFrame,\n",
        "    date_col: str,\n",
        "    n_splits: int = 5,\n",
        "    max_train_size: Optional[int] = None,\n",
        "    test_size: Optional[int] = None, # Added optional test_size\n",
        "    gap: int = 0                 # Added optional gap (in terms of samples)\n",
        ") -> Tuple[TimeSeriesSplit, np.ndarray]:\n",
        "    \"\"\"\n",
        "    Builds and returns an sklearn TimeSeriesSplit instance configured for the\n",
        "    input DataFrame, along with the DataFrame's index sorted by the date column.\n",
        "\n",
        "    Args:\n",
        "        df: The full DataFrame (must include date_col).\n",
        "        date_col: The name of the datetime-like column to sort by.\n",
        "        n_splits: The number of splitting iterations in the cross-validator.\n",
        "        max_train_size: Maximum size for a single training set. None means no limit.\n",
        "        test_size: Used to limit the size of the test set. None means it's determined by n_splits.\n",
        "        gap: Number of samples to exclude between the end of the train set and\n",
        "             the start of the test set.\n",
        "\n",
        "    Returns:\n",
        "        A tuple containing:\n",
        "            - tscv: An initialized TimeSeriesSplit instance.\n",
        "            - ordered_idx: A NumPy array of the original DataFrame index, sorted by date_col.\n",
        "\n",
        "    Raises:\n",
        "        ValueError: If date_col is not found in df or cannot be converted to datetime.\n",
        "        ValueError: If df is empty after handling NaNs in date_col.\n",
        "\n",
        "    Example:\n",
        "        tscv, ordered_idx = get_time_series_splitter(df, 'sale_date', n_splits=4)\n",
        "        # Example usage in a training loop:\n",
        "        for train_indices, test_indices in tscv.split(ordered_idx):\n",
        "            # Use the *original* index values from ordered_idx\n",
        "            train_original_idx = ordered_idx[train_indices]\n",
        "            test_original_idx = ordered_idx[test_indices]\n",
        "            # Select data from the original DataFrame using these indices\n",
        "            train_df = df.loc[train_original_idx]\n",
        "            test_df  = df.loc[test_original_idx]\n",
        "            # ... proceed with training/evaluation ...\n",
        "    \"\"\"\n",
        "    logger.info(f\"Configuring TimeSeriesSplit (n_splits={n_splits}, date_col='{date_col}', max_train={max_train_size}, test_size={test_size}, gap={gap})\")\n",
        "\n",
        "    if date_col not in df.columns:\n",
        "        raise ValueError(f\"Date column '{date_col}' not found in DataFrame.\")\n",
        "\n",
        "    # Attempt to convert to datetime and sort, handling potential errors\n",
        "    try:\n",
        "        # Work on a copy to avoid modifying original df outside function scope\n",
        "        df_sorted = df[[date_col]].copy()\n",
        "        df_sorted[date_col] = pd.to_datetime(df_sorted[date_col], errors='coerce')\n",
        "\n",
        "        # Handle NaNs in the date column before sorting\n",
        "        initial_rows = len(df_sorted)\n",
        "        df_sorted.dropna(subset=[date_col], inplace=True)\n",
        "        if len(df_sorted) < initial_rows:\n",
        "            logger.warning(f\"Dropped {initial_rows - len(df_sorted)} rows with invalid/NaN dates in '{date_col}'.\")\n",
        "\n",
        "        if df_sorted.empty:\n",
        "             raise ValueError(f\"DataFrame is empty after dropping rows with invalid dates in '{date_col}'.\")\n",
        "\n",
        "        df_sorted = df_sorted.sort_values(by=date_col, ascending=True)\n",
        "        ordered_idx = df_sorted.index.to_numpy() # Get the original index values in sorted order\n",
        "\n",
        "    except Exception as e:\n",
        "        logger.error(f\"Error processing date column '{date_col}': {e}\", exc_info=True)\n",
        "        raise ValueError(f\"Could not process date column '{date_col}'. Ensure it's datetime-like.\") from e\n",
        "\n",
        "    # Instantiate TimeSeriesSplit\n",
        "    # Note: TimeSeriesSplit uses the *number* of samples, not dates directly\n",
        "    try:\n",
        "        tscv = TimeSeriesSplit(\n",
        "            n_splits=n_splits,\n",
        "            max_train_size=max_train_size,\n",
        "            test_size=test_size,\n",
        "            gap=gap\n",
        "        )\n",
        "        # Perform a dummy split to check if parameters are valid for the data size\n",
        "        _ = next(tscv.split(ordered_idx))\n",
        "    except ValueError as e_tscv:\n",
        "         logger.error(f\"Error initializing TimeSeriesSplit with given parameters for data size {len(ordered_idx)}: {e_tscv}\", exc_info=True)\n",
        "         raise ValueError(f\"TimeSeriesSplit parameters invalid for data size {len(ordered_idx)}: {e_tscv}\") from e_tscv\n",
        "\n",
        "    logger.info(f\"TimeSeriesSplit configured. Returning splitter and sorted index array (length {len(ordered_idx)}).\")\n",
        "    return tscv, ordered_idx\n",
        "\n",
        "\n",
        "def expanding_window_split(\n",
        "    df: pd.DataFrame,\n",
        "    date_col: str,\n",
        "    initial_train_period: pd.Timedelta,\n",
        "    test_period: pd.Timedelta,\n",
        "    gap: pd.Timedelta = pd.Timedelta(days=0), # Ensure default is Timedelta\n",
        ") -> Iterator[Tuple[np.ndarray, np.ndarray]]:\n",
        "    \"\"\"\n",
        "    Generates integer position indices for expanding-window time series splits.\n",
        "\n",
        "    The training window starts with `initial_train_period` and expands by `test_period`\n",
        "    in each subsequent split. A 'gap' can be introduced between the train and test sets.\n",
        "\n",
        "    Args:\n",
        "        df: The full DataFrame (must include date_col).\n",
        "        date_col: The name of the datetime-like column to determine splits.\n",
        "        initial_train_period: The duration of the initial training set.\n",
        "        test_period: The duration of each test set (and the amount the training\n",
        "                     window expands by each time).\n",
        "        gap: The duration of the gap between the end of the training period and\n",
        "             the start of the test period. Defaults to 0.\n",
        "\n",
        "    Yields:\n",
        "        Iterator of tuples, where each tuple contains:\n",
        "            - train_pos: NumPy array of integer positions (iloc) for the training set\n",
        "                         within the date-sorted DataFrame.\n",
        "            - test_pos: NumPy array of integer positions (iloc) for the test set\n",
        "                        within the date-sorted DataFrame.\n",
        "\n",
        "    Raises:\n",
        "        ValueError: If date_col is not found, cannot be converted to datetime,\n",
        "                    or if periods/gap are invalid.\n",
        "        ValueError: If df is empty after handling NaNs in date_col.\n",
        "\n",
        "    Example:\n",
        "        df_sorted = df.sort_values('sale_date') # Sort beforehand is recommended\n",
        "        splitter = expanding_window_split(\n",
        "            df_sorted, 'sale_date',\n",
        "            initial_train_period=pd.Timedelta('730D'), # 2 years\n",
        "            test_period=pd.Timedelta('180D'),        # 6 months\n",
        "            gap=pd.Timedelta('30D')                 # 1 month gap\n",
        "        )\n",
        "        for train_positions, test_positions in splitter:\n",
        "            train_subset = df_sorted.iloc[train_positions]\n",
        "            test_subset  = df_sorted.iloc[test_positions]\n",
        "            # ... train model on train_subset, evaluate on test_subset ...\n",
        "    \"\"\"\n",
        "    logger.info(f\"Configuring Expanding Window Split (date_col='{date_col}', initial='{initial_train_period}', \"\n",
        "                f\"test='{test_period}', gap='{gap}')\")\n",
        "\n",
        "    # --- Input Validation ---\n",
        "    if date_col not in df.columns:\n",
        "        raise ValueError(f\"Date column '{date_col}' not found in DataFrame.\")\n",
        "    if not isinstance(initial_train_period, pd.Timedelta) or initial_train_period <= pd.Timedelta(0):\n",
        "         raise ValueError(\"initial_train_period must be a positive pd.Timedelta.\")\n",
        "    if not isinstance(test_period, pd.Timedelta) or test_period <= pd.Timedelta(0):\n",
        "         raise ValueError(\"test_period must be a positive pd.Timedelta.\")\n",
        "    if not isinstance(gap, pd.Timedelta) or gap < pd.Timedelta(0):\n",
        "         raise ValueError(\"gap must be a non-negative pd.Timedelta.\")\n",
        "\n",
        "    # --- Data Preparation ---\n",
        "    try:\n",
        "        # Select only necessary columns and work on a copy\n",
        "        df_sorted = df[[date_col]].copy()\n",
        "        df_sorted[date_col] = pd.to_datetime(df_sorted[date_col], errors='coerce')\n",
        "\n",
        "        initial_rows = len(df_sorted)\n",
        "        df_sorted.dropna(subset=[date_col], inplace=True)\n",
        "        if len(df_sorted) < initial_rows:\n",
        "            logger.warning(f\"ExpandingWindow: Dropped {initial_rows - len(df_sorted)} rows with invalid/NaN dates in '{date_col}'.\")\n",
        "\n",
        "        if df_sorted.empty:\n",
        "             raise ValueError(f\"ExpandingWindow: DataFrame is empty after dropping rows with invalid dates in '{date_col}'.\")\n",
        "\n",
        "        df_sorted = df_sorted.sort_values(by=date_col, ascending=True)\n",
        "        # Get the date series for efficient comparison\n",
        "        dates_sorted = df_sorted[date_col]\n",
        "        n_samples = len(dates_sorted)\n",
        "\n",
        "    except Exception as e:\n",
        "        logger.error(f\"Error processing date column '{date_col}' for expanding window: {e}\", exc_info=True)\n",
        "        raise ValueError(f\"Could not process date column '{date_col}'. Ensure it's datetime-like.\") from e\n",
        "\n",
        "    # --- Splitting Logic ---\n",
        "    t_min = dates_sorted.iloc[0]\n",
        "    t_max = dates_sorted.iloc[-1]\n",
        "    k = 0\n",
        "    split_num = 1\n",
        "\n",
        "    while True:\n",
        "        # Calculate time boundaries for the current split\n",
        "        current_train_end = t_min + initial_train_period + (k * test_period)\n",
        "        current_test_start = current_train_end + gap\n",
        "        current_test_end = current_test_start + test_period\n",
        "\n",
        "        # Check if the test period goes beyond the available data\n",
        "        if current_test_start >= t_max:\n",
        "            logger.info(f\"Stopping expanding window: Test start ({current_test_start}) >= max data date ({t_max}). Generated {split_num-1} splits.\")\n",
        "            break\n",
        "\n",
        "        # Find integer positions using boolean indexing (often efficient enough)\n",
        "        # Alternatively, use np.searchsorted for potentially better performance on very large data\n",
        "        train_mask = (dates_sorted < current_train_end)\n",
        "        test_mask = (dates_sorted >= current_test_start) & (dates_sorted < current_test_end)\n",
        "\n",
        "        train_pos = np.where(train_mask)[0]\n",
        "        test_pos = np.where(test_mask)[0]\n",
        "\n",
        "        # Check if the test set is empty for this iteration\n",
        "        if len(test_pos) == 0:\n",
        "            # Check if the train window has already covered all data\n",
        "            if current_train_end >= t_max:\n",
        "                 logger.info(f\"Stopping expanding window: Train end ({current_train_end}) covers all data. Test set empty. Generated {split_num-1} splits.\")\n",
        "            else:\n",
        "                 logger.info(f\"Stopping expanding window: Test set for period [{current_test_start}, {current_test_end}) is empty. Generated {split_num-1} splits.\")\n",
        "            break\n",
        "\n",
        "        # Ensure train set is not empty (could happen with very small initial period)\n",
        "        if len(train_pos) == 0:\n",
        "            logger.warning(f\"Split {split_num}: Training set is empty (train_end={current_train_end}). Check initial_train_period. Skipping this split.\")\n",
        "            k += 1 # Move to the next potential window\n",
        "            continue\n",
        "\n",
        "\n",
        "        logger.debug(f\"Split {split_num}: Train Period [< {current_train_end}], Test Period [{current_test_start}, {current_test_end}) -> Train size: {len(train_pos)}, Test size: {len(test_pos)}\")\n",
        "        yield train_pos, test_pos\n",
        "\n",
        "        k += 1\n",
        "        split_num += 1\n",
        "\n",
        "    if split_num == 1: # Check if the loop never yielded anything\n",
        "        logger.warning(\"Expanding window did not generate any valid splits. Check time periods relative to data range.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2WoeDdJfyaqv",
        "outputId": "97f66715-07db-4bbd-f388-b0014594ac7d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "OpenTSNE found, will be used if enabled.\n"
          ]
        }
      ],
      "source": [
        "# =============================================================================\n",
        "# Latent‑space EDA for a trained VAE (Optimized for Speed & Fixed v2)\n",
        "# -----------------------------------------------------------------------------\n",
        "# • Works with a NumPy array `data_for_encoding` and a Pandas `metadata_df`\n",
        "# • Produces KL‑histogram, PCA plots, t‑SNE, k‑means overlay, cosine heat‑map\n",
        "# • Saves artefacts into `out_dir`\n",
        "# • Incorporates speed optimizations for large datasets.\n",
        "# • Fixes OpenTSNE API call.\n",
        "# • Fixes TypeError when plotting KMeans results.\n",
        "# =============================================================================\n",
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import torch\n",
        "from sklearn.decomposition import PCA\n",
        "# Standard TSNE from sklearn\n",
        "from sklearn.manifold import TSNE as SKL_TSNE\n",
        "from sklearn.cluster import KMeans\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "import time\n",
        "\n",
        "# Attempt to import OpenTSNE for potentially faster t-SNE\n",
        "try:\n",
        "    from openTSNE import TSNE as OpenTSNE_TSNE\n",
        "    from openTSNE import affinity # Usually needed for advanced OpenTSNE usage, good to import\n",
        "    OPEN_TSNE_AVAILABLE = True\n",
        "    print(\"OpenTSNE found, will be used if enabled.\")\n",
        "except ImportError:\n",
        "    OPEN_TSNE_AVAILABLE = False\n",
        "    print(\"OpenTSNE not found, falling back to scikit-learn TSNE.\")\n",
        "\n",
        "# ─────────────────────────────────────────────────────────────────────────────\n",
        "# 1.  Main EDA routine (Optimized & Fixed v2)\n",
        "# ─────────────────────────────────────────────────────────────────────────────\n",
        "def run_latent_eda(model,\n",
        "                   data_for_encoding: np.ndarray,\n",
        "                   metadata_df: pd.DataFrame,\n",
        "                   year_col_name: str,\n",
        "                   x_coord_col_name: str,\n",
        "                   y_coord_col_name: str,\n",
        "                   device='cuda',\n",
        "                   out_dir='latent_eda_out',\n",
        "                   batch_size=1024, # Increased default batch size for faster encoding\n",
        "                   k_clusters=8,\n",
        "                   random_seed=42,\n",
        "                   # Speed/Rigor parameters\n",
        "                   plot_dpi=150, # Lower DPI for speed, increase for publication\n",
        "                   tsne_n_samples=5000, # Max samples for t-SNE\n",
        "                   tsne_n_iter=500,     # Iterations for t-SNE (250-1000 typical)\n",
        "                   tsne_perplexity=30,\n",
        "                   tsne_on_pca_components=50, # Use top N PCs for t-SNE (None to use mu_mat)\n",
        "                   use_opentsne_if_available=True,\n",
        "                   kmeans_n_init=3, # Reduced from 'auto' or 10 for speed\n",
        "                   run_tsne_plot=True, # Flag to run t-SNE at all\n",
        "                   run_cosine_heatmap=True # Flag to run cosine heatmap\n",
        "                   ):\n",
        "    \"\"\"\n",
        "    Perform exploratory analysis of the VAE latent space (Optimized for Speed & Fixed v2).\n",
        "    \"\"\"\n",
        "    print(f\"--- Running Basic Latent EDA (Optimized & Fixed v2) ---\")\n",
        "    print(f\"Parameters: tsne_n_samples={tsne_n_samples}, tsne_n_iter={tsne_n_iter}, tsne_on_pca_components={tsne_on_pca_components}, kmeans_n_init={kmeans_n_init}\")\n",
        "\n",
        "    os.makedirs(out_dir, exist_ok=True)\n",
        "    np.random.seed(random_seed) # For reproducibility in sampling/sklearn\n",
        "    _device = torch.device(device)\n",
        "    model = model.to(_device).eval()\n",
        "\n",
        "    if data_for_encoding.dtype != np.float32:\n",
        "        data_for_encoding = data_for_encoding.astype(np.float32)\n",
        "\n",
        "    pytorch_dataset = TensorDataset(torch.from_numpy(data_for_encoding))\n",
        "\n",
        "    num_loader_workers = 0 # Often safer for speed/compatibility, esp. on Windows & with GPUs\n",
        "    if _device.type != 'cuda' and hasattr(os, 'sched_getaffinity'): # more workers for CPU processing\n",
        "         # Check if sched_getaffinity is available and returns a non-empty set\n",
        "         try:\n",
        "             affinity_set = os.sched_getaffinity(0)\n",
        "             if affinity_set:\n",
        "                 num_loader_workers = len(affinity_set) // 2 if len(affinity_set) > 1 else 0\n",
        "         except NotImplementedError:\n",
        "             # os.sched_getaffinity is not implemented on this platform\n",
        "             num_loader_workers = os.cpu_count() // 2 if os.cpu_count() and os.cpu_count() > 2 else 0\n",
        "\n",
        "\n",
        "    loader = DataLoader(\n",
        "        pytorch_dataset, batch_size=batch_size, shuffle=False,\n",
        "        num_workers=num_loader_workers,\n",
        "        pin_memory=(_device.type == 'cuda')\n",
        "    )\n",
        "\n",
        "    print(f\"Encoding {data_for_encoding.shape[0]} samples...\")\n",
        "    start_time_encode = time.time()\n",
        "    mu_list, logvar_list_for_kl = [], [] # Assuming model.encode returns mu, logvar\n",
        "\n",
        "    # Pre-fetch metadata columns if they exist to speed up the loop\n",
        "    year_series = metadata_df.get(year_col_name)\n",
        "    x_coord_series = metadata_df.get(x_coord_col_name)\n",
        "    y_coord_series = metadata_df.get(y_coord_col_name)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for i, (x_batch_tuple,) in enumerate(loader):\n",
        "            x_batch = x_batch_tuple.to(_device)\n",
        "            # Ensure your VAE's encode method returns (mu, logvar)\n",
        "            # If it only returns mu, then logvar_batch will be problematic.\n",
        "            mu_batch, logvar_batch = model.encode(x_batch) # ASSUMES (mu, logvar) output\n",
        "            mu_list.append(mu_batch.cpu())\n",
        "            logvar_list_for_kl.append(logvar_batch.cpu()) # Collect logvar for KL\n",
        "\n",
        "    mu_mat = torch.cat(mu_list).numpy()\n",
        "    logvar_mat = torch.cat(logvar_list_for_kl).numpy()\n",
        "\n",
        "    # Optimized metadata extraction (after encoding)\n",
        "    all_indices = np.arange(data_for_encoding.shape[0])\n",
        "    # Ensure metadata aligns with the number of encoded samples\n",
        "    if metadata_df.shape[0] != mu_mat.shape[0]:\n",
        "        print(f\"Warning: Metadata rows ({metadata_df.shape[0]}) != Encoded rows ({mu_mat.shape[0]}). Using metadata for first {mu_mat.shape[0]} rows.\")\n",
        "        metadata_df_aligned = metadata_df.iloc[:mu_mat.shape[0]]\n",
        "    else:\n",
        "        metadata_df_aligned = metadata_df\n",
        "\n",
        "    meta_df_out = pd.DataFrame({\n",
        "        'sample_id': all_indices,\n",
        "        # Use .values to avoid potential index mismatch issues if series were filtered\n",
        "        'year_index': metadata_df_aligned[year_col_name].values if year_col_name in metadata_df_aligned else np.nan,\n",
        "        'grid_x': metadata_df_aligned[x_coord_col_name].values if x_coord_col_name in metadata_df_aligned else np.nan,\n",
        "        'grid_y': metadata_df_aligned[y_coord_col_name].values if y_coord_col_name in metadata_df_aligned else np.nan,\n",
        "    }, index=all_indices) # Use original indices if needed later\n",
        "\n",
        "\n",
        "    print(f\"Encoding completed in {time.time() - start_time_encode:.2f}s.\")\n",
        "    print(f'Collected {mu_mat.shape[0]:,} latent vectors (dim {mu_mat.shape[1]}) and logvars.')\n",
        "\n",
        "    z_cols = [f'z_mu_{d:03d}' for d in range(mu_mat.shape[1])] # Clarify these are mu\n",
        "    df_lat = pd.concat([meta_df_out.reset_index(drop=True), pd.DataFrame(mu_mat, columns=z_cols)], axis=1)\n",
        "\n",
        "    # KL Divergence (Standard formula for Gaussian VAE N(0,I) prior)\n",
        "    kl_values = -0.5 * np.sum(1 + logvar_mat - mu_mat**2 - np.exp(logvar_mat), axis=1)\n",
        "    df_lat['kl_divergence'] = kl_values\n",
        "\n",
        "    plt.figure(figsize=(6,4))\n",
        "    sns.histplot(kl_values, bins=50, kde=True)\n",
        "    plt.xlabel('Per‑sample KL Divergence'); plt.title('KL Divergence Distribution'); plt.tight_layout()\n",
        "    plt.savefig(f'{out_dir}/kl_hist.png', dpi=plot_dpi); plt.close()\n",
        "\n",
        "    print(\"Running PCA...\")\n",
        "    start_time_pca = time.time()\n",
        "    n_pca_components = min(min(50, mu_mat.shape[1]), mu_mat.shape[0]) # Use more components if available, cap at 50\n",
        "    if n_pca_components < 1:\n",
        "        print(f\"Skipping PCA as n_components ({n_pca_components}) is less than 1.\")\n",
        "        pc = np.array([]) # Empty array\n",
        "        pca_model = None\n",
        "    else:\n",
        "        pca_model = PCA(n_components=n_pca_components, random_state=random_seed).fit(mu_mat)\n",
        "        pc = pca_model.transform(mu_mat)\n",
        "        plt.figure(figsize=(6,4))\n",
        "        plt.plot(np.cumsum(pca_model.explained_variance_ratio_)*100, marker='o')\n",
        "        plt.ylabel('% variance'); plt.xlabel('# PCs'); plt.title('Latent PCA – cumulative')\n",
        "        plt.grid(); plt.tight_layout(); plt.savefig(f'{out_dir}/pca_cumvar.png', dpi=plot_dpi); plt.close()\n",
        "\n",
        "        # Check if year_index column exists and has non-NA values before plotting\n",
        "        if 'year_index' in df_lat.columns and df_lat['year_index'].notna().any() and pc.shape[0] > 0 and pc.shape[1] >= 2:\n",
        "            plt.figure(figsize=(6,6))\n",
        "            # Convert year to category for discrete colors, handle potential NaNs\n",
        "            hue_data = df_lat['year_index'].astype('category') if pd.api.types.is_numeric_dtype(df_lat['year_index']) else df_lat['year_index']\n",
        "            sns.scatterplot(x=pc[:,0], y=pc[:,1], hue=hue_data,\n",
        "                            s=10, palette='viridis', legend='auto', alpha=0.7)\n",
        "            plt.title('PC1 vs PC2');\n",
        "            plt.legend(title='year', bbox_to_anchor=(1.02,1), loc=\"upper left\")\n",
        "            plt.tight_layout(); plt.savefig(f'{out_dir}/pca_scatter.png', dpi=plot_dpi); plt.close()\n",
        "        else:\n",
        "            print(\"Skipping PCA scatter plot due to missing/invalid year_index or insufficient PCs/data.\")\n",
        "    print(f\"PCA completed in {time.time() - start_time_pca:.2f}s.\")\n",
        "\n",
        "\n",
        "    tsne_embedding = None\n",
        "    if run_tsne_plot:\n",
        "        print(\"Preparing for t-SNE...\")\n",
        "        start_time_tsne = time.time()\n",
        "        data_for_tsne = mu_mat\n",
        "        n_samples_original = mu_mat.shape[0]\n",
        "\n",
        "        indices_tsne = np.arange(n_samples_original)\n",
        "        if n_samples_original > tsne_n_samples:\n",
        "            print(f\"Subsampling {tsne_n_samples} from {n_samples_original} for t-SNE.\")\n",
        "            indices_tsne = np.random.choice(n_samples_original, tsne_n_samples, replace=False)\n",
        "            data_for_tsne = mu_mat[indices_tsne, :]\n",
        "\n",
        "        tsne_input_source = \"mu_mat (latent vectors)\"\n",
        "        if tsne_on_pca_components is not None and pc.shape[0] > 0:\n",
        "             effective_pca_comps = min(tsne_on_pca_components, pc.shape[1])\n",
        "             if effective_pca_comps > 1:\n",
        "                  print(f\"Running t-SNE on top {effective_pca_comps} PCs.\")\n",
        "                  # Ensure we use the correct indices if data was subsampled\n",
        "                  data_for_tsne = pc[indices_tsne, :effective_pca_comps]\n",
        "                  tsne_input_source = f\"top {effective_pca_comps} PCs\"\n",
        "             else: print(f\"Not enough PCA components ({pc.shape[1]}), using original data for t-SNE.\")\n",
        "        elif tsne_on_pca_components is not None: print(\"PCA vectors not available, using original data for t-SNE.\")\n",
        "\n",
        "        current_n_samples_for_tsne = data_for_tsne.shape[0]\n",
        "\n",
        "        # Adjust perplexity: must be less than n_samples\n",
        "        actual_perplexity = min(tsne_perplexity, current_n_samples_for_tsne - 1)\n",
        "        if actual_perplexity < 5 and current_n_samples_for_tsne >=5 : actual_perplexity = 5 # Lower bound if possible\n",
        "\n",
        "        if current_n_samples_for_tsne > 1 and actual_perplexity >= 1 and actual_perplexity < current_n_samples_for_tsne :\n",
        "            tsne_kwargs = {\n",
        "                \"n_components\": 2, \"perplexity\": actual_perplexity,\n",
        "                \"random_state\": random_seed, \"n_jobs\": -1\n",
        "            }\n",
        "            tsne_impl_name = \"scikit-learn TSNE\"\n",
        "\n",
        "            if use_opentsne_if_available and OPEN_TSNE_AVAILABLE:\n",
        "                # OpenTSNE parameters\n",
        "                tsne_kwargs[\"n_iter\"] = tsne_n_iter\n",
        "                tsne_kwargs[\"early_exaggeration_iter\"] = max(50, tsne_n_iter // 4)\n",
        "                tsne_kwargs[\"n_iter\"] = max(100, tsne_n_iter * 3 // 4)\n",
        "                if \"n_jobs\" in tsne_kwargs: del tsne_kwargs[\"n_jobs\"]\n",
        "                tsne_model = OpenTSNE_TSNE(**tsne_kwargs)\n",
        "                tsne_impl_name = \"OpenTSNE\"\n",
        "                print(f\"Running {tsne_impl_name} (Perp={actual_perplexity:.1f}, NIter~{tsne_n_iter}) on {current_n_samples_for_tsne} samples ({tsne_input_source})...\")\n",
        "                # --- FIX: Call OpenTSNE's .fit() method ---\n",
        "                tsne_embedding = tsne_model.fit(data_for_tsne.astype(np.float64))\n",
        "                # --- End Fix ---\n",
        "            else: # Fallback to sklearn\n",
        "                tsne_kwargs[\"init\"] = 'pca'\n",
        "                tsne_kwargs[\"learning_rate\"] = 'auto'\n",
        "                tsne_kwargs[\"n_iter\"] = tsne_n_iter\n",
        "                tsne_model = SKL_TSNE(**tsne_kwargs)\n",
        "                print(f\"Running {tsne_impl_name} (Perp={actual_perplexity:.1f}, NIter={tsne_n_iter}) on {current_n_samples_for_tsne} samples ({tsne_input_source})...\")\n",
        "                tsne_embedding = tsne_model.fit_transform(data_for_tsne.astype(np.float64))\n",
        "\n",
        "            # Plotting t-SNE\n",
        "            tsne_meta_df = df_lat.iloc[indices_tsne].reset_index(drop=True) # Use subsampled indices on df_lat\n",
        "            if 'year_index' in tsne_meta_df.columns and tsne_meta_df['year_index'].notna().any():\n",
        "                plt.figure(figsize=(7,6))\n",
        "                hue_data_tsne = tsne_meta_df['year_index'].astype('category') if pd.api.types.is_numeric_dtype(tsne_meta_df['year_index']) else tsne_meta_df['year_index']\n",
        "                sns.scatterplot(x=tsne_embedding[:,0], y=tsne_embedding[:,1],\n",
        "                                hue=hue_data_tsne, s=10, palette='viridis', legend='auto', alpha=0.7)\n",
        "                plt.title(f'{tsne_impl_name} (Perp {actual_perplexity:.0f}, NIter {tsne_n_iter})')\n",
        "                plt.legend(title='year', bbox_to_anchor=(1.02,1), loc=\"upper left\")\n",
        "                plt.tight_layout(); plt.savefig(f'{out_dir}/tsne.png', dpi=plot_dpi); plt.close()\n",
        "            else: # Basic plot if no year data\n",
        "                plt.figure(figsize=(7,6))\n",
        "                sns.scatterplot(x=tsne_embedding[:,0], y=tsne_embedding[:,1], s=10, alpha=0.7)\n",
        "                plt.title(f'{tsne_impl_name} (Perp {actual_perplexity:.0f}, NIter {tsne_n_iter})')\n",
        "                plt.tight_layout(); plt.savefig(f'{out_dir}/tsne_no_hue.png', dpi=plot_dpi); plt.close()\n",
        "            print(f\"t-SNE completed in {time.time() - start_time_tsne:.2f}s.\")\n",
        "        else:\n",
        "            print(f'Skipping t-SNE plot due to insufficient samples ({current_n_samples_for_tsne}) or invalid perplexity ({actual_perplexity}).')\n",
        "    else:\n",
        "        print(\"Skipping t-SNE plot as per configuration (run_tsne_plot=False).\")\n",
        "\n",
        "    print(\"Running K-Means...\")\n",
        "    start_time_kmeans = time.time()\n",
        "    if pc.shape[0] > 0 and pc.shape[1] >= 2: # K-means overlay on PCA\n",
        "        actual_k_clusters = min(k_clusters, mu_mat.shape[0])\n",
        "        if actual_k_clusters > 1:\n",
        "            kmeans = KMeans(n_clusters=actual_k_clusters, n_init=kmeans_n_init, random_state=random_seed, algorithm=\"lloyd\")\n",
        "            labels = kmeans.fit_predict(mu_mat) # Cluster on original mu_mat\n",
        "            df_lat['cluster'] = labels\n",
        "\n",
        "            plt.figure(figsize=(6,6))\n",
        "            # --- FIX: Remove .astype('category') ---\n",
        "            sns.scatterplot(x=pc[:,0], y=pc[:,1], hue=labels, palette='tab10', s=10, legend='auto', alpha=0.7)\n",
        "            # --- End Fix ---\n",
        "            plt.title(f'PCA scatter with K‑means (k={actual_k_clusters})')\n",
        "            # Add legend if number of clusters is reasonable\n",
        "            if actual_k_clusters <= 20:\n",
        "                 plt.legend(title='Cluster', bbox_to_anchor=(1.02, 1), loc='upper left')\n",
        "            else:\n",
        "                 plt.legend().remove() # Remove legend if too many clusters\n",
        "            plt.tight_layout(); plt.savefig(f'{out_dir}/pca_kmeans.png', dpi=plot_dpi); plt.close()\n",
        "        else:\n",
        "            print(f\"Skipping K-means as k_clusters ({actual_k_clusters}) is too small.\")\n",
        "            df_lat['cluster'] = 0\n",
        "    else:\n",
        "        print(\"Skipping K-means overlay on PCA plot due to insufficient PCs/data.\")\n",
        "        df_lat['cluster'] = 0\n",
        "    print(f\"K-Means completed in {time.time() - start_time_kmeans:.2f}s.\")\n",
        "\n",
        "    if run_cosine_heatmap:\n",
        "        print(\"Running Cosine Similarity Heatmap...\")\n",
        "        start_time_cosine = time.time()\n",
        "        n_show_cosine = min(100, mu_mat.shape[0]) # Keep sample size small for clustermap\n",
        "        if n_show_cosine > 1:\n",
        "            sub_indices_cosine = np.random.choice(mu_mat.shape[0], n_show_cosine, replace=False)\n",
        "            sim = cosine_similarity(mu_mat[sub_indices_cosine])\n",
        "            try:\n",
        "                g = sns.clustermap(sim, cmap='viridis', figsize=(8,8))\n",
        "                g.fig.suptitle(f'Cosine similarity – {n_show_cosine} random latent vectors', y=1.02)\n",
        "                plt.savefig(f'{out_dir}/cosine_heatmap.png', dpi=plot_dpi); plt.close(g.fig)\n",
        "            except Exception as e_cm:\n",
        "                print(f\"Error during clustermap generation: {e_cm}. Skipping.\")\n",
        "                plt.close() # Ensure figure is closed even if save fails\n",
        "        else:\n",
        "            print(f\"Skipping cosine similarity heatmap as n_show_cosine ({n_show_cosine}) is too small.\")\n",
        "        print(f\"Cosine Similarity completed in {time.time() - start_time_cosine:.2f}s.\")\n",
        "    else:\n",
        "        print(\"Skipping Cosine Similarity Heatmap as per configuration.\")\n",
        "\n",
        "    print(\"Saving artifacts...\")\n",
        "    df_lat.to_parquet(f'{out_dir}/latent_table.parquet')\n",
        "    if pc.shape[0] > 0: np.save(f'{out_dir}/pc_top_components.npy', pc)\n",
        "    if tsne_embedding is not None: np.save(f'{out_dir}/tsne_embedding_sampled.npy', tsne_embedding) # Note: this is on sampled data\n",
        "\n",
        "    print(f'All basic EDA artefacts saved to “{out_dir}/”')\n",
        "    print(f\"--- Basic Latent EDA (Optimized & Fixed v2) Finished ---\")\n",
        "    return df_lat\n",
        "\n",
        "# ════════════════════════════════════════════════════════════════════════════\n",
        "# EXAMPLE USAGE (to be adapted and placed in the new pipeline script)\n",
        "# ════════════════════════════════════════════════════════════════════════════\n",
        "# This part needs to be run within the new pipeline script where\n",
        "# vae_model, vae_input_data, and df_for_vae_prep (or equivalent metadata DF)\n",
        "# and coordinate/year column names are defined.\n",
        "\n",
        "# Example placeholder for how it would be called:\n",
        "# Make sure these variables are defined from your new pipeline:\n",
        "# pipeline_vae_model: The trained VAE model from your new pipeline.\n",
        "# pipeline_vae_input_data: The NumPy array used as input for the VAE (e.g., scaled features).\n",
        "# pipeline_metadata_df: The Pandas DataFrame aligned with `pipeline_vae_input_data`,\n",
        "#                       containing original features and metadata.\n",
        "# year_column_for_eda_actual: String name of the year column in `pipeline_metadata_df`.\n",
        "# actual_x_coord: String name of the X coordinate column in `pipeline_metadata_df`.\n",
        "# actual_y_coord: String name of the Y coordinate column in `pipeline_metadata_df`.\n",
        "# DEVICE: 'cuda' or 'cpu'\n",
        "# VAE_BATCH_SIZE: Batch size used in pipeline\n",
        "# RANDOM_SEED: Seed used in pipeline\n",
        "# EDA_PARAMS: Dictionary containing speed parameters like {'plot_dpi': 150, 'tsne_n_samples': 5000, ...}\n",
        "\n",
        "# if 'pipeline_vae_model' in locals() and 'pipeline_vae_input_data' in locals() and 'pipeline_metadata_df' in locals():\n",
        "#     df_latent_output = run_latent_eda(\n",
        "#         model=pipeline_vae_model,\n",
        "#         data_for_encoding=pipeline_vae_input_data,\n",
        "#         metadata_df=pipeline_metadata_df,\n",
        "#         year_col_name=year_column_for_eda_actual,\n",
        "#         x_coord_col_name=actual_x_coord,\n",
        "#         y_coord_col_name=actual_y_coord,\n",
        "#         device=DEVICE,\n",
        "#         out_dir='latent_eda_output_v1_optimized_fixed_v2', # Updated dir name\n",
        "#         batch_size=VAE_BATCH_SIZE,\n",
        "#         random_seed=RANDOM_SEED,\n",
        "#         **EDA_PARAMS # Pass the dictionary of speed parameters\n",
        "#     )\n",
        "#     print(df_latent_output.head())\n",
        "# else:\n",
        "#     print(\"Required variables for run_latent_eda not found.\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RbQFDWjVycm2",
        "outputId": "ad21e832-57f7-40ec-8e43-8a23ce42622f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "OpenTSNE found, will be used if enabled.\n"
          ]
        }
      ],
      "source": [
        "# =============================================================================\n",
        "# Enhanced Latent‑space EDA for a trained VAE (Optimized for Speed & Fixed v2)\n",
        "# -----------------------------------------------------------------------------\n",
        "# • Works with a NumPy array `data_for_encoding` and a Pandas `metadata_df`\n",
        "# • Produces comprehensive latent space analysis with advanced visualizations\n",
        "# • Includes statistical analysis, clustering insights, advanced plots\n",
        "# • Incorporates speed optimizations (Cluster Eval, t-SNE, Latent Traversal).\n",
        "# • Fixes NumPy 2.0 compatibility issue (`np.float_`).\n",
        "# =============================================================================\n",
        "import os\n",
        "import time # Import time for benchmarking sections\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import torch\n",
        "from sklearn.decomposition import PCA\n",
        "# Standard TSNE from sklearn\n",
        "from sklearn.manifold import TSNE as SKL_TSNE\n",
        "# Import KMeans and potentially MiniBatchKMeans\n",
        "from sklearn.cluster import KMeans, AgglomerativeClustering, DBSCAN, MiniBatchKMeans\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "from sklearn.metrics import silhouette_score, calinski_harabasz_score, davies_bouldin_score\n",
        "from scipy.stats import kurtosis, skew\n",
        "from scipy.cluster.hierarchy import dendrogram, linkage\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "# from umap import UMAP # Kept commented\n",
        "import plotly.express as px\n",
        "import plotly.graph_objects as go\n",
        "from plotly.subplots import make_subplots\n",
        "import matplotlib.gridspec as gridspec\n",
        "from matplotlib.colors import LinearSegmentedColormap\n",
        "import pickle\n",
        "import json\n",
        "from datetime import datetime\n",
        "import warnings # To suppress specific warnings if needed\n",
        "\n",
        "# Attempt to import OpenTSNE for potentially faster t-SNE\n",
        "try:\n",
        "    from openTSNE import TSNE as OpenTSNE_TSNE\n",
        "    from openTSNE import affinity # Usually needed for advanced OpenTSNE usage, good to import\n",
        "    OPEN_TSNE_AVAILABLE = True\n",
        "    print(\"OpenTSNE found, will be used if enabled.\")\n",
        "except ImportError:\n",
        "    OPEN_TSNE_AVAILABLE = False\n",
        "    print(\"OpenTSNE not found, falling back to scikit-learn TSNE.\")\n",
        "\n",
        "# Set styling for all plots\n",
        "plt.style.use('seaborn-v0_8-whitegrid')\n",
        "sns.set_context(\"paper\", font_scale=1.2) # Slightly smaller font scale might render faster\n",
        "PALETTE = \"viridis\"\n",
        "CUSTOM_CMAP = LinearSegmentedColormap.from_list(\"custom_viridis\",\n",
        "                                                plt.cm.viridis(np.linspace(0.1, 0.9, 256)))\n",
        "\n",
        "# --- Helper to suppress specific warnings during KMeans if needed ---\n",
        "# from sklearn.exceptions import ConvergenceWarning\n",
        "# warnings.filterwarnings(\"ignore\", category=ConvergenceWarning)\n",
        "\n",
        "# ─────────────────────────────────────────────────────────────────────────────\n",
        "# 2. Extended statistical analysis functions (Unchanged from previous fix)\n",
        "# ─────────────────────────────────────────────────────────────────────────────\n",
        "def analyze_latent_statistics(mu_mat):\n",
        "    \"\"\"Perform detailed statistical analysis of latent dimensions\"\"\"\n",
        "    n_dims = mu_mat.shape[1]\n",
        "    with warnings.catch_warnings():\n",
        "         warnings.simplefilter(\"ignore\", category=RuntimeWarning)\n",
        "         stats = {\n",
        "             'mean': np.mean(mu_mat, axis=0), 'std': np.std(mu_mat, axis=0),\n",
        "             'min': np.min(mu_mat, axis=0), 'max': np.max(mu_mat, axis=0),\n",
        "             'median': np.median(mu_mat, axis=0), 'skewness': skew(mu_mat, axis=0),\n",
        "             'kurtosis': kurtosis(mu_mat, axis=0), 'iqr': np.percentile(mu_mat, 75, axis=0) - np.percentile(mu_mat, 25, axis=0),\n",
        "             'variance': np.var(mu_mat, axis=0), 'activation_ratio': np.mean(np.abs(mu_mat) > 0.1, axis=0)\n",
        "         }\n",
        "    stats_df = pd.DataFrame({k: stats[k] for k in stats}, index=[f'z{i:03d}' for i in range(n_dims)])\n",
        "    stats_df['importance_score'] = ( stats_df['std'].fillna(0) + np.abs(stats_df['skewness'].fillna(0)) * 0.2 + stats_df['activation_ratio'].fillna(0) * 5 )\n",
        "    corr_mat = np.corrcoef(mu_mat.T)\n",
        "    return stats_df, corr_mat\n",
        "\n",
        "def evaluate_clustering_quality(\n",
        "        mu_mat: np.ndarray, labels: np.ndarray, k_range: range = range(2, 15),\n",
        "        kmeans_algo_eval: str = 'KMeans', kmeans_n_init_eval: int = 1,\n",
        "        metric_subsample_size: int = 2000, random_state: int = 42\n",
        "    ) -> pd.DataFrame:\n",
        "    \"\"\" Evaluate clustering quality metrics across multiple k values (Optimized). \"\"\"\n",
        "    print(f\"--- Evaluating Cluster Quality (k={list(k_range)}, algo='{kmeans_algo_eval}', n_init={kmeans_n_init_eval}, metric_subsample={metric_subsample_size}) ---\")\n",
        "    start_time = time.time()\n",
        "    metrics = {'k': [], 'silhouette': [], 'calinski_harabasz': [], 'davies_bouldin': []}\n",
        "    n_samples_total = mu_mat.shape[0]\n",
        "    if n_samples_total <= 1: return pd.DataFrame(metrics)\n",
        "    unique_labels_provided, counts = np.unique(labels, return_counts=True)\n",
        "    n_labels_provided = len(unique_labels_provided)\n",
        "    actual_metric_subsample_size = min(metric_subsample_size, n_samples_total) if n_samples_total > 1 else None\n",
        "\n",
        "    def safe_metric_call(metric_fn, data, current_labels, **kwargs):\n",
        "        try:\n",
        "            num_unique_labs = len(np.unique(current_labels))\n",
        "            if 1 < num_unique_labs < n_samples_total:\n",
        "                call_kwargs = kwargs.copy()\n",
        "                if metric_fn == silhouette_score:\n",
        "                    if actual_metric_subsample_size and actual_metric_subsample_size < n_samples_total:\n",
        "                        call_kwargs['sample_size'] = actual_metric_subsample_size\n",
        "                        call_kwargs['random_state'] = random_state\n",
        "                return metric_fn(data, current_labels, **call_kwargs)\n",
        "            return np.nan\n",
        "        except ValueError: return np.nan\n",
        "\n",
        "    if n_labels_provided > 1:\n",
        "        metrics['k'].append(n_labels_provided)\n",
        "        metrics['silhouette'].append(safe_metric_call(silhouette_score, mu_mat, labels))\n",
        "        metrics['calinski_harabasz'].append(safe_metric_call(calinski_harabasz_score, mu_mat, labels))\n",
        "        metrics['davies_bouldin'].append(safe_metric_call(davies_bouldin_score, mu_mat, labels))\n",
        "\n",
        "    ClusterImpl = KMeans if kmeans_algo_eval == 'KMeans' else MiniBatchKMeans\n",
        "    for k_val in k_range:\n",
        "        if k_val == n_labels_provided: continue\n",
        "        if k_val >= n_samples_total or k_val <= 1: continue\n",
        "        try:\n",
        "             if kmeans_algo_eval == 'KMeans': cluster_model = ClusterImpl(n_clusters=k_val, n_init=kmeans_n_init_eval, random_state=random_state, algorithm=\"lloyd\")\n",
        "             else: mbk_batch_size = min(1024*3, n_samples_total // 2 if n_samples_total > 1 else 1); cluster_model = ClusterImpl(n_clusters=k_val, n_init=kmeans_n_init_eval, random_state=random_state, batch_size=mbk_batch_size, max_iter=100)\n",
        "             current_iter_labels = cluster_model.fit_predict(mu_mat)\n",
        "             metrics['k'].append(k_val)\n",
        "             metrics['silhouette'].append(safe_metric_call(silhouette_score, mu_mat, current_iter_labels))\n",
        "             metrics['calinski_harabasz'].append(safe_metric_call(calinski_harabasz_score, mu_mat, current_iter_labels))\n",
        "             metrics['davies_bouldin'].append(safe_metric_call(davies_bouldin_score, mu_mat, current_iter_labels))\n",
        "        except Exception as e: print(f\"Warning: Clustering/Metric failed for k={k_val}. Error: {e}\"); metrics['k'].append(k_val); metrics['silhouette'].append(np.nan); metrics['calinski_harabasz'].append(np.nan); metrics['davies_bouldin'].append(np.nan)\n",
        "    print(f\"Cluster quality evaluation completed in {time.time() - start_time:.2f}s.\")\n",
        "    return pd.DataFrame(metrics).sort_values(by='k').reset_index(drop=True)\n",
        "\n",
        "def generate_cluster_profiles(mu_mat, logvar_mat, labels, meta_df, year_col_name, x_coord_col_name, y_coord_col_name):\n",
        "    \"\"\"Generate statistical profiles for each cluster (Generally fast)\"\"\"\n",
        "    unique_labels = np.unique(labels); profiles = []\n",
        "    kl_per_sample = -0.5 * np.sum(1 + logvar_mat - mu_mat**2 - np.exp(logvar_mat), axis=1)\n",
        "    temp_df_data = {'label': labels, 'kl': kl_per_sample}\n",
        "    if year_col_name and year_col_name in meta_df.columns: temp_df_data[year_col_name] = meta_df[year_col_name]\n",
        "    if x_coord_col_name and x_coord_col_name in meta_df.columns: temp_df_data[x_coord_col_name] = meta_df[x_coord_col_name]\n",
        "    if y_coord_col_name and y_coord_col_name in meta_df.columns: temp_df_data[y_coord_col_name] = meta_df[y_coord_col_name]\n",
        "    temp_df = pd.DataFrame(temp_df_data)\n",
        "    mu_df = pd.DataFrame(mu_mat, columns=[f'mu_{i}' for i in range(mu_mat.shape[1])])\n",
        "    temp_df = pd.concat([temp_df.reset_index(drop=True), mu_df.reset_index(drop=True)], axis=1)\n",
        "    grouped = temp_df.groupby('label')\n",
        "    for label, group in grouped:\n",
        "        cluster_mu_data = group[[f'mu_{i}' for i in range(mu_mat.shape[1])]].values\n",
        "        profile = {'cluster_id': label, 'size': len(group), 'proportion': len(group) / len(labels), 'center_mu': np.mean(cluster_mu_data, axis=0), 'dispersion_mu': np.mean(np.std(cluster_mu_data, axis=0)), 'avg_kl': group['kl'].mean()}\n",
        "        if year_col_name and year_col_name in group.columns: profile['year_distribution'] = group[year_col_name].value_counts(normalize=True).to_dict()\n",
        "        spatial_dist = {}\n",
        "        if x_coord_col_name and x_coord_col_name in group.columns: spatial_dist['x_mean'] = group[x_coord_col_name].mean(); spatial_dist['x_std'] = group[x_coord_col_name].std()\n",
        "        if y_coord_col_name and y_coord_col_name in group.columns: spatial_dist['y_mean'] = group[y_coord_col_name].mean(); spatial_dist['y_std'] = group[y_coord_col_name].std()\n",
        "        if spatial_dist: profile['spatial_distribution'] = spatial_dist\n",
        "        profiles.append(profile)\n",
        "    return profiles\n",
        "\n",
        "# ─────────────────────────────────────────────────────────────────────────────\n",
        "# 3. Advanced visualization functions (Optimized & Fixed)\n",
        "# ─────────────────────────────────────────────────────────────────────────────\n",
        "def plot_kl_analysis(kl_per_sample, out_dir, plot_dpi=150):\n",
        "    \"\"\"Generate comprehensive KL divergence visualizations\"\"\"\n",
        "    log_kl = np.log1p(np.abs(kl_per_sample)); fig = plt.figure(figsize=(18, 6)); gs = gridspec.GridSpec(1, 3, width_ratios=[1, 1, 1])\n",
        "    ax0 = plt.subplot(gs[0]); sns.histplot(kl_per_sample, bins=50, kde=True, color='darkblue', ax=ax0); ax0.set_xlabel('KL Divergence per Sample'); ax0.set_ylabel('Frequency'); ax0.set_title('KL Distribution')\n",
        "    ax1 = plt.subplot(gs[1]); sns.histplot(log_kl, bins=50, kde=True, color='darkgreen', ax=ax1); ax1.set_xlabel('Log(1+|KL|)'); ax1.set_ylabel('Frequency'); ax1.set_title('Log-Transformed KL Distribution')\n",
        "    ax2 = plt.subplot(gs[2]); sns.ecdfplot(kl_per_sample, ax=ax2, color='darkred'); ax2.set_xlabel('KL Divergence per Sample'); ax2.set_ylabel('Cumulative Proportion'); ax2.set_title('Cumulative KL Distribution')\n",
        "    plt.tight_layout(); plt.savefig(f'{out_dir}/kl_analysis.png', dpi=plot_dpi); plt.close()\n",
        "    return kl_per_sample\n",
        "\n",
        "def plot_pca_analysis(mu_mat, meta_df, labels, year_col_name, out_dir, plot_dpi=150):\n",
        "    \"\"\"Generate comprehensive PCA visualizations (NumPy 2.0 Fix)\"\"\"\n",
        "    start_time = time.time(); pca = PCA(n_components=min(mu_mat.shape[1], mu_mat.shape[0], 100), random_state=42).fit(mu_mat)\n",
        "    pc = pca.transform(mu_mat); explained_variance = pca.explained_variance_ratio_\n",
        "    print(f\"PCA calculation took {time.time() - start_time:.2f}s\")\n",
        "    n_pcs_to_show = min(10, pca.n_components_); pc_df = pd.DataFrame(pc[:, :n_pcs_to_show], columns=[f'PC{i+1}' for i in range(n_pcs_to_show)])\n",
        "    pc_df = pd.concat([meta_df.reset_index(drop=True), pc_df], axis=1); pc_df['cluster'] = labels\n",
        "    fig, ax = plt.subplots(figsize=(12, 6)); num_components_plot = pca.n_components_\n",
        "    ax.bar(range(1, num_components_plot + 1), explained_variance * 100, color=plt.cm.viridis(np.linspace(0, 1, num_components_plot))); ax.plot(range(1, num_components_plot + 1), np.cumsum(explained_variance) * 100, 'o-', color='darkred', linewidth=3)\n",
        "    ax.set_xlabel('Principal Component'); ax.set_ylabel('Explained Variance (%)'); ax.set_title('PCA Explained Variance'); ax.grid(True, linestyle='--', alpha=0.7)\n",
        "    ax2 = ax.twinx(); ax2.set_ylabel('Cumulative Explained Variance (%)', color='darkred'); ax2.tick_params(axis='y', labelcolor='darkred')\n",
        "    max_ev = np.nanmax(explained_variance); ax.set_ylim(0, max_ev * 100 * 1.1 if np.isfinite(max_ev) and max_ev > 0 else 10); ax2.set_ylim(0, 105); ax.set_xlim(0.5, num_components_plot + 0.5)\n",
        "    plt.tight_layout(); plt.savefig(f'{out_dir}/pca_variance.png', dpi=plot_dpi); plt.close()\n",
        "    hover_data_cols = ['sample_id']; # Start with sample_id\n",
        "    if 'grid_x' in pc_df.columns: hover_data_cols.append('grid_x')\n",
        "    if 'grid_y' in pc_df.columns: hover_data_cols.append('grid_y')\n",
        "    if year_col_name and year_col_name in pc_df.columns: hover_data_cols.append(year_col_name)\n",
        "    is_cluster_float = np.issubdtype(pc_df['cluster'].dtype, np.floating) if 'cluster' in pc_df.columns else False\n",
        "    if 'PC1' in pc_df.columns and 'PC2' in pc_df.columns:\n",
        "        fig_pca_2d = px.scatter(pc_df, x='PC1', y='PC2', color='cluster' if 'cluster' in pc_df.columns else None, symbol=year_col_name if year_col_name and year_col_name in pc_df.columns else None, hover_data=hover_data_cols, color_continuous_scale='viridis' if is_cluster_float else None, color_discrete_map={-1: \"lightgrey\"} if -1 in labels else None, title='PCA Visualization (PC1 vs PC2)'); fig_pca_2d.write_html(f'{out_dir}/pca_interactive.html')\n",
        "    if 'PC3' in pc_df.columns:\n",
        "        fig_pca_3d = px.scatter_3d(pc_df, x='PC1', y='PC2', z='PC3', color='cluster' if 'cluster' in pc_df.columns else None, symbol=year_col_name if year_col_name and year_col_name in pc_df.columns else None, hover_data=hover_data_cols, color_continuous_scale='viridis' if is_cluster_float else None, color_discrete_map={-1: \"lightgrey\"} if -1 in labels else None, title='3D PCA Visualization'); fig_pca_3d.write_html(f'{out_dir}/pca_3d.html')\n",
        "    num_pcs_pairplot = min(4, n_pcs_to_show)\n",
        "    if num_pcs_pairplot > 1 and year_col_name and year_col_name in pc_df.columns: sns.pairplot(pc_df, vars=[f'PC{i+1}' for i in range(num_pcs_pairplot)], hue=year_col_name, palette='viridis', plot_kws={'alpha': 0.5, 's': 10}, diag_kind='kde'); plt.savefig(f'{out_dir}/pca_pairplot.png', dpi=plot_dpi); plt.close()\n",
        "    if hasattr(pca, 'components_'):\n",
        "        loadings = pca.components_.T[:, :n_pcs_to_show]; plt.figure(figsize=(max(8, n_pcs_to_show * 0.8), max(6, mu_mat.shape[1] * 0.3))); sns.heatmap(loadings, cmap='coolwarm', center=0, annot=False, fmt=\".2f\", yticklabels=[f'z{i:03d}' for i in range(loadings.shape[0])], xticklabels=[f'PC{i+1}' for i in range(loadings.shape[1])]); plt.title(f'PCA Loadings (Top {n_pcs_to_show} PCs)'); plt.tight_layout(); plt.savefig(f'{out_dir}/pca_loadings.png', dpi=plot_dpi); plt.close()\n",
        "    return pc, pca\n",
        "\n",
        "# def plot_umap_visualization(...): # Kept commented\n",
        "\n",
        "def plot_clustering_analysis(mu_mat, labels, out_dir, cluster_metrics, cluster_profiles, plot_dpi=150, dendrogram_n_samples=2000):\n",
        "    \"\"\"Generate comprehensive clustering analysis visualizations (Optimized Dendrogram)\"\"\"\n",
        "    if not cluster_metrics.empty:\n",
        "        fig, axes = plt.subplots(1, 3, figsize=(18, 5)); plot_metrics = [('silhouette', 'Silhouette Score', 'Higher is better', 'blue'), ('calinski_harabasz', 'Calinski-Harabasz Score', 'Higher is better', 'green'), ('davies_bouldin', 'Davies-Bouldin Score', 'Lower is better', 'red')]\n",
        "        for i, (metric_name, title_name, subtitle, color) in enumerate(plot_metrics):\n",
        "             if metric_name in cluster_metrics.columns: axes[i].plot(cluster_metrics['k'], cluster_metrics[metric_name], 'o-', linewidth=2, color=color); axes[i].set_xlabel('Number of Clusters (k)'); axes[i].set_ylabel(title_name); axes[i].set_title(f'{title_name} by k\\n({subtitle})'); axes[i].grid(True, linestyle='--', alpha=0.7)\n",
        "        plt.tight_layout(); plt.savefig(f'{out_dir}/cluster_evaluation.png', dpi=plot_dpi); plt.close()\n",
        "    else: print(\"Cluster metrics DataFrame is empty. Skipping cluster evaluation plot.\")\n",
        "    if cluster_profiles:\n",
        "         cluster_sizes = np.array([p['size'] for p in cluster_profiles if 'size' in p]); cluster_kl = np.array([p['avg_kl'] for p in cluster_profiles if 'avg_kl' in p]); cluster_dispersion = np.array([p['dispersion_mu'] for p in cluster_profiles if 'dispersion_mu' in p])\n",
        "         if len(cluster_sizes) > 0 and len(cluster_kl) > 0 and len(cluster_dispersion) > 0:\n",
        "              fig, ax = plt.subplots(figsize=(12, 6)); size_norm = cluster_sizes / max(1, max(cluster_sizes))\n",
        "              scatter = ax.scatter(range(len(cluster_profiles)), cluster_kl, s=size_norm * 500 + 50, c=cluster_dispersion, cmap='viridis', alpha=0.7)\n",
        "              for i, profile in enumerate(cluster_profiles): ax.text(i, profile['avg_kl'], str(profile['cluster_id']), ha='center', va='center', fontsize=10, fontweight='bold')\n",
        "              ax.set_xlabel('Cluster ID (index)'); ax.set_xticks(range(len(cluster_profiles))); ax.set_xticklabels([p['cluster_id'] for p in cluster_profiles]); ax.set_ylabel('Average KL Divergence'); ax.set_title('Cluster Profiles: Size, KL, and Dispersion (mu-space)')\n",
        "              cbar = plt.colorbar(scatter); cbar.set_label('Dispersion (Avg Std Dev in mu-space)')\n",
        "              plt.tight_layout(); plt.savefig(f'{out_dir}/cluster_profiles.png', dpi=plot_dpi); plt.close()\n",
        "         else: print(\"Insufficient data in cluster_profiles to plot profiles visualization.\")\n",
        "    if dendrogram_n_samples > 0:\n",
        "        print(f\"Running Dendrogram linkage calculation on sample size: {dendrogram_n_samples}\"); start_time_dendro = time.time()\n",
        "        actual_dendro_samples = min(dendrogram_n_samples, mu_mat.shape[0])\n",
        "        if actual_dendro_samples > 1:\n",
        "            indices_dendro = np.random.choice(mu_mat.shape[0], actual_dendro_samples, replace=False); sample_data_dendro = mu_mat[indices_dendro]\n",
        "            if sample_data_dendro.shape[0] > 1:\n",
        "                try:\n",
        "                    Z = linkage(sample_data_dendro, method='ward'); print(f\"Linkage calculation took {time.time() - start_time_dendro:.2f}s\")\n",
        "                    plt.figure(figsize=(12, 8)); p_truncate = min(30, sample_data_dendro.shape[0]-1 if sample_data_dendro.shape[0]>1 else 1)\n",
        "                    dendrogram(Z, truncate_mode='lastp', p=p_truncate, leaf_rotation=90.); plt.title(f'Hierarchical Clustering Dendrogram (Ward, Sample={actual_dendro_samples})'); plt.xlabel(f'Samples (showing last {p_truncate} merged clusters)'); plt.ylabel('Distance'); plt.tight_layout(); plt.savefig(f'{out_dir}/dendrogram.png', dpi=plot_dpi); plt.close()\n",
        "                except Exception as e_linkage: print(f\"Error during linkage/dendrogram: {e_linkage}\")\n",
        "            else: print(\"Not enough samples for dendrogram after random choice.\")\n",
        "        else: print(\"Not enough samples for dendrogram.\")\n",
        "    else: print(\"Skipping dendrogram plot as per configuration.\")\n",
        "\n",
        "def plot_advanced_similarity_analysis(mu_mat, labels, stats_df, corr_mat, out_dir, plot_dpi=150):\n",
        "    \"\"\"Generate advanced similarity and correlation visualizations\"\"\"\n",
        "    if not stats_df.empty and 'importance_score' in stats_df.columns:\n",
        "         top_dims_series = stats_df.sort_values('importance_score', ascending=False).index[:20]; dim_indices = [int(dim[1:]) for dim in top_dims_series if dim[1:].isdigit()]\n",
        "         if dim_indices:\n",
        "              top_corr = corr_mat[np.ix_(dim_indices, dim_indices)]; plt.figure(figsize=(max(10, len(dim_indices)*0.6), max(8, len(dim_indices)*0.5))); sns.heatmap(top_corr, cmap='coolwarm', center=0, annot=False, fmt='.2f', xticklabels=[stats_df.index[i] for i in dim_indices], yticklabels=[stats_df.index[i] for i in dim_indices]); plt.title(f'Correlation Matrix for Top {len(dim_indices)} Important Dimensions'); plt.tight_layout(); plt.savefig(f'{out_dir}/top_dims_correlation.png', dpi=plot_dpi); plt.close()\n",
        "              top_dims_df = stats_df.loc[[stats_df.index[i] for i in dim_indices]]; plt.figure(figsize=(15, 10)); gs_stats = gridspec.GridSpec(3, 1, height_ratios=[1, 1, 1.5]); ax0 = plt.subplot(gs_stats[0]); ax0.errorbar(range(len(top_dims_df)), top_dims_df['mean'], yerr=top_dims_df['std'], fmt='o', capsize=5, color='darkblue', ecolor='lightblue'); ax0.set_ylabel('Mean Value (± std)'); ax0.set_title('Mean Values of Top Dimensions'); ax0.set_xticks(range(len(top_dims_df))); ax0.set_xticklabels(top_dims_df.index, rotation=45, ha=\"right\"); ax0.grid(True, linestyle='--', alpha=0.7); ax1 = plt.subplot(gs_stats[1]); bar_width = 0.35; positions = np.arange(len(top_dims_df)); ax1.bar(positions - bar_width/2, top_dims_df['skewness'], bar_width, label='Skewness', color='orange'); ax1.bar(positions + bar_width/2, top_dims_df['kurtosis'], bar_width, label='Kurtosis', color='green'); ax1.set_ylabel('Value'); ax1.set_title('Skewness and Kurtosis'); ax1.set_xticks(positions); ax1.set_xticklabels(top_dims_df.index, rotation=45, ha=\"right\"); ax1.legend(); ax1.grid(True, linestyle='--', alpha=0.7); ax2 = plt.subplot(gs_stats[2]); pos_act = np.arange(len(top_dims_df)); ax2.bar(pos_act, top_dims_df['importance_score'], color='purple', alpha=0.7, label='Importance Score'); ax2_twin = ax2.twinx(); ax2_twin.plot(pos_act, top_dims_df['activation_ratio'], 'ro-', linewidth=2, label='Activation Ratio'); ax2.set_xlabel('Dimension'); ax2.set_ylabel('Importance Score', color='purple'); ax2_twin.set_ylabel('Activation Ratio', color='red'); ax2.set_title('Importance and Activation'); ax2.set_xticks(pos_act); ax2.set_xticklabels(top_dims_df.index, rotation=45, ha=\"right\"); lines1, labels1 = ax2.get_legend_handles_labels(); lines2, labels2 = ax2_twin.get_legend_handles_labels(); ax2.legend(lines1 + lines2, labels1 + labels2, loc='upper right'); ax2.grid(True, linestyle='--', alpha=0.7); plt.tight_layout(h_pad=3.0); plt.savefig(f'{out_dir}/dimension_statistics.png', dpi=plot_dpi); plt.close()\n",
        "         else: print(\"No valid top dimensions found for correlation/stats plots.\")\n",
        "    else: print(\"Stats DataFrame is empty or missing 'importance_score'. Skipping some advanced similarity plots.\")\n",
        "    n_show_cosine = min(100, mu_mat.shape[0])\n",
        "    if n_show_cosine > 1:\n",
        "        sub_indices_cosine = np.random.choice(mu_mat.shape[0], n_show_cosine, replace=False); sim = cosine_similarity(mu_mat[sub_indices_cosine])\n",
        "        try: g = sns.clustermap(sim, cmap=CUSTOM_CMAP, standard_scale=None, figsize=(10,10), cbar_kws={\"label\": \"Cosine Similarity\"}, xticklabels=False, yticklabels=False); g.fig.suptitle(f'Cosine Similarity Heatmap - {n_show_cosine} Random Samples', y=1.02, fontsize=14); plt.savefig(f'{out_dir}/enhanced_cosine_heatmap.png', dpi=plot_dpi); plt.close(g.fig)\n",
        "        except Exception as e: print(f\"Clustermap failed: {e}\"); plt.close() # Ensure figure is closed\n",
        "    else: print(f\"Skipping cosine similarity heatmap as n_show_cosine ({n_show_cosine}) is too small.\")\n",
        "    return mu_mat\n",
        "\n",
        "def plot_spatial_year_analysis(mu_mat, logvar_mat, meta_df, labels, pc, year_col_name, x_coord_col_name, y_coord_col_name, out_dir, plot_dpi=150):\n",
        "    \"\"\"Analyze relationship between latent space and spatial/temporal features\"\"\"\n",
        "    kl_per_sample = -0.5 * np.sum(1 + logvar_mat - mu_mat**2 - np.exp(logvar_mat), axis=1)\n",
        "    analysis_df_data = {'cluster': labels, 'kl_divergence': kl_per_sample}\n",
        "    for col_name, data_series in [(year_col_name, meta_df.get(year_col_name)),(x_coord_col_name, meta_df.get(x_coord_col_name)),(y_coord_col_name, meta_df.get(y_coord_col_name))]:\n",
        "         if data_series is not None: analysis_df_data[col_name] = data_series.values\n",
        "    if pc is not None and pc.shape[0] == len(labels):\n",
        "         for i in range(min(pc.shape[1], 3)): analysis_df_data[f'PC{i+1}'] = pc[:, i]\n",
        "    analysis_df = pd.DataFrame(analysis_df_data); analysis_df = analysis_df.reset_index(drop=True)\n",
        "    if x_coord_col_name in analysis_df.columns and y_coord_col_name in analysis_df.columns:\n",
        "        plt.figure(figsize=(10, 8)); scatter_spatial = plt.scatter(analysis_df[x_coord_col_name], analysis_df[y_coord_col_name], c=analysis_df['cluster'], cmap='tab10', alpha=0.6, s=8); plt.colorbar(scatter_spatial, label='Cluster'); plt.title('Spatial Distribution of Clusters'); plt.xlabel(x_coord_col_name); plt.ylabel(y_coord_col_name); plt.tight_layout(); plt.savefig(f'{out_dir}/spatial_clusters.png', dpi=plot_dpi); plt.close()\n",
        "        fig_spatial_plotly = px.scatter(analysis_df, x=x_coord_col_name, y=y_coord_col_name, color='cluster', symbol=year_col_name if year_col_name in analysis_df.columns else None, size='kl_divergence' if 'kl_divergence' in analysis_df.columns else None, hover_data=[col for col in ['PC1', 'PC2', 'kl_divergence'] if col in analysis_df.columns], title='Spatial Distribution (Interactive)', color_discrete_map={-1: \"lightgrey\"}); fig_spatial_plotly.write_html(f'{out_dir}/spatial_interactive.html')\n",
        "    if year_col_name in analysis_df.columns:\n",
        "        year_cluster_counts = analysis_df.groupby([year_col_name, 'cluster']).size().unstack(fill_value=0)\n",
        "        if not year_cluster_counts.empty: year_cluster_prop = year_cluster_counts.div(year_cluster_counts.sum(axis=1), axis=0); year_cluster_prop.plot(kind='bar', stacked=True, colormap='tab10', figsize=(12,7)); plt.title('Cluster Distribution by Year'); plt.xlabel(year_col_name); plt.ylabel('Proportion'); plt.legend(title='Cluster', bbox_to_anchor=(1.05, 1), loc='upper left'); plt.tight_layout(); plt.savefig(f'{out_dir}/temporal_clusters.png', dpi=plot_dpi); plt.close()\n",
        "        sns.catplot(x=year_col_name, y='kl_divergence', data=analysis_df, kind=\"box\", palette='viridis', height=5, aspect=2); plt.title('KL Divergence by Year'); plt.xlabel(year_col_name); plt.ylabel('KL Divergence'); plt.xticks(rotation=45, ha='right'); plt.grid(True, linestyle='--', alpha=0.7); plt.tight_layout(); plt.savefig(f'{out_dir}/kl_by_year.png', dpi=plot_dpi); plt.close()\n",
        "    return analysis_df\n",
        "\n",
        "# --- OPTIMIZED Latent Traversal Plotting ---\n",
        "def plot_latent_traversal(\n",
        "        mu_mat: np.ndarray,\n",
        "        stats_df: pd.DataFrame,\n",
        "        out_dir: str,\n",
        "        plot_dpi: int = 150,\n",
        "        traversal_sample_size: int = 5000, # Subsample for plotting\n",
        "        n_dims_traversal: int = 5, # Plot top N dims\n",
        "        n_pairs_traversal: int = 3  # Plot top N pairs\n",
        "    ):\n",
        "    \"\"\"\n",
        "    Generate latent traversal visualization using histograms (Optimized for Speed).\n",
        "    Plots distributions (histograms) of top important dimensions and their joint distributions.\n",
        "    Uses subsampling for faster plotting.\n",
        "    \"\"\"\n",
        "    print(f\"--- Plotting Latent Traversal (Sample Size: {traversal_sample_size}, Dims: {n_dims_traversal}, Pairs: {n_pairs_traversal}) ---\")\n",
        "    start_time = time.time()\n",
        "    if stats_df.empty or 'importance_score' not in stats_df.columns:\n",
        "        print(\"Stats DataFrame is empty or missing 'importance_score'. Skipping latent traversal plots.\")\n",
        "        return\n",
        "    if mu_mat.shape[0] == 0:\n",
        "        print(\"Mu matrix is empty. Skipping latent traversal plots.\")\n",
        "        return\n",
        "\n",
        "    # Subsample data for plotting\n",
        "    n_total_samples = mu_mat.shape[0]\n",
        "    actual_sample_size = min(traversal_sample_size, n_total_samples)\n",
        "    if actual_sample_size < n_total_samples:\n",
        "        print(f\"Subsampling {actual_sample_size} points for traversal plots.\")\n",
        "        sample_indices = np.random.choice(n_total_samples, actual_sample_size, replace=False)\n",
        "        mu_mat_sampled = mu_mat[sample_indices, :]\n",
        "    else:\n",
        "        mu_mat_sampled = mu_mat\n",
        "\n",
        "    # Get top dimensions based on importance score\n",
        "    top_dims_series = stats_df.sort_values('importance_score', ascending=False).index[:n_dims_traversal]\n",
        "    dim_indices = [int(dim[1:]) for dim in top_dims_series if dim.startswith('z') and dim[1:].isdigit()]\n",
        "\n",
        "    if not dim_indices:\n",
        "        print(\"No valid top dimensions found for latent traversal plots.\")\n",
        "        return\n",
        "\n",
        "    # Plot 1D Histograms for top dimensions\n",
        "    n_dims_to_plot = len(dim_indices)\n",
        "    fig_1d, axes_1d = plt.subplots(n_dims_to_plot, 1, figsize=(10, n_dims_to_plot * 2.5), squeeze=False) # Ensure axes_1d is always 2D\n",
        "    axes_1d = axes_1d.flatten() # Flatten to 1D array for easy iteration\n",
        "\n",
        "    for i, original_dim_idx in enumerate(dim_indices):\n",
        "        dim_name = stats_df.index[original_dim_idx]\n",
        "        # Use histplot instead of kdeplot for speed\n",
        "        sns.histplot(mu_mat_sampled[:, original_dim_idx], ax=axes_1d[i], bins=50, kde=False, color=plt.cm.viridis(i/n_dims_to_plot))\n",
        "        axes_1d[i].set_title(f'Dimension {dim_name} (z{original_dim_idx:03d}) Distribution (Sampled)')\n",
        "        axes_1d[i].set_xlabel('Value')\n",
        "        axes_1d[i].set_ylabel('Frequency')\n",
        "        axes_1d[i].grid(True, linestyle='--', alpha=0.7)\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(f'{out_dir}/top_dimensions_distribution_hist.png', dpi=plot_dpi); plt.close(fig_1d)\n",
        "\n",
        "    # Plot 2D Histograms for top pairs\n",
        "    n_top_for_pairs = min(n_pairs_traversal, len(dim_indices))\n",
        "    if n_top_for_pairs >= 2:\n",
        "        top_pairs_indices = [(dim_indices[i], dim_indices[j]) for i in range(n_top_for_pairs)\n",
        "                             for j in range(i+1, n_top_for_pairs)]\n",
        "\n",
        "        n_pairs_to_plot = len(top_pairs_indices)\n",
        "        if n_pairs_to_plot > 0:\n",
        "            n_cols_pairs = min(2, n_pairs_to_plot) # Max 2 columns\n",
        "            n_rows_pairs = (n_pairs_to_plot + n_cols_pairs - 1) // n_cols_pairs\n",
        "\n",
        "            fig_joint, axes_joint = plt.subplots(n_rows_pairs, n_cols_pairs, figsize=(12, n_rows_pairs * 5), squeeze=False)\n",
        "            axes_joint = axes_joint.flatten()\n",
        "\n",
        "            for i, (dim1_idx, dim2_idx) in enumerate(top_pairs_indices):\n",
        "                if i < len(axes_joint):\n",
        "                    dim1_name = stats_df.index[dim1_idx]\n",
        "                    dim2_name = stats_df.index[dim2_idx]\n",
        "                    # Use histplot (2D) or hist2d for speed\n",
        "                    sns.histplot(\n",
        "                        x=mu_mat_sampled[:, dim1_idx],\n",
        "                        y=mu_mat_sampled[:, dim2_idx],\n",
        "                        ax=axes_joint[i],\n",
        "                        bins=30, # Fewer bins for 2D\n",
        "                        cmap=\"viridis\"\n",
        "                        # Consider adding cbar=True if needed\n",
        "                    )\n",
        "                    axes_joint[i].set_title(f'{dim1_name} vs {dim2_name} (Sampled)')\n",
        "                    axes_joint[i].set_xlabel(dim1_name)\n",
        "                    axes_joint[i].set_ylabel(dim2_name)\n",
        "\n",
        "            for j in range(n_pairs_to_plot, len(axes_joint)): # Hide unused subplots\n",
        "                if j < len(axes_joint): axes_joint[j].axis('off')\n",
        "\n",
        "            plt.tight_layout()\n",
        "            plt.savefig(f'{out_dir}/top_dimensions_joint_hist.png', dpi=plot_dpi); plt.close(fig_joint)\n",
        "        else: print(\"Not enough pairs of top dimensions to plot joint distributions.\")\n",
        "    else: print(\"Not enough top dimensions (need at least 2) for joint distribution plots.\")\n",
        "    print(f\"Latent traversal plotting took {time.time() - start_time:.2f}s\")\n",
        "\n",
        "\n",
        "# ─────────────────────────────────────────────────────────────────────────────\n",
        "# 4. Main EDA routine with advanced analytics (Optimized & Fixed)\n",
        "# ─────────────────────────────────────────────────────────────────────────────\n",
        "def run_enhanced_latent_eda(\n",
        "        model,\n",
        "        data_for_encoding: np.ndarray,\n",
        "        metadata_df: pd.DataFrame,\n",
        "        year_col_name: str,\n",
        "        x_coord_col_name: str,\n",
        "        y_coord_col_name: str,\n",
        "        device='cuda',\n",
        "        out_dir='enhanced_latent_eda_out',\n",
        "        batch_size=1024, # Increased default\n",
        "        k_clusters=8,\n",
        "        k_range=range(2, 11), # Reduced default range\n",
        "        random_seed=42,\n",
        "        # --- Speed/Config Params ---\n",
        "        plot_dpi=150,\n",
        "        # t-SNE params\n",
        "        run_tsne=True,\n",
        "        n_tsne_samples=5000,\n",
        "        tsne_n_iter=500,\n",
        "        tsne_perplexity=30.0,\n",
        "        tsne_on_pca_components=50,\n",
        "        use_opentsne_if_available=True,\n",
        "        # K-Means params\n",
        "        kmeans_n_init_main=3, # n_init for the main clustering\n",
        "        # Cluster evaluation params\n",
        "        run_clustering_evaluation=True, # Flag to run the potentially slow evaluation step\n",
        "        kmeans_algo_eval='MiniBatchKMeans', # Faster algo for eval loop\n",
        "        kmeans_n_init_eval=1, # Very low n_init for eval loop\n",
        "        eval_metric_sample_size=1000, # Subsample size for silhouette in eval loop\n",
        "        # Latent Traversal Params (NEW)\n",
        "        run_latent_traversal_plots=True,\n",
        "        traversal_sample_size=5000, # Subsample for traversal plots\n",
        "        traversal_n_dims=5,         # Number of top dims to plot\n",
        "        traversal_n_pairs=3,        # Number of top pairs to plot\n",
        "        # Other plot params\n",
        "        run_dendrogram=True,\n",
        "        dendrogram_n_samples=2000, # Reduced sample size for dendrogram linkage\n",
        "        run_similarity_analysis=True,\n",
        "        run_spatiotemporal_analysis=True\n",
        "    ):\n",
        "    \"\"\"\n",
        "    Perform comprehensive exploratory analysis of the VAE latent space (Optimized & Fixed).\n",
        "    \"\"\"\n",
        "    print(f\"--- Running Enhanced Latent EDA (Optimized & Fixed v2) ---\")\n",
        "    print(f\"Parameters: run_clustering_evaluation={run_clustering_evaluation}, kmeans_algo_eval='{kmeans_algo_eval}', eval_metric_sample_size={eval_metric_sample_size}, run_tsne={run_tsne}, n_tsne_samples={n_tsne_samples}, tsne_on_pca_components={tsne_on_pca_components}, run_dendrogram={run_dendrogram}, dendrogram_n_samples={dendrogram_n_samples}, run_latent_traversal_plots={run_latent_traversal_plots}, traversal_sample_size={traversal_sample_size}\")\n",
        "    func_start_time = time.time()\n",
        "    # Set seeds\n",
        "    np.random.seed(random_seed); torch.manual_seed(random_seed)\n",
        "    if torch.cuda.is_available(): torch.cuda.manual_seed_all(random_seed)\n",
        "\n",
        "    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
        "    out_dir_stamped = f\"{out_dir}_{timestamp}\"\n",
        "    os.makedirs(out_dir_stamped, exist_ok=True)\n",
        "    print(f\"Output directory: {out_dir_stamped}\")\n",
        "\n",
        "    _device = torch.device(device)\n",
        "    model = model.to(_device).eval()\n",
        "\n",
        "    print(\"Preparing dataset for encoding...\")\n",
        "    if data_for_encoding.dtype != np.float32: data_for_encoding = data_for_encoding.astype(np.float32)\n",
        "    pytorch_dataset = TensorDataset(torch.from_numpy(data_for_encoding))\n",
        "    num_loader_workers = 0\n",
        "    if _device.type != 'cuda' and hasattr(os, 'sched_getaffinity'):\n",
        "        try: # Handle potential NotImplementedError on some platforms\n",
        "            affinity_set = os.sched_getaffinity(0)\n",
        "            if affinity_set: num_loader_workers = len(affinity_set) // 2\n",
        "        except NotImplementedError:\n",
        "             num_loader_workers = os.cpu_count() // 2 if os.cpu_count() and os.cpu_count() > 2 else 0\n",
        "\n",
        "    loader = DataLoader(pytorch_dataset, batch_size=batch_size, shuffle=False, num_workers=num_loader_workers, pin_memory=(_device.type == 'cuda'))\n",
        "\n",
        "    print(f\"Encoding {data_for_encoding.shape[0]} samples...\")\n",
        "    start_time = time.time()\n",
        "    mu_list, logvar_list = [], []\n",
        "    with torch.no_grad():\n",
        "        for i, (x_batch_tuple,) in enumerate(loader):\n",
        "            x_batch = x_batch_tuple.to(_device)\n",
        "            mu_batch, logvar_batch = model.encode(x_batch) # ASSUMES (mu, logvar) output\n",
        "            mu_list.append(mu_batch.cpu()); logvar_list.append(logvar_batch.cpu())\n",
        "    mu_mat = torch.cat(mu_list).numpy(); logvar_mat = torch.cat(logvar_list).numpy()\n",
        "    print(f\"Encoding completed in {time.time() - start_time:.2f}s.\")\n",
        "\n",
        "    # Create Metadata DataFrame - ensure alignment\n",
        "    if metadata_df.shape[0] != mu_mat.shape[0]:\n",
        "         print(f\"Warning: Metadata rows ({metadata_df.shape[0]}) != Encoded rows ({mu_mat.shape[0]}). Attempting to use first {mu_mat.shape[0]} metadata rows.\")\n",
        "         meta_df_internal = metadata_df.iloc[:mu_mat.shape[0]].copy()\n",
        "    else:\n",
        "         meta_df_internal = metadata_df.copy()\n",
        "    meta_df_internal['sample_id'] = meta_df_internal.index\n",
        "\n",
        "    print(\"Creating DataFrame with latent vectors and metadata...\")\n",
        "    z_mu_cols = [f'z_mu_{d:03d}' for d in range(mu_mat.shape[1])]; z_logvar_cols = [f'z_logvar_{d:03d}' for d in range(mu_mat.shape[1])]\n",
        "    df_lat = pd.concat([meta_df_internal.reset_index(drop=True), pd.DataFrame(mu_mat, columns=z_mu_cols), pd.DataFrame(logvar_mat, columns=z_logvar_cols)], axis=1)\n",
        "\n",
        "    print(\"Analyzing latent space statistics (from mu)...\"); start_time = time.time()\n",
        "    stats_df, corr_mat = analyze_latent_statistics(mu_mat)\n",
        "    stats_df.to_csv(f'{out_dir_stamped}/latent_dimension_statistics.csv'); np.save(f'{out_dir_stamped}/dimension_correlation_matrix.npy', corr_mat)\n",
        "    print(f\"Statistics calculation took {time.time() - start_time:.2f}s\")\n",
        "\n",
        "    print(f\"Performing K-means clustering (k={k_clusters}, n_init={kmeans_n_init_main})...\"); start_time = time.time()\n",
        "    actual_k_clusters = min(k_clusters, mu_mat.shape[0]); labels = np.zeros(mu_mat.shape[0], dtype=int); kmeans_model = None\n",
        "    if actual_k_clusters > 1:\n",
        "        kmeans_model = KMeans(n_clusters=actual_k_clusters, n_init=kmeans_n_init_main, random_state=random_seed, algorithm='lloyd')\n",
        "        labels = kmeans_model.fit_predict(mu_mat)\n",
        "    else: print(f\"Skipping K-means clustering as k_clusters ({actual_k_clusters}) is <= 1.\")\n",
        "    print(f\"Main K-Means took {time.time() - start_time:.2f}s\")\n",
        "\n",
        "    cluster_metrics = pd.DataFrame() # Initialize empty\n",
        "    if run_clustering_evaluation:\n",
        "        cluster_metrics = evaluate_clustering_quality( mu_mat, labels, k_range=k_range, kmeans_algo_eval=kmeans_algo_eval, kmeans_n_init_eval=kmeans_n_init_eval, metric_subsample_size=eval_metric_sample_size, random_state=random_seed)\n",
        "        cluster_metrics.to_csv(f'{out_dir_stamped}/cluster_metrics.csv')\n",
        "    else: print(\"Skipping clustering quality evaluation.\")\n",
        "\n",
        "    print(\"Generating cluster profiles...\"); start_time = time.time()\n",
        "    cluster_profiles = generate_cluster_profiles(mu_mat, logvar_mat, labels, meta_df_internal, year_col_name, x_coord_col_name, y_coord_col_name)\n",
        "    def _np_serializer(obj):\n",
        "         if isinstance(obj, np.ndarray): return obj.tolist()\n",
        "         if isinstance(obj, (np.integer, np.floating)): return obj.item()\n",
        "         if isinstance(obj, (datetime, pd.Timestamp)): return obj.isoformat()\n",
        "         try: return str(obj)\n",
        "         except Exception: raise TypeError(f\"Type {type(obj)} not serializable for cluster profiles\")\n",
        "    try:\n",
        "        with open(f'{out_dir_stamped}/cluster_profiles.json', 'w') as f: json.dump(cluster_profiles, f, indent=2, default=_np_serializer)\n",
        "    except TypeError as e: print(f\"Error serializing cluster profiles to JSON: {e}. Profiles might contain unsupported types.\")\n",
        "    print(f\"Cluster profile generation took {time.time() - start_time:.2f}s\")\n",
        "\n",
        "    print(\"Generating KL divergence analysis...\"); start_time = time.time()\n",
        "    kl_divergence_values = -0.5 * np.sum(1 + logvar_mat - mu_mat**2 - np.exp(logvar_mat), axis=1)\n",
        "    plot_kl_analysis(kl_divergence_values, out_dir_stamped, plot_dpi=plot_dpi)\n",
        "    print(f\"KL Analysis plotting took {time.time() - start_time:.2f}s\")\n",
        "\n",
        "    print(\"Running PCA and creating visualizations...\"); start_time = time.time()\n",
        "    pc_vectors, pca_model_fitted = plot_pca_analysis(mu_mat, meta_df_internal, labels, year_col_name, out_dir_stamped, plot_dpi=plot_dpi)\n",
        "    print(f\"PCA + Plotting took {time.time() - start_time:.2f}s\")\n",
        "\n",
        "    if run_clustering_evaluation or cluster_profiles:\n",
        "        print(\"Creating cluster analysis visualizations...\"); start_time = time.time()\n",
        "        plot_clustering_analysis(mu_mat, labels, out_dir_stamped, cluster_metrics, cluster_profiles, plot_dpi=plot_dpi, dendrogram_n_samples=dendrogram_n_samples if run_dendrogram else 0)\n",
        "        print(f\"Cluster analysis plotting took {time.time() - start_time:.2f}s\")\n",
        "\n",
        "    if run_similarity_analysis:\n",
        "        print(\"Generating similarity analysis and dimension statistics...\"); start_time = time.time()\n",
        "        plot_advanced_similarity_analysis(mu_mat, labels, stats_df, corr_mat, out_dir_stamped, plot_dpi=plot_dpi)\n",
        "        print(f\"Similarity/Stats plotting took {time.time() - start_time:.2f}s\")\n",
        "\n",
        "    if run_spatiotemporal_analysis:\n",
        "        print(\"Analyzing spatial and temporal patterns...\"); start_time = time.time()\n",
        "        spatiotemporal_df = plot_spatial_year_analysis(mu_mat, logvar_mat, meta_df_internal, labels, pc_vectors, year_col_name, x_coord_col_name, y_coord_col_name, out_dir_stamped, plot_dpi=plot_dpi)\n",
        "        print(f\"Spatial/Temporal plotting took {time.time() - start_time:.2f}s\")\n",
        "\n",
        "    # --- Optimized Latent Traversal Call ---\n",
        "    if run_latent_traversal_plots:\n",
        "        plot_latent_traversal(\n",
        "            mu_mat, stats_df, out_dir_stamped,\n",
        "            plot_dpi=plot_dpi,\n",
        "            traversal_sample_size=traversal_sample_size, # Pass param\n",
        "            n_dims_traversal=traversal_n_dims,           # Pass param\n",
        "            n_pairs_traversal=traversal_n_pairs          # Pass param\n",
        "        )\n",
        "    else: print(\"Skipping latent traversal plots as per configuration.\")\n",
        "\n",
        "    # --- Optimized t-SNE Section ---\n",
        "    tsne_embedding_final = None; tsne_df_final = None\n",
        "    if run_tsne:\n",
        "        print(\"Preparing for t-SNE...\"); start_time_tsne = time.time()\n",
        "        data_for_tsne = mu_mat; indices_tsne = np.arange(mu_mat.shape[0])\n",
        "        if mu_mat.shape[0] > n_tsne_samples:\n",
        "            print(f\"Subsampling {n_tsne_samples} from {mu_mat.shape[0]} for t-SNE.\")\n",
        "            indices_tsne = np.random.choice(mu_mat.shape[0], n_tsne_samples, replace=False)\n",
        "            data_for_tsne = mu_mat[indices_tsne, :]\n",
        "\n",
        "        tsne_input_dim = data_for_tsne.shape[1]; tsne_input_source = \"mu_mat\"\n",
        "        if tsne_on_pca_components is not None and pc_vectors is not None and pc_vectors.shape[0] == mu_mat.shape[0]:\n",
        "             effective_pca_comps = min(tsne_on_pca_components, pc_vectors.shape[1])\n",
        "             if effective_pca_comps > 1:\n",
        "                  print(f\"Using top {effective_pca_comps} PCs for t-SNE.\")\n",
        "                  data_for_tsne = pc_vectors[indices_tsne, :effective_pca_comps]\n",
        "                  tsne_input_dim = effective_pca_comps; tsne_input_source = f\"{effective_pca_comps} PCs\"\n",
        "             else: print(f\"Not enough PCA components ({pc_vectors.shape[1]}), using original data for t-SNE.\")\n",
        "        elif tsne_on_pca_components is not None: print(\"PCA vectors not available, using original data for t-SNE.\")\n",
        "\n",
        "        current_n_samples_for_tsne = data_for_tsne.shape[0]\n",
        "        actual_perplexity = min(tsne_perplexity, current_n_samples_for_tsne - 1)\n",
        "        if actual_perplexity < 5 and current_n_samples_for_tsne >= 5: actual_perplexity = 5\n",
        "\n",
        "        if current_n_samples_for_tsne > 1 and actual_perplexity >= 1:\n",
        "            tsne_kwargs = {\"n_components\": 2, \"perplexity\": actual_perplexity, \"random_state\": random_seed, \"n_jobs\": -1}\n",
        "            tsne_impl_name = \"scikit-learn TSNE\"\n",
        "            if use_opentsne_if_available and OPEN_TSNE_AVAILABLE:\n",
        "                tsne_kwargs[\"early_exaggeration_iter\"] = max(50, tsne_n_iter // 4); tsne_kwargs[\"n_iter\"] = max(100, tsne_n_iter * 3 // 4)\n",
        "                if \"n_jobs\" in tsne_kwargs: del tsne_kwargs[\"n_jobs\"]\n",
        "                tsne_model = OpenTSNE_TSNE(**tsne_kwargs); tsne_impl_name = \"OpenTSNE\"\n",
        "                print(f\"Running {tsne_impl_name} (Perp={actual_perplexity:.1f}, NIter~{tsne_n_iter}) on {current_n_samples_for_tsne} samples ({tsne_input_source})...\")\n",
        "                tsne_embedding_final = tsne_model.fit(data_for_tsne.astype(np.float64)) # OpenTSNE uses .fit()\n",
        "            else:\n",
        "                tsne_kwargs[\"init\"] = 'pca'; tsne_kwargs[\"learning_rate\"] = 'auto'; tsne_kwargs[\"n_iter\"] = tsne_n_iter\n",
        "                tsne_model = SKL_TSNE(**tsne_kwargs)\n",
        "                print(f\"Running {tsne_impl_name} (Perp={actual_perplexity:.1f}, NIter={tsne_n_iter}) on {current_n_samples_for_tsne} samples ({tsne_input_source})...\")\n",
        "                tsne_embedding_final = tsne_model.fit_transform(data_for_tsne.astype(np.float64)) # Sklearn uses .fit_transform()\n",
        "\n",
        "            print(f\"t-SNE calculation took {time.time() - start_time_tsne:.2f}s.\")\n",
        "\n",
        "            # Create t-SNE DataFrame and plots\n",
        "            tsne_meta_input = meta_df_internal.iloc[indices_tsne].reset_index(drop=True)\n",
        "            tsne_labels_input = labels[indices_tsne] if labels is not None else np.zeros(len(indices_tsne))\n",
        "            tsne_df_data = {'TSNE1': tsne_embedding_final[:, 0], 'TSNE2': tsne_embedding_final[:, 1], 'cluster': tsne_labels_input}\n",
        "            if year_col_name and year_col_name in tsne_meta_input.columns: tsne_df_data[year_col_name] = tsne_meta_input[year_col_name]\n",
        "            if x_coord_col_name and x_coord_col_name in tsne_meta_input.columns: tsne_df_data['grid_x'] = tsne_meta_input[x_coord_col_name]\n",
        "            if y_coord_col_name and y_coord_col_name in tsne_meta_input.columns: tsne_df_data['grid_y'] = tsne_meta_input[y_coord_col_name]\n",
        "            tsne_df_final = pd.DataFrame(tsne_df_data)\n",
        "            plt.figure(figsize=(10, 8)); sns.scatterplot(x='TSNE1', y='TSNE2', hue='cluster', data=tsne_df_final, palette='tab10', alpha=0.7, s=8, legend='auto'); plt.title(f'{tsne_impl_name} (Perp {actual_perplexity:.1f})'); plt.xlabel('t-SNE Dim 1'); plt.ylabel('t-SNE Dim 2'); plt.tight_layout(); plt.savefig(f'{out_dir_stamped}/tsne.png', dpi=plot_dpi); plt.close()\n",
        "            fig_tsne_plotly = px.scatter(tsne_df_final, x='TSNE1', y='TSNE2', color='cluster', symbol=year_col_name if year_col_name and year_col_name in tsne_df_final.columns else None, hover_data=[col for col in ['grid_x', 'grid_y', year_col_name] if col in tsne_df_final.columns], title='t-SNE (Interactive)', color_discrete_map={-1: \"lightgrey\"}); fig_tsne_plotly.write_html(f'{out_dir_stamped}/tsne_interactive.html')\n",
        "        else: print(f\"Skipping t-SNE plot due to insufficient samples/perplexity.\")\n",
        "    else: print(\"Skipping t-SNE as per configuration.\")\n",
        "\n",
        "    print(\"Saving final artifacts...\"); start_time = time.time()\n",
        "    df_lat['cluster'] = labels; df_lat['kl_divergence'] = kl_divergence_values\n",
        "    df_lat.to_parquet(f'{out_dir_stamped}/latent_vectors_with_metadata.parquet')\n",
        "    projection_data_dict = {\n",
        "        'pc_vectors': pc_vectors, 'tsne_embedding_sampled': tsne_embedding_final,\n",
        "        'pca_explained_variance': pca_model_fitted.explained_variance_ratio_ if pca_model_fitted else None,\n",
        "        'cluster_centers_kmeans': kmeans_model.cluster_centers_ if kmeans_model and hasattr(kmeans_model, 'cluster_centers_') else None\n",
        "    }\n",
        "    with open(f'{out_dir_stamped}/projection_data.pkl', 'wb') as f: pickle.dump(projection_data_dict, f)\n",
        "\n",
        "    # Generate summary report (fast)\n",
        "    generate_analysis_summary(model, mu_mat, logvar_mat, stats_df, cluster_profiles, cluster_metrics, out_dir_stamped)\n",
        "    print(f\"Artifact saving took {time.time() - start_time:.2f}s\")\n",
        "\n",
        "    print(f'--- Enhanced EDA Completed in {time.time() - func_start_time:.2f} seconds. Results saved to \"{out_dir_stamped}/\" ---')\n",
        "    return df_lat, stats_df, cluster_profiles\n",
        "\n",
        "\n",
        "# --- Summary Report Generation (Unchanged from previous fix) ---\n",
        "def generate_analysis_summary(model, mu_mat, logvar_mat, stats_df, cluster_profiles, cluster_metrics, out_dir):\n",
        "     \"\"\"Generate a comprehensive analysis summary markdown report\"\"\"\n",
        "     latent_dim = mu_mat.shape[1]; n_samples = mu_mat.shape[0]; kl_per_sample = -0.5 * np.sum(1 + logvar_mat - mu_mat**2 - np.exp(logvar_mat), axis=1); avg_kl_total = np.mean(kl_per_sample); active_dims = 0\n",
        "     if 'std' in stats_df.columns: active_dims = np.sum(stats_df['std'] > 0.1)\n",
        "     best_k_silhouette = 'N/A'\n",
        "     if not cluster_metrics.empty and 'silhouette' in cluster_metrics.columns and cluster_metrics['silhouette'].notna().any():\n",
        "          best_k_row_idx = cluster_metrics['silhouette'].dropna().idxmax()\n",
        "          if pd.notna(best_k_row_idx): best_k_row = cluster_metrics.loc[best_k_row_idx]; best_k_silhouette = int(best_k_row['k']) if pd.notna(best_k_row['k']) else 'N/A'\n",
        "     report = f\"\"\"# Latent Space Analysis Report\\n\\n## Overview\\n- **Date:** {datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")}\\n- **Latent Dimension:** {latent_dim}\\n- **Number of Samples:** {n_samples:,}\\n- **VAE Model Type:** {type(model).__name__}\\n- **Average KL Divergence (Total):** {avg_kl_total:.4f}\\n- **Active Dimensions (std > 0.1):** {active_dims}\\n- **Optimal Cluster Count (Silhouette):** {best_k_silhouette}\\n\\n## Latent Space Statistics (based on mu)\\n\"\"\"\n",
        "     if not stats_df.empty and 'importance_score' in stats_df.columns:\n",
        "          top_dims_summary = stats_df.sort_values('importance_score', ascending=False).head(10)\n",
        "          report += \"\"\"\\n### Top 10 Important Dimensions (mu-space):\\n| Dimension | Importance | Mean   | Std Dev | Skewness | Kurtosis | Activation |\\n|-----------|------------|--------|---------|----------|----------|------------|\\n\"\"\"\n",
        "          for dim_name, row_data in top_dims_summary.iterrows(): report += f\"| {dim_name} | {row_data['importance_score']:.2f} | {row_data['mean']:.2f} | {row_data['std']:.2f} | {row_data['skewness']:.2f} | {row_data['kurtosis']:.2f} | {row_data['activation_ratio']:.2f} |\\n\"\n",
        "     else: report += \"\\nLatent dimension statistics (importance score) not available.\\n\"\n",
        "     if cluster_profiles:\n",
        "          report += \"\"\"\\n### Cluster Analysis Summary:\\n| Cluster ID | Size   | Proportion | Avg KL | Dispersion (mu) |\\n|------------|--------|------------|--------|-----------------|\\n\"\"\"\n",
        "          for profile in cluster_profiles: report += f\"| {profile['cluster_id']} | {profile['size']:,} | {profile['proportion']:.3f} | {profile.get('avg_kl', np.nan):.2f} | {profile.get('dispersion_mu', np.nan):.2f} |\\n\"\n",
        "     else: report += \"\\nCluster profiles not available.\\n\"\n",
        "     report += \"\"\"\\n## Interpretation and Insights (General)\\nThe KL divergence, PCA, t-SNE, and clustering results provide a multi-faceted view of the learned latent space.\\n- **KL Divergence**: Indicates how much each encoded point deviates from the prior. Higher KL samples might be outliers or represent more complex data.\\n- **PCA**: Shows principal directions of variance in the latent space.\\n- **t-SNE/UMAP**: Offers non-linear dimensionality reduction for visualizing clusters and local structure.\\n- **Clustering**: Identifies distinct groups of samples in the latent space, which may correspond to different data archetypes.\\n- **Spatial/Temporal Analysis**: Relates latent representations back to their original spatial or temporal context.\\n\\nFurther investigation should correlate these latent features and clusters with known characteristics of the input data.\\n\"\"\"\n",
        "     with open(f'{out_dir}/analysis_report.md', 'w') as f: f.write(report)\n",
        "     viz_index = \"\"\"# Visualization Index\n",
        "## Dimension Analysis\n",
        "- [Dimension Statistics](dimension_statistics.png)\n",
        "- [Top Dimensions Distribution (Hist)](top_dimensions_distribution_hist.png)\n",
        "- [Top Dimensions Joint Distribution (Hist)](top_dimensions_joint_hist.png)\n",
        "- [Top Dimensions Correlation](top_dims_correlation.png)\n",
        "## KL Divergence Analysis\n",
        "- [KL Analysis](kl_analysis.png)\n",
        "- [KL by Year](kl_by_year.png)\n",
        "## Clustering Analysis\n",
        "- [Cluster Evaluation](cluster_evaluation.png)\n",
        "- [Cluster Profiles](cluster_profiles.png)\n",
        "- [Dendrogram](dendrogram.png)\n",
        "## Projections\n",
        "- [PCA Variance](pca_variance.png)\n",
        "- [PCA Loadings](pca_loadings.png)\n",
        "- [t-SNE](tsne.png)\n",
        "## Spatial-Temporal Analysis\n",
        "- [Spatial Clusters](spatial_clusters.png)\n",
        "- [Temporal Clusters](temporal_clusters.png)\n",
        "## Interactive Visualizations\n",
        "- [PCA Interactive](pca_interactive.html)\n",
        "- [PCA 3D](pca_3d.html)\n",
        "- [t-SNE Interactive](tsne_interactive.html)\n",
        "- [Spatial Interactive](spatial_interactive.html)\n",
        "\"\"\" # Add UMAP links if enabled\n",
        "     with open(f'{out_dir}/visualization_index.md', 'w') as f: f.write(viz_index)\n",
        "\n",
        "\n",
        "# ════════════════════════════════════════════════════════════════════════════\n",
        "# EXAMPLE USAGE WRAPPER (Updated with speed parameters)\n",
        "# ════════════════════════════════════════════════════════════════════════════\n",
        "def analyze_vae_latent_space_enhanced(\n",
        "        model,\n",
        "        data_for_encoding: np.ndarray,\n",
        "        metadata_df: pd.DataFrame,\n",
        "        year_col_name: str,\n",
        "        x_coord_col_name: str,\n",
        "        y_coord_col_name: str,\n",
        "        output_folder=\"enhanced_latent_eda_results\",\n",
        "        device=\"cuda\",\n",
        "        batch_size=1024, # Match run_enhanced_latent_eda default\n",
        "        k_clusters_config=8,\n",
        "        k_range_config=range(2, 11), # Smaller default range\n",
        "        seed=42,\n",
        "        # --- Pass Speed/Config Params ---\n",
        "        eda_plot_dpi=150,\n",
        "        eda_run_tsne=True,\n",
        "        eda_n_tsne_samples=5000,\n",
        "        eda_tsne_n_iter=500,\n",
        "        eda_tsne_perplexity=30.0,\n",
        "        eda_tsne_on_pca_components=50,\n",
        "        eda_use_opentsne=True,\n",
        "        eda_kmeans_n_init_main=3,\n",
        "        eda_run_clustering_evaluation=True,\n",
        "        eda_kmeans_algo_eval='MiniBatchKMeans',\n",
        "        eda_kmeans_n_init_eval=1,\n",
        "        eda_eval_metric_sample_size=1000,\n",
        "        eda_run_dendrogram=True,\n",
        "        eda_dendrogram_n_samples=2000,\n",
        "        eda_run_similarity_analysis=True,\n",
        "        eda_run_spatiotemporal_analysis=True,\n",
        "        # Latent Traversal Params (NEW)\n",
        "        eda_run_latent_traversal_plots=True,\n",
        "        eda_traversal_sample_size=5000,\n",
        "        eda_traversal_n_dims=5,\n",
        "        eda_traversal_n_pairs=3\n",
        "    ):\n",
        "    \"\"\"Convenience function to run the complete enhanced latent space analysis (Optimized & Fixed).\"\"\"\n",
        "    print(f\"Starting Enhanced VAE Latent Space Analysis (Optimized). Output base: {output_folder}\")\n",
        "\n",
        "    # Make sure the main function (run_enhanced_latent_eda) is defined/imported\n",
        "    df_latent_vectors, df_stats, list_cluster_profiles = run_enhanced_latent_eda(\n",
        "        model=model, data_for_encoding=data_for_encoding, metadata_df=metadata_df,\n",
        "        year_col_name=year_col_name, x_coord_col_name=x_coord_col_name, y_coord_col_name=y_coord_col_name,\n",
        "        device=device, out_dir=output_folder, batch_size=batch_size,\n",
        "        k_clusters=k_clusters_config, k_range=k_range_config, random_seed=seed,\n",
        "        # Pass speed/config params\n",
        "        plot_dpi=eda_plot_dpi, run_tsne=eda_run_tsne, n_tsne_samples=eda_n_tsne_samples,\n",
        "        tsne_n_iter=eda_tsne_n_iter, tsne_perplexity=eda_tsne_perplexity,\n",
        "        tsne_on_pca_components=eda_tsne_on_pca_components, use_opentsne_if_available=eda_use_opentsne,\n",
        "        kmeans_n_init_main=eda_kmeans_n_init_main, run_clustering_evaluation=eda_run_clustering_evaluation,\n",
        "        kmeans_algo_eval=eda_kmeans_algo_eval, kmeans_n_init_eval=eda_kmeans_n_init_eval,\n",
        "        eval_metric_sample_size=eda_eval_metric_sample_size, run_dendrogram=eda_run_dendrogram,\n",
        "        dendrogram_n_samples=eda_dendrogram_n_samples, run_similarity_analysis=eda_run_similarity_analysis,\n",
        "        run_spatiotemporal_analysis=eda_run_spatiotemporal_analysis,\n",
        "        # Pass new traversal params\n",
        "        run_latent_traversal_plots=eda_run_latent_traversal_plots,\n",
        "        traversal_sample_size=eda_traversal_sample_size,\n",
        "        traversal_n_dims=eda_traversal_n_dims,\n",
        "        traversal_n_pairs=eda_traversal_n_pairs\n",
        "    )\n",
        "\n",
        "    print(f\"Enhanced analysis complete! Results saved to a timestamped subfolder within '{output_folder}'.\")\n",
        "    if df_latent_vectors is not None and not df_latent_vectors.empty:\n",
        "        print(\"\\nKey insights from enhanced EDA:\")\n",
        "        print(f\"- Number of samples analyzed: {len(df_latent_vectors):,}\")\n",
        "        if df_stats is not None and not df_stats.empty:\n",
        "            print(f\"- Latent dimensions: {df_stats.shape[0]}\")\n",
        "            if 'importance_score' in df_stats.columns:\n",
        "                 print(f\"- Top 3 most important dimensions (mu-space): {', '.join(df_stats.sort_values('importance_score', ascending=False).index[:3])}\")\n",
        "        if list_cluster_profiles:\n",
        "            print(f\"- Clusters found: {len(list_cluster_profiles)}\")\n",
        "            if list_cluster_profiles:\n",
        "                top_cluster_profile = sorted(list_cluster_profiles, key=lambda x: x.get('size', 0), reverse=True)[0]\n",
        "                if top_cluster_profile:\n",
        "                    print(f\"- Largest cluster (ID {top_cluster_profile.get('cluster_id')}) contains {top_cluster_profile.get('size', 0):,} samples ({top_cluster_profile.get('proportion', 0):.1%} of data)\")\n",
        "            else: print(\"- No valid cluster profiles generated.\")\n",
        "\n",
        "    return df_latent_vectors\n",
        "\n",
        "\n",
        "# Example of how to call this from the main pipeline (replace placeholders)\n",
        "# if __name__ == \"__main__\":\n",
        "#      # Assuming main_pipeline_vae_model, main_pipeline_vae_input_data, etc. are defined\n",
        "#      # And assuming the new speed/config parameters are defined in the main pipeline script, e.g.:\n",
        "#      # EDA_ENHANCED_CFG = { 'eda_plot_dpi': 150, 'eda_run_tsne': True, ... etc }\n",
        "#      if 'main_pipeline_vae_model' in locals():\n",
        "#          df_result_enhanced_opt = analyze_vae_latent_space_enhanced(\n",
        "#               model=main_pipeline_vae_model, data_for_encoding=main_pipeline_vae_input_data,\n",
        "#               metadata_df=main_pipeline_df_for_metadata, year_col_name=main_pipeline_year_col,\n",
        "#               x_coord_col_name=main_pipeline_x_col, y_coord_col_name=main_pipeline_y_col,\n",
        "#               device=main_pipeline_device, batch_size=main_pipeline_vae_batch_size,\n",
        "#               # Pass configured parameters\n",
        "#               **EDA_ENHANCED_CFG # Pass the dictionary of speed parameters\n",
        "#          )\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IXOVq3Kc8aSk"
      },
      "outputs": [],
      "source": [
        "PARQUET_PATH = '/content/drive/MyDrive/e6691_2025Spring_nyre_local/data/processed_building_ml.parquet'\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "A1sGh4dJykc8"
      },
      "outputs": [],
      "source": [
        "# --- Data Loading Function ---\n",
        "def load_nyc_data(file_path='building_ml_merged.csv', logger_instance=None):\n",
        "    \"\"\"Loads NYC data from CSV or Parquet.\"\"\"\n",
        "    log = logger_instance or get_logger(\"LoadNYCData\", verbose=True)\n",
        "    try:\n",
        "        log.info(f\"Attempting to load data from: {file_path}\")\n",
        "        if not os.path.exists(file_path):\n",
        "            log.error(f\"File not found: {file_path}\")\n",
        "            return None\n",
        "        # Determine file type and load\n",
        "        if file_path.lower().endswith('.csv'):\n",
        "            df = pd.read_csv(file_path, low_memory=False)\n",
        "        elif file_path.lower().endswith(('.parquet', '.parq')):\n",
        "            df = pd.read_parquet(file_path)\n",
        "        else:\n",
        "            log.error(f\"Unsupported file format: {file_path}.\")\n",
        "            return None\n",
        "        log.info(f\"Loaded data with shape: {df.shape}\")\n",
        "        return df\n",
        "    except Exception as e:\n",
        "        log.error(f\"Error loading data from {file_path}: {e}\", exc_info=True)\n",
        "        return None"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "4G7kPlhBKKbI",
        "outputId": "74279861-660e-4d4f-aa56-a13053d0fc9a"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[1;30;43mStreaming output truncated to the last 5000 lines.\u001b[0m\n",
            "2025-10-23 22:25:42,992 - VAETrainer - INFO - Saving model and trainer state to vae_results/best_model.pth...\n",
            "INFO:VAETrainer:Saving model and trainer state to vae_results/best_model.pth...\n",
            "2025-10-23 22:25:43,006 - VAETrainer - INFO - Successfully saved model and trainer state to vae_results/best_model.pth\n",
            "INFO:VAETrainer:Successfully saved model and trainer state to vae_results/best_model.pth\n",
            "2025-10-23 22:25:45,110 - VAETrainer - INFO - Epoch [12/100] Train TotalLoss: 67.6956, (ReconX: 0.9353, KLD: 0.3569*0.40, LossY: 6.6618*10.00), Time: 1.7s | Val TotalLoss: 60.1959, (ReconX: 0.9197, KLD: 0.3542, LossY: 5.8922)\n",
            "INFO:VAETrainer:Epoch [12/100] Train TotalLoss: 67.6956, (ReconX: 0.9353, KLD: 0.3569*0.40, LossY: 6.6618*10.00), Time: 1.7s | Val TotalLoss: 60.1959, (ReconX: 0.9197, KLD: 0.3542, LossY: 5.8922)\n",
            "2025-10-23 22:25:45,112 - VAETrainer - INFO - Saving model and trainer state to vae_results/best_model.pth...\n",
            "INFO:VAETrainer:Saving model and trainer state to vae_results/best_model.pth...\n",
            "2025-10-23 22:25:45,124 - VAETrainer - INFO - Successfully saved model and trainer state to vae_results/best_model.pth\n",
            "INFO:VAETrainer:Successfully saved model and trainer state to vae_results/best_model.pth\n",
            "2025-10-23 22:25:47,268 - VAETrainer - INFO - Epoch [13/100] Train TotalLoss: 61.4405, (ReconX: 0.9361, KLD: 0.3514*0.43, LossY: 6.0352*10.00), Time: 1.8s | Val TotalLoss: 57.2891, (ReconX: 0.9200, KLD: 0.3496, LossY: 5.6020)\n",
            "INFO:VAETrainer:Epoch [13/100] Train TotalLoss: 61.4405, (ReconX: 0.9361, KLD: 0.3514*0.43, LossY: 6.0352*10.00), Time: 1.8s | Val TotalLoss: 57.2891, (ReconX: 0.9200, KLD: 0.3496, LossY: 5.6020)\n",
            "2025-10-23 22:25:47,270 - VAETrainer - INFO - Saving model and trainer state to vae_results/best_model.pth...\n",
            "INFO:VAETrainer:Saving model and trainer state to vae_results/best_model.pth...\n",
            "2025-10-23 22:25:47,282 - VAETrainer - INFO - Successfully saved model and trainer state to vae_results/best_model.pth\n",
            "INFO:VAETrainer:Successfully saved model and trainer state to vae_results/best_model.pth\n",
            "2025-10-23 22:25:49,304 - VAETrainer - INFO - Epoch [14/100] Train TotalLoss: 56.6749, (ReconX: 0.9367, KLD: 0.3478*0.47, LossY: 5.5576*10.00), Time: 1.7s | Val TotalLoss: 54.1546, (ReconX: 0.9209, KLD: 0.3442, LossY: 5.2890)\n",
            "INFO:VAETrainer:Epoch [14/100] Train TotalLoss: 56.6749, (ReconX: 0.9367, KLD: 0.3478*0.47, LossY: 5.5576*10.00), Time: 1.7s | Val TotalLoss: 54.1546, (ReconX: 0.9209, KLD: 0.3442, LossY: 5.2890)\n",
            "2025-10-23 22:25:49,306 - VAETrainer - INFO - Saving model and trainer state to vae_results/best_model.pth...\n",
            "INFO:VAETrainer:Saving model and trainer state to vae_results/best_model.pth...\n",
            "2025-10-23 22:25:49,319 - VAETrainer - INFO - Successfully saved model and trainer state to vae_results/best_model.pth\n",
            "INFO:VAETrainer:Successfully saved model and trainer state to vae_results/best_model.pth\n",
            "2025-10-23 22:25:51,510 - VAETrainer - INFO - Epoch [15/100] Train TotalLoss: 52.7057, (ReconX: 0.9365, KLD: 0.3423*0.50, LossY: 5.1598*10.00), Time: 1.8s | Val TotalLoss: 50.6252, (ReconX: 0.9228, KLD: 0.3343, LossY: 4.9368)\n",
            "INFO:VAETrainer:Epoch [15/100] Train TotalLoss: 52.7057, (ReconX: 0.9365, KLD: 0.3423*0.50, LossY: 5.1598*10.00), Time: 1.8s | Val TotalLoss: 50.6252, (ReconX: 0.9228, KLD: 0.3343, LossY: 4.9368)\n",
            "2025-10-23 22:25:51,512 - VAETrainer - INFO - Saving model and trainer state to vae_results/best_model.pth...\n",
            "INFO:VAETrainer:Saving model and trainer state to vae_results/best_model.pth...\n",
            "2025-10-23 22:25:51,526 - VAETrainer - INFO - Successfully saved model and trainer state to vae_results/best_model.pth\n",
            "INFO:VAETrainer:Successfully saved model and trainer state to vae_results/best_model.pth\n",
            "2025-10-23 22:25:53,703 - VAETrainer - INFO - Epoch [16/100] Train TotalLoss: 49.3368, (ReconX: 0.9386, KLD: 0.3350*0.53, LossY: 4.8220*10.00), Time: 1.8s | Val TotalLoss: 47.2962, (ReconX: 0.9198, KLD: 0.3358, LossY: 4.6041)\n",
            "INFO:VAETrainer:Epoch [16/100] Train TotalLoss: 49.3368, (ReconX: 0.9386, KLD: 0.3350*0.53, LossY: 4.8220*10.00), Time: 1.8s | Val TotalLoss: 47.2962, (ReconX: 0.9198, KLD: 0.3358, LossY: 4.6041)\n",
            "2025-10-23 22:25:53,704 - VAETrainer - INFO - Saving model and trainer state to vae_results/best_model.pth...\n",
            "INFO:VAETrainer:Saving model and trainer state to vae_results/best_model.pth...\n",
            "2025-10-23 22:25:53,718 - VAETrainer - INFO - Successfully saved model and trainer state to vae_results/best_model.pth\n",
            "INFO:VAETrainer:Successfully saved model and trainer state to vae_results/best_model.pth\n",
            "2025-10-23 22:25:55,894 - VAETrainer - INFO - Epoch [17/100] Train TotalLoss: 46.8788, (ReconX: 0.9381, KLD: 0.3286*0.57, LossY: 4.5754*10.00), Time: 1.8s | Val TotalLoss: 45.3357, (ReconX: 0.9256, KLD: 0.3258, LossY: 4.4084)\n",
            "INFO:VAETrainer:Epoch [17/100] Train TotalLoss: 46.8788, (ReconX: 0.9381, KLD: 0.3286*0.57, LossY: 4.5754*10.00), Time: 1.8s | Val TotalLoss: 45.3357, (ReconX: 0.9256, KLD: 0.3258, LossY: 4.4084)\n",
            "2025-10-23 22:25:55,895 - VAETrainer - INFO - Saving model and trainer state to vae_results/best_model.pth...\n",
            "INFO:VAETrainer:Saving model and trainer state to vae_results/best_model.pth...\n",
            "2025-10-23 22:25:55,908 - VAETrainer - INFO - Successfully saved model and trainer state to vae_results/best_model.pth\n",
            "INFO:VAETrainer:Successfully saved model and trainer state to vae_results/best_model.pth\n",
            "2025-10-23 22:25:57,997 - VAETrainer - INFO - Epoch [18/100] Train TotalLoss: 44.8867, (ReconX: 0.9394, KLD: 0.3227*0.60, LossY: 4.3754*10.00), Time: 1.7s | Val TotalLoss: 42.6895, (ReconX: 0.9254, KLD: 0.3189, LossY: 4.1445)\n",
            "INFO:VAETrainer:Epoch [18/100] Train TotalLoss: 44.8867, (ReconX: 0.9394, KLD: 0.3227*0.60, LossY: 4.3754*10.00), Time: 1.7s | Val TotalLoss: 42.6895, (ReconX: 0.9254, KLD: 0.3189, LossY: 4.1445)\n",
            "2025-10-23 22:25:57,998 - VAETrainer - INFO - Saving model and trainer state to vae_results/best_model.pth...\n",
            "INFO:VAETrainer:Saving model and trainer state to vae_results/best_model.pth...\n",
            "2025-10-23 22:25:58,011 - VAETrainer - INFO - Successfully saved model and trainer state to vae_results/best_model.pth\n",
            "INFO:VAETrainer:Successfully saved model and trainer state to vae_results/best_model.pth\n",
            "2025-10-23 22:26:00,126 - VAETrainer - INFO - Epoch [19/100] Train TotalLoss: 43.2181, (ReconX: 0.9412, KLD: 0.3155*0.63, LossY: 4.2077*10.00), Time: 1.7s | Val TotalLoss: 41.7128, (ReconX: 0.9260, KLD: 0.3095, LossY: 4.0477)\n",
            "INFO:VAETrainer:Epoch [19/100] Train TotalLoss: 43.2181, (ReconX: 0.9412, KLD: 0.3155*0.63, LossY: 4.2077*10.00), Time: 1.7s | Val TotalLoss: 41.7128, (ReconX: 0.9260, KLD: 0.3095, LossY: 4.0477)\n",
            "2025-10-23 22:26:00,128 - VAETrainer - INFO - Saving model and trainer state to vae_results/best_model.pth...\n",
            "INFO:VAETrainer:Saving model and trainer state to vae_results/best_model.pth...\n",
            "2025-10-23 22:26:00,141 - VAETrainer - INFO - Successfully saved model and trainer state to vae_results/best_model.pth\n",
            "INFO:VAETrainer:Successfully saved model and trainer state to vae_results/best_model.pth\n",
            "2025-10-23 22:26:02,261 - VAETrainer - INFO - Epoch [20/100] Train TotalLoss: 41.9772, (ReconX: 0.9423, KLD: 0.3092*0.67, LossY: 4.0829*10.00), Time: 1.7s | Val TotalLoss: 40.1568, (ReconX: 0.9268, KLD: 0.3034, LossY: 3.8927)\n",
            "INFO:VAETrainer:Epoch [20/100] Train TotalLoss: 41.9772, (ReconX: 0.9423, KLD: 0.3092*0.67, LossY: 4.0829*10.00), Time: 1.7s | Val TotalLoss: 40.1568, (ReconX: 0.9268, KLD: 0.3034, LossY: 3.8927)\n",
            "2025-10-23 22:26:02,264 - VAETrainer - INFO - Saving model and trainer state to vae_results/best_model.pth...\n",
            "INFO:VAETrainer:Saving model and trainer state to vae_results/best_model.pth...\n",
            "2025-10-23 22:26:02,277 - VAETrainer - INFO - Successfully saved model and trainer state to vae_results/best_model.pth\n",
            "INFO:VAETrainer:Successfully saved model and trainer state to vae_results/best_model.pth\n",
            "2025-10-23 22:26:04,428 - VAETrainer - INFO - Epoch [21/100] Train TotalLoss: 41.1283, (ReconX: 0.9443, KLD: 0.3023*0.70, LossY: 3.9972*10.00), Time: 1.8s | Val TotalLoss: 39.6949, (ReconX: 0.9261, KLD: 0.2994, LossY: 3.8469)\n",
            "INFO:VAETrainer:Epoch [21/100] Train TotalLoss: 41.1283, (ReconX: 0.9443, KLD: 0.3023*0.70, LossY: 3.9972*10.00), Time: 1.8s | Val TotalLoss: 39.6949, (ReconX: 0.9261, KLD: 0.2994, LossY: 3.8469)\n",
            "2025-10-23 22:26:04,430 - VAETrainer - INFO - Saving model and trainer state to vae_results/best_model.pth...\n",
            "INFO:VAETrainer:Saving model and trainer state to vae_results/best_model.pth...\n",
            "2025-10-23 22:26:04,443 - VAETrainer - INFO - Successfully saved model and trainer state to vae_results/best_model.pth\n",
            "INFO:VAETrainer:Successfully saved model and trainer state to vae_results/best_model.pth\n",
            "2025-10-23 22:26:06,573 - VAETrainer - INFO - Epoch [22/100] Train TotalLoss: 40.2586, (ReconX: 0.9457, KLD: 0.2944*0.73, LossY: 3.9097*10.00), Time: 1.7s | Val TotalLoss: 39.2845, (ReconX: 0.9310, KLD: 0.2856, LossY: 3.8068)\n",
            "INFO:VAETrainer:Epoch [22/100] Train TotalLoss: 40.2586, (ReconX: 0.9457, KLD: 0.2944*0.73, LossY: 3.9097*10.00), Time: 1.7s | Val TotalLoss: 39.2845, (ReconX: 0.9310, KLD: 0.2856, LossY: 3.8068)\n",
            "2025-10-23 22:26:06,575 - VAETrainer - INFO - Saving model and trainer state to vae_results/best_model.pth...\n",
            "INFO:VAETrainer:Saving model and trainer state to vae_results/best_model.pth...\n",
            "2025-10-23 22:26:06,588 - VAETrainer - INFO - Successfully saved model and trainer state to vae_results/best_model.pth\n",
            "INFO:VAETrainer:Successfully saved model and trainer state to vae_results/best_model.pth\n",
            "2025-10-23 22:26:08,647 - VAETrainer - INFO - Epoch [23/100] Train TotalLoss: 39.5575, (ReconX: 0.9464, KLD: 0.2842*0.77, LossY: 3.8393*10.00), Time: 1.7s | Val TotalLoss: 38.7819, (ReconX: 0.9309, KLD: 0.2767, LossY: 3.7574)\n",
            "INFO:VAETrainer:Epoch [23/100] Train TotalLoss: 39.5575, (ReconX: 0.9464, KLD: 0.2842*0.77, LossY: 3.8393*10.00), Time: 1.7s | Val TotalLoss: 38.7819, (ReconX: 0.9309, KLD: 0.2767, LossY: 3.7574)\n",
            "2025-10-23 22:26:08,649 - VAETrainer - INFO - Saving model and trainer state to vae_results/best_model.pth...\n",
            "INFO:VAETrainer:Saving model and trainer state to vae_results/best_model.pth...\n",
            "2025-10-23 22:26:08,662 - VAETrainer - INFO - Successfully saved model and trainer state to vae_results/best_model.pth\n",
            "INFO:VAETrainer:Successfully saved model and trainer state to vae_results/best_model.pth\n",
            "2025-10-23 22:26:10,872 - VAETrainer - INFO - Epoch [24/100] Train TotalLoss: 38.9599, (ReconX: 0.9484, KLD: 0.2755*0.80, LossY: 3.7791*10.00), Time: 1.8s | Val TotalLoss: 38.3236, (ReconX: 0.9317, KLD: 0.2681, LossY: 3.7124)\n",
            "INFO:VAETrainer:Epoch [24/100] Train TotalLoss: 38.9599, (ReconX: 0.9484, KLD: 0.2755*0.80, LossY: 3.7791*10.00), Time: 1.8s | Val TotalLoss: 38.3236, (ReconX: 0.9317, KLD: 0.2681, LossY: 3.7124)\n",
            "2025-10-23 22:26:10,874 - VAETrainer - INFO - Saving model and trainer state to vae_results/best_model.pth...\n",
            "INFO:VAETrainer:Saving model and trainer state to vae_results/best_model.pth...\n",
            "2025-10-23 22:26:10,887 - VAETrainer - INFO - Successfully saved model and trainer state to vae_results/best_model.pth\n",
            "INFO:VAETrainer:Successfully saved model and trainer state to vae_results/best_model.pth\n",
            "2025-10-23 22:26:12,916 - VAETrainer - INFO - Epoch [25/100] Train TotalLoss: 38.6675, (ReconX: 0.9496, KLD: 0.2668*0.83, LossY: 3.7496*10.00), Time: 1.7s | Val TotalLoss: 38.1136, (ReconX: 0.9339, KLD: 0.2591, LossY: 3.6921)\n",
            "INFO:VAETrainer:Epoch [25/100] Train TotalLoss: 38.6675, (ReconX: 0.9496, KLD: 0.2668*0.83, LossY: 3.7496*10.00), Time: 1.7s | Val TotalLoss: 38.1136, (ReconX: 0.9339, KLD: 0.2591, LossY: 3.6921)\n",
            "2025-10-23 22:26:12,918 - VAETrainer - INFO - Saving model and trainer state to vae_results/best_model.pth...\n",
            "INFO:VAETrainer:Saving model and trainer state to vae_results/best_model.pth...\n",
            "2025-10-23 22:26:12,930 - VAETrainer - INFO - Successfully saved model and trainer state to vae_results/best_model.pth\n",
            "INFO:VAETrainer:Successfully saved model and trainer state to vae_results/best_model.pth\n",
            "2025-10-23 22:26:15,101 - VAETrainer - INFO - Epoch [26/100] Train TotalLoss: 38.3188, (ReconX: 0.9519, KLD: 0.2575*0.87, LossY: 3.7144*10.00), Time: 1.8s | Val TotalLoss: 37.9714, (ReconX: 0.9368, KLD: 0.2499, LossY: 3.6785)\n",
            "INFO:VAETrainer:Epoch [26/100] Train TotalLoss: 38.3188, (ReconX: 0.9519, KLD: 0.2575*0.87, LossY: 3.7144*10.00), Time: 1.8s | Val TotalLoss: 37.9714, (ReconX: 0.9368, KLD: 0.2499, LossY: 3.6785)\n",
            "2025-10-23 22:26:15,103 - VAETrainer - INFO - Saving model and trainer state to vae_results/best_model.pth...\n",
            "INFO:VAETrainer:Saving model and trainer state to vae_results/best_model.pth...\n",
            "2025-10-23 22:26:15,121 - VAETrainer - INFO - Successfully saved model and trainer state to vae_results/best_model.pth\n",
            "INFO:VAETrainer:Successfully saved model and trainer state to vae_results/best_model.pth\n",
            "2025-10-23 22:26:17,320 - VAETrainer - INFO - Epoch [27/100] Train TotalLoss: 38.0742, (ReconX: 0.9524, KLD: 0.2453*0.90, LossY: 3.6901*10.00), Time: 1.8s | Val TotalLoss: 37.8394, (ReconX: 0.9390, KLD: 0.2377, LossY: 3.6663)\n",
            "INFO:VAETrainer:Epoch [27/100] Train TotalLoss: 38.0742, (ReconX: 0.9524, KLD: 0.2453*0.90, LossY: 3.6901*10.00), Time: 1.8s | Val TotalLoss: 37.8394, (ReconX: 0.9390, KLD: 0.2377, LossY: 3.6663)\n",
            "2025-10-23 22:26:17,322 - VAETrainer - INFO - Saving model and trainer state to vae_results/best_model.pth...\n",
            "INFO:VAETrainer:Saving model and trainer state to vae_results/best_model.pth...\n",
            "2025-10-23 22:26:17,334 - VAETrainer - INFO - Successfully saved model and trainer state to vae_results/best_model.pth\n",
            "INFO:VAETrainer:Successfully saved model and trainer state to vae_results/best_model.pth\n",
            "2025-10-23 22:26:19,480 - VAETrainer - INFO - Epoch [28/100] Train TotalLoss: 37.8933, (ReconX: 0.9555, KLD: 0.2337*0.93, LossY: 3.6720*10.00), Time: 1.8s | Val TotalLoss: 37.5947, (ReconX: 0.9407, KLD: 0.2275, LossY: 3.6426)\n",
            "INFO:VAETrainer:Epoch [28/100] Train TotalLoss: 37.8933, (ReconX: 0.9555, KLD: 0.2337*0.93, LossY: 3.6720*10.00), Time: 1.8s | Val TotalLoss: 37.5947, (ReconX: 0.9407, KLD: 0.2275, LossY: 3.6426)\n",
            "2025-10-23 22:26:19,482 - VAETrainer - INFO - Saving model and trainer state to vae_results/best_model.pth...\n",
            "INFO:VAETrainer:Saving model and trainer state to vae_results/best_model.pth...\n",
            "2025-10-23 22:26:19,495 - VAETrainer - INFO - Successfully saved model and trainer state to vae_results/best_model.pth\n",
            "INFO:VAETrainer:Successfully saved model and trainer state to vae_results/best_model.pth\n",
            "2025-10-23 22:26:21,626 - VAETrainer - INFO - Epoch [29/100] Train TotalLoss: 37.8371, (ReconX: 0.9566, KLD: 0.2249*0.97, LossY: 3.6663*10.00), Time: 1.8s | Val TotalLoss: 37.4901, (ReconX: 0.9405, KLD: 0.2162, LossY: 3.6333)\n",
            "INFO:VAETrainer:Epoch [29/100] Train TotalLoss: 37.8371, (ReconX: 0.9566, KLD: 0.2249*0.97, LossY: 3.6663*10.00), Time: 1.8s | Val TotalLoss: 37.4901, (ReconX: 0.9405, KLD: 0.2162, LossY: 3.6333)\n",
            "2025-10-23 22:26:21,628 - VAETrainer - INFO - Saving model and trainer state to vae_results/best_model.pth...\n",
            "INFO:VAETrainer:Saving model and trainer state to vae_results/best_model.pth...\n",
            "2025-10-23 22:26:21,641 - VAETrainer - INFO - Successfully saved model and trainer state to vae_results/best_model.pth\n",
            "INFO:VAETrainer:Successfully saved model and trainer state to vae_results/best_model.pth\n",
            "2025-10-23 22:26:23,670 - VAETrainer - INFO - Epoch [30/100] Train TotalLoss: 37.6591, (ReconX: 0.9593, KLD: 0.2143*1.00, LossY: 3.6486*10.00), Time: 1.7s | Val TotalLoss: 37.3409, (ReconX: 0.9436, KLD: 0.2044, LossY: 3.6193)\n",
            "INFO:VAETrainer:Epoch [30/100] Train TotalLoss: 37.6591, (ReconX: 0.9593, KLD: 0.2143*1.00, LossY: 3.6486*10.00), Time: 1.7s | Val TotalLoss: 37.3409, (ReconX: 0.9436, KLD: 0.2044, LossY: 3.6193)\n",
            "2025-10-23 22:26:23,672 - VAETrainer - INFO - Saving model and trainer state to vae_results/best_model.pth...\n",
            "INFO:VAETrainer:Saving model and trainer state to vae_results/best_model.pth...\n",
            "2025-10-23 22:26:23,685 - VAETrainer - INFO - Successfully saved model and trainer state to vae_results/best_model.pth\n",
            "INFO:VAETrainer:Successfully saved model and trainer state to vae_results/best_model.pth\n",
            "2025-10-23 22:26:25,736 - VAETrainer - INFO - Epoch [31/100] Train TotalLoss: 37.5009, (ReconX: 0.9612, KLD: 0.2028*1.00, LossY: 3.6337*10.00), Time: 1.7s | Val TotalLoss: 37.2979, (ReconX: 0.9455, KLD: 0.1907, LossY: 3.6162)\n",
            "INFO:VAETrainer:Epoch [31/100] Train TotalLoss: 37.5009, (ReconX: 0.9612, KLD: 0.2028*1.00, LossY: 3.6337*10.00), Time: 1.7s | Val TotalLoss: 37.2979, (ReconX: 0.9455, KLD: 0.1907, LossY: 3.6162)\n",
            "2025-10-23 22:26:25,738 - VAETrainer - INFO - Saving model and trainer state to vae_results/best_model.pth...\n",
            "INFO:VAETrainer:Saving model and trainer state to vae_results/best_model.pth...\n",
            "2025-10-23 22:26:25,751 - VAETrainer - INFO - Successfully saved model and trainer state to vae_results/best_model.pth\n",
            "INFO:VAETrainer:Successfully saved model and trainer state to vae_results/best_model.pth\n",
            "2025-10-23 22:26:27,908 - VAETrainer - INFO - Epoch [32/100] Train TotalLoss: 37.4827, (ReconX: 0.9630, KLD: 0.1918*1.00, LossY: 3.6328*10.00), Time: 1.8s | Val TotalLoss: 37.2713, (ReconX: 0.9481, KLD: 0.1850, LossY: 3.6138)\n",
            "INFO:VAETrainer:Epoch [32/100] Train TotalLoss: 37.4827, (ReconX: 0.9630, KLD: 0.1918*1.00, LossY: 3.6328*10.00), Time: 1.8s | Val TotalLoss: 37.2713, (ReconX: 0.9481, KLD: 0.1850, LossY: 3.6138)\n",
            "2025-10-23 22:26:27,910 - VAETrainer - INFO - Saving model and trainer state to vae_results/best_model.pth...\n",
            "INFO:VAETrainer:Saving model and trainer state to vae_results/best_model.pth...\n",
            "2025-10-23 22:26:27,923 - VAETrainer - INFO - Successfully saved model and trainer state to vae_results/best_model.pth\n",
            "INFO:VAETrainer:Successfully saved model and trainer state to vae_results/best_model.pth\n",
            "2025-10-23 22:26:30,152 - VAETrainer - INFO - Epoch [33/100] Train TotalLoss: 37.3913, (ReconX: 0.9644, KLD: 0.1814*1.00, LossY: 3.6245*10.00), Time: 1.8s | Val TotalLoss: 37.1870, (ReconX: 0.9498, KLD: 0.1755, LossY: 3.6062)\n",
            "INFO:VAETrainer:Epoch [33/100] Train TotalLoss: 37.3913, (ReconX: 0.9644, KLD: 0.1814*1.00, LossY: 3.6245*10.00), Time: 1.8s | Val TotalLoss: 37.1870, (ReconX: 0.9498, KLD: 0.1755, LossY: 3.6062)\n",
            "2025-10-23 22:26:30,154 - VAETrainer - INFO - Saving model and trainer state to vae_results/best_model.pth...\n",
            "INFO:VAETrainer:Saving model and trainer state to vae_results/best_model.pth...\n",
            "2025-10-23 22:26:30,167 - VAETrainer - INFO - Successfully saved model and trainer state to vae_results/best_model.pth\n",
            "INFO:VAETrainer:Successfully saved model and trainer state to vae_results/best_model.pth\n",
            "2025-10-23 22:26:32,248 - VAETrainer - INFO - Epoch [34/100] Train TotalLoss: 37.3747, (ReconX: 0.9662, KLD: 0.1715*1.00, LossY: 3.6237*10.00), Time: 1.7s | Val TotalLoss: 37.2281, (ReconX: 0.9513, KLD: 0.1676, LossY: 3.6109)\n",
            "INFO:VAETrainer:Epoch [34/100] Train TotalLoss: 37.3747, (ReconX: 0.9662, KLD: 0.1715*1.00, LossY: 3.6237*10.00), Time: 1.7s | Val TotalLoss: 37.2281, (ReconX: 0.9513, KLD: 0.1676, LossY: 3.6109)\n",
            "2025-10-23 22:26:34,319 - VAETrainer - INFO - Epoch [35/100] Train TotalLoss: 37.3736, (ReconX: 0.9679, KLD: 0.1630*1.00, LossY: 3.6243*10.00), Time: 1.7s | Val TotalLoss: 37.1294, (ReconX: 0.9531, KLD: 0.1550, LossY: 3.6021)\n",
            "INFO:VAETrainer:Epoch [35/100] Train TotalLoss: 37.3736, (ReconX: 0.9679, KLD: 0.1630*1.00, LossY: 3.6243*10.00), Time: 1.7s | Val TotalLoss: 37.1294, (ReconX: 0.9531, KLD: 0.1550, LossY: 3.6021)\n",
            "2025-10-23 22:26:34,321 - VAETrainer - INFO - Saving model and trainer state to vae_results/best_model.pth...\n",
            "INFO:VAETrainer:Saving model and trainer state to vae_results/best_model.pth...\n",
            "2025-10-23 22:26:34,335 - VAETrainer - INFO - Successfully saved model and trainer state to vae_results/best_model.pth\n",
            "INFO:VAETrainer:Successfully saved model and trainer state to vae_results/best_model.pth\n",
            "2025-10-23 22:26:36,469 - VAETrainer - INFO - Epoch [36/100] Train TotalLoss: 37.2474, (ReconX: 0.9707, KLD: 0.1543*1.00, LossY: 3.6122*10.00), Time: 1.8s | Val TotalLoss: 37.0865, (ReconX: 0.9561, KLD: 0.1504, LossY: 3.5980)\n",
            "INFO:VAETrainer:Epoch [36/100] Train TotalLoss: 37.2474, (ReconX: 0.9707, KLD: 0.1543*1.00, LossY: 3.6122*10.00), Time: 1.8s | Val TotalLoss: 37.0865, (ReconX: 0.9561, KLD: 0.1504, LossY: 3.5980)\n",
            "2025-10-23 22:26:36,470 - VAETrainer - INFO - Saving model and trainer state to vae_results/best_model.pth...\n",
            "INFO:VAETrainer:Saving model and trainer state to vae_results/best_model.pth...\n",
            "2025-10-23 22:26:36,484 - VAETrainer - INFO - Successfully saved model and trainer state to vae_results/best_model.pth\n",
            "INFO:VAETrainer:Successfully saved model and trainer state to vae_results/best_model.pth\n",
            "2025-10-23 22:26:38,619 - VAETrainer - INFO - Epoch [37/100] Train TotalLoss: 37.2842, (ReconX: 0.9710, KLD: 0.1474*1.00, LossY: 3.6166*10.00), Time: 1.8s | Val TotalLoss: 37.0619, (ReconX: 0.9555, KLD: 0.1439, LossY: 3.5962)\n",
            "INFO:VAETrainer:Epoch [37/100] Train TotalLoss: 37.2842, (ReconX: 0.9710, KLD: 0.1474*1.00, LossY: 3.6166*10.00), Time: 1.8s | Val TotalLoss: 37.0619, (ReconX: 0.9555, KLD: 0.1439, LossY: 3.5962)\n",
            "2025-10-23 22:26:38,620 - VAETrainer - INFO - Saving model and trainer state to vae_results/best_model.pth...\n",
            "INFO:VAETrainer:Saving model and trainer state to vae_results/best_model.pth...\n",
            "2025-10-23 22:26:38,633 - VAETrainer - INFO - Successfully saved model and trainer state to vae_results/best_model.pth\n",
            "INFO:VAETrainer:Successfully saved model and trainer state to vae_results/best_model.pth\n",
            "2025-10-23 22:26:40,937 - VAETrainer - INFO - Epoch [38/100] Train TotalLoss: 37.2037, (ReconX: 0.9730, KLD: 0.1413*1.00, LossY: 3.6089*10.00), Time: 1.9s | Val TotalLoss: 37.0199, (ReconX: 0.9590, KLD: 0.1334, LossY: 3.5927)\n",
            "INFO:VAETrainer:Epoch [38/100] Train TotalLoss: 37.2037, (ReconX: 0.9730, KLD: 0.1413*1.00, LossY: 3.6089*10.00), Time: 1.9s | Val TotalLoss: 37.0199, (ReconX: 0.9590, KLD: 0.1334, LossY: 3.5927)\n",
            "2025-10-23 22:26:40,939 - VAETrainer - INFO - Saving model and trainer state to vae_results/best_model.pth...\n",
            "INFO:VAETrainer:Saving model and trainer state to vae_results/best_model.pth...\n",
            "2025-10-23 22:26:40,957 - VAETrainer - INFO - Successfully saved model and trainer state to vae_results/best_model.pth\n",
            "INFO:VAETrainer:Successfully saved model and trainer state to vae_results/best_model.pth\n",
            "2025-10-23 22:26:43,165 - VAETrainer - INFO - Epoch [39/100] Train TotalLoss: 37.1596, (ReconX: 0.9746, KLD: 0.1328*1.00, LossY: 3.6052*10.00), Time: 1.8s | Val TotalLoss: 36.9606, (ReconX: 0.9607, KLD: 0.1280, LossY: 3.5872)\n",
            "INFO:VAETrainer:Epoch [39/100] Train TotalLoss: 37.1596, (ReconX: 0.9746, KLD: 0.1328*1.00, LossY: 3.6052*10.00), Time: 1.8s | Val TotalLoss: 36.9606, (ReconX: 0.9607, KLD: 0.1280, LossY: 3.5872)\n",
            "2025-10-23 22:26:43,167 - VAETrainer - INFO - Saving model and trainer state to vae_results/best_model.pth...\n",
            "INFO:VAETrainer:Saving model and trainer state to vae_results/best_model.pth...\n",
            "2025-10-23 22:26:43,180 - VAETrainer - INFO - Successfully saved model and trainer state to vae_results/best_model.pth\n",
            "INFO:VAETrainer:Successfully saved model and trainer state to vae_results/best_model.pth\n",
            "2025-10-23 22:26:45,283 - VAETrainer - INFO - Epoch [40/100] Train TotalLoss: 37.0970, (ReconX: 0.9762, KLD: 0.1261*1.00, LossY: 3.5995*10.00), Time: 1.7s | Val TotalLoss: 36.9189, (ReconX: 0.9598, KLD: 0.1197, LossY: 3.5839)\n",
            "INFO:VAETrainer:Epoch [40/100] Train TotalLoss: 37.0970, (ReconX: 0.9762, KLD: 0.1261*1.00, LossY: 3.5995*10.00), Time: 1.7s | Val TotalLoss: 36.9189, (ReconX: 0.9598, KLD: 0.1197, LossY: 3.5839)\n",
            "2025-10-23 22:26:45,286 - VAETrainer - INFO - Saving model and trainer state to vae_results/best_model.pth...\n",
            "INFO:VAETrainer:Saving model and trainer state to vae_results/best_model.pth...\n",
            "2025-10-23 22:26:45,299 - VAETrainer - INFO - Successfully saved model and trainer state to vae_results/best_model.pth\n",
            "INFO:VAETrainer:Successfully saved model and trainer state to vae_results/best_model.pth\n",
            "2025-10-23 22:26:47,376 - VAETrainer - INFO - Epoch [41/100] Train TotalLoss: 37.1187, (ReconX: 0.9774, KLD: 0.1192*1.00, LossY: 3.6022*10.00), Time: 1.7s | Val TotalLoss: 36.8633, (ReconX: 0.9634, KLD: 0.1145, LossY: 3.5785)\n",
            "INFO:VAETrainer:Epoch [41/100] Train TotalLoss: 37.1187, (ReconX: 0.9774, KLD: 0.1192*1.00, LossY: 3.6022*10.00), Time: 1.7s | Val TotalLoss: 36.8633, (ReconX: 0.9634, KLD: 0.1145, LossY: 3.5785)\n",
            "2025-10-23 22:26:47,378 - VAETrainer - INFO - Saving model and trainer state to vae_results/best_model.pth...\n",
            "INFO:VAETrainer:Saving model and trainer state to vae_results/best_model.pth...\n",
            "2025-10-23 22:26:47,391 - VAETrainer - INFO - Successfully saved model and trainer state to vae_results/best_model.pth\n",
            "INFO:VAETrainer:Successfully saved model and trainer state to vae_results/best_model.pth\n",
            "2025-10-23 22:26:49,500 - VAETrainer - INFO - Epoch [42/100] Train TotalLoss: 37.0469, (ReconX: 0.9784, KLD: 0.1142*1.00, LossY: 3.5954*10.00), Time: 1.7s | Val TotalLoss: 36.8609, (ReconX: 0.9629, KLD: 0.1096, LossY: 3.5788)\n",
            "INFO:VAETrainer:Epoch [42/100] Train TotalLoss: 37.0469, (ReconX: 0.9784, KLD: 0.1142*1.00, LossY: 3.5954*10.00), Time: 1.7s | Val TotalLoss: 36.8609, (ReconX: 0.9629, KLD: 0.1096, LossY: 3.5788)\n",
            "2025-10-23 22:26:49,502 - VAETrainer - INFO - Saving model and trainer state to vae_results/best_model.pth...\n",
            "INFO:VAETrainer:Saving model and trainer state to vae_results/best_model.pth...\n",
            "2025-10-23 22:26:49,515 - VAETrainer - INFO - Successfully saved model and trainer state to vae_results/best_model.pth\n",
            "INFO:VAETrainer:Successfully saved model and trainer state to vae_results/best_model.pth\n",
            "2025-10-23 22:26:51,577 - VAETrainer - INFO - Epoch [43/100] Train TotalLoss: 36.9897, (ReconX: 0.9796, KLD: 0.1089*1.00, LossY: 3.5901*10.00), Time: 1.7s | Val TotalLoss: 36.7833, (ReconX: 0.9650, KLD: 0.1059, LossY: 3.5712)\n",
            "INFO:VAETrainer:Epoch [43/100] Train TotalLoss: 36.9897, (ReconX: 0.9796, KLD: 0.1089*1.00, LossY: 3.5901*10.00), Time: 1.7s | Val TotalLoss: 36.7833, (ReconX: 0.9650, KLD: 0.1059, LossY: 3.5712)\n",
            "2025-10-23 22:26:51,579 - VAETrainer - INFO - Saving model and trainer state to vae_results/best_model.pth...\n",
            "INFO:VAETrainer:Saving model and trainer state to vae_results/best_model.pth...\n",
            "2025-10-23 22:26:51,592 - VAETrainer - INFO - Successfully saved model and trainer state to vae_results/best_model.pth\n",
            "INFO:VAETrainer:Successfully saved model and trainer state to vae_results/best_model.pth\n",
            "2025-10-23 22:26:53,784 - VAETrainer - INFO - Epoch [44/100] Train TotalLoss: 36.9568, (ReconX: 0.9802, KLD: 0.1047*1.00, LossY: 3.5872*10.00), Time: 1.8s | Val TotalLoss: 36.7295, (ReconX: 0.9653, KLD: 0.0988, LossY: 3.5665)\n",
            "INFO:VAETrainer:Epoch [44/100] Train TotalLoss: 36.9568, (ReconX: 0.9802, KLD: 0.1047*1.00, LossY: 3.5872*10.00), Time: 1.8s | Val TotalLoss: 36.7295, (ReconX: 0.9653, KLD: 0.0988, LossY: 3.5665)\n",
            "2025-10-23 22:26:53,786 - VAETrainer - INFO - Saving model and trainer state to vae_results/best_model.pth...\n",
            "INFO:VAETrainer:Saving model and trainer state to vae_results/best_model.pth...\n",
            "2025-10-23 22:26:53,799 - VAETrainer - INFO - Successfully saved model and trainer state to vae_results/best_model.pth\n",
            "INFO:VAETrainer:Successfully saved model and trainer state to vae_results/best_model.pth\n",
            "2025-10-23 22:26:55,927 - VAETrainer - INFO - Epoch [45/100] Train TotalLoss: 36.9015, (ReconX: 0.9819, KLD: 0.0998*1.00, LossY: 3.5820*10.00), Time: 1.8s | Val TotalLoss: 36.6530, (ReconX: 0.9669, KLD: 0.0954, LossY: 3.5591)\n",
            "INFO:VAETrainer:Epoch [45/100] Train TotalLoss: 36.9015, (ReconX: 0.9819, KLD: 0.0998*1.00, LossY: 3.5820*10.00), Time: 1.8s | Val TotalLoss: 36.6530, (ReconX: 0.9669, KLD: 0.0954, LossY: 3.5591)\n",
            "2025-10-23 22:26:55,929 - VAETrainer - INFO - Saving model and trainer state to vae_results/best_model.pth...\n",
            "INFO:VAETrainer:Saving model and trainer state to vae_results/best_model.pth...\n",
            "2025-10-23 22:26:55,942 - VAETrainer - INFO - Successfully saved model and trainer state to vae_results/best_model.pth\n",
            "INFO:VAETrainer:Successfully saved model and trainer state to vae_results/best_model.pth\n",
            "2025-10-23 22:26:58,014 - VAETrainer - INFO - Epoch [46/100] Train TotalLoss: 36.8819, (ReconX: 0.9829, KLD: 0.0949*1.00, LossY: 3.5804*10.00), Time: 1.7s | Val TotalLoss: 36.6524, (ReconX: 0.9661, KLD: 0.0944, LossY: 3.5592)\n",
            "INFO:VAETrainer:Epoch [46/100] Train TotalLoss: 36.8819, (ReconX: 0.9829, KLD: 0.0949*1.00, LossY: 3.5804*10.00), Time: 1.7s | Val TotalLoss: 36.6524, (ReconX: 0.9661, KLD: 0.0944, LossY: 3.5592)\n",
            "2025-10-23 22:26:58,015 - VAETrainer - INFO - Saving model and trainer state to vae_results/best_model.pth...\n",
            "INFO:VAETrainer:Saving model and trainer state to vae_results/best_model.pth...\n",
            "2025-10-23 22:26:58,028 - VAETrainer - INFO - Successfully saved model and trainer state to vae_results/best_model.pth\n",
            "INFO:VAETrainer:Successfully saved model and trainer state to vae_results/best_model.pth\n",
            "2025-10-23 22:27:00,166 - VAETrainer - INFO - Epoch [47/100] Train TotalLoss: 36.7201, (ReconX: 0.9828, KLD: 0.0911*1.00, LossY: 3.5646*10.00), Time: 1.8s | Val TotalLoss: 36.5776, (ReconX: 0.9680, KLD: 0.0871, LossY: 3.5522)\n",
            "INFO:VAETrainer:Epoch [47/100] Train TotalLoss: 36.7201, (ReconX: 0.9828, KLD: 0.0911*1.00, LossY: 3.5646*10.00), Time: 1.8s | Val TotalLoss: 36.5776, (ReconX: 0.9680, KLD: 0.0871, LossY: 3.5522)\n",
            "2025-10-23 22:27:00,167 - VAETrainer - INFO - Saving model and trainer state to vae_results/best_model.pth...\n",
            "INFO:VAETrainer:Saving model and trainer state to vae_results/best_model.pth...\n",
            "2025-10-23 22:27:00,182 - VAETrainer - INFO - Successfully saved model and trainer state to vae_results/best_model.pth\n",
            "INFO:VAETrainer:Successfully saved model and trainer state to vae_results/best_model.pth\n",
            "2025-10-23 22:27:02,211 - VAETrainer - INFO - Epoch [48/100] Train TotalLoss: 36.7256, (ReconX: 0.9840, KLD: 0.0876*1.00, LossY: 3.5654*10.00), Time: 1.7s | Val TotalLoss: 36.6125, (ReconX: 0.9686, KLD: 0.0842, LossY: 3.5560)\n",
            "INFO:VAETrainer:Epoch [48/100] Train TotalLoss: 36.7256, (ReconX: 0.9840, KLD: 0.0876*1.00, LossY: 3.5654*10.00), Time: 1.7s | Val TotalLoss: 36.6125, (ReconX: 0.9686, KLD: 0.0842, LossY: 3.5560)\n",
            "2025-10-23 22:27:04,431 - VAETrainer - INFO - Epoch [49/100] Train TotalLoss: 36.5903, (ReconX: 0.9851, KLD: 0.0844*1.00, LossY: 3.5521*10.00), Time: 1.8s | Val TotalLoss: 36.4387, (ReconX: 0.9686, KLD: 0.0824, LossY: 3.5388)\n",
            "INFO:VAETrainer:Epoch [49/100] Train TotalLoss: 36.5903, (ReconX: 0.9851, KLD: 0.0844*1.00, LossY: 3.5521*10.00), Time: 1.8s | Val TotalLoss: 36.4387, (ReconX: 0.9686, KLD: 0.0824, LossY: 3.5388)\n",
            "2025-10-23 22:27:04,433 - VAETrainer - INFO - Saving model and trainer state to vae_results/best_model.pth...\n",
            "INFO:VAETrainer:Saving model and trainer state to vae_results/best_model.pth...\n",
            "2025-10-23 22:27:04,446 - VAETrainer - INFO - Successfully saved model and trainer state to vae_results/best_model.pth\n",
            "INFO:VAETrainer:Successfully saved model and trainer state to vae_results/best_model.pth\n",
            "2025-10-23 22:27:06,640 - VAETrainer - INFO - Epoch [50/100] Train TotalLoss: 36.6224, (ReconX: 0.9856, KLD: 0.0819*1.00, LossY: 3.5555*10.00), Time: 1.8s | Val TotalLoss: 36.4007, (ReconX: 0.9692, KLD: 0.0792, LossY: 3.5352)\n",
            "INFO:VAETrainer:Epoch [50/100] Train TotalLoss: 36.6224, (ReconX: 0.9856, KLD: 0.0819*1.00, LossY: 3.5555*10.00), Time: 1.8s | Val TotalLoss: 36.4007, (ReconX: 0.9692, KLD: 0.0792, LossY: 3.5352)\n",
            "2025-10-23 22:27:06,642 - VAETrainer - INFO - Saving model and trainer state to vae_results/best_model.pth...\n",
            "INFO:VAETrainer:Saving model and trainer state to vae_results/best_model.pth...\n",
            "2025-10-23 22:27:06,655 - VAETrainer - INFO - Successfully saved model and trainer state to vae_results/best_model.pth\n",
            "INFO:VAETrainer:Successfully saved model and trainer state to vae_results/best_model.pth\n",
            "2025-10-23 22:27:08,750 - VAETrainer - INFO - Epoch [51/100] Train TotalLoss: 36.5707, (ReconX: 0.9852, KLD: 0.0793*1.00, LossY: 3.5506*10.00), Time: 1.7s | Val TotalLoss: 36.4312, (ReconX: 0.9727, KLD: 0.0743, LossY: 3.5384)\n",
            "INFO:VAETrainer:Epoch [51/100] Train TotalLoss: 36.5707, (ReconX: 0.9852, KLD: 0.0793*1.00, LossY: 3.5506*10.00), Time: 1.7s | Val TotalLoss: 36.4312, (ReconX: 0.9727, KLD: 0.0743, LossY: 3.5384)\n",
            "2025-10-23 22:27:10,822 - VAETrainer - INFO - Epoch [52/100] Train TotalLoss: 36.4223, (ReconX: 0.9864, KLD: 0.0753*1.00, LossY: 3.5361*10.00), Time: 1.7s | Val TotalLoss: 36.2733, (ReconX: 0.9703, KLD: 0.0724, LossY: 3.5231)\n",
            "INFO:VAETrainer:Epoch [52/100] Train TotalLoss: 36.4223, (ReconX: 0.9864, KLD: 0.0753*1.00, LossY: 3.5361*10.00), Time: 1.7s | Val TotalLoss: 36.2733, (ReconX: 0.9703, KLD: 0.0724, LossY: 3.5231)\n",
            "2025-10-23 22:27:10,824 - VAETrainer - INFO - Saving model and trainer state to vae_results/best_model.pth...\n",
            "INFO:VAETrainer:Saving model and trainer state to vae_results/best_model.pth...\n",
            "2025-10-23 22:27:10,837 - VAETrainer - INFO - Successfully saved model and trainer state to vae_results/best_model.pth\n",
            "INFO:VAETrainer:Successfully saved model and trainer state to vae_results/best_model.pth\n",
            "2025-10-23 22:27:12,988 - VAETrainer - INFO - Epoch [53/100] Train TotalLoss: 36.3604, (ReconX: 0.9873, KLD: 0.0726*1.00, LossY: 3.5300*10.00), Time: 1.8s | Val TotalLoss: 36.2297, (ReconX: 0.9722, KLD: 0.0725, LossY: 3.5185)\n",
            "INFO:VAETrainer:Epoch [53/100] Train TotalLoss: 36.3604, (ReconX: 0.9873, KLD: 0.0726*1.00, LossY: 3.5300*10.00), Time: 1.8s | Val TotalLoss: 36.2297, (ReconX: 0.9722, KLD: 0.0725, LossY: 3.5185)\n",
            "2025-10-23 22:27:12,989 - VAETrainer - INFO - Saving model and trainer state to vae_results/best_model.pth...\n",
            "INFO:VAETrainer:Saving model and trainer state to vae_results/best_model.pth...\n",
            "2025-10-23 22:27:13,003 - VAETrainer - INFO - Successfully saved model and trainer state to vae_results/best_model.pth\n",
            "INFO:VAETrainer:Successfully saved model and trainer state to vae_results/best_model.pth\n",
            "2025-10-23 22:27:15,150 - VAETrainer - INFO - Epoch [54/100] Train TotalLoss: 36.2922, (ReconX: 0.9880, KLD: 0.0713*1.00, LossY: 3.5233*10.00), Time: 1.8s | Val TotalLoss: 36.1042, (ReconX: 0.9724, KLD: 0.0692, LossY: 3.5063)\n",
            "INFO:VAETrainer:Epoch [54/100] Train TotalLoss: 36.2922, (ReconX: 0.9880, KLD: 0.0713*1.00, LossY: 3.5233*10.00), Time: 1.8s | Val TotalLoss: 36.1042, (ReconX: 0.9724, KLD: 0.0692, LossY: 3.5063)\n",
            "2025-10-23 22:27:15,151 - VAETrainer - INFO - Saving model and trainer state to vae_results/best_model.pth...\n",
            "INFO:VAETrainer:Saving model and trainer state to vae_results/best_model.pth...\n",
            "2025-10-23 22:27:15,164 - VAETrainer - INFO - Successfully saved model and trainer state to vae_results/best_model.pth\n",
            "INFO:VAETrainer:Successfully saved model and trainer state to vae_results/best_model.pth\n",
            "2025-10-23 22:27:17,344 - VAETrainer - INFO - Epoch [55/100] Train TotalLoss: 36.2808, (ReconX: 0.9881, KLD: 0.0687*1.00, LossY: 3.5224*10.00), Time: 1.8s | Val TotalLoss: 36.0717, (ReconX: 0.9720, KLD: 0.0676, LossY: 3.5032)\n",
            "INFO:VAETrainer:Epoch [55/100] Train TotalLoss: 36.2808, (ReconX: 0.9881, KLD: 0.0687*1.00, LossY: 3.5224*10.00), Time: 1.8s | Val TotalLoss: 36.0717, (ReconX: 0.9720, KLD: 0.0676, LossY: 3.5032)\n",
            "2025-10-23 22:27:17,346 - VAETrainer - INFO - Saving model and trainer state to vae_results/best_model.pth...\n",
            "INFO:VAETrainer:Saving model and trainer state to vae_results/best_model.pth...\n",
            "2025-10-23 22:27:17,359 - VAETrainer - INFO - Successfully saved model and trainer state to vae_results/best_model.pth\n",
            "INFO:VAETrainer:Successfully saved model and trainer state to vae_results/best_model.pth\n",
            "2025-10-23 22:27:19,542 - VAETrainer - INFO - Epoch [56/100] Train TotalLoss: 36.1494, (ReconX: 0.9893, KLD: 0.0663*1.00, LossY: 3.5094*10.00), Time: 1.8s | Val TotalLoss: 35.9796, (ReconX: 0.9743, KLD: 0.0614, LossY: 3.4944)\n",
            "INFO:VAETrainer:Epoch [56/100] Train TotalLoss: 36.1494, (ReconX: 0.9893, KLD: 0.0663*1.00, LossY: 3.5094*10.00), Time: 1.8s | Val TotalLoss: 35.9796, (ReconX: 0.9743, KLD: 0.0614, LossY: 3.4944)\n",
            "2025-10-23 22:27:19,544 - VAETrainer - INFO - Saving model and trainer state to vae_results/best_model.pth...\n",
            "INFO:VAETrainer:Saving model and trainer state to vae_results/best_model.pth...\n",
            "2025-10-23 22:27:19,557 - VAETrainer - INFO - Successfully saved model and trainer state to vae_results/best_model.pth\n",
            "INFO:VAETrainer:Successfully saved model and trainer state to vae_results/best_model.pth\n",
            "2025-10-23 22:27:21,723 - VAETrainer - INFO - Epoch [57/100] Train TotalLoss: 36.0345, (ReconX: 0.9893, KLD: 0.0629*1.00, LossY: 3.4982*10.00), Time: 1.8s | Val TotalLoss: 35.9337, (ReconX: 0.9738, KLD: 0.0603, LossY: 3.4900)\n",
            "INFO:VAETrainer:Epoch [57/100] Train TotalLoss: 36.0345, (ReconX: 0.9893, KLD: 0.0629*1.00, LossY: 3.4982*10.00), Time: 1.8s | Val TotalLoss: 35.9337, (ReconX: 0.9738, KLD: 0.0603, LossY: 3.4900)\n",
            "2025-10-23 22:27:21,725 - VAETrainer - INFO - Saving model and trainer state to vae_results/best_model.pth...\n",
            "INFO:VAETrainer:Saving model and trainer state to vae_results/best_model.pth...\n",
            "2025-10-23 22:27:21,739 - VAETrainer - INFO - Successfully saved model and trainer state to vae_results/best_model.pth\n",
            "INFO:VAETrainer:Successfully saved model and trainer state to vae_results/best_model.pth\n",
            "2025-10-23 22:27:23,898 - VAETrainer - INFO - Epoch [58/100] Train TotalLoss: 36.0348, (ReconX: 0.9901, KLD: 0.0605*1.00, LossY: 3.4984*10.00), Time: 1.8s | Val TotalLoss: 35.9035, (ReconX: 0.9761, KLD: 0.0590, LossY: 3.4868)\n",
            "INFO:VAETrainer:Epoch [58/100] Train TotalLoss: 36.0348, (ReconX: 0.9901, KLD: 0.0605*1.00, LossY: 3.4984*10.00), Time: 1.8s | Val TotalLoss: 35.9035, (ReconX: 0.9761, KLD: 0.0590, LossY: 3.4868)\n",
            "2025-10-23 22:27:23,900 - VAETrainer - INFO - Saving model and trainer state to vae_results/best_model.pth...\n",
            "INFO:VAETrainer:Saving model and trainer state to vae_results/best_model.pth...\n",
            "2025-10-23 22:27:23,913 - VAETrainer - INFO - Successfully saved model and trainer state to vae_results/best_model.pth\n",
            "INFO:VAETrainer:Successfully saved model and trainer state to vae_results/best_model.pth\n",
            "2025-10-23 22:27:26,014 - VAETrainer - INFO - Epoch [59/100] Train TotalLoss: 35.8807, (ReconX: 0.9901, KLD: 0.0599*1.00, LossY: 3.4831*10.00), Time: 1.7s | Val TotalLoss: 35.8197, (ReconX: 0.9731, KLD: 0.0581, LossY: 3.4788)\n",
            "INFO:VAETrainer:Epoch [59/100] Train TotalLoss: 35.8807, (ReconX: 0.9901, KLD: 0.0599*1.00, LossY: 3.4831*10.00), Time: 1.7s | Val TotalLoss: 35.8197, (ReconX: 0.9731, KLD: 0.0581, LossY: 3.4788)\n",
            "2025-10-23 22:27:26,015 - VAETrainer - INFO - Saving model and trainer state to vae_results/best_model.pth...\n",
            "INFO:VAETrainer:Saving model and trainer state to vae_results/best_model.pth...\n",
            "2025-10-23 22:27:26,028 - VAETrainer - INFO - Successfully saved model and trainer state to vae_results/best_model.pth\n",
            "INFO:VAETrainer:Successfully saved model and trainer state to vae_results/best_model.pth\n",
            "2025-10-23 22:27:28,094 - VAETrainer - INFO - Epoch [60/100] Train TotalLoss: 35.8183, (ReconX: 0.9902, KLD: 0.0579*1.00, LossY: 3.4770*10.00), Time: 1.7s | Val TotalLoss: 35.7695, (ReconX: 0.9744, KLD: 0.0593, LossY: 3.4736)\n",
            "INFO:VAETrainer:Epoch [60/100] Train TotalLoss: 35.8183, (ReconX: 0.9902, KLD: 0.0579*1.00, LossY: 3.4770*10.00), Time: 1.7s | Val TotalLoss: 35.7695, (ReconX: 0.9744, KLD: 0.0593, LossY: 3.4736)\n",
            "2025-10-23 22:27:28,096 - VAETrainer - INFO - Saving model and trainer state to vae_results/best_model.pth...\n",
            "INFO:VAETrainer:Saving model and trainer state to vae_results/best_model.pth...\n",
            "2025-10-23 22:27:28,109 - VAETrainer - INFO - Successfully saved model and trainer state to vae_results/best_model.pth\n",
            "INFO:VAETrainer:Successfully saved model and trainer state to vae_results/best_model.pth\n",
            "2025-10-23 22:27:30,268 - VAETrainer - INFO - Epoch [61/100] Train TotalLoss: 35.7582, (ReconX: 0.9907, KLD: 0.0566*1.00, LossY: 3.4711*10.00), Time: 1.8s | Val TotalLoss: 35.6320, (ReconX: 0.9753, KLD: 0.0560, LossY: 3.4601)\n",
            "INFO:VAETrainer:Epoch [61/100] Train TotalLoss: 35.7582, (ReconX: 0.9907, KLD: 0.0566*1.00, LossY: 3.4711*10.00), Time: 1.8s | Val TotalLoss: 35.6320, (ReconX: 0.9753, KLD: 0.0560, LossY: 3.4601)\n",
            "2025-10-23 22:27:30,270 - VAETrainer - INFO - Saving model and trainer state to vae_results/best_model.pth...\n",
            "INFO:VAETrainer:Saving model and trainer state to vae_results/best_model.pth...\n",
            "2025-10-23 22:27:30,283 - VAETrainer - INFO - Successfully saved model and trainer state to vae_results/best_model.pth\n",
            "INFO:VAETrainer:Successfully saved model and trainer state to vae_results/best_model.pth\n",
            "2025-10-23 22:27:32,445 - VAETrainer - INFO - Epoch [62/100] Train TotalLoss: 35.6815, (ReconX: 0.9911, KLD: 0.0558*1.00, LossY: 3.4635*10.00), Time: 1.8s | Val TotalLoss: 35.5682, (ReconX: 0.9757, KLD: 0.0551, LossY: 3.4537)\n",
            "INFO:VAETrainer:Epoch [62/100] Train TotalLoss: 35.6815, (ReconX: 0.9911, KLD: 0.0558*1.00, LossY: 3.4635*10.00), Time: 1.8s | Val TotalLoss: 35.5682, (ReconX: 0.9757, KLD: 0.0551, LossY: 3.4537)\n",
            "2025-10-23 22:27:32,447 - VAETrainer - INFO - Saving model and trainer state to vae_results/best_model.pth...\n",
            "INFO:VAETrainer:Saving model and trainer state to vae_results/best_model.pth...\n",
            "2025-10-23 22:27:32,463 - VAETrainer - INFO - Successfully saved model and trainer state to vae_results/best_model.pth\n",
            "INFO:VAETrainer:Successfully saved model and trainer state to vae_results/best_model.pth\n",
            "2025-10-23 22:27:34,540 - VAETrainer - INFO - Epoch [63/100] Train TotalLoss: 35.6272, (ReconX: 0.9916, KLD: 0.0548*1.00, LossY: 3.4581*10.00), Time: 1.7s | Val TotalLoss: 35.7316, (ReconX: 0.9769, KLD: 0.0541, LossY: 3.4701)\n",
            "INFO:VAETrainer:Epoch [63/100] Train TotalLoss: 35.6272, (ReconX: 0.9916, KLD: 0.0548*1.00, LossY: 3.4581*10.00), Time: 1.7s | Val TotalLoss: 35.7316, (ReconX: 0.9769, KLD: 0.0541, LossY: 3.4701)\n",
            "2025-10-23 22:27:36,642 - VAETrainer - INFO - Epoch [64/100] Train TotalLoss: 35.5257, (ReconX: 0.9910, KLD: 0.0561*1.00, LossY: 3.4479*10.00), Time: 1.7s | Val TotalLoss: 35.4407, (ReconX: 0.9751, KLD: 0.0538, LossY: 3.4412)\n",
            "INFO:VAETrainer:Epoch [64/100] Train TotalLoss: 35.5257, (ReconX: 0.9910, KLD: 0.0561*1.00, LossY: 3.4479*10.00), Time: 1.7s | Val TotalLoss: 35.4407, (ReconX: 0.9751, KLD: 0.0538, LossY: 3.4412)\n",
            "2025-10-23 22:27:36,644 - VAETrainer - INFO - Saving model and trainer state to vae_results/best_model.pth...\n",
            "INFO:VAETrainer:Saving model and trainer state to vae_results/best_model.pth...\n",
            "2025-10-23 22:27:36,658 - VAETrainer - INFO - Successfully saved model and trainer state to vae_results/best_model.pth\n",
            "INFO:VAETrainer:Successfully saved model and trainer state to vae_results/best_model.pth\n",
            "2025-10-23 22:27:38,757 - VAETrainer - INFO - Epoch [65/100] Train TotalLoss: 35.4780, (ReconX: 0.9914, KLD: 0.0537*1.00, LossY: 3.4433*10.00), Time: 1.7s | Val TotalLoss: 35.4359, (ReconX: 0.9763, KLD: 0.0516, LossY: 3.4408)\n",
            "INFO:VAETrainer:Epoch [65/100] Train TotalLoss: 35.4780, (ReconX: 0.9914, KLD: 0.0537*1.00, LossY: 3.4433*10.00), Time: 1.7s | Val TotalLoss: 35.4359, (ReconX: 0.9763, KLD: 0.0516, LossY: 3.4408)\n",
            "2025-10-23 22:27:38,759 - VAETrainer - INFO - Saving model and trainer state to vae_results/best_model.pth...\n",
            "INFO:VAETrainer:Saving model and trainer state to vae_results/best_model.pth...\n",
            "2025-10-23 22:27:38,772 - VAETrainer - INFO - Successfully saved model and trainer state to vae_results/best_model.pth\n",
            "INFO:VAETrainer:Successfully saved model and trainer state to vae_results/best_model.pth\n",
            "2025-10-23 22:27:40,894 - VAETrainer - INFO - Epoch [66/100] Train TotalLoss: 35.4491, (ReconX: 0.9917, KLD: 0.0527*1.00, LossY: 3.4405*10.00), Time: 1.7s | Val TotalLoss: 35.4408, (ReconX: 0.9765, KLD: 0.0526, LossY: 3.4412)\n",
            "INFO:VAETrainer:Epoch [66/100] Train TotalLoss: 35.4491, (ReconX: 0.9917, KLD: 0.0527*1.00, LossY: 3.4405*10.00), Time: 1.7s | Val TotalLoss: 35.4408, (ReconX: 0.9765, KLD: 0.0526, LossY: 3.4412)\n",
            "2025-10-23 22:27:43,074 - VAETrainer - INFO - Epoch [67/100] Train TotalLoss: 35.3032, (ReconX: 0.9917, KLD: 0.0518*1.00, LossY: 3.4260*10.00), Time: 1.8s | Val TotalLoss: 35.3002, (ReconX: 0.9741, KLD: 0.0530, LossY: 3.4273)\n",
            "INFO:VAETrainer:Epoch [67/100] Train TotalLoss: 35.3032, (ReconX: 0.9917, KLD: 0.0518*1.00, LossY: 3.4260*10.00), Time: 1.8s | Val TotalLoss: 35.3002, (ReconX: 0.9741, KLD: 0.0530, LossY: 3.4273)\n",
            "2025-10-23 22:27:43,076 - VAETrainer - INFO - Saving model and trainer state to vae_results/best_model.pth...\n",
            "INFO:VAETrainer:Saving model and trainer state to vae_results/best_model.pth...\n",
            "2025-10-23 22:27:43,090 - VAETrainer - INFO - Successfully saved model and trainer state to vae_results/best_model.pth\n",
            "INFO:VAETrainer:Successfully saved model and trainer state to vae_results/best_model.pth\n",
            "2025-10-23 22:27:45,316 - VAETrainer - INFO - Epoch [68/100] Train TotalLoss: 35.2685, (ReconX: 0.9923, KLD: 0.0515*1.00, LossY: 3.4225*10.00), Time: 1.8s | Val TotalLoss: 35.2730, (ReconX: 0.9762, KLD: 0.0500, LossY: 3.4247)\n",
            "INFO:VAETrainer:Epoch [68/100] Train TotalLoss: 35.2685, (ReconX: 0.9923, KLD: 0.0515*1.00, LossY: 3.4225*10.00), Time: 1.8s | Val TotalLoss: 35.2730, (ReconX: 0.9762, KLD: 0.0500, LossY: 3.4247)\n",
            "2025-10-23 22:27:45,318 - VAETrainer - INFO - Saving model and trainer state to vae_results/best_model.pth...\n",
            "INFO:VAETrainer:Saving model and trainer state to vae_results/best_model.pth...\n",
            "2025-10-23 22:27:45,331 - VAETrainer - INFO - Successfully saved model and trainer state to vae_results/best_model.pth\n",
            "INFO:VAETrainer:Successfully saved model and trainer state to vae_results/best_model.pth\n",
            "2025-10-23 22:27:47,435 - VAETrainer - INFO - Epoch [69/100] Train TotalLoss: 35.1877, (ReconX: 0.9917, KLD: 0.0514*1.00, LossY: 3.4145*10.00), Time: 1.7s | Val TotalLoss: 35.2750, (ReconX: 0.9753, KLD: 0.0522, LossY: 3.4248)\n",
            "INFO:VAETrainer:Epoch [69/100] Train TotalLoss: 35.1877, (ReconX: 0.9917, KLD: 0.0514*1.00, LossY: 3.4145*10.00), Time: 1.7s | Val TotalLoss: 35.2750, (ReconX: 0.9753, KLD: 0.0522, LossY: 3.4248)\n",
            "2025-10-23 22:27:49,536 - VAETrainer - INFO - Epoch [70/100] Train TotalLoss: 35.1897, (ReconX: 0.9922, KLD: 0.0516*1.00, LossY: 3.4146*10.00), Time: 1.7s | Val TotalLoss: 35.1027, (ReconX: 0.9758, KLD: 0.0503, LossY: 3.4077)\n",
            "INFO:VAETrainer:Epoch [70/100] Train TotalLoss: 35.1897, (ReconX: 0.9922, KLD: 0.0516*1.00, LossY: 3.4146*10.00), Time: 1.7s | Val TotalLoss: 35.1027, (ReconX: 0.9758, KLD: 0.0503, LossY: 3.4077)\n",
            "2025-10-23 22:27:49,538 - VAETrainer - INFO - Saving model and trainer state to vae_results/best_model.pth...\n",
            "INFO:VAETrainer:Saving model and trainer state to vae_results/best_model.pth...\n",
            "2025-10-23 22:27:49,551 - VAETrainer - INFO - Successfully saved model and trainer state to vae_results/best_model.pth\n",
            "INFO:VAETrainer:Successfully saved model and trainer state to vae_results/best_model.pth\n",
            "2025-10-23 22:27:51,657 - VAETrainer - INFO - Epoch [71/100] Train TotalLoss: 35.1708, (ReconX: 0.9915, KLD: 0.0515*1.00, LossY: 3.4128*10.00), Time: 1.7s | Val TotalLoss: 35.1291, (ReconX: 0.9758, KLD: 0.0495, LossY: 3.4104)\n",
            "INFO:VAETrainer:Epoch [71/100] Train TotalLoss: 35.1708, (ReconX: 0.9915, KLD: 0.0515*1.00, LossY: 3.4128*10.00), Time: 1.7s | Val TotalLoss: 35.1291, (ReconX: 0.9758, KLD: 0.0495, LossY: 3.4104)\n",
            "2025-10-23 22:27:53,817 - VAETrainer - INFO - Epoch [72/100] Train TotalLoss: 34.9966, (ReconX: 0.9923, KLD: 0.0506*1.00, LossY: 3.3954*10.00), Time: 1.8s | Val TotalLoss: 35.0277, (ReconX: 0.9758, KLD: 0.0495, LossY: 3.4002)\n",
            "INFO:VAETrainer:Epoch [72/100] Train TotalLoss: 34.9966, (ReconX: 0.9923, KLD: 0.0506*1.00, LossY: 3.3954*10.00), Time: 1.8s | Val TotalLoss: 35.0277, (ReconX: 0.9758, KLD: 0.0495, LossY: 3.4002)\n",
            "2025-10-23 22:27:53,819 - VAETrainer - INFO - Saving model and trainer state to vae_results/best_model.pth...\n",
            "INFO:VAETrainer:Saving model and trainer state to vae_results/best_model.pth...\n",
            "2025-10-23 22:27:53,832 - VAETrainer - INFO - Successfully saved model and trainer state to vae_results/best_model.pth\n",
            "INFO:VAETrainer:Successfully saved model and trainer state to vae_results/best_model.pth\n",
            "2025-10-23 22:27:56,043 - VAETrainer - INFO - Epoch [73/100] Train TotalLoss: 35.0056, (ReconX: 0.9920, KLD: 0.0500*1.00, LossY: 3.3964*10.00), Time: 1.8s | Val TotalLoss: 35.0205, (ReconX: 0.9776, KLD: 0.0496, LossY: 3.3993)\n",
            "INFO:VAETrainer:Epoch [73/100] Train TotalLoss: 35.0056, (ReconX: 0.9920, KLD: 0.0500*1.00, LossY: 3.3964*10.00), Time: 1.8s | Val TotalLoss: 35.0205, (ReconX: 0.9776, KLD: 0.0496, LossY: 3.3993)\n",
            "2025-10-23 22:27:56,045 - VAETrainer - INFO - Saving model and trainer state to vae_results/best_model.pth...\n",
            "INFO:VAETrainer:Saving model and trainer state to vae_results/best_model.pth...\n",
            "2025-10-23 22:27:56,058 - VAETrainer - INFO - Successfully saved model and trainer state to vae_results/best_model.pth\n",
            "INFO:VAETrainer:Successfully saved model and trainer state to vae_results/best_model.pth\n",
            "2025-10-23 22:27:58,251 - VAETrainer - INFO - Epoch [74/100] Train TotalLoss: 34.8963, (ReconX: 0.9926, KLD: 0.0492*1.00, LossY: 3.3854*10.00), Time: 1.8s | Val TotalLoss: 34.9286, (ReconX: 0.9760, KLD: 0.0475, LossY: 3.3905)\n",
            "INFO:VAETrainer:Epoch [74/100] Train TotalLoss: 34.8963, (ReconX: 0.9926, KLD: 0.0492*1.00, LossY: 3.3854*10.00), Time: 1.8s | Val TotalLoss: 34.9286, (ReconX: 0.9760, KLD: 0.0475, LossY: 3.3905)\n",
            "2025-10-23 22:27:58,253 - VAETrainer - INFO - Saving model and trainer state to vae_results/best_model.pth...\n",
            "INFO:VAETrainer:Saving model and trainer state to vae_results/best_model.pth...\n",
            "2025-10-23 22:27:58,266 - VAETrainer - INFO - Successfully saved model and trainer state to vae_results/best_model.pth\n",
            "INFO:VAETrainer:Successfully saved model and trainer state to vae_results/best_model.pth\n",
            "2025-10-23 22:28:00,427 - VAETrainer - INFO - Epoch [75/100] Train TotalLoss: 34.8853, (ReconX: 0.9928, KLD: 0.0486*1.00, LossY: 3.3844*10.00), Time: 1.8s | Val TotalLoss: 34.8507, (ReconX: 0.9769, KLD: 0.0477, LossY: 3.3826)\n",
            "INFO:VAETrainer:Epoch [75/100] Train TotalLoss: 34.8853, (ReconX: 0.9928, KLD: 0.0486*1.00, LossY: 3.3844*10.00), Time: 1.8s | Val TotalLoss: 34.8507, (ReconX: 0.9769, KLD: 0.0477, LossY: 3.3826)\n",
            "2025-10-23 22:28:00,429 - VAETrainer - INFO - Saving model and trainer state to vae_results/best_model.pth...\n",
            "INFO:VAETrainer:Saving model and trainer state to vae_results/best_model.pth...\n",
            "2025-10-23 22:28:00,443 - VAETrainer - INFO - Successfully saved model and trainer state to vae_results/best_model.pth\n",
            "INFO:VAETrainer:Successfully saved model and trainer state to vae_results/best_model.pth\n",
            "2025-10-23 22:28:02,509 - VAETrainer - INFO - Epoch [76/100] Train TotalLoss: 34.8222, (ReconX: 0.9926, KLD: 0.0478*1.00, LossY: 3.3782*10.00), Time: 1.7s | Val TotalLoss: 34.8147, (ReconX: 0.9783, KLD: 0.0481, LossY: 3.3788)\n",
            "INFO:VAETrainer:Epoch [76/100] Train TotalLoss: 34.8222, (ReconX: 0.9926, KLD: 0.0478*1.00, LossY: 3.3782*10.00), Time: 1.7s | Val TotalLoss: 34.8147, (ReconX: 0.9783, KLD: 0.0481, LossY: 3.3788)\n",
            "2025-10-23 22:28:02,511 - VAETrainer - INFO - Saving model and trainer state to vae_results/best_model.pth...\n",
            "INFO:VAETrainer:Saving model and trainer state to vae_results/best_model.pth...\n",
            "2025-10-23 22:28:02,527 - VAETrainer - INFO - Successfully saved model and trainer state to vae_results/best_model.pth\n",
            "INFO:VAETrainer:Successfully saved model and trainer state to vae_results/best_model.pth\n",
            "2025-10-23 22:28:04,668 - VAETrainer - INFO - Epoch [77/100] Train TotalLoss: 34.7904, (ReconX: 0.9925, KLD: 0.0480*1.00, LossY: 3.3750*10.00), Time: 1.8s | Val TotalLoss: 34.7403, (ReconX: 0.9768, KLD: 0.0468, LossY: 3.3717)\n",
            "INFO:VAETrainer:Epoch [77/100] Train TotalLoss: 34.7904, (ReconX: 0.9925, KLD: 0.0480*1.00, LossY: 3.3750*10.00), Time: 1.8s | Val TotalLoss: 34.7403, (ReconX: 0.9768, KLD: 0.0468, LossY: 3.3717)\n",
            "2025-10-23 22:28:04,669 - VAETrainer - INFO - Saving model and trainer state to vae_results/best_model.pth...\n",
            "INFO:VAETrainer:Saving model and trainer state to vae_results/best_model.pth...\n",
            "2025-10-23 22:28:04,683 - VAETrainer - INFO - Successfully saved model and trainer state to vae_results/best_model.pth\n",
            "INFO:VAETrainer:Successfully saved model and trainer state to vae_results/best_model.pth\n",
            "2025-10-23 22:28:06,812 - VAETrainer - INFO - Epoch [78/100] Train TotalLoss: 34.7669, (ReconX: 0.9928, KLD: 0.0473*1.00, LossY: 3.3727*10.00), Time: 1.7s | Val TotalLoss: 35.1763, (ReconX: 0.9775, KLD: 0.0461, LossY: 3.4153)\n",
            "INFO:VAETrainer:Epoch [78/100] Train TotalLoss: 34.7669, (ReconX: 0.9928, KLD: 0.0473*1.00, LossY: 3.3727*10.00), Time: 1.7s | Val TotalLoss: 35.1763, (ReconX: 0.9775, KLD: 0.0461, LossY: 3.4153)\n",
            "2025-10-23 22:28:09,002 - VAETrainer - INFO - Epoch [79/100] Train TotalLoss: 34.7046, (ReconX: 0.9929, KLD: 0.0471*1.00, LossY: 3.3665*10.00), Time: 1.8s | Val TotalLoss: 34.7715, (ReconX: 0.9778, KLD: 0.0476, LossY: 3.3746)\n",
            "INFO:VAETrainer:Epoch [79/100] Train TotalLoss: 34.7046, (ReconX: 0.9929, KLD: 0.0471*1.00, LossY: 3.3665*10.00), Time: 1.8s | Val TotalLoss: 34.7715, (ReconX: 0.9778, KLD: 0.0476, LossY: 3.3746)\n",
            "2025-10-23 22:28:11,131 - VAETrainer - INFO - Epoch [80/100] Train TotalLoss: 34.6690, (ReconX: 0.9928, KLD: 0.0470*1.00, LossY: 3.3629*10.00), Time: 1.8s | Val TotalLoss: 34.6965, (ReconX: 0.9763, KLD: 0.0473, LossY: 3.3673)\n",
            "INFO:VAETrainer:Epoch [80/100] Train TotalLoss: 34.6690, (ReconX: 0.9928, KLD: 0.0470*1.00, LossY: 3.3629*10.00), Time: 1.8s | Val TotalLoss: 34.6965, (ReconX: 0.9763, KLD: 0.0473, LossY: 3.3673)\n",
            "2025-10-23 22:28:11,133 - VAETrainer - INFO - Saving model and trainer state to vae_results/best_model.pth...\n",
            "INFO:VAETrainer:Saving model and trainer state to vae_results/best_model.pth...\n",
            "2025-10-23 22:28:11,146 - VAETrainer - INFO - Successfully saved model and trainer state to vae_results/best_model.pth\n",
            "INFO:VAETrainer:Successfully saved model and trainer state to vae_results/best_model.pth\n",
            "2025-10-23 22:28:13,235 - VAETrainer - INFO - Epoch [81/100] Train TotalLoss: 34.6095, (ReconX: 0.9927, KLD: 0.0475*1.00, LossY: 3.3569*10.00), Time: 1.7s | Val TotalLoss: 34.5658, (ReconX: 0.9767, KLD: 0.0474, LossY: 3.3542)\n",
            "INFO:VAETrainer:Epoch [81/100] Train TotalLoss: 34.6095, (ReconX: 0.9927, KLD: 0.0475*1.00, LossY: 3.3569*10.00), Time: 1.7s | Val TotalLoss: 34.5658, (ReconX: 0.9767, KLD: 0.0474, LossY: 3.3542)\n",
            "2025-10-23 22:28:13,237 - VAETrainer - INFO - Saving model and trainer state to vae_results/best_model.pth...\n",
            "INFO:VAETrainer:Saving model and trainer state to vae_results/best_model.pth...\n",
            "2025-10-23 22:28:13,251 - VAETrainer - INFO - Successfully saved model and trainer state to vae_results/best_model.pth\n",
            "INFO:VAETrainer:Successfully saved model and trainer state to vae_results/best_model.pth\n",
            "2025-10-23 22:28:15,297 - VAETrainer - INFO - Epoch [82/100] Train TotalLoss: 34.5517, (ReconX: 0.9927, KLD: 0.0472*1.00, LossY: 3.3512*10.00), Time: 1.7s | Val TotalLoss: 34.5307, (ReconX: 0.9764, KLD: 0.0474, LossY: 3.3507)\n",
            "INFO:VAETrainer:Epoch [82/100] Train TotalLoss: 34.5517, (ReconX: 0.9927, KLD: 0.0472*1.00, LossY: 3.3512*10.00), Time: 1.7s | Val TotalLoss: 34.5307, (ReconX: 0.9764, KLD: 0.0474, LossY: 3.3507)\n",
            "2025-10-23 22:28:15,298 - VAETrainer - INFO - Saving model and trainer state to vae_results/best_model.pth...\n",
            "INFO:VAETrainer:Saving model and trainer state to vae_results/best_model.pth...\n",
            "2025-10-23 22:28:15,311 - VAETrainer - INFO - Successfully saved model and trainer state to vae_results/best_model.pth\n",
            "INFO:VAETrainer:Successfully saved model and trainer state to vae_results/best_model.pth\n",
            "2025-10-23 22:28:17,457 - VAETrainer - INFO - Epoch [83/100] Train TotalLoss: 34.5531, (ReconX: 0.9935, KLD: 0.0459*1.00, LossY: 3.3514*10.00), Time: 1.8s | Val TotalLoss: 34.5210, (ReconX: 0.9773, KLD: 0.0466, LossY: 3.3497)\n",
            "INFO:VAETrainer:Epoch [83/100] Train TotalLoss: 34.5531, (ReconX: 0.9935, KLD: 0.0459*1.00, LossY: 3.3514*10.00), Time: 1.8s | Val TotalLoss: 34.5210, (ReconX: 0.9773, KLD: 0.0466, LossY: 3.3497)\n",
            "2025-10-23 22:28:17,459 - VAETrainer - INFO - Saving model and trainer state to vae_results/best_model.pth...\n",
            "INFO:VAETrainer:Saving model and trainer state to vae_results/best_model.pth...\n",
            "2025-10-23 22:28:17,473 - VAETrainer - INFO - Successfully saved model and trainer state to vae_results/best_model.pth\n",
            "INFO:VAETrainer:Successfully saved model and trainer state to vae_results/best_model.pth\n",
            "2025-10-23 22:28:19,687 - VAETrainer - INFO - Epoch [84/100] Train TotalLoss: 34.5091, (ReconX: 0.9926, KLD: 0.0465*1.00, LossY: 3.3470*10.00), Time: 1.8s | Val TotalLoss: 34.6533, (ReconX: 0.9766, KLD: 0.0449, LossY: 3.3632)\n",
            "INFO:VAETrainer:Epoch [84/100] Train TotalLoss: 34.5091, (ReconX: 0.9926, KLD: 0.0465*1.00, LossY: 3.3470*10.00), Time: 1.8s | Val TotalLoss: 34.6533, (ReconX: 0.9766, KLD: 0.0449, LossY: 3.3632)\n",
            "2025-10-23 22:28:21,817 - VAETrainer - INFO - Epoch [85/100] Train TotalLoss: 34.5321, (ReconX: 0.9929, KLD: 0.0461*1.00, LossY: 3.3493*10.00), Time: 1.7s | Val TotalLoss: 34.4792, (ReconX: 0.9764, KLD: 0.0445, LossY: 3.3458)\n",
            "INFO:VAETrainer:Epoch [85/100] Train TotalLoss: 34.5321, (ReconX: 0.9929, KLD: 0.0461*1.00, LossY: 3.3493*10.00), Time: 1.7s | Val TotalLoss: 34.4792, (ReconX: 0.9764, KLD: 0.0445, LossY: 3.3458)\n",
            "2025-10-23 22:28:21,819 - VAETrainer - INFO - Saving model and trainer state to vae_results/best_model.pth...\n",
            "INFO:VAETrainer:Saving model and trainer state to vae_results/best_model.pth...\n",
            "2025-10-23 22:28:21,832 - VAETrainer - INFO - Successfully saved model and trainer state to vae_results/best_model.pth\n",
            "INFO:VAETrainer:Successfully saved model and trainer state to vae_results/best_model.pth\n",
            "2025-10-23 22:28:24,074 - VAETrainer - INFO - Epoch [86/100] Train TotalLoss: 34.4432, (ReconX: 0.9933, KLD: 0.0443*1.00, LossY: 3.3406*10.00), Time: 1.9s | Val TotalLoss: 34.3743, (ReconX: 0.9767, KLD: 0.0447, LossY: 3.3353)\n",
            "INFO:VAETrainer:Epoch [86/100] Train TotalLoss: 34.4432, (ReconX: 0.9933, KLD: 0.0443*1.00, LossY: 3.3406*10.00), Time: 1.9s | Val TotalLoss: 34.3743, (ReconX: 0.9767, KLD: 0.0447, LossY: 3.3353)\n",
            "2025-10-23 22:28:24,076 - VAETrainer - INFO - Saving model and trainer state to vae_results/best_model.pth...\n",
            "INFO:VAETrainer:Saving model and trainer state to vae_results/best_model.pth...\n",
            "2025-10-23 22:28:24,089 - VAETrainer - INFO - Successfully saved model and trainer state to vae_results/best_model.pth\n",
            "INFO:VAETrainer:Successfully saved model and trainer state to vae_results/best_model.pth\n",
            "2025-10-23 22:28:26,123 - VAETrainer - INFO - Epoch [87/100] Train TotalLoss: 34.3808, (ReconX: 0.9935, KLD: 0.0442*1.00, LossY: 3.3343*10.00), Time: 1.7s | Val TotalLoss: 34.4545, (ReconX: 0.9774, KLD: 0.0434, LossY: 3.3434)\n",
            "INFO:VAETrainer:Epoch [87/100] Train TotalLoss: 34.3808, (ReconX: 0.9935, KLD: 0.0442*1.00, LossY: 3.3343*10.00), Time: 1.7s | Val TotalLoss: 34.4545, (ReconX: 0.9774, KLD: 0.0434, LossY: 3.3434)\n",
            "2025-10-23 22:28:28,289 - VAETrainer - INFO - Epoch [88/100] Train TotalLoss: 34.3519, (ReconX: 0.9934, KLD: 0.0444*1.00, LossY: 3.3314*10.00), Time: 1.8s | Val TotalLoss: 34.4089, (ReconX: 0.9780, KLD: 0.0446, LossY: 3.3386)\n",
            "INFO:VAETrainer:Epoch [88/100] Train TotalLoss: 34.3519, (ReconX: 0.9934, KLD: 0.0444*1.00, LossY: 3.3314*10.00), Time: 1.8s | Val TotalLoss: 34.4089, (ReconX: 0.9780, KLD: 0.0446, LossY: 3.3386)\n",
            "2025-10-23 22:28:30,405 - VAETrainer - INFO - Epoch [89/100] Train TotalLoss: 34.3651, (ReconX: 0.9934, KLD: 0.0444*1.00, LossY: 3.3327*10.00), Time: 1.7s | Val TotalLoss: 34.3338, (ReconX: 0.9756, KLD: 0.0446, LossY: 3.3314)\n",
            "INFO:VAETrainer:Epoch [89/100] Train TotalLoss: 34.3651, (ReconX: 0.9934, KLD: 0.0444*1.00, LossY: 3.3327*10.00), Time: 1.7s | Val TotalLoss: 34.3338, (ReconX: 0.9756, KLD: 0.0446, LossY: 3.3314)\n",
            "2025-10-23 22:28:30,406 - VAETrainer - INFO - Saving model and trainer state to vae_results/best_model.pth...\n",
            "INFO:VAETrainer:Saving model and trainer state to vae_results/best_model.pth...\n",
            "2025-10-23 22:28:30,420 - VAETrainer - INFO - Successfully saved model and trainer state to vae_results/best_model.pth\n",
            "INFO:VAETrainer:Successfully saved model and trainer state to vae_results/best_model.pth\n",
            "2025-10-23 22:28:32,622 - VAETrainer - INFO - Epoch [90/100] Train TotalLoss: 34.3379, (ReconX: 0.9937, KLD: 0.0441*1.00, LossY: 3.3300*10.00), Time: 1.8s | Val TotalLoss: 34.3118, (ReconX: 0.9773, KLD: 0.0426, LossY: 3.3292)\n",
            "INFO:VAETrainer:Epoch [90/100] Train TotalLoss: 34.3379, (ReconX: 0.9937, KLD: 0.0441*1.00, LossY: 3.3300*10.00), Time: 1.8s | Val TotalLoss: 34.3118, (ReconX: 0.9773, KLD: 0.0426, LossY: 3.3292)\n",
            "2025-10-23 22:28:32,625 - VAETrainer - INFO - Saving model and trainer state to vae_results/best_model.pth...\n",
            "INFO:VAETrainer:Saving model and trainer state to vae_results/best_model.pth...\n",
            "2025-10-23 22:28:32,638 - VAETrainer - INFO - Successfully saved model and trainer state to vae_results/best_model.pth\n",
            "INFO:VAETrainer:Successfully saved model and trainer state to vae_results/best_model.pth\n",
            "2025-10-23 22:28:34,752 - VAETrainer - INFO - Epoch [91/100] Train TotalLoss: 34.3219, (ReconX: 0.9933, KLD: 0.0428*1.00, LossY: 3.3286*10.00), Time: 1.7s | Val TotalLoss: 34.3173, (ReconX: 0.9780, KLD: 0.0441, LossY: 3.3295)\n",
            "INFO:VAETrainer:Epoch [91/100] Train TotalLoss: 34.3219, (ReconX: 0.9933, KLD: 0.0428*1.00, LossY: 3.3286*10.00), Time: 1.7s | Val TotalLoss: 34.3173, (ReconX: 0.9780, KLD: 0.0441, LossY: 3.3295)\n",
            "2025-10-23 22:28:36,886 - VAETrainer - INFO - Epoch [92/100] Train TotalLoss: 34.2520, (ReconX: 0.9938, KLD: 0.0437*1.00, LossY: 3.3215*10.00), Time: 1.8s | Val TotalLoss: 34.2224, (ReconX: 0.9773, KLD: 0.0422, LossY: 3.3203)\n",
            "INFO:VAETrainer:Epoch [92/100] Train TotalLoss: 34.2520, (ReconX: 0.9938, KLD: 0.0437*1.00, LossY: 3.3215*10.00), Time: 1.8s | Val TotalLoss: 34.2224, (ReconX: 0.9773, KLD: 0.0422, LossY: 3.3203)\n",
            "2025-10-23 22:28:36,888 - VAETrainer - INFO - Saving model and trainer state to vae_results/best_model.pth...\n",
            "INFO:VAETrainer:Saving model and trainer state to vae_results/best_model.pth...\n",
            "2025-10-23 22:28:36,901 - VAETrainer - INFO - Successfully saved model and trainer state to vae_results/best_model.pth\n",
            "INFO:VAETrainer:Successfully saved model and trainer state to vae_results/best_model.pth\n",
            "2025-10-23 22:28:39,068 - VAETrainer - INFO - Epoch [93/100] Train TotalLoss: 34.2606, (ReconX: 0.9937, KLD: 0.0426*1.00, LossY: 3.3224*10.00), Time: 1.8s | Val TotalLoss: 34.2391, (ReconX: 0.9783, KLD: 0.0430, LossY: 3.3218)\n",
            "INFO:VAETrainer:Epoch [93/100] Train TotalLoss: 34.2606, (ReconX: 0.9937, KLD: 0.0426*1.00, LossY: 3.3224*10.00), Time: 1.8s | Val TotalLoss: 34.2391, (ReconX: 0.9783, KLD: 0.0430, LossY: 3.3218)\n",
            "2025-10-23 22:28:41,182 - VAETrainer - INFO - Epoch [94/100] Train TotalLoss: 34.2170, (ReconX: 0.9940, KLD: 0.0431*1.00, LossY: 3.3180*10.00), Time: 1.7s | Val TotalLoss: 34.4273, (ReconX: 0.9777, KLD: 0.0414, LossY: 3.3408)\n",
            "INFO:VAETrainer:Epoch [94/100] Train TotalLoss: 34.2170, (ReconX: 0.9940, KLD: 0.0431*1.00, LossY: 3.3180*10.00), Time: 1.7s | Val TotalLoss: 34.4273, (ReconX: 0.9777, KLD: 0.0414, LossY: 3.3408)\n",
            "2025-10-23 22:28:43,304 - VAETrainer - INFO - Epoch [95/100] Train TotalLoss: 34.1412, (ReconX: 0.9933, KLD: 0.0423*1.00, LossY: 3.3106*10.00), Time: 1.7s | Val TotalLoss: 34.3325, (ReconX: 0.9778, KLD: 0.0425, LossY: 3.3312)\n",
            "INFO:VAETrainer:Epoch [95/100] Train TotalLoss: 34.1412, (ReconX: 0.9933, KLD: 0.0423*1.00, LossY: 3.3106*10.00), Time: 1.7s | Val TotalLoss: 34.3325, (ReconX: 0.9778, KLD: 0.0425, LossY: 3.3312)\n",
            "2025-10-23 22:28:45,450 - VAETrainer - INFO - Epoch [96/100] Train TotalLoss: 34.1314, (ReconX: 0.9937, KLD: 0.0418*1.00, LossY: 3.3096*10.00), Time: 1.8s | Val TotalLoss: 34.3482, (ReconX: 0.9773, KLD: 0.0424, LossY: 3.3328)\n",
            "INFO:VAETrainer:Epoch [96/100] Train TotalLoss: 34.1314, (ReconX: 0.9937, KLD: 0.0418*1.00, LossY: 3.3096*10.00), Time: 1.8s | Val TotalLoss: 34.3482, (ReconX: 0.9773, KLD: 0.0424, LossY: 3.3328)\n",
            "2025-10-23 22:28:47,524 - VAETrainer - INFO - Epoch [97/100] Train TotalLoss: 34.0273, (ReconX: 0.9934, KLD: 0.0415*1.00, LossY: 3.2992*10.00), Time: 1.7s | Val TotalLoss: 34.0940, (ReconX: 0.9779, KLD: 0.0401, LossY: 3.3076)\n",
            "INFO:VAETrainer:Epoch [97/100] Train TotalLoss: 34.0273, (ReconX: 0.9934, KLD: 0.0415*1.00, LossY: 3.2992*10.00), Time: 1.7s | Val TotalLoss: 34.0940, (ReconX: 0.9779, KLD: 0.0401, LossY: 3.3076)\n",
            "2025-10-23 22:28:47,526 - VAETrainer - INFO - Saving model and trainer state to vae_results/best_model.pth...\n",
            "INFO:VAETrainer:Saving model and trainer state to vae_results/best_model.pth...\n",
            "2025-10-23 22:28:47,541 - VAETrainer - INFO - Successfully saved model and trainer state to vae_results/best_model.pth\n",
            "INFO:VAETrainer:Successfully saved model and trainer state to vae_results/best_model.pth\n",
            "2025-10-23 22:28:49,601 - VAETrainer - INFO - Epoch [98/100] Train TotalLoss: 34.0460, (ReconX: 0.9942, KLD: 0.0404*1.00, LossY: 3.3011*10.00), Time: 1.7s | Val TotalLoss: 34.0286, (ReconX: 0.9772, KLD: 0.0413, LossY: 3.3010)\n",
            "INFO:VAETrainer:Epoch [98/100] Train TotalLoss: 34.0460, (ReconX: 0.9942, KLD: 0.0404*1.00, LossY: 3.3011*10.00), Time: 1.7s | Val TotalLoss: 34.0286, (ReconX: 0.9772, KLD: 0.0413, LossY: 3.3010)\n",
            "2025-10-23 22:28:49,602 - VAETrainer - INFO - Saving model and trainer state to vae_results/best_model.pth...\n",
            "INFO:VAETrainer:Saving model and trainer state to vae_results/best_model.pth...\n",
            "2025-10-23 22:28:49,616 - VAETrainer - INFO - Successfully saved model and trainer state to vae_results/best_model.pth\n",
            "INFO:VAETrainer:Successfully saved model and trainer state to vae_results/best_model.pth\n",
            "2025-10-23 22:28:51,726 - VAETrainer - INFO - Epoch [99/100] Train TotalLoss: 33.9933, (ReconX: 0.9941, KLD: 0.0412*1.00, LossY: 3.2958*10.00), Time: 1.7s | Val TotalLoss: 34.0343, (ReconX: 0.9787, KLD: 0.0406, LossY: 3.3015)\n",
            "INFO:VAETrainer:Epoch [99/100] Train TotalLoss: 33.9933, (ReconX: 0.9941, KLD: 0.0412*1.00, LossY: 3.2958*10.00), Time: 1.7s | Val TotalLoss: 34.0343, (ReconX: 0.9787, KLD: 0.0406, LossY: 3.3015)\n",
            "2025-10-23 22:28:53,844 - VAETrainer - INFO - Epoch [100/100] Train TotalLoss: 33.9541, (ReconX: 0.9943, KLD: 0.0403*1.00, LossY: 3.2920*10.00), Time: 1.7s | Val TotalLoss: 34.0251, (ReconX: 0.9793, KLD: 0.0390, LossY: 3.3007)\n",
            "INFO:VAETrainer:Epoch [100/100] Train TotalLoss: 33.9541, (ReconX: 0.9943, KLD: 0.0403*1.00, LossY: 3.2920*10.00), Time: 1.7s | Val TotalLoss: 34.0251, (ReconX: 0.9793, KLD: 0.0390, LossY: 3.3007)\n",
            "2025-10-23 22:28:53,846 - VAETrainer - INFO - Saving model and trainer state to vae_results/best_model.pth...\n",
            "INFO:VAETrainer:Saving model and trainer state to vae_results/best_model.pth...\n",
            "2025-10-23 22:28:53,859 - VAETrainer - INFO - Successfully saved model and trainer state to vae_results/best_model.pth\n",
            "INFO:VAETrainer:Successfully saved model and trainer state to vae_results/best_model.pth\n",
            "2025-10-23 22:28:53,861 - VAETrainer - INFO - Training loop finished.\n",
            "INFO:VAETrainer:Training loop finished.\n",
            "2025-10-23 22:28:53,862 - VAETrainer - INFO - Loading best model state from 'vae_results/best_model.pth' (Val Loss: 34.0251).\n",
            "INFO:VAETrainer:Loading best model state from 'vae_results/best_model.pth' (Val Loss: 34.0251).\n",
            "2025-10-23 22:28:53,863 - VAETrainer_Loader - INFO - Loading VAE model and trainer state from vae_results/best_model.pth to device cuda...\n",
            "INFO:VAETrainer_Loader:Loading VAE model and trainer state from vae_results/best_model.pth to device cuda...\n",
            "2025-10-23 22:28:53,903 - SemiSupMIWAE(X:42,Y:1,Lat:3) - INFO - SemiSupMIWAE Initialized: InputXDim=42, TargetYDim=1, LatentDim=3, AlphaPriceLoss=10.0, Device=cuda\n",
            "INFO:SemiSupMIWAE(X:42,Y:1,Lat:3):SemiSupMIWAE Initialized: InputXDim=42, TargetYDim=1, LatentDim=3, AlphaPriceLoss=10.0, Device=cuda\n",
            "2025-10-23 22:28:53,906 - VAETrainer_Loader - INFO - SemiSupMIWAE model created and state loaded successfully.\n",
            "INFO:VAETrainer_Loader:SemiSupMIWAE model created and state loaded successfully.\n",
            "2025-10-23 22:28:53,908 - VAETrainer - INFO - VAETrainer initialized for SemiSupMIWAE. Device: cuda. Optimizer: adam. LR: 0.0003. ReconLoss(X): 'mse'. Alpha(Y Loss): 10.0. LossType(Y): 'gaussian_nll'\n",
            "INFO:VAETrainer:VAETrainer initialized for SemiSupMIWAE. Device: cuda. Optimizer: adam. LR: 0.0003. ReconLoss(X): 'mse'. Alpha(Y Loss): 10.0. LossType(Y): 'gaussian_nll'\n",
            "2025-10-23 22:28:53,910 - VAETrainer_Loader - INFO - VAETrainer instance created and state loaded for model from vae_results/best_model.pth.\n",
            "INFO:VAETrainer_Loader:VAETrainer instance created and state loaded for model from vae_results/best_model.pth.\n",
            "2025-10-23 22:28:53,924 - VAETrainer - INFO - Predicting target (Y) mean and log-variance for 12272 samples...\n",
            "INFO:VAETrainer:Predicting target (Y) mean and log-variance for 12272 samples...\n",
            "2025-10-23 22:28:54,012 - MainRealEstateAnalysis_MIWAE - INFO - [Fold 1] Stored test predictions for 12272 rows.\n",
            "INFO:MainRealEstateAnalysis_MIWAE:[Fold 1] Stored test predictions for 12272 rows.\n",
            "2025-10-23 22:28:54,026 - MainRealEstateAnalysis_MIWAE - INFO - [Fold 2/10] sizes → Train=98169 (80.0%), Val=12271 (10.0%), Test=12272 (10.0%)\n",
            "INFO:MainRealEstateAnalysis_MIWAE:[Fold 2/10] sizes → Train=98169 (80.0%), Val=12271 (10.0%), Test=12272 (10.0%)\n",
            "2025-10-23 22:28:54,027 - __main__.create_vae_from_artifacts - INFO - Creating VAE model. Target column for supervision: log_sale_price.\n",
            "2025-10-23 22:28:54,027 - __main__.create_vae_from_artifacts - INFO - Creating VAE model. Target column for supervision: log_sale_price.\n",
            "INFO:__main__.create_vae_from_artifacts:Creating VAE model. Target column for supervision: log_sale_price.\n",
            "2025-10-23 22:28:54,028 - __main__.create_vae_from_artifacts - INFO - Number of input features for VAE (X): 42 (Features: ['lotarea', 'assess_total', 'exempt_total', 'assessland', 'exemptland']...)\n",
            "2025-10-23 22:28:54,028 - __main__.create_vae_from_artifacts - INFO - Number of input features for VAE (X): 42 (Features: ['lotarea', 'assess_total', 'exempt_total', 'assessland', 'exemptland']...)\n",
            "INFO:__main__.create_vae_from_artifacts:Number of input features for VAE (X): 42 (Features: ['lotarea', 'assess_total', 'exempt_total', 'assessland', 'exemptland']...)\n",
            "2025-10-23 22:28:54,030 - __main__.create_vae_from_artifacts - INFO - Semi-supervised mode: Target 'log_sale_price' (y_dim=1). Alpha for price loss: 10.0.\n",
            "2025-10-23 22:28:54,030 - __main__.create_vae_from_artifacts - INFO - Semi-supervised mode: Target 'log_sale_price' (y_dim=1). Alpha for price loss: 10.0.\n",
            "INFO:__main__.create_vae_from_artifacts:Semi-supervised mode: Target 'log_sale_price' (y_dim=1). Alpha for price loss: 10.0.\n",
            "2025-10-23 22:28:54,031 - __main__.create_vae_from_artifacts - INFO - Using overridden latent_dim: 3\n",
            "2025-10-23 22:28:54,031 - __main__.create_vae_from_artifacts - INFO - Using overridden latent_dim: 3\n",
            "INFO:__main__.create_vae_from_artifacts:Using overridden latent_dim: 3\n",
            "2025-10-23 22:28:54,033 - __main__.create_vae_from_artifacts - INFO - Final effective Latent Dimension: 3\n",
            "2025-10-23 22:28:54,033 - __main__.create_vae_from_artifacts - INFO - Final effective Latent Dimension: 3\n",
            "INFO:__main__.create_vae_from_artifacts:Final effective Latent Dimension: 3\n",
            "2025-10-23 22:28:54,034 - __main__.create_vae_from_artifacts - INFO - Encoder layers: [21], Decoder layers: [21]\n",
            "2025-10-23 22:28:54,034 - __main__.create_vae_from_artifacts - INFO - Encoder layers: [21], Decoder layers: [21]\n",
            "INFO:__main__.create_vae_from_artifacts:Encoder layers: [21], Decoder layers: [21]\n",
            "2025-10-23 22:28:54,036 - __main__.create_vae_from_artifacts - INFO - Using determined prior: Type=student_t_mixture, K=2 (from overrides or artifact inference).\n",
            "2025-10-23 22:28:54,036 - __main__.create_vae_from_artifacts - INFO - Using determined prior: Type=student_t_mixture, K=2 (from overrides or artifact inference).\n",
            "INFO:__main__.create_vae_from_artifacts:Using determined prior: Type=student_t_mixture, K=2 (from overrides or artifact inference).\n",
            "2025-10-23 22:28:54,037 - __main__.create_vae_from_artifacts - INFO - Initializing parameters for student_t_mixture with K=2 components...\n",
            "2025-10-23 22:28:54,037 - __main__.create_vae_from_artifacts - INFO - Initializing parameters for student_t_mixture with K=2 components...\n",
            "INFO:__main__.create_vae_from_artifacts:Initializing parameters for student_t_mixture with K=2 components...\n",
            "2025-10-23 22:28:54,038 - __main__.create_vae_from_artifacts - WARNING - DP-GMM artifacts are unsuitable or insufficient for initializing mixture prior parameters. Default parameters (e.g., zeros for means/logits, identity for covariances) will be used by the VAE.\n",
            "2025-10-23 22:28:54,038 - __main__.create_vae_from_artifacts - WARNING - DP-GMM artifacts are unsuitable or insufficient for initializing mixture prior parameters. Default parameters (e.g., zeros for means/logits, identity for covariances) will be used by the VAE.\n",
            "WARNING:__main__.create_vae_from_artifacts:DP-GMM artifacts are unsuitable or insufficient for initializing mixture prior parameters. Default parameters (e.g., zeros for means/logits, identity for covariances) will be used by the VAE.\n",
            "2025-10-23 22:28:54,039 - __main__.create_vae_from_artifacts - INFO - Setting Student-t df to 4.0 for prior.\n",
            "2025-10-23 22:28:54,039 - __main__.create_vae_from_artifacts - INFO - Setting Student-t df to 4.0 for prior.\n",
            "INFO:__main__.create_vae_from_artifacts:Setting Student-t df to 4.0 for prior.\n",
            "2025-10-23 22:28:54,040 - __main__.create_vae_from_artifacts - INFO - Instantiating SemiSupMIWAE model...\n",
            "2025-10-23 22:28:54,040 - __main__.create_vae_from_artifacts - INFO - Instantiating SemiSupMIWAE model...\n",
            "INFO:__main__.create_vae_from_artifacts:Instantiating SemiSupMIWAE model...\n",
            "2025-10-23 22:28:54,042 - __main__.create_vae_from_artifacts - INFO - Price head layers for SemiSupMIWAE: [8, 4]\n",
            "2025-10-23 22:28:54,042 - __main__.create_vae_from_artifacts - INFO - Price head layers for SemiSupMIWAE: [8, 4]\n",
            "INFO:__main__.create_vae_from_artifacts:Price head layers for SemiSupMIWAE: [8, 4]\n",
            "2025-10-23 22:28:54,050 - SemiSupMIWAE(X:42,Y:1,Lat:3) - INFO - SemiSupMIWAE Initialized: InputXDim=42, TargetYDim=1, LatentDim=3, AlphaPriceLoss=10.0, Device=cuda\n",
            "INFO:SemiSupMIWAE(X:42,Y:1,Lat:3):SemiSupMIWAE Initialized: InputXDim=42, TargetYDim=1, LatentDim=3, AlphaPriceLoss=10.0, Device=cuda\n",
            "2025-10-23 22:28:54,050 - __main__.create_vae_from_artifacts - INFO - SemiSupMIWAE model created successfully on device: cuda.\n",
            "2025-10-23 22:28:54,050 - __main__.create_vae_from_artifacts - INFO - SemiSupMIWAE model created successfully on device: cuda.\n",
            "INFO:__main__.create_vae_from_artifacts:SemiSupMIWAE model created successfully on device: cuda.\n",
            "2025-10-23 22:28:54,053 - VAETrainer - INFO - VAETrainer initialized for SemiSupMIWAE. Device: cuda. Optimizer: adam. LR: 0.0003. ReconLoss(X): 'mse'. Alpha(Y Loss): 10.0. LossType(Y): 'gaussian_nll'\n",
            "INFO:VAETrainer:VAETrainer initialized for SemiSupMIWAE. Device: cuda. Optimizer: adam. LR: 0.0003. ReconLoss(X): 'mse'. Alpha(Y Loss): 10.0. LossType(Y): 'gaussian_nll'\n",
            "2025-10-23 22:28:54,054 - VAETrainer - INFO - Starting training: Epochs=100, BatchSize=512, KLD_weight=1.0, KLD_AnnealEpochs=30, Alpha(Y_Loss)=10.0.\n",
            "INFO:VAETrainer:Starting training: Epochs=100, BatchSize=512, KLD_weight=1.0, KLD_AnnealEpochs=30, Alpha(Y_Loss)=10.0.\n",
            "2025-10-23 22:28:54,054 - VAETrainer - INFO - Train loader: 98169 samples, 192 batches. Drop last: False\n",
            "INFO:VAETrainer:Train loader: 98169 samples, 192 batches. Drop last: False\n",
            "2025-10-23 22:28:54,055 - VAETrainer - INFO - Validation loader: 12271 samples, 24 batches.\n",
            "INFO:VAETrainer:Validation loader: 12271 samples, 24 batches.\n",
            "2025-10-23 22:28:56,209 - VAETrainer - INFO - Epoch [1/100] Train TotalLoss: 495.2009, (ReconX: 1.0631, KLD: 0.4894*0.03, LossY: 49.4121*10.00), Time: 1.8s | Val TotalLoss: 404.9403, (ReconX: 0.9912, KLD: 0.5533, LossY: 40.3396)\n",
            "INFO:VAETrainer:Epoch [1/100] Train TotalLoss: 495.2009, (ReconX: 1.0631, KLD: 0.4894*0.03, LossY: 49.4121*10.00), Time: 1.8s | Val TotalLoss: 404.9403, (ReconX: 0.9912, KLD: 0.5533, LossY: 40.3396)\n",
            "2025-10-23 22:28:56,211 - VAETrainer - INFO - Saving model and trainer state to vae_results/best_model.pth...\n",
            "INFO:VAETrainer:Saving model and trainer state to vae_results/best_model.pth...\n",
            "2025-10-23 22:28:56,228 - VAETrainer - INFO - Successfully saved model and trainer state to vae_results/best_model.pth\n",
            "INFO:VAETrainer:Successfully saved model and trainer state to vae_results/best_model.pth\n",
            "2025-10-23 22:28:58,349 - VAETrainer - INFO - Epoch [2/100] Train TotalLoss: 343.9476, (ReconX: 0.9686, KLD: 0.5133*0.07, LossY: 34.2945*10.00), Time: 1.7s | Val TotalLoss: 274.9948, (ReconX: 0.9440, KLD: 0.5463, LossY: 27.3505)\n",
            "INFO:VAETrainer:Epoch [2/100] Train TotalLoss: 343.9476, (ReconX: 0.9686, KLD: 0.5133*0.07, LossY: 34.2945*10.00), Time: 1.7s | Val TotalLoss: 274.9948, (ReconX: 0.9440, KLD: 0.5463, LossY: 27.3505)\n",
            "2025-10-23 22:28:58,351 - VAETrainer - INFO - Saving model and trainer state to vae_results/best_model.pth...\n",
            "INFO:VAETrainer:Saving model and trainer state to vae_results/best_model.pth...\n",
            "2025-10-23 22:28:58,364 - VAETrainer - INFO - Successfully saved model and trainer state to vae_results/best_model.pth\n",
            "INFO:VAETrainer:Successfully saved model and trainer state to vae_results/best_model.pth\n",
            "2025-10-23 22:29:00,482 - VAETrainer - INFO - Epoch [3/100] Train TotalLoss: 245.3241, (ReconX: 0.9344, KLD: 0.5293*0.10, LossY: 24.4337*10.00), Time: 1.7s | Val TotalLoss: 212.0795, (ReconX: 0.9256, KLD: 0.5140, LossY: 21.0640)\n",
            "INFO:VAETrainer:Epoch [3/100] Train TotalLoss: 245.3241, (ReconX: 0.9344, KLD: 0.5293*0.10, LossY: 24.4337*10.00), Time: 1.7s | Val TotalLoss: 212.0795, (ReconX: 0.9256, KLD: 0.5140, LossY: 21.0640)\n",
            "2025-10-23 22:29:00,484 - VAETrainer - INFO - Saving model and trainer state to vae_results/best_model.pth...\n",
            "INFO:VAETrainer:Saving model and trainer state to vae_results/best_model.pth...\n",
            "2025-10-23 22:29:00,498 - VAETrainer - INFO - Successfully saved model and trainer state to vae_results/best_model.pth\n",
            "INFO:VAETrainer:Successfully saved model and trainer state to vae_results/best_model.pth\n",
            "2025-10-23 22:29:02,619 - VAETrainer - INFO - Epoch [4/100] Train TotalLoss: 189.4379, (ReconX: 0.9311, KLD: 0.4740*0.13, LossY: 18.8444*10.00), Time: 1.7s | Val TotalLoss: 166.3318, (ReconX: 0.9285, KLD: 0.4556, LossY: 16.4948)\n",
            "INFO:VAETrainer:Epoch [4/100] Train TotalLoss: 189.4379, (ReconX: 0.9311, KLD: 0.4740*0.13, LossY: 18.8444*10.00), Time: 1.7s | Val TotalLoss: 166.3318, (ReconX: 0.9285, KLD: 0.4556, LossY: 16.4948)\n",
            "2025-10-23 22:29:02,621 - VAETrainer - INFO - Saving model and trainer state to vae_results/best_model.pth...\n",
            "INFO:VAETrainer:Saving model and trainer state to vae_results/best_model.pth...\n",
            "2025-10-23 22:29:02,634 - VAETrainer - INFO - Successfully saved model and trainer state to vae_results/best_model.pth\n",
            "INFO:VAETrainer:Successfully saved model and trainer state to vae_results/best_model.pth\n",
            "2025-10-23 22:29:04,704 - VAETrainer - INFO - Epoch [5/100] Train TotalLoss: 148.5331, (ReconX: 0.9347, KLD: 0.4115*0.17, LossY: 14.7530*10.00), Time: 1.7s | Val TotalLoss: 132.3341, (ReconX: 0.9308, KLD: 0.3835, LossY: 13.1020)\n",
            "INFO:VAETrainer:Epoch [5/100] Train TotalLoss: 148.5331, (ReconX: 0.9347, KLD: 0.4115*0.17, LossY: 14.7530*10.00), Time: 1.7s | Val TotalLoss: 132.3341, (ReconX: 0.9308, KLD: 0.3835, LossY: 13.1020)\n",
            "2025-10-23 22:29:04,705 - VAETrainer - INFO - Saving model and trainer state to vae_results/best_model.pth...\n",
            "INFO:VAETrainer:Saving model and trainer state to vae_results/best_model.pth...\n",
            "2025-10-23 22:29:04,719 - VAETrainer - INFO - Successfully saved model and trainer state to vae_results/best_model.pth\n",
            "INFO:VAETrainer:Successfully saved model and trainer state to vae_results/best_model.pth\n",
            "2025-10-23 22:29:06,770 - VAETrainer - INFO - Epoch [6/100] Train TotalLoss: 119.7701, (ReconX: 0.9367, KLD: 0.3680*0.20, LossY: 11.8760*10.00), Time: 1.7s | Val TotalLoss: 108.7032, (ReconX: 0.9332, KLD: 0.3503, LossY: 10.7420)\n",
            "INFO:VAETrainer:Epoch [6/100] Train TotalLoss: 119.7701, (ReconX: 0.9367, KLD: 0.3680*0.20, LossY: 11.8760*10.00), Time: 1.7s | Val TotalLoss: 108.7032, (ReconX: 0.9332, KLD: 0.3503, LossY: 10.7420)\n",
            "2025-10-23 22:29:06,772 - VAETrainer - INFO - Saving model and trainer state to vae_results/best_model.pth...\n",
            "INFO:VAETrainer:Saving model and trainer state to vae_results/best_model.pth...\n",
            "2025-10-23 22:29:06,785 - VAETrainer - INFO - Successfully saved model and trainer state to vae_results/best_model.pth\n",
            "INFO:VAETrainer:Successfully saved model and trainer state to vae_results/best_model.pth\n",
            "2025-10-23 22:29:08,945 - VAETrainer - INFO - Epoch [7/100] Train TotalLoss: 99.1160, (ReconX: 0.9372, KLD: 0.3498*0.23, LossY: 9.8097*10.00), Time: 1.8s | Val TotalLoss: 90.9345, (ReconX: 0.9328, KLD: 0.3369, LossY: 8.9665)\n",
            "INFO:VAETrainer:Epoch [7/100] Train TotalLoss: 99.1160, (ReconX: 0.9372, KLD: 0.3498*0.23, LossY: 9.8097*10.00), Time: 1.8s | Val TotalLoss: 90.9345, (ReconX: 0.9328, KLD: 0.3369, LossY: 8.9665)\n",
            "2025-10-23 22:29:08,947 - VAETrainer - INFO - Saving model and trainer state to vae_results/best_model.pth...\n",
            "INFO:VAETrainer:Saving model and trainer state to vae_results/best_model.pth...\n",
            "2025-10-23 22:29:08,961 - VAETrainer - INFO - Successfully saved model and trainer state to vae_results/best_model.pth\n",
            "INFO:VAETrainer:Successfully saved model and trainer state to vae_results/best_model.pth\n",
            "2025-10-23 22:29:11,104 - VAETrainer - INFO - Epoch [8/100] Train TotalLoss: 83.6524, (ReconX: 0.9370, KLD: 0.3464*0.27, LossY: 8.2623*10.00), Time: 1.8s | Val TotalLoss: 77.9254, (ReconX: 0.9332, KLD: 0.3433, LossY: 7.6649)\n",
            "INFO:VAETrainer:Epoch [8/100] Train TotalLoss: 83.6524, (ReconX: 0.9370, KLD: 0.3464*0.27, LossY: 8.2623*10.00), Time: 1.8s | Val TotalLoss: 77.9254, (ReconX: 0.9332, KLD: 0.3433, LossY: 7.6649)\n",
            "2025-10-23 22:29:11,105 - VAETrainer - INFO - Saving model and trainer state to vae_results/best_model.pth...\n",
            "INFO:VAETrainer:Saving model and trainer state to vae_results/best_model.pth...\n",
            "2025-10-23 22:29:11,118 - VAETrainer - INFO - Successfully saved model and trainer state to vae_results/best_model.pth\n",
            "INFO:VAETrainer:Successfully saved model and trainer state to vae_results/best_model.pth\n",
            "2025-10-23 22:29:13,226 - VAETrainer - INFO - Epoch [9/100] Train TotalLoss: 72.0984, (ReconX: 0.9362, KLD: 0.3491*0.30, LossY: 7.1057*10.00), Time: 1.7s | Val TotalLoss: 67.2769, (ReconX: 0.9346, KLD: 0.3433, LossY: 6.5999)\n",
            "INFO:VAETrainer:Epoch [9/100] Train TotalLoss: 72.0984, (ReconX: 0.9362, KLD: 0.3491*0.30, LossY: 7.1057*10.00), Time: 1.7s | Val TotalLoss: 67.2769, (ReconX: 0.9346, KLD: 0.3433, LossY: 6.5999)\n",
            "2025-10-23 22:29:13,227 - VAETrainer - INFO - Saving model and trainer state to vae_results/best_model.pth...\n",
            "INFO:VAETrainer:Saving model and trainer state to vae_results/best_model.pth...\n",
            "2025-10-23 22:29:13,241 - VAETrainer - INFO - Successfully saved model and trainer state to vae_results/best_model.pth\n",
            "INFO:VAETrainer:Successfully saved model and trainer state to vae_results/best_model.pth\n",
            "2025-10-23 22:29:15,394 - VAETrainer - INFO - Epoch [10/100] Train TotalLoss: 63.0465, (ReconX: 0.9361, KLD: 0.3521*0.33, LossY: 6.1993*10.00), Time: 1.8s | Val TotalLoss: 58.7623, (ReconX: 0.9348, KLD: 0.3410, LossY: 5.7486)\n",
            "INFO:VAETrainer:Epoch [10/100] Train TotalLoss: 63.0465, (ReconX: 0.9361, KLD: 0.3521*0.33, LossY: 6.1993*10.00), Time: 1.8s | Val TotalLoss: 58.7623, (ReconX: 0.9348, KLD: 0.3410, LossY: 5.7486)\n",
            "2025-10-23 22:29:15,395 - VAETrainer - INFO - Saving model and trainer state to vae_results/best_model.pth...\n",
            "INFO:VAETrainer:Saving model and trainer state to vae_results/best_model.pth...\n",
            "2025-10-23 22:29:15,408 - VAETrainer - INFO - Successfully saved model and trainer state to vae_results/best_model.pth\n",
            "INFO:VAETrainer:Successfully saved model and trainer state to vae_results/best_model.pth\n",
            "2025-10-23 22:29:17,459 - VAETrainer - INFO - Epoch [11/100] Train TotalLoss: 56.3332, (ReconX: 0.9352, KLD: 0.3526*0.37, LossY: 5.5269*10.00), Time: 1.7s | Val TotalLoss: 52.7376, (ReconX: 0.9335, KLD: 0.3408, LossY: 5.1463)\n",
            "INFO:VAETrainer:Epoch [11/100] Train TotalLoss: 56.3332, (ReconX: 0.9352, KLD: 0.3526*0.37, LossY: 5.5269*10.00), Time: 1.7s | Val TotalLoss: 52.7376, (ReconX: 0.9335, KLD: 0.3408, LossY: 5.1463)\n",
            "2025-10-23 22:29:17,460 - VAETrainer - INFO - Saving model and trainer state to vae_results/best_model.pth...\n",
            "INFO:VAETrainer:Saving model and trainer state to vae_results/best_model.pth...\n",
            "2025-10-23 22:29:17,473 - VAETrainer - INFO - Successfully saved model and trainer state to vae_results/best_model.pth\n",
            "INFO:VAETrainer:Successfully saved model and trainer state to vae_results/best_model.pth\n",
            "2025-10-23 22:29:19,504 - VAETrainer - INFO - Epoch [12/100] Train TotalLoss: 51.1578, (ReconX: 0.9357, KLD: 0.3513*0.40, LossY: 5.0082*10.00), Time: 1.7s | Val TotalLoss: 47.7318, (ReconX: 0.9342, KLD: 0.3398, LossY: 4.6458)\n",
            "INFO:VAETrainer:Epoch [12/100] Train TotalLoss: 51.1578, (ReconX: 0.9357, KLD: 0.3513*0.40, LossY: 5.0082*10.00), Time: 1.7s | Val TotalLoss: 47.7318, (ReconX: 0.9342, KLD: 0.3398, LossY: 4.6458)\n",
            "2025-10-23 22:29:19,506 - VAETrainer - INFO - Saving model and trainer state to vae_results/best_model.pth...\n",
            "INFO:VAETrainer:Saving model and trainer state to vae_results/best_model.pth...\n",
            "2025-10-23 22:29:19,518 - VAETrainer - INFO - Successfully saved model and trainer state to vae_results/best_model.pth\n",
            "INFO:VAETrainer:Successfully saved model and trainer state to vae_results/best_model.pth\n",
            "2025-10-23 22:29:21,679 - VAETrainer - INFO - Epoch [13/100] Train TotalLoss: 47.2488, (ReconX: 0.9365, KLD: 0.3495*0.43, LossY: 4.6161*10.00), Time: 1.8s | Val TotalLoss: 44.7666, (ReconX: 0.9366, KLD: 0.3399, LossY: 4.3490)\n",
            "INFO:VAETrainer:Epoch [13/100] Train TotalLoss: 47.2488, (ReconX: 0.9365, KLD: 0.3495*0.43, LossY: 4.6161*10.00), Time: 1.8s | Val TotalLoss: 44.7666, (ReconX: 0.9366, KLD: 0.3399, LossY: 4.3490)\n",
            "2025-10-23 22:29:21,680 - VAETrainer - INFO - Saving model and trainer state to vae_results/best_model.pth...\n",
            "INFO:VAETrainer:Saving model and trainer state to vae_results/best_model.pth...\n",
            "2025-10-23 22:29:21,695 - VAETrainer - INFO - Successfully saved model and trainer state to vae_results/best_model.pth\n",
            "INFO:VAETrainer:Successfully saved model and trainer state to vae_results/best_model.pth\n",
            "2025-10-23 22:29:23,840 - VAETrainer - INFO - Epoch [14/100] Train TotalLoss: 44.0981, (ReconX: 0.9362, KLD: 0.3462*0.47, LossY: 4.3000*10.00), Time: 1.8s | Val TotalLoss: 42.0814, (ReconX: 0.9350, KLD: 0.3311, LossY: 4.0815)\n",
            "INFO:VAETrainer:Epoch [14/100] Train TotalLoss: 44.0981, (ReconX: 0.9362, KLD: 0.3462*0.47, LossY: 4.3000*10.00), Time: 1.8s | Val TotalLoss: 42.0814, (ReconX: 0.9350, KLD: 0.3311, LossY: 4.0815)\n",
            "2025-10-23 22:29:23,842 - VAETrainer - INFO - Saving model and trainer state to vae_results/best_model.pth...\n",
            "INFO:VAETrainer:Saving model and trainer state to vae_results/best_model.pth...\n",
            "2025-10-23 22:29:23,854 - VAETrainer - INFO - Successfully saved model and trainer state to vae_results/best_model.pth\n",
            "INFO:VAETrainer:Successfully saved model and trainer state to vae_results/best_model.pth\n",
            "2025-10-23 22:29:25,963 - VAETrainer - INFO - Epoch [15/100] Train TotalLoss: 41.7399, (ReconX: 0.9376, KLD: 0.3418*0.50, LossY: 4.0631*10.00), Time: 1.7s | Val TotalLoss: 40.1522, (ReconX: 0.9366, KLD: 0.3294, LossY: 3.8886)\n",
            "INFO:VAETrainer:Epoch [15/100] Train TotalLoss: 41.7399, (ReconX: 0.9376, KLD: 0.3418*0.50, LossY: 4.0631*10.00), Time: 1.7s | Val TotalLoss: 40.1522, (ReconX: 0.9366, KLD: 0.3294, LossY: 3.8886)\n",
            "2025-10-23 22:29:25,964 - VAETrainer - INFO - Saving model and trainer state to vae_results/best_model.pth...\n",
            "INFO:VAETrainer:Saving model and trainer state to vae_results/best_model.pth...\n",
            "2025-10-23 22:29:25,977 - VAETrainer - INFO - Successfully saved model and trainer state to vae_results/best_model.pth\n",
            "INFO:VAETrainer:Successfully saved model and trainer state to vae_results/best_model.pth\n",
            "2025-10-23 22:29:28,084 - VAETrainer - INFO - Epoch [16/100] Train TotalLoss: 39.8537, (ReconX: 0.9384, KLD: 0.3367*0.53, LossY: 3.8736*10.00), Time: 1.7s | Val TotalLoss: 38.6956, (ReconX: 0.9369, KLD: 0.3287, LossY: 3.7430)\n",
            "INFO:VAETrainer:Epoch [16/100] Train TotalLoss: 39.8537, (ReconX: 0.9384, KLD: 0.3367*0.53, LossY: 3.8736*10.00), Time: 1.7s | Val TotalLoss: 38.6956, (ReconX: 0.9369, KLD: 0.3287, LossY: 3.7430)\n",
            "2025-10-23 22:29:28,087 - VAETrainer - INFO - Saving model and trainer state to vae_results/best_model.pth...\n",
            "INFO:VAETrainer:Saving model and trainer state to vae_results/best_model.pth...\n",
            "2025-10-23 22:29:28,103 - VAETrainer - INFO - Successfully saved model and trainer state to vae_results/best_model.pth\n",
            "INFO:VAETrainer:Successfully saved model and trainer state to vae_results/best_model.pth\n",
            "2025-10-23 22:29:30,245 - VAETrainer - INFO - Epoch [17/100] Train TotalLoss: 38.2109, (ReconX: 0.9386, KLD: 0.3302*0.57, LossY: 3.7085*10.00), Time: 1.8s | Val TotalLoss: 37.4910, (ReconX: 0.9392, KLD: 0.3198, LossY: 3.6232)\n",
            "INFO:VAETrainer:Epoch [17/100] Train TotalLoss: 38.2109, (ReconX: 0.9386, KLD: 0.3302*0.57, LossY: 3.7085*10.00), Time: 1.8s | Val TotalLoss: 37.4910, (ReconX: 0.9392, KLD: 0.3198, LossY: 3.6232)\n",
            "2025-10-23 22:29:30,247 - VAETrainer - INFO - Saving model and trainer state to vae_results/best_model.pth...\n",
            "INFO:VAETrainer:Saving model and trainer state to vae_results/best_model.pth...\n",
            "2025-10-23 22:29:30,261 - VAETrainer - INFO - Successfully saved model and trainer state to vae_results/best_model.pth\n",
            "INFO:VAETrainer:Successfully saved model and trainer state to vae_results/best_model.pth\n",
            "2025-10-23 22:29:32,554 - VAETrainer - INFO - Epoch [18/100] Train TotalLoss: 36.9211, (ReconX: 0.9397, KLD: 0.3240*0.60, LossY: 3.5787*10.00), Time: 1.9s | Val TotalLoss: 35.9442, (ReconX: 0.9372, KLD: 0.3123, LossY: 3.4695)\n",
            "INFO:VAETrainer:Epoch [18/100] Train TotalLoss: 36.9211, (ReconX: 0.9397, KLD: 0.3240*0.60, LossY: 3.5787*10.00), Time: 1.9s | Val TotalLoss: 35.9442, (ReconX: 0.9372, KLD: 0.3123, LossY: 3.4695)\n",
            "2025-10-23 22:29:32,556 - VAETrainer - INFO - Saving model and trainer state to vae_results/best_model.pth...\n",
            "INFO:VAETrainer:Saving model and trainer state to vae_results/best_model.pth...\n",
            "2025-10-23 22:29:32,570 - VAETrainer - INFO - Successfully saved model and trainer state to vae_results/best_model.pth\n",
            "INFO:VAETrainer:Successfully saved model and trainer state to vae_results/best_model.pth\n",
            "2025-10-23 22:29:34,787 - VAETrainer - INFO - Epoch [19/100] Train TotalLoss: 35.9236, (ReconX: 0.9415, KLD: 0.3174*0.63, LossY: 3.4781*10.00), Time: 1.8s | Val TotalLoss: 35.1680, (ReconX: 0.9390, KLD: 0.3026, LossY: 3.3926)\n",
            "INFO:VAETrainer:Epoch [19/100] Train TotalLoss: 35.9236, (ReconX: 0.9415, KLD: 0.3174*0.63, LossY: 3.4781*10.00), Time: 1.8s | Val TotalLoss: 35.1680, (ReconX: 0.9390, KLD: 0.3026, LossY: 3.3926)\n",
            "2025-10-23 22:29:34,789 - VAETrainer - INFO - Saving model and trainer state to vae_results/best_model.pth...\n",
            "INFO:VAETrainer:Saving model and trainer state to vae_results/best_model.pth...\n",
            "2025-10-23 22:29:34,802 - VAETrainer - INFO - Successfully saved model and trainer state to vae_results/best_model.pth\n",
            "INFO:VAETrainer:Successfully saved model and trainer state to vae_results/best_model.pth\n",
            "2025-10-23 22:29:36,976 - VAETrainer - INFO - Epoch [20/100] Train TotalLoss: 35.1285, (ReconX: 0.9425, KLD: 0.3073*0.67, LossY: 3.3981*10.00), Time: 1.8s | Val TotalLoss: 34.5533, (ReconX: 0.9401, KLD: 0.2919, LossY: 3.3321)\n",
            "INFO:VAETrainer:Epoch [20/100] Train TotalLoss: 35.1285, (ReconX: 0.9425, KLD: 0.3073*0.67, LossY: 3.3981*10.00), Time: 1.8s | Val TotalLoss: 34.5533, (ReconX: 0.9401, KLD: 0.2919, LossY: 3.3321)\n",
            "2025-10-23 22:29:36,977 - VAETrainer - INFO - Saving model and trainer state to vae_results/best_model.pth...\n",
            "INFO:VAETrainer:Saving model and trainer state to vae_results/best_model.pth...\n",
            "2025-10-23 22:29:36,991 - VAETrainer - INFO - Successfully saved model and trainer state to vae_results/best_model.pth\n",
            "INFO:VAETrainer:Successfully saved model and trainer state to vae_results/best_model.pth\n",
            "2025-10-23 22:29:39,189 - VAETrainer - INFO - Epoch [21/100] Train TotalLoss: 34.4524, (ReconX: 0.9433, KLD: 0.2991*0.70, LossY: 3.3300*10.00), Time: 1.8s | Val TotalLoss: 33.6474, (ReconX: 0.9387, KLD: 0.2864, LossY: 3.2422)\n",
            "INFO:VAETrainer:Epoch [21/100] Train TotalLoss: 34.4524, (ReconX: 0.9433, KLD: 0.2991*0.70, LossY: 3.3300*10.00), Time: 1.8s | Val TotalLoss: 33.6474, (ReconX: 0.9387, KLD: 0.2864, LossY: 3.2422)\n",
            "2025-10-23 22:29:39,191 - VAETrainer - INFO - Saving model and trainer state to vae_results/best_model.pth...\n",
            "INFO:VAETrainer:Saving model and trainer state to vae_results/best_model.pth...\n",
            "2025-10-23 22:29:39,204 - VAETrainer - INFO - Successfully saved model and trainer state to vae_results/best_model.pth\n",
            "INFO:VAETrainer:Successfully saved model and trainer state to vae_results/best_model.pth\n",
            "2025-10-23 22:29:41,406 - VAETrainer - INFO - Epoch [22/100] Train TotalLoss: 33.8036, (ReconX: 0.9447, KLD: 0.2885*0.73, LossY: 3.2647*10.00), Time: 1.8s | Val TotalLoss: 33.1120, (ReconX: 0.9425, KLD: 0.2759, LossY: 3.1894)\n",
            "INFO:VAETrainer:Epoch [22/100] Train TotalLoss: 33.8036, (ReconX: 0.9447, KLD: 0.2885*0.73, LossY: 3.2647*10.00), Time: 1.8s | Val TotalLoss: 33.1120, (ReconX: 0.9425, KLD: 0.2759, LossY: 3.1894)\n",
            "2025-10-23 22:29:41,408 - VAETrainer - INFO - Saving model and trainer state to vae_results/best_model.pth...\n",
            "INFO:VAETrainer:Saving model and trainer state to vae_results/best_model.pth...\n",
            "2025-10-23 22:29:41,421 - VAETrainer - INFO - Successfully saved model and trainer state to vae_results/best_model.pth\n",
            "INFO:VAETrainer:Successfully saved model and trainer state to vae_results/best_model.pth\n",
            "2025-10-23 22:29:43,635 - VAETrainer - INFO - Epoch [23/100] Train TotalLoss: 33.1748, (ReconX: 0.9470, KLD: 0.2775*0.77, LossY: 3.2015*10.00), Time: 1.8s | Val TotalLoss: 32.5139, (ReconX: 0.9462, KLD: 0.2661, LossY: 3.1302)\n",
            "INFO:VAETrainer:Epoch [23/100] Train TotalLoss: 33.1748, (ReconX: 0.9470, KLD: 0.2775*0.77, LossY: 3.2015*10.00), Time: 1.8s | Val TotalLoss: 32.5139, (ReconX: 0.9462, KLD: 0.2661, LossY: 3.1302)\n",
            "2025-10-23 22:29:43,637 - VAETrainer - INFO - Saving model and trainer state to vae_results/best_model.pth...\n",
            "INFO:VAETrainer:Saving model and trainer state to vae_results/best_model.pth...\n",
            "2025-10-23 22:29:43,650 - VAETrainer - INFO - Successfully saved model and trainer state to vae_results/best_model.pth\n",
            "INFO:VAETrainer:Successfully saved model and trainer state to vae_results/best_model.pth\n",
            "2025-10-23 22:29:45,821 - VAETrainer - INFO - Epoch [24/100] Train TotalLoss: 32.6871, (ReconX: 0.9485, KLD: 0.2671*0.80, LossY: 3.1525*10.00), Time: 1.8s | Val TotalLoss: 32.0418, (ReconX: 0.9471, KLD: 0.2537, LossY: 3.0841)\n",
            "INFO:VAETrainer:Epoch [24/100] Train TotalLoss: 32.6871, (ReconX: 0.9485, KLD: 0.2671*0.80, LossY: 3.1525*10.00), Time: 1.8s | Val TotalLoss: 32.0418, (ReconX: 0.9471, KLD: 0.2537, LossY: 3.0841)\n",
            "2025-10-23 22:29:45,823 - VAETrainer - INFO - Saving model and trainer state to vae_results/best_model.pth...\n",
            "INFO:VAETrainer:Saving model and trainer state to vae_results/best_model.pth...\n",
            "2025-10-23 22:29:45,835 - VAETrainer - INFO - Successfully saved model and trainer state to vae_results/best_model.pth\n",
            "INFO:VAETrainer:Successfully saved model and trainer state to vae_results/best_model.pth\n",
            "2025-10-23 22:29:48,010 - VAETrainer - INFO - Epoch [25/100] Train TotalLoss: 32.1064, (ReconX: 0.9506, KLD: 0.2562*0.83, LossY: 3.0942*10.00), Time: 1.8s | Val TotalLoss: 31.5388, (ReconX: 0.9478, KLD: 0.2420, LossY: 3.0349)\n",
            "INFO:VAETrainer:Epoch [25/100] Train TotalLoss: 32.1064, (ReconX: 0.9506, KLD: 0.2562*0.83, LossY: 3.0942*10.00), Time: 1.8s | Val TotalLoss: 31.5388, (ReconX: 0.9478, KLD: 0.2420, LossY: 3.0349)\n",
            "2025-10-23 22:29:48,011 - VAETrainer - INFO - Saving model and trainer state to vae_results/best_model.pth...\n",
            "INFO:VAETrainer:Saving model and trainer state to vae_results/best_model.pth...\n",
            "2025-10-23 22:29:48,025 - VAETrainer - INFO - Successfully saved model and trainer state to vae_results/best_model.pth\n",
            "INFO:VAETrainer:Successfully saved model and trainer state to vae_results/best_model.pth\n",
            "2025-10-23 22:29:50,162 - VAETrainer - INFO - Epoch [26/100] Train TotalLoss: 31.6494, (ReconX: 0.9521, KLD: 0.2445*0.87, LossY: 3.0485*10.00), Time: 1.7s | Val TotalLoss: 31.0493, (ReconX: 0.9520, KLD: 0.2304, LossY: 2.9867)\n",
            "INFO:VAETrainer:Epoch [26/100] Train TotalLoss: 31.6494, (ReconX: 0.9521, KLD: 0.2445*0.87, LossY: 3.0485*10.00), Time: 1.7s | Val TotalLoss: 31.0493, (ReconX: 0.9520, KLD: 0.2304, LossY: 2.9867)\n",
            "2025-10-23 22:29:50,163 - VAETrainer - INFO - Saving model and trainer state to vae_results/best_model.pth...\n",
            "INFO:VAETrainer:Saving model and trainer state to vae_results/best_model.pth...\n",
            "2025-10-23 22:29:50,176 - VAETrainer - INFO - Successfully saved model and trainer state to vae_results/best_model.pth\n",
            "INFO:VAETrainer:Successfully saved model and trainer state to vae_results/best_model.pth\n",
            "2025-10-23 22:29:52,314 - VAETrainer - INFO - Epoch [27/100] Train TotalLoss: 31.2278, (ReconX: 0.9531, KLD: 0.2328*0.90, LossY: 3.0065*10.00), Time: 1.8s | Val TotalLoss: 30.6768, (ReconX: 0.9498, KLD: 0.2243, LossY: 2.9503)\n",
            "INFO:VAETrainer:Epoch [27/100] Train TotalLoss: 31.2278, (ReconX: 0.9531, KLD: 0.2328*0.90, LossY: 3.0065*10.00), Time: 1.8s | Val TotalLoss: 30.6768, (ReconX: 0.9498, KLD: 0.2243, LossY: 2.9503)\n",
            "2025-10-23 22:29:52,316 - VAETrainer - INFO - Saving model and trainer state to vae_results/best_model.pth...\n",
            "INFO:VAETrainer:Saving model and trainer state to vae_results/best_model.pth...\n",
            "2025-10-23 22:29:52,329 - VAETrainer - INFO - Successfully saved model and trainer state to vae_results/best_model.pth\n",
            "INFO:VAETrainer:Successfully saved model and trainer state to vae_results/best_model.pth\n",
            "2025-10-23 22:29:54,473 - VAETrainer - INFO - Epoch [28/100] Train TotalLoss: 30.5905, (ReconX: 0.9559, KLD: 0.2228*0.93, LossY: 2.9427*10.00), Time: 1.8s | Val TotalLoss: 29.9668, (ReconX: 0.9546, KLD: 0.2146, LossY: 2.8798)\n",
            "INFO:VAETrainer:Epoch [28/100] Train TotalLoss: 30.5905, (ReconX: 0.9559, KLD: 0.2228*0.93, LossY: 2.9427*10.00), Time: 1.8s | Val TotalLoss: 29.9668, (ReconX: 0.9546, KLD: 0.2146, LossY: 2.8798)\n",
            "2025-10-23 22:29:54,475 - VAETrainer - INFO - Saving model and trainer state to vae_results/best_model.pth...\n",
            "INFO:VAETrainer:Saving model and trainer state to vae_results/best_model.pth...\n",
            "2025-10-23 22:29:54,488 - VAETrainer - INFO - Successfully saved model and trainer state to vae_results/best_model.pth\n",
            "INFO:VAETrainer:Successfully saved model and trainer state to vae_results/best_model.pth\n",
            "2025-10-23 22:29:56,602 - VAETrainer - INFO - Epoch [29/100] Train TotalLoss: 29.8895, (ReconX: 0.9578, KLD: 0.2132*0.97, LossY: 2.8726*10.00), Time: 1.7s | Val TotalLoss: 28.9685, (ReconX: 0.9542, KLD: 0.2034, LossY: 2.7811)\n",
            "INFO:VAETrainer:Epoch [29/100] Train TotalLoss: 29.8895, (ReconX: 0.9578, KLD: 0.2132*0.97, LossY: 2.8726*10.00), Time: 1.7s | Val TotalLoss: 28.9685, (ReconX: 0.9542, KLD: 0.2034, LossY: 2.7811)\n",
            "2025-10-23 22:29:56,603 - VAETrainer - INFO - Saving model and trainer state to vae_results/best_model.pth...\n",
            "INFO:VAETrainer:Saving model and trainer state to vae_results/best_model.pth...\n",
            "2025-10-23 22:29:56,617 - VAETrainer - INFO - Successfully saved model and trainer state to vae_results/best_model.pth\n",
            "INFO:VAETrainer:Successfully saved model and trainer state to vae_results/best_model.pth\n",
            "2025-10-23 22:29:58,864 - VAETrainer - INFO - Epoch [30/100] Train TotalLoss: 28.7098, (ReconX: 0.9593, KLD: 0.2036*1.00, LossY: 2.7547*10.00), Time: 1.8s | Val TotalLoss: 27.3295, (ReconX: 0.9571, KLD: 0.1946, LossY: 2.6178)\n",
            "INFO:VAETrainer:Epoch [30/100] Train TotalLoss: 28.7098, (ReconX: 0.9593, KLD: 0.2036*1.00, LossY: 2.7547*10.00), Time: 1.8s | Val TotalLoss: 27.3295, (ReconX: 0.9571, KLD: 0.1946, LossY: 2.6178)\n",
            "2025-10-23 22:29:58,866 - VAETrainer - INFO - Saving model and trainer state to vae_results/best_model.pth...\n",
            "INFO:VAETrainer:Saving model and trainer state to vae_results/best_model.pth...\n",
            "2025-10-23 22:29:58,880 - VAETrainer - INFO - Successfully saved model and trainer state to vae_results/best_model.pth\n",
            "INFO:VAETrainer:Successfully saved model and trainer state to vae_results/best_model.pth\n",
            "2025-10-23 22:30:01,111 - VAETrainer - INFO - Epoch [31/100] Train TotalLoss: 26.6739, (ReconX: 0.9601, KLD: 0.1954*1.00, LossY: 2.5518*10.00), Time: 1.8s | Val TotalLoss: 25.1555, (ReconX: 0.9592, KLD: 0.1860, LossY: 2.4010)\n",
            "INFO:VAETrainer:Epoch [31/100] Train TotalLoss: 26.6739, (ReconX: 0.9601, KLD: 0.1954*1.00, LossY: 2.5518*10.00), Time: 1.8s | Val TotalLoss: 25.1555, (ReconX: 0.9592, KLD: 0.1860, LossY: 2.4010)\n",
            "2025-10-23 22:30:01,113 - VAETrainer - INFO - Saving model and trainer state to vae_results/best_model.pth...\n",
            "INFO:VAETrainer:Saving model and trainer state to vae_results/best_model.pth...\n",
            "2025-10-23 22:30:01,127 - VAETrainer - INFO - Successfully saved model and trainer state to vae_results/best_model.pth\n",
            "INFO:VAETrainer:Successfully saved model and trainer state to vae_results/best_model.pth\n",
            "2025-10-23 22:30:03,380 - VAETrainer - INFO - Epoch [32/100] Train TotalLoss: 24.3177, (ReconX: 0.9606, KLD: 0.1899*1.00, LossY: 2.3167*10.00), Time: 1.9s | Val TotalLoss: 23.0495, (ReconX: 0.9590, KLD: 0.1833, LossY: 2.1907)\n",
            "INFO:VAETrainer:Epoch [32/100] Train TotalLoss: 24.3177, (ReconX: 0.9606, KLD: 0.1899*1.00, LossY: 2.3167*10.00), Time: 1.9s | Val TotalLoss: 23.0495, (ReconX: 0.9590, KLD: 0.1833, LossY: 2.1907)\n",
            "2025-10-23 22:30:03,383 - VAETrainer - INFO - Saving model and trainer state to vae_results/best_model.pth...\n",
            "INFO:VAETrainer:Saving model and trainer state to vae_results/best_model.pth...\n",
            "2025-10-23 22:30:03,396 - VAETrainer - INFO - Successfully saved model and trainer state to vae_results/best_model.pth\n",
            "INFO:VAETrainer:Successfully saved model and trainer state to vae_results/best_model.pth\n",
            "2025-10-23 22:30:05,619 - VAETrainer - INFO - Epoch [33/100] Train TotalLoss: 22.4404, (ReconX: 0.9619, KLD: 0.1809*1.00, LossY: 2.1298*10.00), Time: 1.8s | Val TotalLoss: 21.5533, (ReconX: 0.9598, KLD: 0.1703, LossY: 2.0423)\n",
            "INFO:VAETrainer:Epoch [33/100] Train TotalLoss: 22.4404, (ReconX: 0.9619, KLD: 0.1809*1.00, LossY: 2.1298*10.00), Time: 1.8s | Val TotalLoss: 21.5533, (ReconX: 0.9598, KLD: 0.1703, LossY: 2.0423)\n",
            "2025-10-23 22:30:05,621 - VAETrainer - INFO - Saving model and trainer state to vae_results/best_model.pth...\n",
            "INFO:VAETrainer:Saving model and trainer state to vae_results/best_model.pth...\n",
            "2025-10-23 22:30:05,634 - VAETrainer - INFO - Successfully saved model and trainer state to vae_results/best_model.pth\n",
            "INFO:VAETrainer:Successfully saved model and trainer state to vae_results/best_model.pth\n",
            "2025-10-23 22:30:07,792 - VAETrainer - INFO - Epoch [34/100] Train TotalLoss: 20.4598, (ReconX: 0.9646, KLD: 0.1668*1.00, LossY: 1.9328*10.00), Time: 1.8s | Val TotalLoss: 18.8855, (ReconX: 0.9648, KLD: 0.1579, LossY: 1.7763)\n",
            "INFO:VAETrainer:Epoch [34/100] Train TotalLoss: 20.4598, (ReconX: 0.9646, KLD: 0.1668*1.00, LossY: 1.9328*10.00), Time: 1.8s | Val TotalLoss: 18.8855, (ReconX: 0.9648, KLD: 0.1579, LossY: 1.7763)\n",
            "2025-10-23 22:30:07,794 - VAETrainer - INFO - Saving model and trainer state to vae_results/best_model.pth...\n",
            "INFO:VAETrainer:Saving model and trainer state to vae_results/best_model.pth...\n",
            "2025-10-23 22:30:07,807 - VAETrainer - INFO - Successfully saved model and trainer state to vae_results/best_model.pth\n",
            "INFO:VAETrainer:Successfully saved model and trainer state to vae_results/best_model.pth\n",
            "2025-10-23 22:30:10,041 - VAETrainer - INFO - Epoch [35/100] Train TotalLoss: 19.2587, (ReconX: 0.9664, KLD: 0.1555*1.00, LossY: 1.8137*10.00), Time: 1.9s | Val TotalLoss: 18.3544, (ReconX: 0.9647, KLD: 0.1490, LossY: 1.7241)\n",
            "INFO:VAETrainer:Epoch [35/100] Train TotalLoss: 19.2587, (ReconX: 0.9664, KLD: 0.1555*1.00, LossY: 1.8137*10.00), Time: 1.9s | Val TotalLoss: 18.3544, (ReconX: 0.9647, KLD: 0.1490, LossY: 1.7241)\n",
            "2025-10-23 22:30:10,042 - VAETrainer - INFO - Saving model and trainer state to vae_results/best_model.pth...\n",
            "INFO:VAETrainer:Saving model and trainer state to vae_results/best_model.pth...\n",
            "2025-10-23 22:30:10,056 - VAETrainer - INFO - Successfully saved model and trainer state to vae_results/best_model.pth\n",
            "INFO:VAETrainer:Successfully saved model and trainer state to vae_results/best_model.pth\n",
            "2025-10-23 22:30:12,299 - VAETrainer - INFO - Epoch [36/100] Train TotalLoss: 18.6199, (ReconX: 0.9681, KLD: 0.1498*1.00, LossY: 1.7502*10.00), Time: 1.9s | Val TotalLoss: 17.7424, (ReconX: 0.9650, KLD: 0.1445, LossY: 1.6633)\n",
            "INFO:VAETrainer:Epoch [36/100] Train TotalLoss: 18.6199, (ReconX: 0.9681, KLD: 0.1498*1.00, LossY: 1.7502*10.00), Time: 1.9s | Val TotalLoss: 17.7424, (ReconX: 0.9650, KLD: 0.1445, LossY: 1.6633)\n",
            "2025-10-23 22:30:12,300 - VAETrainer - INFO - Saving model and trainer state to vae_results/best_model.pth...\n",
            "INFO:VAETrainer:Saving model and trainer state to vae_results/best_model.pth...\n",
            "2025-10-23 22:30:12,314 - VAETrainer - INFO - Successfully saved model and trainer state to vae_results/best_model.pth\n",
            "INFO:VAETrainer:Successfully saved model and trainer state to vae_results/best_model.pth\n",
            "2025-10-23 22:30:14,457 - VAETrainer - INFO - Epoch [37/100] Train TotalLoss: 18.1726, (ReconX: 0.9689, KLD: 0.1471*1.00, LossY: 1.7057*10.00), Time: 1.8s | Val TotalLoss: 17.3412, (ReconX: 0.9667, KLD: 0.1399, LossY: 1.6235)\n",
            "INFO:VAETrainer:Epoch [37/100] Train TotalLoss: 18.1726, (ReconX: 0.9689, KLD: 0.1471*1.00, LossY: 1.7057*10.00), Time: 1.8s | Val TotalLoss: 17.3412, (ReconX: 0.9667, KLD: 0.1399, LossY: 1.6235)\n",
            "2025-10-23 22:30:14,459 - VAETrainer - INFO - Saving model and trainer state to vae_results/best_model.pth...\n",
            "INFO:VAETrainer:Saving model and trainer state to vae_results/best_model.pth...\n",
            "2025-10-23 22:30:14,473 - VAETrainer - INFO - Successfully saved model and trainer state to vae_results/best_model.pth\n",
            "INFO:VAETrainer:Successfully saved model and trainer state to vae_results/best_model.pth\n",
            "2025-10-23 22:30:16,536 - VAETrainer - INFO - Epoch [38/100] Train TotalLoss: 17.9284, (ReconX: 0.9697, KLD: 0.1439*1.00, LossY: 1.6815*10.00), Time: 1.7s | Val TotalLoss: 17.0724, (ReconX: 0.9667, KLD: 0.1415, LossY: 1.5964)\n",
            "INFO:VAETrainer:Epoch [38/100] Train TotalLoss: 17.9284, (ReconX: 0.9697, KLD: 0.1439*1.00, LossY: 1.6815*10.00), Time: 1.7s | Val TotalLoss: 17.0724, (ReconX: 0.9667, KLD: 0.1415, LossY: 1.5964)\n",
            "2025-10-23 22:30:16,538 - VAETrainer - INFO - Saving model and trainer state to vae_results/best_model.pth...\n",
            "INFO:VAETrainer:Saving model and trainer state to vae_results/best_model.pth...\n",
            "2025-10-23 22:30:16,550 - VAETrainer - INFO - Successfully saved model and trainer state to vae_results/best_model.pth\n",
            "INFO:VAETrainer:Successfully saved model and trainer state to vae_results/best_model.pth\n",
            "2025-10-23 22:30:18,576 - VAETrainer - INFO - Epoch [39/100] Train TotalLoss: 17.4440, (ReconX: 0.9704, KLD: 0.1414*1.00, LossY: 1.6332*10.00), Time: 1.7s | Val TotalLoss: 16.7843, (ReconX: 0.9674, KLD: 0.1329, LossY: 1.5684)\n",
            "INFO:VAETrainer:Epoch [39/100] Train TotalLoss: 17.4440, (ReconX: 0.9704, KLD: 0.1414*1.00, LossY: 1.6332*10.00), Time: 1.7s | Val TotalLoss: 16.7843, (ReconX: 0.9674, KLD: 0.1329, LossY: 1.5684)\n",
            "2025-10-23 22:30:18,578 - VAETrainer - INFO - Saving model and trainer state to vae_results/best_model.pth...\n",
            "INFO:VAETrainer:Saving model and trainer state to vae_results/best_model.pth...\n",
            "2025-10-23 22:30:18,592 - VAETrainer - INFO - Successfully saved model and trainer state to vae_results/best_model.pth\n",
            "INFO:VAETrainer:Successfully saved model and trainer state to vae_results/best_model.pth\n",
            "2025-10-23 22:30:20,680 - VAETrainer - INFO - Epoch [40/100] Train TotalLoss: 17.5206, (ReconX: 0.9701, KLD: 0.1394*1.00, LossY: 1.6411*10.00), Time: 1.7s | Val TotalLoss: 16.3898, (ReconX: 0.9669, KLD: 0.1368, LossY: 1.5286)\n",
            "INFO:VAETrainer:Epoch [40/100] Train TotalLoss: 17.5206, (ReconX: 0.9701, KLD: 0.1394*1.00, LossY: 1.6411*10.00), Time: 1.7s | Val TotalLoss: 16.3898, (ReconX: 0.9669, KLD: 0.1368, LossY: 1.5286)\n",
            "2025-10-23 22:30:20,683 - VAETrainer - INFO - Saving model and trainer state to vae_results/best_model.pth...\n",
            "INFO:VAETrainer:Saving model and trainer state to vae_results/best_model.pth...\n",
            "2025-10-23 22:30:20,696 - VAETrainer - INFO - Successfully saved model and trainer state to vae_results/best_model.pth\n",
            "INFO:VAETrainer:Successfully saved model and trainer state to vae_results/best_model.pth\n",
            "2025-10-23 22:30:22,864 - VAETrainer - INFO - Epoch [41/100] Train TotalLoss: 17.2066, (ReconX: 0.9707, KLD: 0.1384*1.00, LossY: 1.6098*10.00), Time: 1.8s | Val TotalLoss: 16.4431, (ReconX: 0.9678, KLD: 0.1333, LossY: 1.5342)\n",
            "INFO:VAETrainer:Epoch [41/100] Train TotalLoss: 17.2066, (ReconX: 0.9707, KLD: 0.1384*1.00, LossY: 1.6098*10.00), Time: 1.8s | Val TotalLoss: 16.4431, (ReconX: 0.9678, KLD: 0.1333, LossY: 1.5342)\n",
            "2025-10-23 22:30:24,985 - VAETrainer - INFO - Epoch [42/100] Train TotalLoss: 17.0013, (ReconX: 0.9708, KLD: 0.1365*1.00, LossY: 1.5894*10.00), Time: 1.7s | Val TotalLoss: 16.1018, (ReconX: 0.9662, KLD: 0.1341, LossY: 1.5001)\n",
            "INFO:VAETrainer:Epoch [42/100] Train TotalLoss: 17.0013, (ReconX: 0.9708, KLD: 0.1365*1.00, LossY: 1.5894*10.00), Time: 1.7s | Val TotalLoss: 16.1018, (ReconX: 0.9662, KLD: 0.1341, LossY: 1.5001)\n",
            "2025-10-23 22:30:24,987 - VAETrainer - INFO - Saving model and trainer state to vae_results/best_model.pth...\n",
            "INFO:VAETrainer:Saving model and trainer state to vae_results/best_model.pth...\n",
            "2025-10-23 22:30:25,000 - VAETrainer - INFO - Successfully saved model and trainer state to vae_results/best_model.pth\n",
            "INFO:VAETrainer:Successfully saved model and trainer state to vae_results/best_model.pth\n",
            "2025-10-23 22:30:27,178 - VAETrainer - INFO - Epoch [43/100] Train TotalLoss: 16.9540, (ReconX: 0.9705, KLD: 0.1361*1.00, LossY: 1.5847*10.00), Time: 1.8s | Val TotalLoss: 15.8264, (ReconX: 0.9672, KLD: 0.1350, LossY: 1.4724)\n",
            "INFO:VAETrainer:Epoch [43/100] Train TotalLoss: 16.9540, (ReconX: 0.9705, KLD: 0.1361*1.00, LossY: 1.5847*10.00), Time: 1.8s | Val TotalLoss: 15.8264, (ReconX: 0.9672, KLD: 0.1350, LossY: 1.4724)\n",
            "2025-10-23 22:30:27,179 - VAETrainer - INFO - Saving model and trainer state to vae_results/best_model.pth...\n",
            "INFO:VAETrainer:Saving model and trainer state to vae_results/best_model.pth...\n",
            "2025-10-23 22:30:27,195 - VAETrainer - INFO - Successfully saved model and trainer state to vae_results/best_model.pth\n",
            "INFO:VAETrainer:Successfully saved model and trainer state to vae_results/best_model.pth\n",
            "2025-10-23 22:30:29,319 - VAETrainer - INFO - Epoch [44/100] Train TotalLoss: 16.8450, (ReconX: 0.9712, KLD: 0.1353*1.00, LossY: 1.5738*10.00), Time: 1.8s | Val TotalLoss: 15.7742, (ReconX: 0.9681, KLD: 0.1291, LossY: 1.4677)\n",
            "INFO:VAETrainer:Epoch [44/100] Train TotalLoss: 16.8450, (ReconX: 0.9712, KLD: 0.1353*1.00, LossY: 1.5738*10.00), Time: 1.8s | Val TotalLoss: 15.7742, (ReconX: 0.9681, KLD: 0.1291, LossY: 1.4677)\n",
            "2025-10-23 22:30:29,321 - VAETrainer - INFO - Saving model and trainer state to vae_results/best_model.pth...\n",
            "INFO:VAETrainer:Saving model and trainer state to vae_results/best_model.pth...\n",
            "2025-10-23 22:30:29,335 - VAETrainer - INFO - Successfully saved model and trainer state to vae_results/best_model.pth\n",
            "INFO:VAETrainer:Successfully saved model and trainer state to vae_results/best_model.pth\n",
            "2025-10-23 22:30:31,374 - VAETrainer - INFO - Epoch [45/100] Train TotalLoss: 16.5420, (ReconX: 0.9717, KLD: 0.1328*1.00, LossY: 1.5438*10.00), Time: 1.7s | Val TotalLoss: 15.6269, (ReconX: 0.9692, KLD: 0.1293, LossY: 1.4528)\n",
            "INFO:VAETrainer:Epoch [45/100] Train TotalLoss: 16.5420, (ReconX: 0.9717, KLD: 0.1328*1.00, LossY: 1.5438*10.00), Time: 1.7s | Val TotalLoss: 15.6269, (ReconX: 0.9692, KLD: 0.1293, LossY: 1.4528)\n",
            "2025-10-23 22:30:31,375 - VAETrainer - INFO - Saving model and trainer state to vae_results/best_model.pth...\n",
            "INFO:VAETrainer:Saving model and trainer state to vae_results/best_model.pth...\n",
            "2025-10-23 22:30:31,391 - VAETrainer - INFO - Successfully saved model and trainer state to vae_results/best_model.pth\n",
            "INFO:VAETrainer:Successfully saved model and trainer state to vae_results/best_model.pth\n",
            "2025-10-23 22:30:33,501 - VAETrainer - INFO - Epoch [46/100] Train TotalLoss: 16.6247, (ReconX: 0.9712, KLD: 0.1321*1.00, LossY: 1.5521*10.00), Time: 1.7s | Val TotalLoss: 15.4872, (ReconX: 0.9677, KLD: 0.1292, LossY: 1.4390)\n",
            "INFO:VAETrainer:Epoch [46/100] Train TotalLoss: 16.6247, (ReconX: 0.9712, KLD: 0.1321*1.00, LossY: 1.5521*10.00), Time: 1.7s | Val TotalLoss: 15.4872, (ReconX: 0.9677, KLD: 0.1292, LossY: 1.4390)\n",
            "2025-10-23 22:30:33,503 - VAETrainer - INFO - Saving model and trainer state to vae_results/best_model.pth...\n",
            "INFO:VAETrainer:Saving model and trainer state to vae_results/best_model.pth...\n",
            "2025-10-23 22:30:33,517 - VAETrainer - INFO - Successfully saved model and trainer state to vae_results/best_model.pth\n",
            "INFO:VAETrainer:Successfully saved model and trainer state to vae_results/best_model.pth\n",
            "2025-10-23 22:30:35,723 - VAETrainer - INFO - Epoch [47/100] Train TotalLoss: 16.5521, (ReconX: 0.9721, KLD: 0.1318*1.00, LossY: 1.5448*10.00), Time: 1.8s | Val TotalLoss: 15.6643, (ReconX: 0.9682, KLD: 0.1264, LossY: 1.4570)\n",
            "INFO:VAETrainer:Epoch [47/100] Train TotalLoss: 16.5521, (ReconX: 0.9721, KLD: 0.1318*1.00, LossY: 1.5448*10.00), Time: 1.8s | Val TotalLoss: 15.6643, (ReconX: 0.9682, KLD: 0.1264, LossY: 1.4570)\n",
            "2025-10-23 22:30:37,796 - VAETrainer - INFO - Epoch [48/100] Train TotalLoss: 16.2379, (ReconX: 0.9715, KLD: 0.1304*1.00, LossY: 1.5136*10.00), Time: 1.7s | Val TotalLoss: 15.8655, (ReconX: 0.9706, KLD: 0.1241, LossY: 1.4771)\n",
            "INFO:VAETrainer:Epoch [48/100] Train TotalLoss: 16.2379, (ReconX: 0.9715, KLD: 0.1304*1.00, LossY: 1.5136*10.00), Time: 1.7s | Val TotalLoss: 15.8655, (ReconX: 0.9706, KLD: 0.1241, LossY: 1.4771)\n",
            "2025-10-23 22:30:39,859 - VAETrainer - INFO - Epoch [49/100] Train TotalLoss: 16.0260, (ReconX: 0.9728, KLD: 0.1283*1.00, LossY: 1.4925*10.00), Time: 1.7s | Val TotalLoss: 15.3001, (ReconX: 0.9690, KLD: 0.1233, LossY: 1.4208)\n",
            "INFO:VAETrainer:Epoch [49/100] Train TotalLoss: 16.0260, (ReconX: 0.9728, KLD: 0.1283*1.00, LossY: 1.4925*10.00), Time: 1.7s | Val TotalLoss: 15.3001, (ReconX: 0.9690, KLD: 0.1233, LossY: 1.4208)\n",
            "2025-10-23 22:30:39,861 - VAETrainer - INFO - Saving model and trainer state to vae_results/best_model.pth...\n",
            "INFO:VAETrainer:Saving model and trainer state to vae_results/best_model.pth...\n",
            "2025-10-23 22:30:39,876 - VAETrainer - INFO - Successfully saved model and trainer state to vae_results/best_model.pth\n",
            "INFO:VAETrainer:Successfully saved model and trainer state to vae_results/best_model.pth\n",
            "2025-10-23 22:30:41,929 - VAETrainer - INFO - Epoch [50/100] Train TotalLoss: 16.1652, (ReconX: 0.9736, KLD: 0.1276*1.00, LossY: 1.5064*10.00), Time: 1.7s | Val TotalLoss: 15.2293, (ReconX: 0.9689, KLD: 0.1245, LossY: 1.4136)\n",
            "INFO:VAETrainer:Epoch [50/100] Train TotalLoss: 16.1652, (ReconX: 0.9736, KLD: 0.1276*1.00, LossY: 1.5064*10.00), Time: 1.7s | Val TotalLoss: 15.2293, (ReconX: 0.9689, KLD: 0.1245, LossY: 1.4136)\n",
            "2025-10-23 22:30:41,931 - VAETrainer - INFO - Saving model and trainer state to vae_results/best_model.pth...\n",
            "INFO:VAETrainer:Saving model and trainer state to vae_results/best_model.pth...\n",
            "2025-10-23 22:30:41,944 - VAETrainer - INFO - Successfully saved model and trainer state to vae_results/best_model.pth\n",
            "INFO:VAETrainer:Successfully saved model and trainer state to vae_results/best_model.pth\n",
            "2025-10-23 22:30:44,148 - VAETrainer - INFO - Epoch [51/100] Train TotalLoss: 15.8861, (ReconX: 0.9725, KLD: 0.1277*1.00, LossY: 1.4786*10.00), Time: 1.8s | Val TotalLoss: 16.0311, (ReconX: 0.9694, KLD: 0.1233, LossY: 1.4938)\n",
            "INFO:VAETrainer:Epoch [51/100] Train TotalLoss: 15.8861, (ReconX: 0.9725, KLD: 0.1277*1.00, LossY: 1.4786*10.00), Time: 1.8s | Val TotalLoss: 16.0311, (ReconX: 0.9694, KLD: 0.1233, LossY: 1.4938)\n",
            "2025-10-23 22:30:46,214 - VAETrainer - INFO - Epoch [52/100] Train TotalLoss: 15.7553, (ReconX: 0.9728, KLD: 0.1263*1.00, LossY: 1.4656*10.00), Time: 1.7s | Val TotalLoss: 15.1483, (ReconX: 0.9706, KLD: 0.1220, LossY: 1.4056)\n",
            "INFO:VAETrainer:Epoch [52/100] Train TotalLoss: 15.7553, (ReconX: 0.9728, KLD: 0.1263*1.00, LossY: 1.4656*10.00), Time: 1.7s | Val TotalLoss: 15.1483, (ReconX: 0.9706, KLD: 0.1220, LossY: 1.4056)\n",
            "2025-10-23 22:30:46,217 - VAETrainer - INFO - Saving model and trainer state to vae_results/best_model.pth...\n",
            "INFO:VAETrainer:Saving model and trainer state to vae_results/best_model.pth...\n",
            "2025-10-23 22:30:46,230 - VAETrainer - INFO - Successfully saved model and trainer state to vae_results/best_model.pth\n",
            "INFO:VAETrainer:Successfully saved model and trainer state to vae_results/best_model.pth\n",
            "2025-10-23 22:30:48,405 - VAETrainer - INFO - Epoch [53/100] Train TotalLoss: 15.6545, (ReconX: 0.9733, KLD: 0.1243*1.00, LossY: 1.4557*10.00), Time: 1.8s | Val TotalLoss: 14.7548, (ReconX: 0.9709, KLD: 0.1209, LossY: 1.3663)\n",
            "INFO:VAETrainer:Epoch [53/100] Train TotalLoss: 15.6545, (ReconX: 0.9733, KLD: 0.1243*1.00, LossY: 1.4557*10.00), Time: 1.8s | Val TotalLoss: 14.7548, (ReconX: 0.9709, KLD: 0.1209, LossY: 1.3663)\n",
            "2025-10-23 22:30:48,406 - VAETrainer - INFO - Saving model and trainer state to vae_results/best_model.pth...\n",
            "INFO:VAETrainer:Saving model and trainer state to vae_results/best_model.pth...\n",
            "2025-10-23 22:30:48,423 - VAETrainer - INFO - Successfully saved model and trainer state to vae_results/best_model.pth\n",
            "INFO:VAETrainer:Successfully saved model and trainer state to vae_results/best_model.pth\n",
            "2025-10-23 22:30:50,479 - VAETrainer - INFO - Epoch [54/100] Train TotalLoss: 15.4764, (ReconX: 0.9749, KLD: 0.1210*1.00, LossY: 1.4380*10.00), Time: 1.7s | Val TotalLoss: 14.5816, (ReconX: 0.9715, KLD: 0.1202, LossY: 1.3490)\n",
            "INFO:VAETrainer:Epoch [54/100] Train TotalLoss: 15.4764, (ReconX: 0.9749, KLD: 0.1210*1.00, LossY: 1.4380*10.00), Time: 1.7s | Val TotalLoss: 14.5816, (ReconX: 0.9715, KLD: 0.1202, LossY: 1.3490)\n",
            "2025-10-23 22:30:50,481 - VAETrainer - INFO - Saving model and trainer state to vae_results/best_model.pth...\n",
            "INFO:VAETrainer:Saving model and trainer state to vae_results/best_model.pth...\n",
            "2025-10-23 22:30:50,494 - VAETrainer - INFO - Successfully saved model and trainer state to vae_results/best_model.pth\n",
            "INFO:VAETrainer:Successfully saved model and trainer state to vae_results/best_model.pth\n",
            "2025-10-23 22:30:52,529 - VAETrainer - INFO - Epoch [55/100] Train TotalLoss: 15.4187, (ReconX: 0.9745, KLD: 0.1203*1.00, LossY: 1.4324*10.00), Time: 1.7s | Val TotalLoss: 15.0501, (ReconX: 0.9716, KLD: 0.1166, LossY: 1.3962)\n",
            "INFO:VAETrainer:Epoch [55/100] Train TotalLoss: 15.4187, (ReconX: 0.9745, KLD: 0.1203*1.00, LossY: 1.4324*10.00), Time: 1.7s | Val TotalLoss: 15.0501, (ReconX: 0.9716, KLD: 0.1166, LossY: 1.3962)\n",
            "2025-10-23 22:30:54,626 - VAETrainer - INFO - Epoch [56/100] Train TotalLoss: 15.4947, (ReconX: 0.9753, KLD: 0.1190*1.00, LossY: 1.4400*10.00), Time: 1.7s | Val TotalLoss: 15.6681, (ReconX: 0.9724, KLD: 0.1134, LossY: 1.4582)\n",
            "INFO:VAETrainer:Epoch [56/100] Train TotalLoss: 15.4947, (ReconX: 0.9753, KLD: 0.1190*1.00, LossY: 1.4400*10.00), Time: 1.7s | Val TotalLoss: 15.6681, (ReconX: 0.9724, KLD: 0.1134, LossY: 1.4582)\n",
            "2025-10-23 22:30:56,782 - VAETrainer - INFO - Epoch [57/100] Train TotalLoss: 15.5298, (ReconX: 0.9744, KLD: 0.1188*1.00, LossY: 1.4437*10.00), Time: 1.8s | Val TotalLoss: 14.6922, (ReconX: 0.9735, KLD: 0.1144, LossY: 1.3604)\n",
            "INFO:VAETrainer:Epoch [57/100] Train TotalLoss: 15.5298, (ReconX: 0.9744, KLD: 0.1188*1.00, LossY: 1.4437*10.00), Time: 1.8s | Val TotalLoss: 14.6922, (ReconX: 0.9735, KLD: 0.1144, LossY: 1.3604)\n",
            "2025-10-23 22:30:58,862 - VAETrainer - INFO - Epoch [58/100] Train TotalLoss: 15.3624, (ReconX: 0.9753, KLD: 0.1175*1.00, LossY: 1.4270*10.00), Time: 1.7s | Val TotalLoss: 14.5766, (ReconX: 0.9716, KLD: 0.1152, LossY: 1.3490)\n",
            "INFO:VAETrainer:Epoch [58/100] Train TotalLoss: 15.3624, (ReconX: 0.9753, KLD: 0.1175*1.00, LossY: 1.4270*10.00), Time: 1.7s | Val TotalLoss: 14.5766, (ReconX: 0.9716, KLD: 0.1152, LossY: 1.3490)\n",
            "2025-10-23 22:30:58,864 - VAETrainer - INFO - Saving model and trainer state to vae_results/best_model.pth...\n",
            "INFO:VAETrainer:Saving model and trainer state to vae_results/best_model.pth...\n",
            "2025-10-23 22:30:58,877 - VAETrainer - INFO - Successfully saved model and trainer state to vae_results/best_model.pth\n",
            "INFO:VAETrainer:Successfully saved model and trainer state to vae_results/best_model.pth\n",
            "2025-10-23 22:31:01,029 - VAETrainer - INFO - Epoch [59/100] Train TotalLoss: 15.1940, (ReconX: 0.9755, KLD: 0.1173*1.00, LossY: 1.4101*10.00), Time: 1.8s | Val TotalLoss: 14.4024, (ReconX: 0.9733, KLD: 0.1126, LossY: 1.3317)\n",
            "INFO:VAETrainer:Epoch [59/100] Train TotalLoss: 15.1940, (ReconX: 0.9755, KLD: 0.1173*1.00, LossY: 1.4101*10.00), Time: 1.8s | Val TotalLoss: 14.4024, (ReconX: 0.9733, KLD: 0.1126, LossY: 1.3317)\n",
            "2025-10-23 22:31:01,031 - VAETrainer - INFO - Saving model and trainer state to vae_results/best_model.pth...\n",
            "INFO:VAETrainer:Saving model and trainer state to vae_results/best_model.pth...\n",
            "2025-10-23 22:31:01,045 - VAETrainer - INFO - Successfully saved model and trainer state to vae_results/best_model.pth\n",
            "INFO:VAETrainer:Successfully saved model and trainer state to vae_results/best_model.pth\n",
            "2025-10-23 22:31:03,145 - VAETrainer - INFO - Epoch [60/100] Train TotalLoss: 15.3482, (ReconX: 0.9755, KLD: 0.1152*1.00, LossY: 1.4258*10.00), Time: 1.7s | Val TotalLoss: 14.5580, (ReconX: 0.9746, KLD: 0.1105, LossY: 1.3473)\n",
            "INFO:VAETrainer:Epoch [60/100] Train TotalLoss: 15.3482, (ReconX: 0.9755, KLD: 0.1152*1.00, LossY: 1.4258*10.00), Time: 1.7s | Val TotalLoss: 14.5580, (ReconX: 0.9746, KLD: 0.1105, LossY: 1.3473)\n",
            "2025-10-23 22:31:05,244 - VAETrainer - INFO - Epoch [61/100] Train TotalLoss: 15.1760, (ReconX: 0.9752, KLD: 0.1153*1.00, LossY: 1.4086*10.00), Time: 1.7s | Val TotalLoss: 14.3873, (ReconX: 0.9721, KLD: 0.1104, LossY: 1.3305)\n",
            "INFO:VAETrainer:Epoch [61/100] Train TotalLoss: 15.1760, (ReconX: 0.9752, KLD: 0.1153*1.00, LossY: 1.4086*10.00), Time: 1.7s | Val TotalLoss: 14.3873, (ReconX: 0.9721, KLD: 0.1104, LossY: 1.3305)\n",
            "2025-10-23 22:31:05,245 - VAETrainer - INFO - Saving model and trainer state to vae_results/best_model.pth...\n",
            "INFO:VAETrainer:Saving model and trainer state to vae_results/best_model.pth...\n",
            "2025-10-23 22:31:05,258 - VAETrainer - INFO - Successfully saved model and trainer state to vae_results/best_model.pth\n",
            "INFO:VAETrainer:Successfully saved model and trainer state to vae_results/best_model.pth\n",
            "2025-10-23 22:31:07,342 - VAETrainer - INFO - Epoch [62/100] Train TotalLoss: 15.3293, (ReconX: 0.9764, KLD: 0.1140*1.00, LossY: 1.4239*10.00), Time: 1.7s | Val TotalLoss: 14.3698, (ReconX: 0.9724, KLD: 0.1111, LossY: 1.3286)\n",
            "INFO:VAETrainer:Epoch [62/100] Train TotalLoss: 15.3293, (ReconX: 0.9764, KLD: 0.1140*1.00, LossY: 1.4239*10.00), Time: 1.7s | Val TotalLoss: 14.3698, (ReconX: 0.9724, KLD: 0.1111, LossY: 1.3286)\n",
            "2025-10-23 22:31:07,344 - VAETrainer - INFO - Saving model and trainer state to vae_results/best_model.pth...\n",
            "INFO:VAETrainer:Saving model and trainer state to vae_results/best_model.pth...\n",
            "2025-10-23 22:31:07,358 - VAETrainer - INFO - Successfully saved model and trainer state to vae_results/best_model.pth\n",
            "INFO:VAETrainer:Successfully saved model and trainer state to vae_results/best_model.pth\n",
            "2025-10-23 22:31:09,464 - VAETrainer - INFO - Epoch [63/100] Train TotalLoss: 15.1005, (ReconX: 0.9761, KLD: 0.1140*1.00, LossY: 1.4010*10.00), Time: 1.7s | Val TotalLoss: 14.9793, (ReconX: 0.9724, KLD: 0.1106, LossY: 1.3896)\n",
            "INFO:VAETrainer:Epoch [63/100] Train TotalLoss: 15.1005, (ReconX: 0.9761, KLD: 0.1140*1.00, LossY: 1.4010*10.00), Time: 1.7s | Val TotalLoss: 14.9793, (ReconX: 0.9724, KLD: 0.1106, LossY: 1.3896)\n",
            "2025-10-23 22:31:11,646 - VAETrainer - INFO - Epoch [64/100] Train TotalLoss: 15.2043, (ReconX: 0.9754, KLD: 0.1133*1.00, LossY: 1.4116*10.00), Time: 1.8s | Val TotalLoss: 14.4309, (ReconX: 0.9730, KLD: 0.1099, LossY: 1.3348)\n",
            "INFO:VAETrainer:Epoch [64/100] Train TotalLoss: 15.2043, (ReconX: 0.9754, KLD: 0.1133*1.00, LossY: 1.4116*10.00), Time: 1.8s | Val TotalLoss: 14.4309, (ReconX: 0.9730, KLD: 0.1099, LossY: 1.3348)\n",
            "2025-10-23 22:31:13,824 - VAETrainer - INFO - Epoch [65/100] Train TotalLoss: 14.8973, (ReconX: 0.9768, KLD: 0.1125*1.00, LossY: 1.3808*10.00), Time: 1.8s | Val TotalLoss: 14.4591, (ReconX: 0.9726, KLD: 0.1076, LossY: 1.3379)\n",
            "INFO:VAETrainer:Epoch [65/100] Train TotalLoss: 14.8973, (ReconX: 0.9768, KLD: 0.1125*1.00, LossY: 1.3808*10.00), Time: 1.8s | Val TotalLoss: 14.4591, (ReconX: 0.9726, KLD: 0.1076, LossY: 1.3379)\n",
            "2025-10-23 22:31:15,934 - VAETrainer - INFO - Epoch [66/100] Train TotalLoss: 14.9438, (ReconX: 0.9768, KLD: 0.1113*1.00, LossY: 1.3856*10.00), Time: 1.7s | Val TotalLoss: 14.1195, (ReconX: 0.9758, KLD: 0.1094, LossY: 1.3034)\n",
            "INFO:VAETrainer:Epoch [66/100] Train TotalLoss: 14.9438, (ReconX: 0.9768, KLD: 0.1113*1.00, LossY: 1.3856*10.00), Time: 1.7s | Val TotalLoss: 14.1195, (ReconX: 0.9758, KLD: 0.1094, LossY: 1.3034)\n",
            "2025-10-23 22:31:15,936 - VAETrainer - INFO - Saving model and trainer state to vae_results/best_model.pth...\n",
            "INFO:VAETrainer:Saving model and trainer state to vae_results/best_model.pth...\n",
            "2025-10-23 22:31:15,949 - VAETrainer - INFO - Successfully saved model and trainer state to vae_results/best_model.pth\n",
            "INFO:VAETrainer:Successfully saved model and trainer state to vae_results/best_model.pth\n",
            "2025-10-23 22:31:18,095 - VAETrainer - INFO - Epoch [67/100] Train TotalLoss: 14.8485, (ReconX: 0.9765, KLD: 0.1109*1.00, LossY: 1.3761*10.00), Time: 1.8s | Val TotalLoss: 14.2993, (ReconX: 0.9738, KLD: 0.1078, LossY: 1.3218)\n",
            "INFO:VAETrainer:Epoch [67/100] Train TotalLoss: 14.8485, (ReconX: 0.9765, KLD: 0.1109*1.00, LossY: 1.3761*10.00), Time: 1.8s | Val TotalLoss: 14.2993, (ReconX: 0.9738, KLD: 0.1078, LossY: 1.3218)\n",
            "2025-10-23 22:31:20,194 - VAETrainer - INFO - Epoch [68/100] Train TotalLoss: 14.8513, (ReconX: 0.9770, KLD: 0.1098*1.00, LossY: 1.3764*10.00), Time: 1.7s | Val TotalLoss: 14.1142, (ReconX: 0.9746, KLD: 0.1070, LossY: 1.3033)\n",
            "INFO:VAETrainer:Epoch [68/100] Train TotalLoss: 14.8513, (ReconX: 0.9770, KLD: 0.1098*1.00, LossY: 1.3764*10.00), Time: 1.7s | Val TotalLoss: 14.1142, (ReconX: 0.9746, KLD: 0.1070, LossY: 1.3033)\n",
            "2025-10-23 22:31:20,195 - VAETrainer - INFO - Saving model and trainer state to vae_results/best_model.pth...\n",
            "INFO:VAETrainer:Saving model and trainer state to vae_results/best_model.pth...\n",
            "2025-10-23 22:31:20,210 - VAETrainer - INFO - Successfully saved model and trainer state to vae_results/best_model.pth\n",
            "INFO:VAETrainer:Successfully saved model and trainer state to vae_results/best_model.pth\n",
            "2025-10-23 22:31:22,273 - VAETrainer - INFO - Epoch [69/100] Train TotalLoss: 14.8674, (ReconX: 0.9774, KLD: 0.1089*1.00, LossY: 1.3781*10.00), Time: 1.7s | Val TotalLoss: 14.1550, (ReconX: 0.9756, KLD: 0.1069, LossY: 1.3072)\n",
            "INFO:VAETrainer:Epoch [69/100] Train TotalLoss: 14.8674, (ReconX: 0.9774, KLD: 0.1089*1.00, LossY: 1.3781*10.00), Time: 1.7s | Val TotalLoss: 14.1550, (ReconX: 0.9756, KLD: 0.1069, LossY: 1.3072)\n",
            "2025-10-23 22:31:24,401 - VAETrainer - INFO - Epoch [70/100] Train TotalLoss: 15.3511, (ReconX: 0.9779, KLD: 0.1082*1.00, LossY: 1.4265*10.00), Time: 1.7s | Val TotalLoss: 14.0704, (ReconX: 0.9714, KLD: 0.1077, LossY: 1.2991)\n",
            "INFO:VAETrainer:Epoch [70/100] Train TotalLoss: 15.3511, (ReconX: 0.9779, KLD: 0.1082*1.00, LossY: 1.4265*10.00), Time: 1.7s | Val TotalLoss: 14.0704, (ReconX: 0.9714, KLD: 0.1077, LossY: 1.2991)\n",
            "2025-10-23 22:31:24,403 - VAETrainer - INFO - Saving model and trainer state to vae_results/best_model.pth...\n",
            "INFO:VAETrainer:Saving model and trainer state to vae_results/best_model.pth...\n",
            "2025-10-23 22:31:24,416 - VAETrainer - INFO - Successfully saved model and trainer state to vae_results/best_model.pth\n",
            "INFO:VAETrainer:Successfully saved model and trainer state to vae_results/best_model.pth\n",
            "2025-10-23 22:31:26,549 - VAETrainer - INFO - Epoch [71/100] Train TotalLoss: 14.7435, (ReconX: 0.9778, KLD: 0.1078*1.00, LossY: 1.3658*10.00), Time: 1.8s | Val TotalLoss: 14.4647, (ReconX: 0.9738, KLD: 0.1028, LossY: 1.3388)\n",
            "INFO:VAETrainer:Epoch [71/100] Train TotalLoss: 14.7435, (ReconX: 0.9778, KLD: 0.1078*1.00, LossY: 1.3658*10.00), Time: 1.8s | Val TotalLoss: 14.4647, (ReconX: 0.9738, KLD: 0.1028, LossY: 1.3388)\n",
            "2025-10-23 22:31:28,693 - VAETrainer - INFO - Epoch [72/100] Train TotalLoss: 14.7282, (ReconX: 0.9782, KLD: 0.1065*1.00, LossY: 1.3643*10.00), Time: 1.8s | Val TotalLoss: 14.2147, (ReconX: 0.9751, KLD: 0.1023, LossY: 1.3137)\n",
            "INFO:VAETrainer:Epoch [72/100] Train TotalLoss: 14.7282, (ReconX: 0.9782, KLD: 0.1065*1.00, LossY: 1.3643*10.00), Time: 1.8s | Val TotalLoss: 14.2147, (ReconX: 0.9751, KLD: 0.1023, LossY: 1.3137)\n",
            "2025-10-23 22:31:30,868 - VAETrainer - INFO - Epoch [73/100] Train TotalLoss: 14.8106, (ReconX: 0.9781, KLD: 0.1067*1.00, LossY: 1.3726*10.00), Time: 1.8s | Val TotalLoss: 13.7964, (ReconX: 0.9750, KLD: 0.1043, LossY: 1.2717)\n",
            "INFO:VAETrainer:Epoch [73/100] Train TotalLoss: 14.8106, (ReconX: 0.9781, KLD: 0.1067*1.00, LossY: 1.3726*10.00), Time: 1.8s | Val TotalLoss: 13.7964, (ReconX: 0.9750, KLD: 0.1043, LossY: 1.2717)\n",
            "2025-10-23 22:31:30,870 - VAETrainer - INFO - Saving model and trainer state to vae_results/best_model.pth...\n",
            "INFO:VAETrainer:Saving model and trainer state to vae_results/best_model.pth...\n",
            "2025-10-23 22:31:30,883 - VAETrainer - INFO - Successfully saved model and trainer state to vae_results/best_model.pth\n",
            "INFO:VAETrainer:Successfully saved model and trainer state to vae_results/best_model.pth\n",
            "2025-10-23 22:31:32,940 - VAETrainer - INFO - Epoch [74/100] Train TotalLoss: 14.8534, (ReconX: 0.9780, KLD: 0.1061*1.00, LossY: 1.3769*10.00), Time: 1.7s | Val TotalLoss: 13.7885, (ReconX: 0.9756, KLD: 0.1040, LossY: 1.2709)\n",
            "INFO:VAETrainer:Epoch [74/100] Train TotalLoss: 14.8534, (ReconX: 0.9780, KLD: 0.1061*1.00, LossY: 1.3769*10.00), Time: 1.7s | Val TotalLoss: 13.7885, (ReconX: 0.9756, KLD: 0.1040, LossY: 1.2709)\n",
            "2025-10-23 22:31:32,942 - VAETrainer - INFO - Saving model and trainer state to vae_results/best_model.pth...\n",
            "INFO:VAETrainer:Saving model and trainer state to vae_results/best_model.pth...\n",
            "2025-10-23 22:31:32,955 - VAETrainer - INFO - Successfully saved model and trainer state to vae_results/best_model.pth\n",
            "INFO:VAETrainer:Successfully saved model and trainer state to vae_results/best_model.pth\n",
            "2025-10-23 22:31:34,995 - VAETrainer - INFO - Epoch [75/100] Train TotalLoss: 15.2758, (ReconX: 0.9783, KLD: 0.1056*1.00, LossY: 1.4192*10.00), Time: 1.7s | Val TotalLoss: 14.9089, (ReconX: 0.9743, KLD: 0.1014, LossY: 1.3833)\n",
            "INFO:VAETrainer:Epoch [75/100] Train TotalLoss: 15.2758, (ReconX: 0.9783, KLD: 0.1056*1.00, LossY: 1.4192*10.00), Time: 1.7s | Val TotalLoss: 14.9089, (ReconX: 0.9743, KLD: 0.1014, LossY: 1.3833)\n",
            "2025-10-23 22:31:37,121 - VAETrainer - INFO - Epoch [76/100] Train TotalLoss: 14.9633, (ReconX: 0.9791, KLD: 0.1057*1.00, LossY: 1.3878*10.00), Time: 1.8s | Val TotalLoss: 14.0585, (ReconX: 0.9740, KLD: 0.1025, LossY: 1.2982)\n",
            "INFO:VAETrainer:Epoch [76/100] Train TotalLoss: 14.9633, (ReconX: 0.9791, KLD: 0.1057*1.00, LossY: 1.3878*10.00), Time: 1.8s | Val TotalLoss: 14.0585, (ReconX: 0.9740, KLD: 0.1025, LossY: 1.2982)\n",
            "2025-10-23 22:31:39,320 - VAETrainer - INFO - Epoch [77/100] Train TotalLoss: 14.8689, (ReconX: 0.9784, KLD: 0.1051*1.00, LossY: 1.3785*10.00), Time: 1.8s | Val TotalLoss: 13.8548, (ReconX: 0.9754, KLD: 0.1015, LossY: 1.2778)\n",
            "INFO:VAETrainer:Epoch [77/100] Train TotalLoss: 14.8689, (ReconX: 0.9784, KLD: 0.1051*1.00, LossY: 1.3785*10.00), Time: 1.8s | Val TotalLoss: 13.8548, (ReconX: 0.9754, KLD: 0.1015, LossY: 1.2778)\n",
            "2025-10-23 22:31:41,392 - VAETrainer - INFO - Epoch [78/100] Train TotalLoss: 14.7088, (ReconX: 0.9784, KLD: 0.1039*1.00, LossY: 1.3626*10.00), Time: 1.7s | Val TotalLoss: 13.6682, (ReconX: 0.9765, KLD: 0.1009, LossY: 1.2591)\n",
            "INFO:VAETrainer:Epoch [78/100] Train TotalLoss: 14.7088, (ReconX: 0.9784, KLD: 0.1039*1.00, LossY: 1.3626*10.00), Time: 1.7s | Val TotalLoss: 13.6682, (ReconX: 0.9765, KLD: 0.1009, LossY: 1.2591)\n",
            "2025-10-23 22:31:41,393 - VAETrainer - INFO - Saving model and trainer state to vae_results/best_model.pth...\n",
            "INFO:VAETrainer:Saving model and trainer state to vae_results/best_model.pth...\n",
            "2025-10-23 22:31:41,407 - VAETrainer - INFO - Successfully saved model and trainer state to vae_results/best_model.pth\n",
            "INFO:VAETrainer:Successfully saved model and trainer state to vae_results/best_model.pth\n",
            "2025-10-23 22:31:43,466 - VAETrainer - INFO - Epoch [79/100] Train TotalLoss: 14.6957, (ReconX: 0.9787, KLD: 0.1034*1.00, LossY: 1.3614*10.00), Time: 1.7s | Val TotalLoss: 13.6709, (ReconX: 0.9748, KLD: 0.1003, LossY: 1.2596)\n",
            "INFO:VAETrainer:Epoch [79/100] Train TotalLoss: 14.6957, (ReconX: 0.9787, KLD: 0.1034*1.00, LossY: 1.3614*10.00), Time: 1.7s | Val TotalLoss: 13.6709, (ReconX: 0.9748, KLD: 0.1003, LossY: 1.2596)\n",
            "2025-10-23 22:31:45,562 - VAETrainer - INFO - Epoch [80/100] Train TotalLoss: 14.7971, (ReconX: 0.9783, KLD: 0.1032*1.00, LossY: 1.3716*10.00), Time: 1.7s | Val TotalLoss: 13.9402, (ReconX: 0.9744, KLD: 0.1022, LossY: 1.2864)\n",
            "INFO:VAETrainer:Epoch [80/100] Train TotalLoss: 14.7971, (ReconX: 0.9783, KLD: 0.1032*1.00, LossY: 1.3716*10.00), Time: 1.7s | Val TotalLoss: 13.9402, (ReconX: 0.9744, KLD: 0.1022, LossY: 1.2864)\n",
            "2025-10-23 22:31:47,634 - VAETrainer - INFO - Epoch [81/100] Train TotalLoss: 14.5781, (ReconX: 0.9785, KLD: 0.1039*1.00, LossY: 1.3496*10.00), Time: 1.7s | Val TotalLoss: 13.8933, (ReconX: 0.9740, KLD: 0.1016, LossY: 1.2818)\n",
            "INFO:VAETrainer:Epoch [81/100] Train TotalLoss: 14.5781, (ReconX: 0.9785, KLD: 0.1039*1.00, LossY: 1.3496*10.00), Time: 1.7s | Val TotalLoss: 13.8933, (ReconX: 0.9740, KLD: 0.1016, LossY: 1.2818)\n",
            "2025-10-23 22:31:49,775 - VAETrainer - INFO - Epoch [82/100] Train TotalLoss: 14.5998, (ReconX: 0.9784, KLD: 0.1036*1.00, LossY: 1.3518*10.00), Time: 1.7s | Val TotalLoss: 13.9144, (ReconX: 0.9759, KLD: 0.1008, LossY: 1.2838)\n",
            "INFO:VAETrainer:Epoch [82/100] Train TotalLoss: 14.5998, (ReconX: 0.9784, KLD: 0.1036*1.00, LossY: 1.3518*10.00), Time: 1.7s | Val TotalLoss: 13.9144, (ReconX: 0.9759, KLD: 0.1008, LossY: 1.2838)\n",
            "2025-10-23 22:31:51,853 - VAETrainer - INFO - Epoch [83/100] Train TotalLoss: 14.6473, (ReconX: 0.9787, KLD: 0.1028*1.00, LossY: 1.3566*10.00), Time: 1.7s | Val TotalLoss: 13.8231, (ReconX: 0.9751, KLD: 0.1004, LossY: 1.2748)\n",
            "INFO:VAETrainer:Epoch [83/100] Train TotalLoss: 14.6473, (ReconX: 0.9787, KLD: 0.1028*1.00, LossY: 1.3566*10.00), Time: 1.7s | Val TotalLoss: 13.8231, (ReconX: 0.9751, KLD: 0.1004, LossY: 1.2748)\n",
            "2025-10-23 22:31:53,903 - VAETrainer - INFO - Epoch [84/100] Train TotalLoss: 14.4084, (ReconX: 0.9787, KLD: 0.1020*1.00, LossY: 1.3328*10.00), Time: 1.7s | Val TotalLoss: 13.7017, (ReconX: 0.9746, KLD: 0.0978, LossY: 1.2629)\n",
            "INFO:VAETrainer:Epoch [84/100] Train TotalLoss: 14.4084, (ReconX: 0.9787, KLD: 0.1020*1.00, LossY: 1.3328*10.00), Time: 1.7s | Val TotalLoss: 13.7017, (ReconX: 0.9746, KLD: 0.0978, LossY: 1.2629)\n",
            "2025-10-23 22:31:56,072 - VAETrainer - INFO - Epoch [85/100] Train TotalLoss: 14.3167, (ReconX: 0.9788, KLD: 0.1011*1.00, LossY: 1.3237*10.00), Time: 1.8s | Val TotalLoss: 13.4200, (ReconX: 0.9761, KLD: 0.0975, LossY: 1.2346)\n",
            "INFO:VAETrainer:Epoch [85/100] Train TotalLoss: 14.3167, (ReconX: 0.9788, KLD: 0.1011*1.00, LossY: 1.3237*10.00), Time: 1.8s | Val TotalLoss: 13.4200, (ReconX: 0.9761, KLD: 0.0975, LossY: 1.2346)\n",
            "2025-10-23 22:31:56,074 - VAETrainer - INFO - Saving model and trainer state to vae_results/best_model.pth...\n",
            "INFO:VAETrainer:Saving model and trainer state to vae_results/best_model.pth...\n",
            "2025-10-23 22:31:56,090 - VAETrainer - INFO - Successfully saved model and trainer state to vae_results/best_model.pth\n",
            "INFO:VAETrainer:Successfully saved model and trainer state to vae_results/best_model.pth\n",
            "2025-10-23 22:31:58,181 - VAETrainer - INFO - Epoch [86/100] Train TotalLoss: 14.2035, (ReconX: 0.9787, KLD: 0.1002*1.00, LossY: 1.3125*10.00), Time: 1.7s | Val TotalLoss: 13.5406, (ReconX: 0.9777, KLD: 0.0985, LossY: 1.2464)\n",
            "INFO:VAETrainer:Epoch [86/100] Train TotalLoss: 14.2035, (ReconX: 0.9787, KLD: 0.1002*1.00, LossY: 1.3125*10.00), Time: 1.7s | Val TotalLoss: 13.5406, (ReconX: 0.9777, KLD: 0.0985, LossY: 1.2464)\n",
            "2025-10-23 22:32:00,336 - VAETrainer - INFO - Epoch [87/100] Train TotalLoss: 14.2313, (ReconX: 0.9790, KLD: 0.0999*1.00, LossY: 1.3152*10.00), Time: 1.8s | Val TotalLoss: 13.5103, (ReconX: 0.9762, KLD: 0.0960, LossY: 1.2438)\n",
            "INFO:VAETrainer:Epoch [87/100] Train TotalLoss: 14.2313, (ReconX: 0.9790, KLD: 0.0999*1.00, LossY: 1.3152*10.00), Time: 1.8s | Val TotalLoss: 13.5103, (ReconX: 0.9762, KLD: 0.0960, LossY: 1.2438)\n",
            "2025-10-23 22:32:02,427 - VAETrainer - INFO - Epoch [88/100] Train TotalLoss: 14.2435, (ReconX: 0.9792, KLD: 0.0998*1.00, LossY: 1.3165*10.00), Time: 1.7s | Val TotalLoss: 13.4674, (ReconX: 0.9749, KLD: 0.0978, LossY: 1.2395)\n",
            "INFO:VAETrainer:Epoch [88/100] Train TotalLoss: 14.2435, (ReconX: 0.9792, KLD: 0.0998*1.00, LossY: 1.3165*10.00), Time: 1.7s | Val TotalLoss: 13.4674, (ReconX: 0.9749, KLD: 0.0978, LossY: 1.2395)\n",
            "2025-10-23 22:32:04,584 - VAETrainer - INFO - Epoch [89/100] Train TotalLoss: 14.3913, (ReconX: 0.9793, KLD: 0.0998*1.00, LossY: 1.3312*10.00), Time: 1.8s | Val TotalLoss: 13.4494, (ReconX: 0.9763, KLD: 0.0970, LossY: 1.2376)\n",
            "INFO:VAETrainer:Epoch [89/100] Train TotalLoss: 14.3913, (ReconX: 0.9793, KLD: 0.0998*1.00, LossY: 1.3312*10.00), Time: 1.8s | Val TotalLoss: 13.4494, (ReconX: 0.9763, KLD: 0.0970, LossY: 1.2376)\n",
            "2025-10-23 22:32:06,706 - VAETrainer - INFO - Epoch [90/100] Train TotalLoss: 14.2338, (ReconX: 0.9788, KLD: 0.0999*1.00, LossY: 1.3155*10.00), Time: 1.7s | Val TotalLoss: 13.4912, (ReconX: 0.9758, KLD: 0.0982, LossY: 1.2417)\n",
            "INFO:VAETrainer:Epoch [90/100] Train TotalLoss: 14.2338, (ReconX: 0.9788, KLD: 0.0999*1.00, LossY: 1.3155*10.00), Time: 1.7s | Val TotalLoss: 13.4912, (ReconX: 0.9758, KLD: 0.0982, LossY: 1.2417)\n",
            "2025-10-23 22:32:08,847 - VAETrainer - INFO - Epoch [91/100] Train TotalLoss: 15.0471, (ReconX: 0.9786, KLD: 0.0998*1.00, LossY: 1.3969*10.00), Time: 1.7s | Val TotalLoss: 13.8470, (ReconX: 0.9761, KLD: 0.0955, LossY: 1.2775)\n",
            "INFO:VAETrainer:Epoch [91/100] Train TotalLoss: 15.0471, (ReconX: 0.9786, KLD: 0.0998*1.00, LossY: 1.3969*10.00), Time: 1.7s | Val TotalLoss: 13.8470, (ReconX: 0.9761, KLD: 0.0955, LossY: 1.2775)\n",
            "2025-10-23 22:32:10,958 - VAETrainer - INFO - Epoch [92/100] Train TotalLoss: 14.2234, (ReconX: 0.9792, KLD: 0.0990*1.00, LossY: 1.3145*10.00), Time: 1.7s | Val TotalLoss: 13.6890, (ReconX: 0.9765, KLD: 0.0970, LossY: 1.2616)\n",
            "INFO:VAETrainer:Epoch [92/100] Train TotalLoss: 14.2234, (ReconX: 0.9792, KLD: 0.0990*1.00, LossY: 1.3145*10.00), Time: 1.7s | Val TotalLoss: 13.6890, (ReconX: 0.9765, KLD: 0.0970, LossY: 1.2616)\n",
            "2025-10-23 22:32:13,029 - VAETrainer - INFO - Epoch [93/100] Train TotalLoss: 14.5105, (ReconX: 0.9797, KLD: 0.0992*1.00, LossY: 1.3432*10.00), Time: 1.7s | Val TotalLoss: 13.5785, (ReconX: 0.9749, KLD: 0.0970, LossY: 1.2507)\n",
            "INFO:VAETrainer:Epoch [93/100] Train TotalLoss: 14.5105, (ReconX: 0.9797, KLD: 0.0992*1.00, LossY: 1.3432*10.00), Time: 1.7s | Val TotalLoss: 13.5785, (ReconX: 0.9749, KLD: 0.0970, LossY: 1.2507)\n",
            "2025-10-23 22:32:15,242 - VAETrainer - INFO - Epoch [94/100] Train TotalLoss: 14.3209, (ReconX: 0.9787, KLD: 0.0996*1.00, LossY: 1.3243*10.00), Time: 1.8s | Val TotalLoss: 13.5536, (ReconX: 0.9765, KLD: 0.0977, LossY: 1.2479)\n",
            "INFO:VAETrainer:Epoch [94/100] Train TotalLoss: 14.3209, (ReconX: 0.9787, KLD: 0.0996*1.00, LossY: 1.3243*10.00), Time: 1.8s | Val TotalLoss: 13.5536, (ReconX: 0.9765, KLD: 0.0977, LossY: 1.2479)\n",
            "2025-10-23 22:32:17,370 - VAETrainer - INFO - Epoch [95/100] Train TotalLoss: 13.9093, (ReconX: 0.9797, KLD: 0.0997*1.00, LossY: 1.2830*10.00), Time: 1.7s | Val TotalLoss: 13.5689, (ReconX: 0.9768, KLD: 0.0967, LossY: 1.2495)\n",
            "INFO:VAETrainer:Epoch [95/100] Train TotalLoss: 13.9093, (ReconX: 0.9797, KLD: 0.0997*1.00, LossY: 1.2830*10.00), Time: 1.7s | Val TotalLoss: 13.5689, (ReconX: 0.9768, KLD: 0.0967, LossY: 1.2495)\n",
            "2025-10-23 22:32:19,422 - VAETrainer - INFO - Epoch [96/100] Train TotalLoss: 14.2621, (ReconX: 0.9797, KLD: 0.0995*1.00, LossY: 1.3183*10.00), Time: 1.7s | Val TotalLoss: 13.4793, (ReconX: 0.9752, KLD: 0.0961, LossY: 1.2408)\n",
            "INFO:VAETrainer:Epoch [96/100] Train TotalLoss: 14.2621, (ReconX: 0.9797, KLD: 0.0995*1.00, LossY: 1.3183*10.00), Time: 1.7s | Val TotalLoss: 13.4793, (ReconX: 0.9752, KLD: 0.0961, LossY: 1.2408)\n",
            "2025-10-23 22:32:21,456 - VAETrainer - INFO - Epoch [97/100] Train TotalLoss: 14.1644, (ReconX: 0.9792, KLD: 0.0995*1.00, LossY: 1.3086*10.00), Time: 1.7s | Val TotalLoss: 13.4236, (ReconX: 0.9754, KLD: 0.0972, LossY: 1.2351)\n",
            "INFO:VAETrainer:Epoch [97/100] Train TotalLoss: 14.1644, (ReconX: 0.9792, KLD: 0.0995*1.00, LossY: 1.3086*10.00), Time: 1.7s | Val TotalLoss: 13.4236, (ReconX: 0.9754, KLD: 0.0972, LossY: 1.2351)\n",
            "2025-10-23 22:32:23,629 - VAETrainer - INFO - Epoch [98/100] Train TotalLoss: 14.1532, (ReconX: 0.9791, KLD: 0.0994*1.00, LossY: 1.3075*10.00), Time: 1.8s | Val TotalLoss: 13.6506, (ReconX: 0.9772, KLD: 0.0958, LossY: 1.2578)\n",
            "INFO:VAETrainer:Epoch [98/100] Train TotalLoss: 14.1532, (ReconX: 0.9791, KLD: 0.0994*1.00, LossY: 1.3075*10.00), Time: 1.8s | Val TotalLoss: 13.6506, (ReconX: 0.9772, KLD: 0.0958, LossY: 1.2578)\n",
            "2025-10-23 22:32:25,777 - VAETrainer - INFO - Epoch [99/100] Train TotalLoss: 14.1414, (ReconX: 0.9792, KLD: 0.0996*1.00, LossY: 1.3063*10.00), Time: 1.8s | Val TotalLoss: 13.5860, (ReconX: 0.9743, KLD: 0.0969, LossY: 1.2515)\n",
            "INFO:VAETrainer:Epoch [99/100] Train TotalLoss: 14.1414, (ReconX: 0.9792, KLD: 0.0996*1.00, LossY: 1.3063*10.00), Time: 1.8s | Val TotalLoss: 13.5860, (ReconX: 0.9743, KLD: 0.0969, LossY: 1.2515)\n",
            "2025-10-23 22:32:27,987 - VAETrainer - INFO - Epoch [100/100] Train TotalLoss: 14.0702, (ReconX: 0.9791, KLD: 0.0995*1.00, LossY: 1.2992*10.00), Time: 1.8s | Val TotalLoss: 13.5116, (ReconX: 0.9756, KLD: 0.0963, LossY: 1.2440)\n",
            "INFO:VAETrainer:Epoch [100/100] Train TotalLoss: 14.0702, (ReconX: 0.9791, KLD: 0.0995*1.00, LossY: 1.2992*10.00), Time: 1.8s | Val TotalLoss: 13.5116, (ReconX: 0.9756, KLD: 0.0963, LossY: 1.2440)\n",
            "2025-10-23 22:32:27,989 - VAETrainer - INFO - Early stopping triggered at epoch 100. Best val_loss: 13.4200\n",
            "INFO:VAETrainer:Early stopping triggered at epoch 100. Best val_loss: 13.4200\n",
            "2025-10-23 22:32:27,990 - VAETrainer - INFO - Training loop finished.\n",
            "INFO:VAETrainer:Training loop finished.\n",
            "2025-10-23 22:32:27,991 - VAETrainer - INFO - Loading best model state from 'vae_results/best_model.pth' (Val Loss: 13.4200).\n",
            "INFO:VAETrainer:Loading best model state from 'vae_results/best_model.pth' (Val Loss: 13.4200).\n",
            "2025-10-23 22:32:27,993 - VAETrainer_Loader - INFO - Loading VAE model and trainer state from vae_results/best_model.pth to device cuda...\n",
            "INFO:VAETrainer_Loader:Loading VAE model and trainer state from vae_results/best_model.pth to device cuda...\n",
            "2025-10-23 22:32:28,035 - SemiSupMIWAE(X:42,Y:1,Lat:3) - INFO - SemiSupMIWAE Initialized: InputXDim=42, TargetYDim=1, LatentDim=3, AlphaPriceLoss=10.0, Device=cuda\n",
            "INFO:SemiSupMIWAE(X:42,Y:1,Lat:3):SemiSupMIWAE Initialized: InputXDim=42, TargetYDim=1, LatentDim=3, AlphaPriceLoss=10.0, Device=cuda\n",
            "2025-10-23 22:32:28,039 - VAETrainer_Loader - INFO - SemiSupMIWAE model created and state loaded successfully.\n",
            "INFO:VAETrainer_Loader:SemiSupMIWAE model created and state loaded successfully.\n",
            "2025-10-23 22:32:28,041 - VAETrainer - INFO - VAETrainer initialized for SemiSupMIWAE. Device: cuda. Optimizer: adam. LR: 0.0003. ReconLoss(X): 'mse'. Alpha(Y Loss): 10.0. LossType(Y): 'gaussian_nll'\n",
            "INFO:VAETrainer:VAETrainer initialized for SemiSupMIWAE. Device: cuda. Optimizer: adam. LR: 0.0003. ReconLoss(X): 'mse'. Alpha(Y Loss): 10.0. LossType(Y): 'gaussian_nll'\n",
            "2025-10-23 22:32:28,043 - VAETrainer_Loader - INFO - VAETrainer instance created and state loaded for model from vae_results/best_model.pth.\n",
            "INFO:VAETrainer_Loader:VAETrainer instance created and state loaded for model from vae_results/best_model.pth.\n",
            "2025-10-23 22:32:28,053 - VAETrainer - INFO - Predicting target (Y) mean and log-variance for 12272 samples...\n",
            "INFO:VAETrainer:Predicting target (Y) mean and log-variance for 12272 samples...\n",
            "2025-10-23 22:32:28,118 - MainRealEstateAnalysis_MIWAE - INFO - [Fold 2] Stored test predictions for 12272 rows.\n",
            "INFO:MainRealEstateAnalysis_MIWAE:[Fold 2] Stored test predictions for 12272 rows.\n",
            "2025-10-23 22:32:28,130 - MainRealEstateAnalysis_MIWAE - INFO - [Fold 3/10] sizes → Train=98170 (80.0%), Val=12271 (10.0%), Test=12271 (10.0%)\n",
            "INFO:MainRealEstateAnalysis_MIWAE:[Fold 3/10] sizes → Train=98170 (80.0%), Val=12271 (10.0%), Test=12271 (10.0%)\n",
            "2025-10-23 22:32:28,131 - __main__.create_vae_from_artifacts - INFO - Creating VAE model. Target column for supervision: log_sale_price.\n",
            "2025-10-23 22:32:28,131 - __main__.create_vae_from_artifacts - INFO - Creating VAE model. Target column for supervision: log_sale_price.\n",
            "INFO:__main__.create_vae_from_artifacts:Creating VAE model. Target column for supervision: log_sale_price.\n",
            "2025-10-23 22:32:28,133 - __main__.create_vae_from_artifacts - INFO - Number of input features for VAE (X): 42 (Features: ['lotarea', 'assess_total', 'exempt_total', 'assessland', 'exemptland']...)\n",
            "2025-10-23 22:32:28,133 - __main__.create_vae_from_artifacts - INFO - Number of input features for VAE (X): 42 (Features: ['lotarea', 'assess_total', 'exempt_total', 'assessland', 'exemptland']...)\n",
            "INFO:__main__.create_vae_from_artifacts:Number of input features for VAE (X): 42 (Features: ['lotarea', 'assess_total', 'exempt_total', 'assessland', 'exemptland']...)\n",
            "2025-10-23 22:32:28,134 - __main__.create_vae_from_artifacts - INFO - Semi-supervised mode: Target 'log_sale_price' (y_dim=1). Alpha for price loss: 10.0.\n",
            "2025-10-23 22:32:28,134 - __main__.create_vae_from_artifacts - INFO - Semi-supervised mode: Target 'log_sale_price' (y_dim=1). Alpha for price loss: 10.0.\n",
            "INFO:__main__.create_vae_from_artifacts:Semi-supervised mode: Target 'log_sale_price' (y_dim=1). Alpha for price loss: 10.0.\n",
            "2025-10-23 22:32:28,135 - __main__.create_vae_from_artifacts - INFO - Using overridden latent_dim: 3\n",
            "2025-10-23 22:32:28,135 - __main__.create_vae_from_artifacts - INFO - Using overridden latent_dim: 3\n",
            "INFO:__main__.create_vae_from_artifacts:Using overridden latent_dim: 3\n",
            "2025-10-23 22:32:28,136 - __main__.create_vae_from_artifacts - INFO - Final effective Latent Dimension: 3\n",
            "2025-10-23 22:32:28,136 - __main__.create_vae_from_artifacts - INFO - Final effective Latent Dimension: 3\n",
            "INFO:__main__.create_vae_from_artifacts:Final effective Latent Dimension: 3\n",
            "2025-10-23 22:32:28,137 - __main__.create_vae_from_artifacts - INFO - Encoder layers: [21], Decoder layers: [21]\n",
            "2025-10-23 22:32:28,137 - __main__.create_vae_from_artifacts - INFO - Encoder layers: [21], Decoder layers: [21]\n",
            "INFO:__main__.create_vae_from_artifacts:Encoder layers: [21], Decoder layers: [21]\n",
            "2025-10-23 22:32:28,141 - __main__.create_vae_from_artifacts - INFO - Using determined prior: Type=student_t_mixture, K=2 (from overrides or artifact inference).\n",
            "2025-10-23 22:32:28,141 - __main__.create_vae_from_artifacts - INFO - Using determined prior: Type=student_t_mixture, K=2 (from overrides or artifact inference).\n",
            "INFO:__main__.create_vae_from_artifacts:Using determined prior: Type=student_t_mixture, K=2 (from overrides or artifact inference).\n",
            "2025-10-23 22:32:28,142 - __main__.create_vae_from_artifacts - INFO - Initializing parameters for student_t_mixture with K=2 components...\n",
            "2025-10-23 22:32:28,142 - __main__.create_vae_from_artifacts - INFO - Initializing parameters for student_t_mixture with K=2 components...\n",
            "INFO:__main__.create_vae_from_artifacts:Initializing parameters for student_t_mixture with K=2 components...\n",
            "2025-10-23 22:32:28,143 - __main__.create_vae_from_artifacts - WARNING - DP-GMM artifacts are unsuitable or insufficient for initializing mixture prior parameters. Default parameters (e.g., zeros for means/logits, identity for covariances) will be used by the VAE.\n",
            "2025-10-23 22:32:28,143 - __main__.create_vae_from_artifacts - WARNING - DP-GMM artifacts are unsuitable or insufficient for initializing mixture prior parameters. Default parameters (e.g., zeros for means/logits, identity for covariances) will be used by the VAE.\n",
            "WARNING:__main__.create_vae_from_artifacts:DP-GMM artifacts are unsuitable or insufficient for initializing mixture prior parameters. Default parameters (e.g., zeros for means/logits, identity for covariances) will be used by the VAE.\n",
            "2025-10-23 22:32:28,145 - __main__.create_vae_from_artifacts - INFO - Setting Student-t df to 4.0 for prior.\n",
            "2025-10-23 22:32:28,145 - __main__.create_vae_from_artifacts - INFO - Setting Student-t df to 4.0 for prior.\n",
            "INFO:__main__.create_vae_from_artifacts:Setting Student-t df to 4.0 for prior.\n",
            "2025-10-23 22:32:28,146 - __main__.create_vae_from_artifacts - INFO - Instantiating SemiSupMIWAE model...\n",
            "2025-10-23 22:32:28,146 - __main__.create_vae_from_artifacts - INFO - Instantiating SemiSupMIWAE model...\n",
            "INFO:__main__.create_vae_from_artifacts:Instantiating SemiSupMIWAE model...\n",
            "2025-10-23 22:32:28,147 - __main__.create_vae_from_artifacts - INFO - Price head layers for SemiSupMIWAE: [8, 4]\n",
            "2025-10-23 22:32:28,147 - __main__.create_vae_from_artifacts - INFO - Price head layers for SemiSupMIWAE: [8, 4]\n",
            "INFO:__main__.create_vae_from_artifacts:Price head layers for SemiSupMIWAE: [8, 4]\n",
            "2025-10-23 22:32:28,153 - SemiSupMIWAE(X:42,Y:1,Lat:3) - INFO - SemiSupMIWAE Initialized: InputXDim=42, TargetYDim=1, LatentDim=3, AlphaPriceLoss=10.0, Device=cuda\n",
            "INFO:SemiSupMIWAE(X:42,Y:1,Lat:3):SemiSupMIWAE Initialized: InputXDim=42, TargetYDim=1, LatentDim=3, AlphaPriceLoss=10.0, Device=cuda\n",
            "2025-10-23 22:32:28,154 - __main__.create_vae_from_artifacts - INFO - SemiSupMIWAE model created successfully on device: cuda.\n",
            "2025-10-23 22:32:28,154 - __main__.create_vae_from_artifacts - INFO - SemiSupMIWAE model created successfully on device: cuda.\n",
            "INFO:__main__.create_vae_from_artifacts:SemiSupMIWAE model created successfully on device: cuda.\n",
            "2025-10-23 22:32:28,156 - VAETrainer - INFO - VAETrainer initialized for SemiSupMIWAE. Device: cuda. Optimizer: adam. LR: 0.0003. ReconLoss(X): 'mse'. Alpha(Y Loss): 10.0. LossType(Y): 'gaussian_nll'\n",
            "INFO:VAETrainer:VAETrainer initialized for SemiSupMIWAE. Device: cuda. Optimizer: adam. LR: 0.0003. ReconLoss(X): 'mse'. Alpha(Y Loss): 10.0. LossType(Y): 'gaussian_nll'\n",
            "2025-10-23 22:32:28,157 - VAETrainer - INFO - Starting training: Epochs=100, BatchSize=512, KLD_weight=1.0, KLD_AnnealEpochs=30, Alpha(Y_Loss)=10.0.\n",
            "INFO:VAETrainer:Starting training: Epochs=100, BatchSize=512, KLD_weight=1.0, KLD_AnnealEpochs=30, Alpha(Y_Loss)=10.0.\n",
            "2025-10-23 22:32:28,158 - VAETrainer - INFO - Train loader: 98170 samples, 192 batches. Drop last: False\n",
            "INFO:VAETrainer:Train loader: 98170 samples, 192 batches. Drop last: False\n",
            "2025-10-23 22:32:28,159 - VAETrainer - INFO - Validation loader: 12271 samples, 24 batches.\n",
            "INFO:VAETrainer:Validation loader: 12271 samples, 24 batches.\n",
            "2025-10-23 22:32:30,265 - VAETrainer - INFO - Epoch [1/100] Train TotalLoss: 557.9083, (ReconX: 1.0766, KLD: 0.3460*0.03, LossY: 55.6820*10.00), Time: 1.7s | Val TotalLoss: 432.9071, (ReconX: 1.0133, KLD: 0.4184, LossY: 43.1475)\n",
            "INFO:VAETrainer:Epoch [1/100] Train TotalLoss: 557.9083, (ReconX: 1.0766, KLD: 0.3460*0.03, LossY: 55.6820*10.00), Time: 1.7s | Val TotalLoss: 432.9071, (ReconX: 1.0133, KLD: 0.4184, LossY: 43.1475)\n",
            "2025-10-23 22:32:30,266 - VAETrainer - INFO - Saving model and trainer state to vae_results/best_model.pth...\n",
            "INFO:VAETrainer:Saving model and trainer state to vae_results/best_model.pth...\n",
            "2025-10-23 22:32:30,280 - VAETrainer - INFO - Successfully saved model and trainer state to vae_results/best_model.pth\n",
            "INFO:VAETrainer:Successfully saved model and trainer state to vae_results/best_model.pth\n",
            "2025-10-23 22:32:32,494 - VAETrainer - INFO - Epoch [2/100] Train TotalLoss: 381.8463, (ReconX: 0.9704, KLD: 0.4584*0.07, LossY: 38.0845*10.00), Time: 1.8s | Val TotalLoss: 334.6584, (ReconX: 0.9453, KLD: 0.5214, LossY: 33.3192)\n",
            "INFO:VAETrainer:Epoch [2/100] Train TotalLoss: 381.8463, (ReconX: 0.9704, KLD: 0.4584*0.07, LossY: 38.0845*10.00), Time: 1.8s | Val TotalLoss: 334.6584, (ReconX: 0.9453, KLD: 0.5214, LossY: 33.3192)\n",
            "2025-10-23 22:32:32,496 - VAETrainer - INFO - Saving model and trainer state to vae_results/best_model.pth...\n",
            "INFO:VAETrainer:Saving model and trainer state to vae_results/best_model.pth...\n",
            "2025-10-23 22:32:32,509 - VAETrainer - INFO - Successfully saved model and trainer state to vae_results/best_model.pth\n",
            "INFO:VAETrainer:Successfully saved model and trainer state to vae_results/best_model.pth\n",
            "2025-10-23 22:32:34,684 - VAETrainer - INFO - Epoch [3/100] Train TotalLoss: 300.2396, (ReconX: 0.9272, KLD: 0.5243*0.10, LossY: 29.9260*10.00), Time: 1.8s | Val TotalLoss: 263.7268, (ReconX: 0.9249, KLD: 0.5363, LossY: 26.2266)\n",
            "INFO:VAETrainer:Epoch [3/100] Train TotalLoss: 300.2396, (ReconX: 0.9272, KLD: 0.5243*0.10, LossY: 29.9260*10.00), Time: 1.8s | Val TotalLoss: 263.7268, (ReconX: 0.9249, KLD: 0.5363, LossY: 26.2266)\n",
            "2025-10-23 22:32:34,686 - VAETrainer - INFO - Saving model and trainer state to vae_results/best_model.pth...\n",
            "INFO:VAETrainer:Saving model and trainer state to vae_results/best_model.pth...\n",
            "2025-10-23 22:32:34,699 - VAETrainer - INFO - Successfully saved model and trainer state to vae_results/best_model.pth\n",
            "INFO:VAETrainer:Successfully saved model and trainer state to vae_results/best_model.pth\n",
            "2025-10-23 22:32:36,918 - VAETrainer - INFO - Epoch [4/100] Train TotalLoss: 235.8949, (ReconX: 0.9177, KLD: 0.5064*0.13, LossY: 23.4910*10.00), Time: 1.8s | Val TotalLoss: 202.9269, (ReconX: 0.9263, KLD: 0.4937, LossY: 20.1507)\n",
            "INFO:VAETrainer:Epoch [4/100] Train TotalLoss: 235.8949, (ReconX: 0.9177, KLD: 0.5064*0.13, LossY: 23.4910*10.00), Time: 1.8s | Val TotalLoss: 202.9269, (ReconX: 0.9263, KLD: 0.4937, LossY: 20.1507)\n",
            "2025-10-23 22:32:36,920 - VAETrainer - INFO - Saving model and trainer state to vae_results/best_model.pth...\n",
            "INFO:VAETrainer:Saving model and trainer state to vae_results/best_model.pth...\n",
            "2025-10-23 22:32:36,934 - VAETrainer - INFO - Successfully saved model and trainer state to vae_results/best_model.pth\n",
            "INFO:VAETrainer:Successfully saved model and trainer state to vae_results/best_model.pth\n",
            "2025-10-23 22:32:39,181 - VAETrainer - INFO - Epoch [5/100] Train TotalLoss: 182.3033, (ReconX: 0.9232, KLD: 0.4655*0.17, LossY: 18.1303*10.00), Time: 1.8s | Val TotalLoss: 158.8035, (ReconX: 0.9299, KLD: 0.4575, LossY: 15.7416)\n",
            "INFO:VAETrainer:Epoch [5/100] Train TotalLoss: 182.3033, (ReconX: 0.9232, KLD: 0.4655*0.17, LossY: 18.1303*10.00), Time: 1.8s | Val TotalLoss: 158.8035, (ReconX: 0.9299, KLD: 0.4575, LossY: 15.7416)\n",
            "2025-10-23 22:32:39,183 - VAETrainer - INFO - Saving model and trainer state to vae_results/best_model.pth...\n",
            "INFO:VAETrainer:Saving model and trainer state to vae_results/best_model.pth...\n",
            "2025-10-23 22:32:39,196 - VAETrainer - INFO - Successfully saved model and trainer state to vae_results/best_model.pth\n",
            "INFO:VAETrainer:Successfully saved model and trainer state to vae_results/best_model.pth\n",
            "2025-10-23 22:32:41,394 - VAETrainer - INFO - Epoch [6/100] Train TotalLoss: 143.5424, (ReconX: 0.9237, KLD: 0.4500*0.20, LossY: 14.2529*10.00), Time: 1.8s | Val TotalLoss: 126.9376, (ReconX: 0.9286, KLD: 0.4581, LossY: 12.5551)\n",
            "INFO:VAETrainer:Epoch [6/100] Train TotalLoss: 143.5424, (ReconX: 0.9237, KLD: 0.4500*0.20, LossY: 14.2529*10.00), Time: 1.8s | Val TotalLoss: 126.9376, (ReconX: 0.9286, KLD: 0.4581, LossY: 12.5551)\n",
            "2025-10-23 22:32:41,395 - VAETrainer - INFO - Saving model and trainer state to vae_results/best_model.pth...\n",
            "INFO:VAETrainer:Saving model and trainer state to vae_results/best_model.pth...\n",
            "2025-10-23 22:32:41,408 - VAETrainer - INFO - Successfully saved model and trainer state to vae_results/best_model.pth\n",
            "INFO:VAETrainer:Successfully saved model and trainer state to vae_results/best_model.pth\n",
            "2025-10-23 22:32:43,543 - VAETrainer - INFO - Epoch [7/100] Train TotalLoss: 118.0080, (ReconX: 0.9226, KLD: 0.4502*0.23, LossY: 11.6980*10.00), Time: 1.8s | Val TotalLoss: 105.8303, (ReconX: 0.9266, KLD: 0.4563, LossY: 10.4447)\n",
            "INFO:VAETrainer:Epoch [7/100] Train TotalLoss: 118.0080, (ReconX: 0.9226, KLD: 0.4502*0.23, LossY: 11.6980*10.00), Time: 1.8s | Val TotalLoss: 105.8303, (ReconX: 0.9266, KLD: 0.4563, LossY: 10.4447)\n",
            "2025-10-23 22:32:43,544 - VAETrainer - INFO - Saving model and trainer state to vae_results/best_model.pth...\n",
            "INFO:VAETrainer:Saving model and trainer state to vae_results/best_model.pth...\n",
            "2025-10-23 22:32:43,561 - VAETrainer - INFO - Successfully saved model and trainer state to vae_results/best_model.pth\n",
            "INFO:VAETrainer:Successfully saved model and trainer state to vae_results/best_model.pth\n",
            "2025-10-23 22:32:45,742 - VAETrainer - INFO - Epoch [8/100] Train TotalLoss: 99.2777, (ReconX: 0.9222, KLD: 0.4492*0.27, LossY: 9.8236*10.00), Time: 1.8s | Val TotalLoss: 90.6692, (ReconX: 0.9287, KLD: 0.4406, LossY: 8.9300)\n",
            "INFO:VAETrainer:Epoch [8/100] Train TotalLoss: 99.2777, (ReconX: 0.9222, KLD: 0.4492*0.27, LossY: 9.8236*10.00), Time: 1.8s | Val TotalLoss: 90.6692, (ReconX: 0.9287, KLD: 0.4406, LossY: 8.9300)\n",
            "2025-10-23 22:32:45,743 - VAETrainer - INFO - Saving model and trainer state to vae_results/best_model.pth...\n",
            "INFO:VAETrainer:Saving model and trainer state to vae_results/best_model.pth...\n",
            "2025-10-23 22:32:45,756 - VAETrainer - INFO - Successfully saved model and trainer state to vae_results/best_model.pth\n",
            "INFO:VAETrainer:Successfully saved model and trainer state to vae_results/best_model.pth\n",
            "2025-10-23 22:32:47,880 - VAETrainer - INFO - Epoch [9/100] Train TotalLoss: 85.4288, (ReconX: 0.9230, KLD: 0.4459*0.30, LossY: 8.4372*10.00), Time: 1.7s | Val TotalLoss: 78.9109, (ReconX: 0.9299, KLD: 0.4490, LossY: 7.7532)\n",
            "INFO:VAETrainer:Epoch [9/100] Train TotalLoss: 85.4288, (ReconX: 0.9230, KLD: 0.4459*0.30, LossY: 8.4372*10.00), Time: 1.7s | Val TotalLoss: 78.9109, (ReconX: 0.9299, KLD: 0.4490, LossY: 7.7532)\n",
            "2025-10-23 22:32:47,882 - VAETrainer - INFO - Saving model and trainer state to vae_results/best_model.pth...\n",
            "INFO:VAETrainer:Saving model and trainer state to vae_results/best_model.pth...\n",
            "2025-10-23 22:32:47,895 - VAETrainer - INFO - Successfully saved model and trainer state to vae_results/best_model.pth\n",
            "INFO:VAETrainer:Successfully saved model and trainer state to vae_results/best_model.pth\n",
            "2025-10-23 22:32:50,092 - VAETrainer - INFO - Epoch [10/100] Train TotalLoss: 74.9101, (ReconX: 0.9226, KLD: 0.4462*0.33, LossY: 7.3839*10.00), Time: 1.8s | Val TotalLoss: 69.0279, (ReconX: 0.9280, KLD: 0.4467, LossY: 6.7653)\n",
            "INFO:VAETrainer:Epoch [10/100] Train TotalLoss: 74.9101, (ReconX: 0.9226, KLD: 0.4462*0.33, LossY: 7.3839*10.00), Time: 1.8s | Val TotalLoss: 69.0279, (ReconX: 0.9280, KLD: 0.4467, LossY: 6.7653)\n",
            "2025-10-23 22:32:50,093 - VAETrainer - INFO - Saving model and trainer state to vae_results/best_model.pth...\n",
            "INFO:VAETrainer:Saving model and trainer state to vae_results/best_model.pth...\n",
            "2025-10-23 22:32:50,106 - VAETrainer - INFO - Successfully saved model and trainer state to vae_results/best_model.pth\n",
            "INFO:VAETrainer:Successfully saved model and trainer state to vae_results/best_model.pth\n",
            "2025-10-23 22:32:52,336 - VAETrainer - INFO - Epoch [11/100] Train TotalLoss: 66.6730, (ReconX: 0.9226, KLD: 0.4458*0.37, LossY: 6.5587*10.00), Time: 1.8s | Val TotalLoss: 62.0700, (ReconX: 0.9273, KLD: 0.4448, LossY: 6.0698)\n",
            "INFO:VAETrainer:Epoch [11/100] Train TotalLoss: 66.6730, (ReconX: 0.9226, KLD: 0.4458*0.37, LossY: 6.5587*10.00), Time: 1.8s | Val TotalLoss: 62.0700, (ReconX: 0.9273, KLD: 0.4448, LossY: 6.0698)\n",
            "2025-10-23 22:32:52,337 - VAETrainer - INFO - Saving model and trainer state to vae_results/best_model.pth...\n",
            "INFO:VAETrainer:Saving model and trainer state to vae_results/best_model.pth...\n",
            "2025-10-23 22:32:52,353 - VAETrainer - INFO - Successfully saved model and trainer state to vae_results/best_model.pth\n",
            "INFO:VAETrainer:Successfully saved model and trainer state to vae_results/best_model.pth\n",
            "2025-10-23 22:32:54,630 - VAETrainer - INFO - Epoch [12/100] Train TotalLoss: 60.1817, (ReconX: 0.9232, KLD: 0.4436*0.40, LossY: 5.9081*10.00), Time: 1.9s | Val TotalLoss: 57.5838, (ReconX: 0.9295, KLD: 0.4450, LossY: 5.6209)\n",
            "INFO:VAETrainer:Epoch [12/100] Train TotalLoss: 60.1817, (ReconX: 0.9232, KLD: 0.4436*0.40, LossY: 5.9081*10.00), Time: 1.9s | Val TotalLoss: 57.5838, (ReconX: 0.9295, KLD: 0.4450, LossY: 5.6209)\n",
            "2025-10-23 22:32:54,632 - VAETrainer - INFO - Saving model and trainer state to vae_results/best_model.pth...\n",
            "INFO:VAETrainer:Saving model and trainer state to vae_results/best_model.pth...\n",
            "2025-10-23 22:32:54,645 - VAETrainer - INFO - Successfully saved model and trainer state to vae_results/best_model.pth\n",
            "INFO:VAETrainer:Successfully saved model and trainer state to vae_results/best_model.pth\n",
            "2025-10-23 22:32:56,715 - VAETrainer - INFO - Epoch [13/100] Train TotalLoss: 55.0114, (ReconX: 0.9237, KLD: 0.4386*0.43, LossY: 5.3898*10.00), Time: 1.7s | Val TotalLoss: 52.7514, (ReconX: 0.9294, KLD: 0.4353, LossY: 5.1387)\n",
            "INFO:VAETrainer:Epoch [13/100] Train TotalLoss: 55.0114, (ReconX: 0.9237, KLD: 0.4386*0.43, LossY: 5.3898*10.00), Time: 1.7s | Val TotalLoss: 52.7514, (ReconX: 0.9294, KLD: 0.4353, LossY: 5.1387)\n",
            "2025-10-23 22:32:56,717 - VAETrainer - INFO - Saving model and trainer state to vae_results/best_model.pth...\n",
            "INFO:VAETrainer:Saving model and trainer state to vae_results/best_model.pth...\n",
            "2025-10-23 22:32:56,730 - VAETrainer - INFO - Successfully saved model and trainer state to vae_results/best_model.pth\n",
            "INFO:VAETrainer:Successfully saved model and trainer state to vae_results/best_model.pth\n",
            "2025-10-23 22:32:58,944 - VAETrainer - INFO - Epoch [14/100] Train TotalLoss: 51.0229, (ReconX: 0.9244, KLD: 0.4318*0.47, LossY: 4.9897*10.00), Time: 1.8s | Val TotalLoss: 48.5535, (ReconX: 0.9327, KLD: 0.4241, LossY: 4.7197)\n",
            "INFO:VAETrainer:Epoch [14/100] Train TotalLoss: 51.0229, (ReconX: 0.9244, KLD: 0.4318*0.47, LossY: 4.9897*10.00), Time: 1.8s | Val TotalLoss: 48.5535, (ReconX: 0.9327, KLD: 0.4241, LossY: 4.7197)\n",
            "2025-10-23 22:32:58,945 - VAETrainer - INFO - Saving model and trainer state to vae_results/best_model.pth...\n",
            "INFO:VAETrainer:Saving model and trainer state to vae_results/best_model.pth...\n",
            "2025-10-23 22:32:58,961 - VAETrainer - INFO - Successfully saved model and trainer state to vae_results/best_model.pth\n",
            "INFO:VAETrainer:Successfully saved model and trainer state to vae_results/best_model.pth\n",
            "2025-10-23 22:33:01,076 - VAETrainer - INFO - Epoch [15/100] Train TotalLoss: 47.7704, (ReconX: 0.9259, KLD: 0.4223*0.50, LossY: 4.6633*10.00), Time: 1.7s | Val TotalLoss: 45.6642, (ReconX: 0.9304, KLD: 0.4175, LossY: 4.4316)\n",
            "INFO:VAETrainer:Epoch [15/100] Train TotalLoss: 47.7704, (ReconX: 0.9259, KLD: 0.4223*0.50, LossY: 4.6633*10.00), Time: 1.7s | Val TotalLoss: 45.6642, (ReconX: 0.9304, KLD: 0.4175, LossY: 4.4316)\n",
            "2025-10-23 22:33:01,078 - VAETrainer - INFO - Saving model and trainer state to vae_results/best_model.pth...\n",
            "INFO:VAETrainer:Saving model and trainer state to vae_results/best_model.pth...\n",
            "2025-10-23 22:33:01,090 - VAETrainer - INFO - Successfully saved model and trainer state to vae_results/best_model.pth\n",
            "INFO:VAETrainer:Successfully saved model and trainer state to vae_results/best_model.pth\n",
            "2025-10-23 22:33:03,419 - VAETrainer - INFO - Epoch [16/100] Train TotalLoss: 45.2397, (ReconX: 0.9273, KLD: 0.4162*0.53, LossY: 4.4091*10.00), Time: 1.9s | Val TotalLoss: 44.0095, (ReconX: 0.9326, KLD: 0.4105, LossY: 4.2666)\n",
            "INFO:VAETrainer:Epoch [16/100] Train TotalLoss: 45.2397, (ReconX: 0.9273, KLD: 0.4162*0.53, LossY: 4.4091*10.00), Time: 1.9s | Val TotalLoss: 44.0095, (ReconX: 0.9326, KLD: 0.4105, LossY: 4.2666)\n",
            "2025-10-23 22:33:03,421 - VAETrainer - INFO - Saving model and trainer state to vae_results/best_model.pth...\n",
            "INFO:VAETrainer:Saving model and trainer state to vae_results/best_model.pth...\n",
            "2025-10-23 22:33:03,434 - VAETrainer - INFO - Successfully saved model and trainer state to vae_results/best_model.pth\n",
            "INFO:VAETrainer:Successfully saved model and trainer state to vae_results/best_model.pth\n",
            "2025-10-23 22:33:05,636 - VAETrainer - INFO - Epoch [17/100] Train TotalLoss: 43.0324, (ReconX: 0.9276, KLD: 0.4063*0.57, LossY: 4.1875*10.00), Time: 1.8s | Val TotalLoss: 41.9220, (ReconX: 0.9360, KLD: 0.3981, LossY: 4.0588)\n",
            "INFO:VAETrainer:Epoch [17/100] Train TotalLoss: 43.0324, (ReconX: 0.9276, KLD: 0.4063*0.57, LossY: 4.1875*10.00), Time: 1.8s | Val TotalLoss: 41.9220, (ReconX: 0.9360, KLD: 0.3981, LossY: 4.0588)\n",
            "2025-10-23 22:33:05,638 - VAETrainer - INFO - Saving model and trainer state to vae_results/best_model.pth...\n",
            "INFO:VAETrainer:Saving model and trainer state to vae_results/best_model.pth...\n",
            "2025-10-23 22:33:05,650 - VAETrainer - INFO - Successfully saved model and trainer state to vae_results/best_model.pth\n",
            "INFO:VAETrainer:Successfully saved model and trainer state to vae_results/best_model.pth\n",
            "2025-10-23 22:33:07,745 - VAETrainer - INFO - Epoch [18/100] Train TotalLoss: 41.4456, (ReconX: 0.9287, KLD: 0.4002*0.60, LossY: 4.0277*10.00), Time: 1.7s | Val TotalLoss: 39.9237, (ReconX: 0.9355, KLD: 0.3954, LossY: 3.8593)\n",
            "INFO:VAETrainer:Epoch [18/100] Train TotalLoss: 41.4456, (ReconX: 0.9287, KLD: 0.4002*0.60, LossY: 4.0277*10.00), Time: 1.7s | Val TotalLoss: 39.9237, (ReconX: 0.9355, KLD: 0.3954, LossY: 3.8593)\n",
            "2025-10-23 22:33:07,747 - VAETrainer - INFO - Saving model and trainer state to vae_results/best_model.pth...\n",
            "INFO:VAETrainer:Saving model and trainer state to vae_results/best_model.pth...\n",
            "2025-10-23 22:33:07,760 - VAETrainer - INFO - Successfully saved model and trainer state to vae_results/best_model.pth\n",
            "INFO:VAETrainer:Successfully saved model and trainer state to vae_results/best_model.pth\n",
            "2025-10-23 22:33:09,896 - VAETrainer - INFO - Epoch [19/100] Train TotalLoss: 40.0779, (ReconX: 0.9293, KLD: 0.3964*0.63, LossY: 3.8898*10.00), Time: 1.7s | Val TotalLoss: 39.0495, (ReconX: 0.9363, KLD: 0.3901, LossY: 3.7723)\n",
            "INFO:VAETrainer:Epoch [19/100] Train TotalLoss: 40.0779, (ReconX: 0.9293, KLD: 0.3964*0.63, LossY: 3.8898*10.00), Time: 1.7s | Val TotalLoss: 39.0495, (ReconX: 0.9363, KLD: 0.3901, LossY: 3.7723)\n",
            "2025-10-23 22:33:09,898 - VAETrainer - INFO - Saving model and trainer state to vae_results/best_model.pth...\n",
            "INFO:VAETrainer:Saving model and trainer state to vae_results/best_model.pth...\n",
            "2025-10-23 22:33:09,911 - VAETrainer - INFO - Successfully saved model and trainer state to vae_results/best_model.pth\n",
            "INFO:VAETrainer:Successfully saved model and trainer state to vae_results/best_model.pth\n",
            "2025-10-23 22:33:12,124 - VAETrainer - INFO - Epoch [20/100] Train TotalLoss: 38.8969, (ReconX: 0.9309, KLD: 0.3812*0.67, LossY: 3.7712*10.00), Time: 1.8s | Val TotalLoss: 38.2863, (ReconX: 0.9385, KLD: 0.3690, LossY: 3.6979)\n",
            "INFO:VAETrainer:Epoch [20/100] Train TotalLoss: 38.8969, (ReconX: 0.9309, KLD: 0.3812*0.67, LossY: 3.7712*10.00), Time: 1.8s | Val TotalLoss: 38.2863, (ReconX: 0.9385, KLD: 0.3690, LossY: 3.6979)\n",
            "2025-10-23 22:33:12,125 - VAETrainer - INFO - Saving model and trainer state to vae_results/best_model.pth...\n",
            "INFO:VAETrainer:Saving model and trainer state to vae_results/best_model.pth...\n",
            "2025-10-23 22:33:12,141 - VAETrainer - INFO - Successfully saved model and trainer state to vae_results/best_model.pth\n",
            "INFO:VAETrainer:Successfully saved model and trainer state to vae_results/best_model.pth\n",
            "2025-10-23 22:33:14,217 - VAETrainer - INFO - Epoch [21/100] Train TotalLoss: 38.0454, (ReconX: 0.9328, KLD: 0.3690*0.70, LossY: 3.6854*10.00), Time: 1.7s | Val TotalLoss: 37.1574, (ReconX: 0.9408, KLD: 0.3562, LossY: 3.5861)\n",
            "INFO:VAETrainer:Epoch [21/100] Train TotalLoss: 38.0454, (ReconX: 0.9328, KLD: 0.3690*0.70, LossY: 3.6854*10.00), Time: 1.7s | Val TotalLoss: 37.1574, (ReconX: 0.9408, KLD: 0.3562, LossY: 3.5861)\n",
            "2025-10-23 22:33:14,219 - VAETrainer - INFO - Saving model and trainer state to vae_results/best_model.pth...\n",
            "INFO:VAETrainer:Saving model and trainer state to vae_results/best_model.pth...\n",
            "2025-10-23 22:33:14,232 - VAETrainer - INFO - Successfully saved model and trainer state to vae_results/best_model.pth\n",
            "INFO:VAETrainer:Successfully saved model and trainer state to vae_results/best_model.pth\n",
            "2025-10-23 22:33:16,395 - VAETrainer - INFO - Epoch [22/100] Train TotalLoss: 37.2997, (ReconX: 0.9341, KLD: 0.3559*0.73, LossY: 3.6105*10.00), Time: 1.8s | Val TotalLoss: 36.5192, (ReconX: 0.9417, KLD: 0.3446, LossY: 3.5233)\n",
            "INFO:VAETrainer:Epoch [22/100] Train TotalLoss: 37.2997, (ReconX: 0.9341, KLD: 0.3559*0.73, LossY: 3.6105*10.00), Time: 1.8s | Val TotalLoss: 36.5192, (ReconX: 0.9417, KLD: 0.3446, LossY: 3.5233)\n",
            "2025-10-23 22:33:16,397 - VAETrainer - INFO - Saving model and trainer state to vae_results/best_model.pth...\n",
            "INFO:VAETrainer:Saving model and trainer state to vae_results/best_model.pth...\n",
            "2025-10-23 22:33:16,410 - VAETrainer - INFO - Successfully saved model and trainer state to vae_results/best_model.pth\n",
            "INFO:VAETrainer:Successfully saved model and trainer state to vae_results/best_model.pth\n",
            "2025-10-23 22:33:18,655 - VAETrainer - INFO - Epoch [23/100] Train TotalLoss: 36.7046, (ReconX: 0.9358, KLD: 0.3389*0.77, LossY: 3.5509*10.00), Time: 1.9s | Val TotalLoss: 36.0188, (ReconX: 0.9451, KLD: 0.3284, LossY: 3.4745)\n",
            "INFO:VAETrainer:Epoch [23/100] Train TotalLoss: 36.7046, (ReconX: 0.9358, KLD: 0.3389*0.77, LossY: 3.5509*10.00), Time: 1.9s | Val TotalLoss: 36.0188, (ReconX: 0.9451, KLD: 0.3284, LossY: 3.4745)\n",
            "2025-10-23 22:33:18,657 - VAETrainer - INFO - Saving model and trainer state to vae_results/best_model.pth...\n",
            "INFO:VAETrainer:Saving model and trainer state to vae_results/best_model.pth...\n",
            "2025-10-23 22:33:18,671 - VAETrainer - INFO - Successfully saved model and trainer state to vae_results/best_model.pth\n",
            "INFO:VAETrainer:Successfully saved model and trainer state to vae_results/best_model.pth\n",
            "2025-10-23 22:33:20,859 - VAETrainer - INFO - Epoch [24/100] Train TotalLoss: 36.1792, (ReconX: 0.9381, KLD: 0.3224*0.80, LossY: 3.4983*10.00), Time: 1.8s | Val TotalLoss: 35.6444, (ReconX: 0.9467, KLD: 0.3110, LossY: 3.4387)\n",
            "INFO:VAETrainer:Epoch [24/100] Train TotalLoss: 36.1792, (ReconX: 0.9381, KLD: 0.3224*0.80, LossY: 3.4983*10.00), Time: 1.8s | Val TotalLoss: 35.6444, (ReconX: 0.9467, KLD: 0.3110, LossY: 3.4387)\n",
            "2025-10-23 22:33:20,861 - VAETrainer - INFO - Saving model and trainer state to vae_results/best_model.pth...\n",
            "INFO:VAETrainer:Saving model and trainer state to vae_results/best_model.pth...\n",
            "2025-10-23 22:33:20,875 - VAETrainer - INFO - Successfully saved model and trainer state to vae_results/best_model.pth\n",
            "INFO:VAETrainer:Successfully saved model and trainer state to vae_results/best_model.pth\n",
            "2025-10-23 22:33:23,007 - VAETrainer - INFO - Epoch [25/100] Train TotalLoss: 35.7722, (ReconX: 0.9401, KLD: 0.3064*0.83, LossY: 3.4577*10.00), Time: 1.8s | Val TotalLoss: 35.1481, (ReconX: 0.9493, KLD: 0.2906, LossY: 3.3908)\n",
            "INFO:VAETrainer:Epoch [25/100] Train TotalLoss: 35.7722, (ReconX: 0.9401, KLD: 0.3064*0.83, LossY: 3.4577*10.00), Time: 1.8s | Val TotalLoss: 35.1481, (ReconX: 0.9493, KLD: 0.2906, LossY: 3.3908)\n",
            "2025-10-23 22:33:23,009 - VAETrainer - INFO - Saving model and trainer state to vae_results/best_model.pth...\n",
            "INFO:VAETrainer:Saving model and trainer state to vae_results/best_model.pth...\n",
            "2025-10-23 22:33:23,022 - VAETrainer - INFO - Successfully saved model and trainer state to vae_results/best_model.pth\n",
            "INFO:VAETrainer:Successfully saved model and trainer state to vae_results/best_model.pth\n",
            "2025-10-23 22:33:25,224 - VAETrainer - INFO - Epoch [26/100] Train TotalLoss: 35.3796, (ReconX: 0.9425, KLD: 0.2875*0.87, LossY: 3.4188*10.00), Time: 1.8s | Val TotalLoss: 34.9593, (ReconX: 0.9516, KLD: 0.2766, LossY: 3.3731)\n",
            "INFO:VAETrainer:Epoch [26/100] Train TotalLoss: 35.3796, (ReconX: 0.9425, KLD: 0.2875*0.87, LossY: 3.4188*10.00), Time: 1.8s | Val TotalLoss: 34.9593, (ReconX: 0.9516, KLD: 0.2766, LossY: 3.3731)\n",
            "2025-10-23 22:33:25,226 - VAETrainer - INFO - Saving model and trainer state to vae_results/best_model.pth...\n",
            "INFO:VAETrainer:Saving model and trainer state to vae_results/best_model.pth...\n",
            "2025-10-23 22:33:25,239 - VAETrainer - INFO - Successfully saved model and trainer state to vae_results/best_model.pth\n",
            "INFO:VAETrainer:Successfully saved model and trainer state to vae_results/best_model.pth\n",
            "2025-10-23 22:33:27,402 - VAETrainer - INFO - Epoch [27/100] Train TotalLoss: 35.1586, (ReconX: 0.9455, KLD: 0.2701*0.90, LossY: 3.3970*10.00), Time: 1.8s | Val TotalLoss: 34.7645, (ReconX: 0.9524, KLD: 0.2669, LossY: 3.3545)\n",
            "INFO:VAETrainer:Epoch [27/100] Train TotalLoss: 35.1586, (ReconX: 0.9455, KLD: 0.2701*0.90, LossY: 3.3970*10.00), Time: 1.8s | Val TotalLoss: 34.7645, (ReconX: 0.9524, KLD: 0.2669, LossY: 3.3545)\n",
            "2025-10-23 22:33:27,403 - VAETrainer - INFO - Saving model and trainer state to vae_results/best_model.pth...\n",
            "INFO:VAETrainer:Saving model and trainer state to vae_results/best_model.pth...\n",
            "2025-10-23 22:33:27,416 - VAETrainer - INFO - Successfully saved model and trainer state to vae_results/best_model.pth\n",
            "INFO:VAETrainer:Successfully saved model and trainer state to vae_results/best_model.pth\n",
            "2025-10-23 22:33:29,681 - VAETrainer - INFO - Epoch [28/100] Train TotalLoss: 34.9306, (ReconX: 0.9483, KLD: 0.2573*0.93, LossY: 3.3742*10.00), Time: 1.9s | Val TotalLoss: 34.4071, (ReconX: 0.9552, KLD: 0.2462, LossY: 3.3206)\n",
            "INFO:VAETrainer:Epoch [28/100] Train TotalLoss: 34.9306, (ReconX: 0.9483, KLD: 0.2573*0.93, LossY: 3.3742*10.00), Time: 1.9s | Val TotalLoss: 34.4071, (ReconX: 0.9552, KLD: 0.2462, LossY: 3.3206)\n",
            "2025-10-23 22:33:29,683 - VAETrainer - INFO - Saving model and trainer state to vae_results/best_model.pth...\n",
            "INFO:VAETrainer:Saving model and trainer state to vae_results/best_model.pth...\n",
            "2025-10-23 22:33:29,697 - VAETrainer - INFO - Successfully saved model and trainer state to vae_results/best_model.pth\n",
            "INFO:VAETrainer:Successfully saved model and trainer state to vae_results/best_model.pth\n",
            "2025-10-23 22:33:31,845 - VAETrainer - INFO - Epoch [29/100] Train TotalLoss: 34.6948, (ReconX: 0.9514, KLD: 0.2375*0.97, LossY: 3.3514*10.00), Time: 1.8s | Val TotalLoss: 34.2378, (ReconX: 0.9576, KLD: 0.2302, LossY: 3.3050)\n",
            "INFO:VAETrainer:Epoch [29/100] Train TotalLoss: 34.6948, (ReconX: 0.9514, KLD: 0.2375*0.97, LossY: 3.3514*10.00), Time: 1.8s | Val TotalLoss: 34.2378, (ReconX: 0.9576, KLD: 0.2302, LossY: 3.3050)\n",
            "2025-10-23 22:33:31,847 - VAETrainer - INFO - Saving model and trainer state to vae_results/best_model.pth...\n",
            "INFO:VAETrainer:Saving model and trainer state to vae_results/best_model.pth...\n",
            "2025-10-23 22:33:31,860 - VAETrainer - INFO - Successfully saved model and trainer state to vae_results/best_model.pth\n",
            "INFO:VAETrainer:Successfully saved model and trainer state to vae_results/best_model.pth\n",
            "2025-10-23 22:33:34,006 - VAETrainer - INFO - Epoch [30/100] Train TotalLoss: 34.5408, (ReconX: 0.9537, KLD: 0.2243*1.00, LossY: 3.3363*10.00), Time: 1.8s | Val TotalLoss: 34.0664, (ReconX: 0.9582, KLD: 0.2138, LossY: 3.2894)\n",
            "INFO:VAETrainer:Epoch [30/100] Train TotalLoss: 34.5408, (ReconX: 0.9537, KLD: 0.2243*1.00, LossY: 3.3363*10.00), Time: 1.8s | Val TotalLoss: 34.0664, (ReconX: 0.9582, KLD: 0.2138, LossY: 3.2894)\n",
            "2025-10-23 22:33:34,008 - VAETrainer - INFO - Saving model and trainer state to vae_results/best_model.pth...\n",
            "INFO:VAETrainer:Saving model and trainer state to vae_results/best_model.pth...\n",
            "2025-10-23 22:33:34,022 - VAETrainer - INFO - Successfully saved model and trainer state to vae_results/best_model.pth\n",
            "INFO:VAETrainer:Successfully saved model and trainer state to vae_results/best_model.pth\n",
            "2025-10-23 22:33:36,155 - VAETrainer - INFO - Epoch [31/100] Train TotalLoss: 34.3683, (ReconX: 0.9558, KLD: 0.2092*1.00, LossY: 3.3203*10.00), Time: 1.8s | Val TotalLoss: 34.0173, (ReconX: 0.9631, KLD: 0.2016, LossY: 3.2853)\n",
            "INFO:VAETrainer:Epoch [31/100] Train TotalLoss: 34.3683, (ReconX: 0.9558, KLD: 0.2092*1.00, LossY: 3.3203*10.00), Time: 1.8s | Val TotalLoss: 34.0173, (ReconX: 0.9631, KLD: 0.2016, LossY: 3.2853)\n",
            "2025-10-23 22:33:36,157 - VAETrainer - INFO - Saving model and trainer state to vae_results/best_model.pth...\n",
            "INFO:VAETrainer:Saving model and trainer state to vae_results/best_model.pth...\n",
            "2025-10-23 22:33:36,170 - VAETrainer - INFO - Successfully saved model and trainer state to vae_results/best_model.pth\n",
            "INFO:VAETrainer:Successfully saved model and trainer state to vae_results/best_model.pth\n",
            "2025-10-23 22:33:38,333 - VAETrainer - INFO - Epoch [32/100] Train TotalLoss: 34.2228, (ReconX: 0.9583, KLD: 0.1969*1.00, LossY: 3.3067*10.00), Time: 1.8s | Val TotalLoss: 33.8194, (ReconX: 0.9649, KLD: 0.1920, LossY: 3.2663)\n",
            "INFO:VAETrainer:Epoch [32/100] Train TotalLoss: 34.2228, (ReconX: 0.9583, KLD: 0.1969*1.00, LossY: 3.3067*10.00), Time: 1.8s | Val TotalLoss: 33.8194, (ReconX: 0.9649, KLD: 0.1920, LossY: 3.2663)\n",
            "2025-10-23 22:33:38,335 - VAETrainer - INFO - Saving model and trainer state to vae_results/best_model.pth...\n",
            "INFO:VAETrainer:Saving model and trainer state to vae_results/best_model.pth...\n",
            "2025-10-23 22:33:38,348 - VAETrainer - INFO - Successfully saved model and trainer state to vae_results/best_model.pth\n",
            "INFO:VAETrainer:Successfully saved model and trainer state to vae_results/best_model.pth\n",
            "2025-10-23 22:33:40,575 - VAETrainer - INFO - Epoch [33/100] Train TotalLoss: 34.0436, (ReconX: 0.9599, KLD: 0.1848*1.00, LossY: 3.2899*10.00), Time: 1.8s | Val TotalLoss: 33.6289, (ReconX: 0.9676, KLD: 0.1760, LossY: 3.2485)\n",
            "INFO:VAETrainer:Epoch [33/100] Train TotalLoss: 34.0436, (ReconX: 0.9599, KLD: 0.1848*1.00, LossY: 3.2899*10.00), Time: 1.8s | Val TotalLoss: 33.6289, (ReconX: 0.9676, KLD: 0.1760, LossY: 3.2485)\n",
            "2025-10-23 22:33:40,576 - VAETrainer - INFO - Saving model and trainer state to vae_results/best_model.pth...\n",
            "INFO:VAETrainer:Saving model and trainer state to vae_results/best_model.pth...\n",
            "2025-10-23 22:33:40,589 - VAETrainer - INFO - Successfully saved model and trainer state to vae_results/best_model.pth\n",
            "INFO:VAETrainer:Successfully saved model and trainer state to vae_results/best_model.pth\n",
            "2025-10-23 22:33:42,779 - VAETrainer - INFO - Epoch [34/100] Train TotalLoss: 33.8785, (ReconX: 0.9621, KLD: 0.1728*1.00, LossY: 3.2744*10.00), Time: 1.8s | Val TotalLoss: 33.4236, (ReconX: 0.9700, KLD: 0.1624, LossY: 3.2291)\n",
            "INFO:VAETrainer:Epoch [34/100] Train TotalLoss: 33.8785, (ReconX: 0.9621, KLD: 0.1728*1.00, LossY: 3.2744*10.00), Time: 1.8s | Val TotalLoss: 33.4236, (ReconX: 0.9700, KLD: 0.1624, LossY: 3.2291)\n",
            "2025-10-23 22:33:42,781 - VAETrainer - INFO - Saving model and trainer state to vae_results/best_model.pth...\n",
            "INFO:VAETrainer:Saving model and trainer state to vae_results/best_model.pth...\n",
            "2025-10-23 22:33:42,794 - VAETrainer - INFO - Successfully saved model and trainer state to vae_results/best_model.pth\n",
            "INFO:VAETrainer:Successfully saved model and trainer state to vae_results/best_model.pth\n",
            "2025-10-23 22:33:45,010 - VAETrainer - INFO - Epoch [35/100] Train TotalLoss: 33.7062, (ReconX: 0.9651, KLD: 0.1619*1.00, LossY: 3.2579*10.00), Time: 1.8s | Val TotalLoss: 33.2782, (ReconX: 0.9686, KLD: 0.1626, LossY: 3.2147)\n",
            "INFO:VAETrainer:Epoch [35/100] Train TotalLoss: 33.7062, (ReconX: 0.9651, KLD: 0.1619*1.00, LossY: 3.2579*10.00), Time: 1.8s | Val TotalLoss: 33.2782, (ReconX: 0.9686, KLD: 0.1626, LossY: 3.2147)\n",
            "2025-10-23 22:33:45,011 - VAETrainer - INFO - Saving model and trainer state to vae_results/best_model.pth...\n",
            "INFO:VAETrainer:Saving model and trainer state to vae_results/best_model.pth...\n",
            "2025-10-23 22:33:45,025 - VAETrainer - INFO - Successfully saved model and trainer state to vae_results/best_model.pth\n",
            "INFO:VAETrainer:Successfully saved model and trainer state to vae_results/best_model.pth\n",
            "2025-10-23 22:33:47,200 - VAETrainer - INFO - Epoch [36/100] Train TotalLoss: 33.4723, (ReconX: 0.9658, KLD: 0.1515*1.00, LossY: 3.2355*10.00), Time: 1.8s | Val TotalLoss: 33.1891, (ReconX: 0.9746, KLD: 0.1408, LossY: 3.2074)\n",
            "INFO:VAETrainer:Epoch [36/100] Train TotalLoss: 33.4723, (ReconX: 0.9658, KLD: 0.1515*1.00, LossY: 3.2355*10.00), Time: 1.8s | Val TotalLoss: 33.1891, (ReconX: 0.9746, KLD: 0.1408, LossY: 3.2074)\n",
            "2025-10-23 22:33:47,201 - VAETrainer - INFO - Saving model and trainer state to vae_results/best_model.pth...\n",
            "INFO:VAETrainer:Saving model and trainer state to vae_results/best_model.pth...\n",
            "2025-10-23 22:33:47,216 - VAETrainer - INFO - Successfully saved model and trainer state to vae_results/best_model.pth\n",
            "INFO:VAETrainer:Successfully saved model and trainer state to vae_results/best_model.pth\n",
            "2025-10-23 22:33:49,360 - VAETrainer - INFO - Epoch [37/100] Train TotalLoss: 33.3217, (ReconX: 0.9680, KLD: 0.1438*1.00, LossY: 3.2210*10.00), Time: 1.8s | Val TotalLoss: 33.0626, (ReconX: 0.9738, KLD: 0.1402, LossY: 3.1949)\n",
            "INFO:VAETrainer:Epoch [37/100] Train TotalLoss: 33.3217, (ReconX: 0.9680, KLD: 0.1438*1.00, LossY: 3.2210*10.00), Time: 1.8s | Val TotalLoss: 33.0626, (ReconX: 0.9738, KLD: 0.1402, LossY: 3.1949)\n",
            "2025-10-23 22:33:49,362 - VAETrainer - INFO - Saving model and trainer state to vae_results/best_model.pth...\n",
            "INFO:VAETrainer:Saving model and trainer state to vae_results/best_model.pth...\n",
            "2025-10-23 22:33:49,378 - VAETrainer - INFO - Successfully saved model and trainer state to vae_results/best_model.pth\n",
            "INFO:VAETrainer:Successfully saved model and trainer state to vae_results/best_model.pth\n",
            "2025-10-23 22:33:51,459 - VAETrainer - INFO - Epoch [38/100] Train TotalLoss: 33.0221, (ReconX: 0.9697, KLD: 0.1326*1.00, LossY: 3.1920*10.00), Time: 1.7s | Val TotalLoss: 32.6800, (ReconX: 0.9792, KLD: 0.1271, LossY: 3.1574)\n",
            "INFO:VAETrainer:Epoch [38/100] Train TotalLoss: 33.0221, (ReconX: 0.9697, KLD: 0.1326*1.00, LossY: 3.1920*10.00), Time: 1.7s | Val TotalLoss: 32.6800, (ReconX: 0.9792, KLD: 0.1271, LossY: 3.1574)\n",
            "2025-10-23 22:33:51,460 - VAETrainer - INFO - Saving model and trainer state to vae_results/best_model.pth...\n",
            "INFO:VAETrainer:Saving model and trainer state to vae_results/best_model.pth...\n",
            "2025-10-23 22:33:51,474 - VAETrainer - INFO - Successfully saved model and trainer state to vae_results/best_model.pth\n",
            "INFO:VAETrainer:Successfully saved model and trainer state to vae_results/best_model.pth\n",
            "2025-10-23 22:33:53,664 - VAETrainer - INFO - Epoch [39/100] Train TotalLoss: 32.7415, (ReconX: 0.9726, KLD: 0.1228*1.00, LossY: 3.1646*10.00), Time: 1.8s | Val TotalLoss: 32.4527, (ReconX: 0.9790, KLD: 0.1211, LossY: 3.1353)\n",
            "INFO:VAETrainer:Epoch [39/100] Train TotalLoss: 32.7415, (ReconX: 0.9726, KLD: 0.1228*1.00, LossY: 3.1646*10.00), Time: 1.8s | Val TotalLoss: 32.4527, (ReconX: 0.9790, KLD: 0.1211, LossY: 3.1353)\n",
            "2025-10-23 22:33:53,666 - VAETrainer - INFO - Saving model and trainer state to vae_results/best_model.pth...\n",
            "INFO:VAETrainer:Saving model and trainer state to vae_results/best_model.pth...\n",
            "2025-10-23 22:33:53,680 - VAETrainer - INFO - Successfully saved model and trainer state to vae_results/best_model.pth\n",
            "INFO:VAETrainer:Successfully saved model and trainer state to vae_results/best_model.pth\n",
            "2025-10-23 22:33:55,939 - VAETrainer - INFO - Epoch [40/100] Train TotalLoss: 32.2918, (ReconX: 0.9735, KLD: 0.1162*1.00, LossY: 3.1202*10.00), Time: 1.9s | Val TotalLoss: 31.7648, (ReconX: 0.9820, KLD: 0.1104, LossY: 3.0672)\n",
            "INFO:VAETrainer:Epoch [40/100] Train TotalLoss: 32.2918, (ReconX: 0.9735, KLD: 0.1162*1.00, LossY: 3.1202*10.00), Time: 1.9s | Val TotalLoss: 31.7648, (ReconX: 0.9820, KLD: 0.1104, LossY: 3.0672)\n",
            "2025-10-23 22:33:55,940 - VAETrainer - INFO - Saving model and trainer state to vae_results/best_model.pth...\n",
            "INFO:VAETrainer:Saving model and trainer state to vae_results/best_model.pth...\n",
            "2025-10-23 22:33:55,954 - VAETrainer - INFO - Successfully saved model and trainer state to vae_results/best_model.pth\n",
            "INFO:VAETrainer:Successfully saved model and trainer state to vae_results/best_model.pth\n",
            "2025-10-23 22:33:58,043 - VAETrainer - INFO - Epoch [41/100] Train TotalLoss: 31.6295, (ReconX: 0.9733, KLD: 0.1125*1.00, LossY: 3.0544*10.00), Time: 1.7s | Val TotalLoss: 31.4892, (ReconX: 0.9822, KLD: 0.1145, LossY: 3.0392)\n",
            "INFO:VAETrainer:Epoch [41/100] Train TotalLoss: 31.6295, (ReconX: 0.9733, KLD: 0.1125*1.00, LossY: 3.0544*10.00), Time: 1.7s | Val TotalLoss: 31.4892, (ReconX: 0.9822, KLD: 0.1145, LossY: 3.0392)\n",
            "2025-10-23 22:33:58,044 - VAETrainer - INFO - Saving model and trainer state to vae_results/best_model.pth...\n",
            "INFO:VAETrainer:Saving model and trainer state to vae_results/best_model.pth...\n",
            "2025-10-23 22:33:58,057 - VAETrainer - INFO - Successfully saved model and trainer state to vae_results/best_model.pth\n",
            "INFO:VAETrainer:Successfully saved model and trainer state to vae_results/best_model.pth\n",
            "2025-10-23 22:34:00,196 - VAETrainer - INFO - Epoch [42/100] Train TotalLoss: 30.7522, (ReconX: 0.9730, KLD: 0.1166*1.00, LossY: 2.9663*10.00), Time: 1.7s | Val TotalLoss: 30.2145, (ReconX: 0.9783, KLD: 0.1172, LossY: 2.9119)\n",
            "INFO:VAETrainer:Epoch [42/100] Train TotalLoss: 30.7522, (ReconX: 0.9730, KLD: 0.1166*1.00, LossY: 2.9663*10.00), Time: 1.7s | Val TotalLoss: 30.2145, (ReconX: 0.9783, KLD: 0.1172, LossY: 2.9119)\n",
            "2025-10-23 22:34:00,197 - VAETrainer - INFO - Saving model and trainer state to vae_results/best_model.pth...\n",
            "INFO:VAETrainer:Saving model and trainer state to vae_results/best_model.pth...\n",
            "2025-10-23 22:34:00,213 - VAETrainer - INFO - Successfully saved model and trainer state to vae_results/best_model.pth\n",
            "INFO:VAETrainer:Successfully saved model and trainer state to vae_results/best_model.pth\n",
            "2025-10-23 22:34:02,319 - VAETrainer - INFO - Epoch [43/100] Train TotalLoss: 29.7273, (ReconX: 0.9720, KLD: 0.1169*1.00, LossY: 2.8638*10.00), Time: 1.7s | Val TotalLoss: 28.5122, (ReconX: 0.9800, KLD: 0.1111, LossY: 2.7421)\n",
            "INFO:VAETrainer:Epoch [43/100] Train TotalLoss: 29.7273, (ReconX: 0.9720, KLD: 0.1169*1.00, LossY: 2.8638*10.00), Time: 1.7s | Val TotalLoss: 28.5122, (ReconX: 0.9800, KLD: 0.1111, LossY: 2.7421)\n",
            "2025-10-23 22:34:02,321 - VAETrainer - INFO - Saving model and trainer state to vae_results/best_model.pth...\n",
            "INFO:VAETrainer:Saving model and trainer state to vae_results/best_model.pth...\n",
            "2025-10-23 22:34:02,336 - VAETrainer - INFO - Successfully saved model and trainer state to vae_results/best_model.pth\n",
            "INFO:VAETrainer:Successfully saved model and trainer state to vae_results/best_model.pth\n",
            "2025-10-23 22:34:04,448 - VAETrainer - INFO - Epoch [44/100] Train TotalLoss: 28.1010, (ReconX: 0.9732, KLD: 0.1142*1.00, LossY: 2.7014*10.00), Time: 1.7s | Val TotalLoss: 26.5704, (ReconX: 0.9804, KLD: 0.1124, LossY: 2.5478)\n",
            "INFO:VAETrainer:Epoch [44/100] Train TotalLoss: 28.1010, (ReconX: 0.9732, KLD: 0.1142*1.00, LossY: 2.7014*10.00), Time: 1.7s | Val TotalLoss: 26.5704, (ReconX: 0.9804, KLD: 0.1124, LossY: 2.5478)\n",
            "2025-10-23 22:34:04,450 - VAETrainer - INFO - Saving model and trainer state to vae_results/best_model.pth...\n",
            "INFO:VAETrainer:Saving model and trainer state to vae_results/best_model.pth...\n",
            "2025-10-23 22:34:04,465 - VAETrainer - INFO - Successfully saved model and trainer state to vae_results/best_model.pth\n",
            "INFO:VAETrainer:Successfully saved model and trainer state to vae_results/best_model.pth\n",
            "2025-10-23 22:34:06,680 - VAETrainer - INFO - Epoch [45/100] Train TotalLoss: 26.7183, (ReconX: 0.9748, KLD: 0.1068*1.00, LossY: 2.5637*10.00), Time: 1.8s | Val TotalLoss: 25.8459, (ReconX: 0.9833, KLD: 0.1033, LossY: 2.4759)\n",
            "INFO:VAETrainer:Epoch [45/100] Train TotalLoss: 26.7183, (ReconX: 0.9748, KLD: 0.1068*1.00, LossY: 2.5637*10.00), Time: 1.8s | Val TotalLoss: 25.8459, (ReconX: 0.9833, KLD: 0.1033, LossY: 2.4759)\n",
            "2025-10-23 22:34:06,682 - VAETrainer - INFO - Saving model and trainer state to vae_results/best_model.pth...\n",
            "INFO:VAETrainer:Saving model and trainer state to vae_results/best_model.pth...\n",
            "2025-10-23 22:34:06,697 - VAETrainer - INFO - Successfully saved model and trainer state to vae_results/best_model.pth\n",
            "INFO:VAETrainer:Successfully saved model and trainer state to vae_results/best_model.pth\n",
            "2025-10-23 22:34:08,889 - VAETrainer - INFO - Epoch [46/100] Train TotalLoss: 25.3819, (ReconX: 0.9762, KLD: 0.1012*1.00, LossY: 2.4304*10.00), Time: 1.8s | Val TotalLoss: 26.5565, (ReconX: 0.9818, KLD: 0.0979, LossY: 2.5477)\n",
            "INFO:VAETrainer:Epoch [46/100] Train TotalLoss: 25.3819, (ReconX: 0.9762, KLD: 0.1012*1.00, LossY: 2.4304*10.00), Time: 1.8s | Val TotalLoss: 26.5565, (ReconX: 0.9818, KLD: 0.0979, LossY: 2.5477)\n",
            "2025-10-23 22:34:11,048 - VAETrainer - INFO - Epoch [47/100] Train TotalLoss: 23.5781, (ReconX: 0.9778, KLD: 0.0907*1.00, LossY: 2.2510*10.00), Time: 1.8s | Val TotalLoss: 24.4294, (ReconX: 0.9876, KLD: 0.0823, LossY: 2.3360)\n",
            "INFO:VAETrainer:Epoch [47/100] Train TotalLoss: 23.5781, (ReconX: 0.9778, KLD: 0.0907*1.00, LossY: 2.2510*10.00), Time: 1.8s | Val TotalLoss: 24.4294, (ReconX: 0.9876, KLD: 0.0823, LossY: 2.3360)\n",
            "2025-10-23 22:34:11,050 - VAETrainer - INFO - Saving model and trainer state to vae_results/best_model.pth...\n",
            "INFO:VAETrainer:Saving model and trainer state to vae_results/best_model.pth...\n",
            "2025-10-23 22:34:11,062 - VAETrainer - INFO - Successfully saved model and trainer state to vae_results/best_model.pth\n",
            "INFO:VAETrainer:Successfully saved model and trainer state to vae_results/best_model.pth\n",
            "2025-10-23 22:34:13,154 - VAETrainer - INFO - Epoch [48/100] Train TotalLoss: 21.2279, (ReconX: 0.9815, KLD: 0.0802*1.00, LossY: 2.0166*10.00), Time: 1.7s | Val TotalLoss: 20.3016, (ReconX: 0.9874, KLD: 0.0775, LossY: 1.9237)\n",
            "INFO:VAETrainer:Epoch [48/100] Train TotalLoss: 21.2279, (ReconX: 0.9815, KLD: 0.0802*1.00, LossY: 2.0166*10.00), Time: 1.7s | Val TotalLoss: 20.3016, (ReconX: 0.9874, KLD: 0.0775, LossY: 1.9237)\n",
            "2025-10-23 22:34:13,156 - VAETrainer - INFO - Saving model and trainer state to vae_results/best_model.pth...\n",
            "INFO:VAETrainer:Saving model and trainer state to vae_results/best_model.pth...\n",
            "2025-10-23 22:34:13,169 - VAETrainer - INFO - Successfully saved model and trainer state to vae_results/best_model.pth\n",
            "INFO:VAETrainer:Successfully saved model and trainer state to vae_results/best_model.pth\n",
            "2025-10-23 22:34:15,350 - VAETrainer - INFO - Epoch [49/100] Train TotalLoss: 19.8177, (ReconX: 0.9824, KLD: 0.0762*1.00, LossY: 1.8759*10.00), Time: 1.8s | Val TotalLoss: 19.8175, (ReconX: 0.9897, KLD: 0.0746, LossY: 1.8753)\n",
            "INFO:VAETrainer:Epoch [49/100] Train TotalLoss: 19.8177, (ReconX: 0.9824, KLD: 0.0762*1.00, LossY: 1.8759*10.00), Time: 1.8s | Val TotalLoss: 19.8175, (ReconX: 0.9897, KLD: 0.0746, LossY: 1.8753)\n",
            "2025-10-23 22:34:15,353 - VAETrainer - INFO - Saving model and trainer state to vae_results/best_model.pth...\n",
            "INFO:VAETrainer:Saving model and trainer state to vae_results/best_model.pth...\n",
            "2025-10-23 22:34:15,365 - VAETrainer - INFO - Successfully saved model and trainer state to vae_results/best_model.pth\n",
            "INFO:VAETrainer:Successfully saved model and trainer state to vae_results/best_model.pth\n",
            "2025-10-23 22:34:17,567 - VAETrainer - INFO - Epoch [50/100] Train TotalLoss: 19.3763, (ReconX: 0.9839, KLD: 0.0741*1.00, LossY: 1.8318*10.00), Time: 1.8s | Val TotalLoss: 19.0227, (ReconX: 0.9908, KLD: 0.0736, LossY: 1.7958)\n",
            "INFO:VAETrainer:Epoch [50/100] Train TotalLoss: 19.3763, (ReconX: 0.9839, KLD: 0.0741*1.00, LossY: 1.8318*10.00), Time: 1.8s | Val TotalLoss: 19.0227, (ReconX: 0.9908, KLD: 0.0736, LossY: 1.7958)\n",
            "2025-10-23 22:34:17,570 - VAETrainer - INFO - Saving model and trainer state to vae_results/best_model.pth...\n",
            "INFO:VAETrainer:Saving model and trainer state to vae_results/best_model.pth...\n",
            "2025-10-23 22:34:17,584 - VAETrainer - INFO - Successfully saved model and trainer state to vae_results/best_model.pth\n",
            "INFO:VAETrainer:Successfully saved model and trainer state to vae_results/best_model.pth\n",
            "2025-10-23 22:34:19,801 - VAETrainer - INFO - Epoch [51/100] Train TotalLoss: 19.0426, (ReconX: 0.9837, KLD: 0.0724*1.00, LossY: 1.7987*10.00), Time: 1.8s | Val TotalLoss: 20.2081, (ReconX: 0.9903, KLD: 0.0709, LossY: 1.9147)\n",
            "INFO:VAETrainer:Epoch [51/100] Train TotalLoss: 19.0426, (ReconX: 0.9837, KLD: 0.0724*1.00, LossY: 1.7987*10.00), Time: 1.8s | Val TotalLoss: 20.2081, (ReconX: 0.9903, KLD: 0.0709, LossY: 1.9147)\n",
            "2025-10-23 22:34:21,896 - VAETrainer - INFO - Epoch [52/100] Train TotalLoss: 18.8509, (ReconX: 0.9844, KLD: 0.0707*1.00, LossY: 1.7796*10.00), Time: 1.7s | Val TotalLoss: 18.6963, (ReconX: 0.9904, KLD: 0.0697, LossY: 1.7636)\n",
            "INFO:VAETrainer:Epoch [52/100] Train TotalLoss: 18.8509, (ReconX: 0.9844, KLD: 0.0707*1.00, LossY: 1.7796*10.00), Time: 1.7s | Val TotalLoss: 18.6963, (ReconX: 0.9904, KLD: 0.0697, LossY: 1.7636)\n",
            "2025-10-23 22:34:21,898 - VAETrainer - INFO - Saving model and trainer state to vae_results/best_model.pth...\n",
            "INFO:VAETrainer:Saving model and trainer state to vae_results/best_model.pth...\n",
            "2025-10-23 22:34:21,911 - VAETrainer - INFO - Successfully saved model and trainer state to vae_results/best_model.pth\n",
            "INFO:VAETrainer:Successfully saved model and trainer state to vae_results/best_model.pth\n",
            "2025-10-23 22:34:24,013 - VAETrainer - INFO - Epoch [53/100] Train TotalLoss: 18.5943, (ReconX: 0.9842, KLD: 0.0704*1.00, LossY: 1.7540*10.00), Time: 1.7s | Val TotalLoss: 18.0307, (ReconX: 0.9915, KLD: 0.0673, LossY: 1.6972)\n",
            "INFO:VAETrainer:Epoch [53/100] Train TotalLoss: 18.5943, (ReconX: 0.9842, KLD: 0.0704*1.00, LossY: 1.7540*10.00), Time: 1.7s | Val TotalLoss: 18.0307, (ReconX: 0.9915, KLD: 0.0673, LossY: 1.6972)\n",
            "2025-10-23 22:34:24,015 - VAETrainer - INFO - Saving model and trainer state to vae_results/best_model.pth...\n",
            "INFO:VAETrainer:Saving model and trainer state to vae_results/best_model.pth...\n",
            "2025-10-23 22:34:24,030 - VAETrainer - INFO - Successfully saved model and trainer state to vae_results/best_model.pth\n",
            "INFO:VAETrainer:Successfully saved model and trainer state to vae_results/best_model.pth\n",
            "2025-10-23 22:34:26,205 - VAETrainer - INFO - Epoch [54/100] Train TotalLoss: 18.4400, (ReconX: 0.9839, KLD: 0.0694*1.00, LossY: 1.7387*10.00), Time: 1.8s | Val TotalLoss: 28.5383, (ReconX: 0.9885, KLD: 0.0682, LossY: 2.7482)\n",
            "INFO:VAETrainer:Epoch [54/100] Train TotalLoss: 18.4400, (ReconX: 0.9839, KLD: 0.0694*1.00, LossY: 1.7387*10.00), Time: 1.8s | Val TotalLoss: 28.5383, (ReconX: 0.9885, KLD: 0.0682, LossY: 2.7482)\n",
            "2025-10-23 22:34:28,487 - VAETrainer - INFO - Epoch [55/100] Train TotalLoss: 17.7799, (ReconX: 0.9848, KLD: 0.0674*1.00, LossY: 1.6728*10.00), Time: 1.9s | Val TotalLoss: 17.4233, (ReconX: 0.9909, KLD: 0.0666, LossY: 1.6366)\n",
            "INFO:VAETrainer:Epoch [55/100] Train TotalLoss: 17.7799, (ReconX: 0.9848, KLD: 0.0674*1.00, LossY: 1.6728*10.00), Time: 1.9s | Val TotalLoss: 17.4233, (ReconX: 0.9909, KLD: 0.0666, LossY: 1.6366)\n",
            "2025-10-23 22:34:28,489 - VAETrainer - INFO - Saving model and trainer state to vae_results/best_model.pth...\n",
            "INFO:VAETrainer:Saving model and trainer state to vae_results/best_model.pth...\n",
            "2025-10-23 22:34:28,503 - VAETrainer - INFO - Successfully saved model and trainer state to vae_results/best_model.pth\n",
            "INFO:VAETrainer:Successfully saved model and trainer state to vae_results/best_model.pth\n",
            "2025-10-23 22:34:30,691 - VAETrainer - INFO - Epoch [56/100] Train TotalLoss: 17.4558, (ReconX: 0.9853, KLD: 0.0666*1.00, LossY: 1.6404*10.00), Time: 1.8s | Val TotalLoss: 17.4846, (ReconX: 0.9920, KLD: 0.0650, LossY: 1.6428)\n",
            "INFO:VAETrainer:Epoch [56/100] Train TotalLoss: 17.4558, (ReconX: 0.9853, KLD: 0.0666*1.00, LossY: 1.6404*10.00), Time: 1.8s | Val TotalLoss: 17.4846, (ReconX: 0.9920, KLD: 0.0650, LossY: 1.6428)\n",
            "2025-10-23 22:34:32,966 - VAETrainer - INFO - Epoch [57/100] Train TotalLoss: 17.2627, (ReconX: 0.9849, KLD: 0.0655*1.00, LossY: 1.6212*10.00), Time: 1.9s | Val TotalLoss: 16.0733, (ReconX: 0.9921, KLD: 0.0640, LossY: 1.5017)\n",
            "INFO:VAETrainer:Epoch [57/100] Train TotalLoss: 17.2627, (ReconX: 0.9849, KLD: 0.0655*1.00, LossY: 1.6212*10.00), Time: 1.9s | Val TotalLoss: 16.0733, (ReconX: 0.9921, KLD: 0.0640, LossY: 1.5017)\n",
            "2025-10-23 22:34:32,967 - VAETrainer - INFO - Saving model and trainer state to vae_results/best_model.pth...\n",
            "INFO:VAETrainer:Saving model and trainer state to vae_results/best_model.pth...\n",
            "2025-10-23 22:34:32,982 - VAETrainer - INFO - Successfully saved model and trainer state to vae_results/best_model.pth\n",
            "INFO:VAETrainer:Successfully saved model and trainer state to vae_results/best_model.pth\n",
            "2025-10-23 22:34:35,152 - VAETrainer - INFO - Epoch [58/100] Train TotalLoss: 16.8209, (ReconX: 0.9861, KLD: 0.0647*1.00, LossY: 1.5770*10.00), Time: 1.8s | Val TotalLoss: 21.6624, (ReconX: 0.9917, KLD: 0.0628, LossY: 2.0608)\n",
            "INFO:VAETrainer:Epoch [58/100] Train TotalLoss: 16.8209, (ReconX: 0.9861, KLD: 0.0647*1.00, LossY: 1.5770*10.00), Time: 1.8s | Val TotalLoss: 21.6624, (ReconX: 0.9917, KLD: 0.0628, LossY: 2.0608)\n",
            "2025-10-23 22:34:37,327 - VAETrainer - INFO - Epoch [59/100] Train TotalLoss: 16.6067, (ReconX: 0.9855, KLD: 0.0650*1.00, LossY: 1.5556*10.00), Time: 1.8s | Val TotalLoss: 18.4757, (ReconX: 0.9910, KLD: 0.0646, LossY: 1.7420)\n",
            "INFO:VAETrainer:Epoch [59/100] Train TotalLoss: 16.6067, (ReconX: 0.9855, KLD: 0.0650*1.00, LossY: 1.5556*10.00), Time: 1.8s | Val TotalLoss: 18.4757, (ReconX: 0.9910, KLD: 0.0646, LossY: 1.7420)\n",
            "2025-10-23 22:34:39,420 - VAETrainer - INFO - Epoch [60/100] Train TotalLoss: 16.2958, (ReconX: 0.9860, KLD: 0.0639*1.00, LossY: 1.5246*10.00), Time: 1.7s | Val TotalLoss: 15.3025, (ReconX: 0.9922, KLD: 0.0606, LossY: 1.4250)\n",
            "INFO:VAETrainer:Epoch [60/100] Train TotalLoss: 16.2958, (ReconX: 0.9860, KLD: 0.0639*1.00, LossY: 1.5246*10.00), Time: 1.7s | Val TotalLoss: 15.3025, (ReconX: 0.9922, KLD: 0.0606, LossY: 1.4250)\n",
            "2025-10-23 22:34:39,422 - VAETrainer - INFO - Saving model and trainer state to vae_results/best_model.pth...\n",
            "INFO:VAETrainer:Saving model and trainer state to vae_results/best_model.pth...\n",
            "2025-10-23 22:34:39,435 - VAETrainer - INFO - Successfully saved model and trainer state to vae_results/best_model.pth\n",
            "INFO:VAETrainer:Successfully saved model and trainer state to vae_results/best_model.pth\n",
            "2025-10-23 22:34:41,629 - VAETrainer - INFO - Epoch [61/100] Train TotalLoss: 16.1090, (ReconX: 0.9862, KLD: 0.0638*1.00, LossY: 1.5059*10.00), Time: 1.8s | Val TotalLoss: 16.2474, (ReconX: 0.9923, KLD: 0.0636, LossY: 1.5192)\n",
            "INFO:VAETrainer:Epoch [61/100] Train TotalLoss: 16.1090, (ReconX: 0.9862, KLD: 0.0638*1.00, LossY: 1.5059*10.00), Time: 1.8s | Val TotalLoss: 16.2474, (ReconX: 0.9923, KLD: 0.0636, LossY: 1.5192)\n",
            "2025-10-23 22:34:43,835 - VAETrainer - INFO - Epoch [62/100] Train TotalLoss: 15.8919, (ReconX: 0.9855, KLD: 0.0641*1.00, LossY: 1.4842*10.00), Time: 1.8s | Val TotalLoss: 16.7737, (ReconX: 0.9929, KLD: 0.0618, LossY: 1.5719)\n",
            "INFO:VAETrainer:Epoch [62/100] Train TotalLoss: 15.8919, (ReconX: 0.9855, KLD: 0.0641*1.00, LossY: 1.4842*10.00), Time: 1.8s | Val TotalLoss: 16.7737, (ReconX: 0.9929, KLD: 0.0618, LossY: 1.5719)\n",
            "2025-10-23 22:34:46,012 - VAETrainer - INFO - Epoch [63/100] Train TotalLoss: 15.8645, (ReconX: 0.9858, KLD: 0.0631*1.00, LossY: 1.4816*10.00), Time: 1.8s | Val TotalLoss: 17.1639, (ReconX: 0.9919, KLD: 0.0626, LossY: 1.6109)\n",
            "INFO:VAETrainer:Epoch [63/100] Train TotalLoss: 15.8645, (ReconX: 0.9858, KLD: 0.0631*1.00, LossY: 1.4816*10.00), Time: 1.8s | Val TotalLoss: 17.1639, (ReconX: 0.9919, KLD: 0.0626, LossY: 1.6109)\n",
            "2025-10-23 22:34:48,176 - VAETrainer - INFO - Epoch [64/100] Train TotalLoss: 15.5567, (ReconX: 0.9863, KLD: 0.0626*1.00, LossY: 1.4508*10.00), Time: 1.8s | Val TotalLoss: 23.4506, (ReconX: 0.9916, KLD: 0.0612, LossY: 2.2398)\n",
            "INFO:VAETrainer:Epoch [64/100] Train TotalLoss: 15.5567, (ReconX: 0.9863, KLD: 0.0626*1.00, LossY: 1.4508*10.00), Time: 1.8s | Val TotalLoss: 23.4506, (ReconX: 0.9916, KLD: 0.0612, LossY: 2.2398)\n",
            "2025-10-23 22:34:50,230 - VAETrainer - INFO - Epoch [65/100] Train TotalLoss: 15.6275, (ReconX: 0.9858, KLD: 0.0623*1.00, LossY: 1.4579*10.00), Time: 1.7s | Val TotalLoss: 17.5727, (ReconX: 0.9936, KLD: 0.0619, LossY: 1.6517)\n",
            "INFO:VAETrainer:Epoch [65/100] Train TotalLoss: 15.6275, (ReconX: 0.9858, KLD: 0.0623*1.00, LossY: 1.4579*10.00), Time: 1.7s | Val TotalLoss: 17.5727, (ReconX: 0.9936, KLD: 0.0619, LossY: 1.6517)\n",
            "2025-10-23 22:34:52,389 - VAETrainer - INFO - Epoch [66/100] Train TotalLoss: 15.3474, (ReconX: 0.9861, KLD: 0.0626*1.00, LossY: 1.4299*10.00), Time: 1.8s | Val TotalLoss: 19.3949, (ReconX: 0.9922, KLD: 0.0629, LossY: 1.8340)\n",
            "INFO:VAETrainer:Epoch [66/100] Train TotalLoss: 15.3474, (ReconX: 0.9861, KLD: 0.0626*1.00, LossY: 1.4299*10.00), Time: 1.8s | Val TotalLoss: 19.3949, (ReconX: 0.9922, KLD: 0.0629, LossY: 1.8340)\n",
            "2025-10-23 22:34:54,584 - VAETrainer - INFO - Epoch [67/100] Train TotalLoss: 15.2164, (ReconX: 0.9864, KLD: 0.0625*1.00, LossY: 1.4168*10.00), Time: 1.8s | Val TotalLoss: 14.1482, (ReconX: 0.9937, KLD: 0.0614, LossY: 1.3093)\n",
            "INFO:VAETrainer:Epoch [67/100] Train TotalLoss: 15.2164, (ReconX: 0.9864, KLD: 0.0625*1.00, LossY: 1.4168*10.00), Time: 1.8s | Val TotalLoss: 14.1482, (ReconX: 0.9937, KLD: 0.0614, LossY: 1.3093)\n",
            "2025-10-23 22:34:54,586 - VAETrainer - INFO - Saving model and trainer state to vae_results/best_model.pth...\n",
            "INFO:VAETrainer:Saving model and trainer state to vae_results/best_model.pth...\n",
            "2025-10-23 22:34:54,600 - VAETrainer - INFO - Successfully saved model and trainer state to vae_results/best_model.pth\n",
            "INFO:VAETrainer:Successfully saved model and trainer state to vae_results/best_model.pth\n",
            "2025-10-23 22:34:56,760 - VAETrainer - INFO - Epoch [68/100] Train TotalLoss: 15.0424, (ReconX: 0.9861, KLD: 0.0620*1.00, LossY: 1.3994*10.00), Time: 1.8s | Val TotalLoss: 14.1147, (ReconX: 0.9920, KLD: 0.0618, LossY: 1.3061)\n",
            "INFO:VAETrainer:Epoch [68/100] Train TotalLoss: 15.0424, (ReconX: 0.9861, KLD: 0.0620*1.00, LossY: 1.3994*10.00), Time: 1.8s | Val TotalLoss: 14.1147, (ReconX: 0.9920, KLD: 0.0618, LossY: 1.3061)\n",
            "2025-10-23 22:34:56,761 - VAETrainer - INFO - Saving model and trainer state to vae_results/best_model.pth...\n",
            "INFO:VAETrainer:Saving model and trainer state to vae_results/best_model.pth...\n",
            "2025-10-23 22:34:56,777 - VAETrainer - INFO - Successfully saved model and trainer state to vae_results/best_model.pth\n",
            "INFO:VAETrainer:Successfully saved model and trainer state to vae_results/best_model.pth\n",
            "2025-10-23 22:34:58,940 - VAETrainer - INFO - Epoch [69/100] Train TotalLoss: 15.1144, (ReconX: 0.9866, KLD: 0.0618*1.00, LossY: 1.4066*10.00), Time: 1.8s | Val TotalLoss: 15.0091, (ReconX: 0.9911, KLD: 0.0604, LossY: 1.3958)\n",
            "INFO:VAETrainer:Epoch [69/100] Train TotalLoss: 15.1144, (ReconX: 0.9866, KLD: 0.0618*1.00, LossY: 1.4066*10.00), Time: 1.8s | Val TotalLoss: 15.0091, (ReconX: 0.9911, KLD: 0.0604, LossY: 1.3958)\n",
            "2025-10-23 22:35:00,995 - VAETrainer - INFO - Epoch [70/100] Train TotalLoss: 14.9474, (ReconX: 0.9862, KLD: 0.0614*1.00, LossY: 1.3900*10.00), Time: 1.7s | Val TotalLoss: 14.3796, (ReconX: 0.9924, KLD: 0.0607, LossY: 1.3327)\n",
            "INFO:VAETrainer:Epoch [70/100] Train TotalLoss: 14.9474, (ReconX: 0.9862, KLD: 0.0614*1.00, LossY: 1.3900*10.00), Time: 1.7s | Val TotalLoss: 14.3796, (ReconX: 0.9924, KLD: 0.0607, LossY: 1.3327)\n",
            "2025-10-23 22:35:03,041 - VAETrainer - INFO - Epoch [71/100] Train TotalLoss: 15.0041, (ReconX: 0.9860, KLD: 0.0614*1.00, LossY: 1.3957*10.00), Time: 1.7s | Val TotalLoss: 14.0814, (ReconX: 0.9920, KLD: 0.0600, LossY: 1.3029)\n",
            "INFO:VAETrainer:Epoch [71/100] Train TotalLoss: 15.0041, (ReconX: 0.9860, KLD: 0.0614*1.00, LossY: 1.3957*10.00), Time: 1.7s | Val TotalLoss: 14.0814, (ReconX: 0.9920, KLD: 0.0600, LossY: 1.3029)\n",
            "2025-10-23 22:35:03,043 - VAETrainer - INFO - Saving model and trainer state to vae_results/best_model.pth...\n",
            "INFO:VAETrainer:Saving model and trainer state to vae_results/best_model.pth...\n",
            "2025-10-23 22:35:03,057 - VAETrainer - INFO - Successfully saved model and trainer state to vae_results/best_model.pth\n",
            "INFO:VAETrainer:Successfully saved model and trainer state to vae_results/best_model.pth\n",
            "2025-10-23 22:35:05,178 - VAETrainer - INFO - Epoch [72/100] Train TotalLoss: 14.9142, (ReconX: 0.9863, KLD: 0.0609*1.00, LossY: 1.3867*10.00), Time: 1.7s | Val TotalLoss: 14.3109, (ReconX: 0.9915, KLD: 0.0608, LossY: 1.3259)\n",
            "INFO:VAETrainer:Epoch [72/100] Train TotalLoss: 14.9142, (ReconX: 0.9863, KLD: 0.0609*1.00, LossY: 1.3867*10.00), Time: 1.7s | Val TotalLoss: 14.3109, (ReconX: 0.9915, KLD: 0.0608, LossY: 1.3259)\n",
            "2025-10-23 22:35:07,318 - VAETrainer - INFO - Epoch [73/100] Train TotalLoss: 14.9434, (ReconX: 0.9865, KLD: 0.0610*1.00, LossY: 1.3896*10.00), Time: 1.8s | Val TotalLoss: 14.6483, (ReconX: 0.9936, KLD: 0.0602, LossY: 1.3594)\n",
            "INFO:VAETrainer:Epoch [73/100] Train TotalLoss: 14.9434, (ReconX: 0.9865, KLD: 0.0610*1.00, LossY: 1.3896*10.00), Time: 1.8s | Val TotalLoss: 14.6483, (ReconX: 0.9936, KLD: 0.0602, LossY: 1.3594)\n",
            "2025-10-23 22:35:09,474 - VAETrainer - INFO - Epoch [74/100] Train TotalLoss: 14.8873, (ReconX: 0.9861, KLD: 0.0607*1.00, LossY: 1.3840*10.00), Time: 1.8s | Val TotalLoss: 14.2145, (ReconX: 0.9923, KLD: 0.0599, LossY: 1.3162)\n",
            "INFO:VAETrainer:Epoch [74/100] Train TotalLoss: 14.8873, (ReconX: 0.9861, KLD: 0.0607*1.00, LossY: 1.3840*10.00), Time: 1.8s | Val TotalLoss: 14.2145, (ReconX: 0.9923, KLD: 0.0599, LossY: 1.3162)\n",
            "2025-10-23 22:35:11,725 - VAETrainer - INFO - Epoch [75/100] Train TotalLoss: 14.8512, (ReconX: 0.9863, KLD: 0.0606*1.00, LossY: 1.3804*10.00), Time: 1.9s | Val TotalLoss: 13.9494, (ReconX: 0.9937, KLD: 0.0595, LossY: 1.2896)\n",
            "INFO:VAETrainer:Epoch [75/100] Train TotalLoss: 14.8512, (ReconX: 0.9863, KLD: 0.0606*1.00, LossY: 1.3804*10.00), Time: 1.9s | Val TotalLoss: 13.9494, (ReconX: 0.9937, KLD: 0.0595, LossY: 1.2896)\n",
            "2025-10-23 22:35:11,727 - VAETrainer - INFO - Saving model and trainer state to vae_results/best_model.pth...\n",
            "INFO:VAETrainer:Saving model and trainer state to vae_results/best_model.pth...\n",
            "2025-10-23 22:35:11,742 - VAETrainer - INFO - Successfully saved model and trainer state to vae_results/best_model.pth\n",
            "INFO:VAETrainer:Successfully saved model and trainer state to vae_results/best_model.pth\n",
            "2025-10-23 22:35:13,802 - VAETrainer - INFO - Epoch [76/100] Train TotalLoss: 14.8073, (ReconX: 0.9865, KLD: 0.0604*1.00, LossY: 1.3760*10.00), Time: 1.7s | Val TotalLoss: 13.9442, (ReconX: 0.9932, KLD: 0.0594, LossY: 1.2892)\n",
            "INFO:VAETrainer:Epoch [76/100] Train TotalLoss: 14.8073, (ReconX: 0.9865, KLD: 0.0604*1.00, LossY: 1.3760*10.00), Time: 1.7s | Val TotalLoss: 13.9442, (ReconX: 0.9932, KLD: 0.0594, LossY: 1.2892)\n",
            "2025-10-23 22:35:13,805 - VAETrainer - INFO - Saving model and trainer state to vae_results/best_model.pth...\n",
            "INFO:VAETrainer:Saving model and trainer state to vae_results/best_model.pth...\n",
            "2025-10-23 22:35:13,818 - VAETrainer - INFO - Successfully saved model and trainer state to vae_results/best_model.pth\n",
            "INFO:VAETrainer:Successfully saved model and trainer state to vae_results/best_model.pth\n",
            "2025-10-23 22:35:15,868 - VAETrainer - INFO - Epoch [77/100] Train TotalLoss: 14.9868, (ReconX: 0.9865, KLD: 0.0602*1.00, LossY: 1.3940*10.00), Time: 1.7s | Val TotalLoss: 14.1760, (ReconX: 0.9932, KLD: 0.0590, LossY: 1.3124)\n",
            "INFO:VAETrainer:Epoch [77/100] Train TotalLoss: 14.9868, (ReconX: 0.9865, KLD: 0.0602*1.00, LossY: 1.3940*10.00), Time: 1.7s | Val TotalLoss: 14.1760, (ReconX: 0.9932, KLD: 0.0590, LossY: 1.3124)\n",
            "2025-10-23 22:35:18,111 - VAETrainer - INFO - Epoch [78/100] Train TotalLoss: 14.7753, (ReconX: 0.9870, KLD: 0.0600*1.00, LossY: 1.3728*10.00), Time: 1.9s | Val TotalLoss: 14.0053, (ReconX: 0.9935, KLD: 0.0591, LossY: 1.2953)\n",
            "INFO:VAETrainer:Epoch [78/100] Train TotalLoss: 14.7753, (ReconX: 0.9870, KLD: 0.0600*1.00, LossY: 1.3728*10.00), Time: 1.9s | Val TotalLoss: 14.0053, (ReconX: 0.9935, KLD: 0.0591, LossY: 1.2953)\n",
            "2025-10-23 22:35:20,299 - VAETrainer - INFO - Epoch [79/100] Train TotalLoss: 15.1554, (ReconX: 0.9867, KLD: 0.0603*1.00, LossY: 1.4109*10.00), Time: 1.8s | Val TotalLoss: 16.3052, (ReconX: 0.9932, KLD: 0.0601, LossY: 1.5252)\n",
            "INFO:VAETrainer:Epoch [79/100] Train TotalLoss: 15.1554, (ReconX: 0.9867, KLD: 0.0603*1.00, LossY: 1.4109*10.00), Time: 1.8s | Val TotalLoss: 16.3052, (ReconX: 0.9932, KLD: 0.0601, LossY: 1.5252)\n",
            "2025-10-23 22:35:22,406 - VAETrainer - INFO - Epoch [80/100] Train TotalLoss: 14.9951, (ReconX: 0.9865, KLD: 0.0606*1.00, LossY: 1.3948*10.00), Time: 1.7s | Val TotalLoss: 14.9286, (ReconX: 0.9930, KLD: 0.0594, LossY: 1.3876)\n",
            "INFO:VAETrainer:Epoch [80/100] Train TotalLoss: 14.9951, (ReconX: 0.9865, KLD: 0.0606*1.00, LossY: 1.3948*10.00), Time: 1.7s | Val TotalLoss: 14.9286, (ReconX: 0.9930, KLD: 0.0594, LossY: 1.3876)\n",
            "2025-10-23 22:35:24,602 - VAETrainer - INFO - Epoch [81/100] Train TotalLoss: 15.0591, (ReconX: 0.9863, KLD: 0.0604*1.00, LossY: 1.4012*10.00), Time: 1.8s | Val TotalLoss: 14.0787, (ReconX: 0.9925, KLD: 0.0597, LossY: 1.3026)\n",
            "INFO:VAETrainer:Epoch [81/100] Train TotalLoss: 15.0591, (ReconX: 0.9863, KLD: 0.0604*1.00, LossY: 1.4012*10.00), Time: 1.8s | Val TotalLoss: 14.0787, (ReconX: 0.9925, KLD: 0.0597, LossY: 1.3026)\n",
            "2025-10-23 22:35:26,743 - VAETrainer - INFO - Epoch [82/100] Train TotalLoss: 14.9309, (ReconX: 0.9869, KLD: 0.0600*1.00, LossY: 1.3884*10.00), Time: 1.8s | Val TotalLoss: 14.0572, (ReconX: 0.9916, KLD: 0.0589, LossY: 1.3007)\n",
            "INFO:VAETrainer:Epoch [82/100] Train TotalLoss: 14.9309, (ReconX: 0.9869, KLD: 0.0600*1.00, LossY: 1.3884*10.00), Time: 1.8s | Val TotalLoss: 14.0572, (ReconX: 0.9916, KLD: 0.0589, LossY: 1.3007)\n",
            "2025-10-23 22:35:28,896 - VAETrainer - INFO - Epoch [83/100] Train TotalLoss: 14.7797, (ReconX: 0.9864, KLD: 0.0599*1.00, LossY: 1.3733*10.00), Time: 1.8s | Val TotalLoss: 13.9102, (ReconX: 0.9928, KLD: 0.0591, LossY: 1.2858)\n",
            "INFO:VAETrainer:Epoch [83/100] Train TotalLoss: 14.7797, (ReconX: 0.9864, KLD: 0.0599*1.00, LossY: 1.3733*10.00), Time: 1.8s | Val TotalLoss: 13.9102, (ReconX: 0.9928, KLD: 0.0591, LossY: 1.2858)\n",
            "2025-10-23 22:35:28,897 - VAETrainer - INFO - Saving model and trainer state to vae_results/best_model.pth...\n",
            "INFO:VAETrainer:Saving model and trainer state to vae_results/best_model.pth...\n",
            "2025-10-23 22:35:28,911 - VAETrainer - INFO - Successfully saved model and trainer state to vae_results/best_model.pth\n",
            "INFO:VAETrainer:Successfully saved model and trainer state to vae_results/best_model.pth\n",
            "2025-10-23 22:35:31,087 - VAETrainer - INFO - Epoch [84/100] Train TotalLoss: 14.8736, (ReconX: 0.9867, KLD: 0.0598*1.00, LossY: 1.3827*10.00), Time: 1.8s | Val TotalLoss: 13.9776, (ReconX: 0.9931, KLD: 0.0589, LossY: 1.2926)\n",
            "INFO:VAETrainer:Epoch [84/100] Train TotalLoss: 14.8736, (ReconX: 0.9867, KLD: 0.0598*1.00, LossY: 1.3827*10.00), Time: 1.8s | Val TotalLoss: 13.9776, (ReconX: 0.9931, KLD: 0.0589, LossY: 1.2926)\n",
            "2025-10-23 22:35:33,334 - VAETrainer - INFO - Epoch [85/100] Train TotalLoss: 14.8467, (ReconX: 0.9863, KLD: 0.0597*1.00, LossY: 1.3801*10.00), Time: 1.9s | Val TotalLoss: 13.9523, (ReconX: 0.9934, KLD: 0.0586, LossY: 1.2900)\n",
            "INFO:VAETrainer:Epoch [85/100] Train TotalLoss: 14.8467, (ReconX: 0.9863, KLD: 0.0597*1.00, LossY: 1.3801*10.00), Time: 1.9s | Val TotalLoss: 13.9523, (ReconX: 0.9934, KLD: 0.0586, LossY: 1.2900)\n",
            "2025-10-23 22:35:35,426 - VAETrainer - INFO - Epoch [86/100] Train TotalLoss: 14.8745, (ReconX: 0.9865, KLD: 0.0597*1.00, LossY: 1.3828*10.00), Time: 1.7s | Val TotalLoss: 13.9032, (ReconX: 0.9929, KLD: 0.0587, LossY: 1.2851)\n",
            "INFO:VAETrainer:Epoch [86/100] Train TotalLoss: 14.8745, (ReconX: 0.9865, KLD: 0.0597*1.00, LossY: 1.3828*10.00), Time: 1.7s | Val TotalLoss: 13.9032, (ReconX: 0.9929, KLD: 0.0587, LossY: 1.2851)\n",
            "2025-10-23 22:35:35,427 - VAETrainer - INFO - Saving model and trainer state to vae_results/best_model.pth...\n",
            "INFO:VAETrainer:Saving model and trainer state to vae_results/best_model.pth...\n",
            "2025-10-23 22:35:35,443 - VAETrainer - INFO - Successfully saved model and trainer state to vae_results/best_model.pth\n",
            "INFO:VAETrainer:Successfully saved model and trainer state to vae_results/best_model.pth\n",
            "2025-10-23 22:35:37,571 - VAETrainer - INFO - Epoch [87/100] Train TotalLoss: 14.8100, (ReconX: 0.9864, KLD: 0.0596*1.00, LossY: 1.3764*10.00), Time: 1.8s | Val TotalLoss: 13.9379, (ReconX: 0.9928, KLD: 0.0587, LossY: 1.2886)\n",
            "INFO:VAETrainer:Epoch [87/100] Train TotalLoss: 14.8100, (ReconX: 0.9864, KLD: 0.0596*1.00, LossY: 1.3764*10.00), Time: 1.8s | Val TotalLoss: 13.9379, (ReconX: 0.9928, KLD: 0.0587, LossY: 1.2886)\n",
            "2025-10-23 22:35:39,793 - VAETrainer - INFO - Epoch [88/100] Train TotalLoss: 14.7902, (ReconX: 0.9864, KLD: 0.0596*1.00, LossY: 1.3744*10.00), Time: 1.8s | Val TotalLoss: 13.8841, (ReconX: 0.9944, KLD: 0.0586, LossY: 1.2831)\n",
            "INFO:VAETrainer:Epoch [88/100] Train TotalLoss: 14.7902, (ReconX: 0.9864, KLD: 0.0596*1.00, LossY: 1.3744*10.00), Time: 1.8s | Val TotalLoss: 13.8841, (ReconX: 0.9944, KLD: 0.0586, LossY: 1.2831)\n",
            "2025-10-23 22:35:39,795 - VAETrainer - INFO - Saving model and trainer state to vae_results/best_model.pth...\n",
            "INFO:VAETrainer:Saving model and trainer state to vae_results/best_model.pth...\n",
            "2025-10-23 22:35:39,808 - VAETrainer - INFO - Successfully saved model and trainer state to vae_results/best_model.pth\n",
            "INFO:VAETrainer:Successfully saved model and trainer state to vae_results/best_model.pth\n",
            "2025-10-23 22:35:41,988 - VAETrainer - INFO - Epoch [89/100] Train TotalLoss: 14.6714, (ReconX: 0.9862, KLD: 0.0595*1.00, LossY: 1.3626*10.00), Time: 1.8s | Val TotalLoss: 13.8708, (ReconX: 0.9926, KLD: 0.0589, LossY: 1.2819)\n",
            "INFO:VAETrainer:Epoch [89/100] Train TotalLoss: 14.6714, (ReconX: 0.9862, KLD: 0.0595*1.00, LossY: 1.3626*10.00), Time: 1.8s | Val TotalLoss: 13.8708, (ReconX: 0.9926, KLD: 0.0589, LossY: 1.2819)\n",
            "2025-10-23 22:35:41,990 - VAETrainer - INFO - Saving model and trainer state to vae_results/best_model.pth...\n",
            "INFO:VAETrainer:Saving model and trainer state to vae_results/best_model.pth...\n",
            "2025-10-23 22:35:42,003 - VAETrainer - INFO - Successfully saved model and trainer state to vae_results/best_model.pth\n",
            "INFO:VAETrainer:Successfully saved model and trainer state to vae_results/best_model.pth\n",
            "2025-10-23 22:35:44,200 - VAETrainer - INFO - Epoch [90/100] Train TotalLoss: 14.8514, (ReconX: 0.9866, KLD: 0.0595*1.00, LossY: 1.3805*10.00), Time: 1.8s | Val TotalLoss: 14.0838, (ReconX: 0.9929, KLD: 0.0589, LossY: 1.3032)\n",
            "INFO:VAETrainer:Epoch [90/100] Train TotalLoss: 14.8514, (ReconX: 0.9866, KLD: 0.0595*1.00, LossY: 1.3805*10.00), Time: 1.8s | Val TotalLoss: 14.0838, (ReconX: 0.9929, KLD: 0.0589, LossY: 1.3032)\n",
            "2025-10-23 22:35:46,327 - VAETrainer - INFO - Epoch [91/100] Train TotalLoss: 14.6643, (ReconX: 0.9869, KLD: 0.0594*1.00, LossY: 1.3618*10.00), Time: 1.8s | Val TotalLoss: 13.9587, (ReconX: 0.9934, KLD: 0.0593, LossY: 1.2906)\n",
            "INFO:VAETrainer:Epoch [91/100] Train TotalLoss: 14.6643, (ReconX: 0.9869, KLD: 0.0594*1.00, LossY: 1.3618*10.00), Time: 1.8s | Val TotalLoss: 13.9587, (ReconX: 0.9934, KLD: 0.0593, LossY: 1.2906)\n",
            "2025-10-23 22:35:48,542 - VAETrainer - INFO - Epoch [92/100] Train TotalLoss: 14.6113, (ReconX: 0.9870, KLD: 0.0593*1.00, LossY: 1.3565*10.00), Time: 1.8s | Val TotalLoss: 13.8639, (ReconX: 0.9948, KLD: 0.0583, LossY: 1.2811)\n",
            "INFO:VAETrainer:Epoch [92/100] Train TotalLoss: 14.6113, (ReconX: 0.9870, KLD: 0.0593*1.00, LossY: 1.3565*10.00), Time: 1.8s | Val TotalLoss: 13.8639, (ReconX: 0.9948, KLD: 0.0583, LossY: 1.2811)\n",
            "2025-10-23 22:35:48,544 - VAETrainer - INFO - Saving model and trainer state to vae_results/best_model.pth...\n",
            "INFO:VAETrainer:Saving model and trainer state to vae_results/best_model.pth...\n",
            "2025-10-23 22:35:48,560 - VAETrainer - INFO - Successfully saved model and trainer state to vae_results/best_model.pth\n",
            "INFO:VAETrainer:Successfully saved model and trainer state to vae_results/best_model.pth\n",
            "2025-10-23 22:35:50,719 - VAETrainer - INFO - Epoch [93/100] Train TotalLoss: 14.7940, (ReconX: 0.9867, KLD: 0.0594*1.00, LossY: 1.3748*10.00), Time: 1.8s | Val TotalLoss: 13.8443, (ReconX: 0.9920, KLD: 0.0588, LossY: 1.2793)\n",
            "INFO:VAETrainer:Epoch [93/100] Train TotalLoss: 14.7940, (ReconX: 0.9867, KLD: 0.0594*1.00, LossY: 1.3748*10.00), Time: 1.8s | Val TotalLoss: 13.8443, (ReconX: 0.9920, KLD: 0.0588, LossY: 1.2793)\n",
            "2025-10-23 22:35:50,721 - VAETrainer - INFO - Saving model and trainer state to vae_results/best_model.pth...\n",
            "INFO:VAETrainer:Saving model and trainer state to vae_results/best_model.pth...\n",
            "2025-10-23 22:35:50,736 - VAETrainer - INFO - Successfully saved model and trainer state to vae_results/best_model.pth\n",
            "INFO:VAETrainer:Successfully saved model and trainer state to vae_results/best_model.pth\n",
            "2025-10-23 22:35:52,849 - VAETrainer - INFO - Epoch [94/100] Train TotalLoss: 14.6076, (ReconX: 0.9867, KLD: 0.0594*1.00, LossY: 1.3562*10.00), Time: 1.7s | Val TotalLoss: 13.8176, (ReconX: 0.9939, KLD: 0.0587, LossY: 1.2765)\n",
            "INFO:VAETrainer:Epoch [94/100] Train TotalLoss: 14.6076, (ReconX: 0.9867, KLD: 0.0594*1.00, LossY: 1.3562*10.00), Time: 1.7s | Val TotalLoss: 13.8176, (ReconX: 0.9939, KLD: 0.0587, LossY: 1.2765)\n",
            "2025-10-23 22:35:52,851 - VAETrainer - INFO - Saving model and trainer state to vae_results/best_model.pth...\n",
            "INFO:VAETrainer:Saving model and trainer state to vae_results/best_model.pth...\n",
            "2025-10-23 22:35:52,865 - VAETrainer - INFO - Successfully saved model and trainer state to vae_results/best_model.pth\n",
            "INFO:VAETrainer:Successfully saved model and trainer state to vae_results/best_model.pth\n",
            "2025-10-23 22:35:54,909 - VAETrainer - INFO - Epoch [95/100] Train TotalLoss: 14.8656, (ReconX: 0.9868, KLD: 0.0594*1.00, LossY: 1.3819*10.00), Time: 1.7s | Val TotalLoss: 13.8570, (ReconX: 0.9928, KLD: 0.0588, LossY: 1.2805)\n",
            "INFO:VAETrainer:Epoch [95/100] Train TotalLoss: 14.8656, (ReconX: 0.9868, KLD: 0.0594*1.00, LossY: 1.3819*10.00), Time: 1.7s | Val TotalLoss: 13.8570, (ReconX: 0.9928, KLD: 0.0588, LossY: 1.2805)\n",
            "2025-10-23 22:35:57,115 - VAETrainer - INFO - Epoch [96/100] Train TotalLoss: 14.7692, (ReconX: 0.9867, KLD: 0.0593*1.00, LossY: 1.3723*10.00), Time: 1.8s | Val TotalLoss: 13.9252, (ReconX: 0.9924, KLD: 0.0588, LossY: 1.2874)\n",
            "INFO:VAETrainer:Epoch [96/100] Train TotalLoss: 14.7692, (ReconX: 0.9867, KLD: 0.0593*1.00, LossY: 1.3723*10.00), Time: 1.8s | Val TotalLoss: 13.9252, (ReconX: 0.9924, KLD: 0.0588, LossY: 1.2874)\n",
            "2025-10-23 22:35:59,212 - VAETrainer - INFO - Epoch [97/100] Train TotalLoss: 14.7324, (ReconX: 0.9865, KLD: 0.0594*1.00, LossY: 1.3687*10.00), Time: 1.7s | Val TotalLoss: 13.8437, (ReconX: 0.9933, KLD: 0.0585, LossY: 1.2792)\n",
            "INFO:VAETrainer:Epoch [97/100] Train TotalLoss: 14.7324, (ReconX: 0.9865, KLD: 0.0594*1.00, LossY: 1.3687*10.00), Time: 1.7s | Val TotalLoss: 13.8437, (ReconX: 0.9933, KLD: 0.0585, LossY: 1.2792)\n",
            "2025-10-23 22:36:01,356 - VAETrainer - INFO - Epoch [98/100] Train TotalLoss: 14.7841, (ReconX: 0.9865, KLD: 0.0594*1.00, LossY: 1.3738*10.00), Time: 1.8s | Val TotalLoss: 13.8582, (ReconX: 0.9929, KLD: 0.0589, LossY: 1.2806)\n",
            "INFO:VAETrainer:Epoch [98/100] Train TotalLoss: 14.7841, (ReconX: 0.9865, KLD: 0.0594*1.00, LossY: 1.3738*10.00), Time: 1.8s | Val TotalLoss: 13.8582, (ReconX: 0.9929, KLD: 0.0589, LossY: 1.2806)\n",
            "2025-10-23 22:36:03,520 - VAETrainer - INFO - Epoch [99/100] Train TotalLoss: 14.7703, (ReconX: 0.9865, KLD: 0.0593*1.00, LossY: 1.3724*10.00), Time: 1.8s | Val TotalLoss: 13.8884, (ReconX: 0.9924, KLD: 0.0589, LossY: 1.2837)\n",
            "INFO:VAETrainer:Epoch [99/100] Train TotalLoss: 14.7703, (ReconX: 0.9865, KLD: 0.0593*1.00, LossY: 1.3724*10.00), Time: 1.8s | Val TotalLoss: 13.8884, (ReconX: 0.9924, KLD: 0.0589, LossY: 1.2837)\n",
            "2025-10-23 22:36:05,588 - VAETrainer - INFO - Epoch [100/100] Train TotalLoss: 14.7017, (ReconX: 0.9861, KLD: 0.0592*1.00, LossY: 1.3656*10.00), Time: 1.7s | Val TotalLoss: 14.1107, (ReconX: 0.9928, KLD: 0.0587, LossY: 1.3059)\n",
            "INFO:VAETrainer:Epoch [100/100] Train TotalLoss: 14.7017, (ReconX: 0.9861, KLD: 0.0592*1.00, LossY: 1.3656*10.00), Time: 1.7s | Val TotalLoss: 14.1107, (ReconX: 0.9928, KLD: 0.0587, LossY: 1.3059)\n",
            "2025-10-23 22:36:05,589 - VAETrainer - INFO - Training loop finished.\n",
            "INFO:VAETrainer:Training loop finished.\n",
            "2025-10-23 22:36:05,590 - VAETrainer - INFO - Loading best model state from 'vae_results/best_model.pth' (Val Loss: 13.8176).\n",
            "INFO:VAETrainer:Loading best model state from 'vae_results/best_model.pth' (Val Loss: 13.8176).\n",
            "2025-10-23 22:36:05,592 - VAETrainer_Loader - INFO - Loading VAE model and trainer state from vae_results/best_model.pth to device cuda...\n",
            "INFO:VAETrainer_Loader:Loading VAE model and trainer state from vae_results/best_model.pth to device cuda...\n",
            "2025-10-23 22:36:05,631 - SemiSupMIWAE(X:42,Y:1,Lat:3) - INFO - SemiSupMIWAE Initialized: InputXDim=42, TargetYDim=1, LatentDim=3, AlphaPriceLoss=10.0, Device=cuda\n",
            "INFO:SemiSupMIWAE(X:42,Y:1,Lat:3):SemiSupMIWAE Initialized: InputXDim=42, TargetYDim=1, LatentDim=3, AlphaPriceLoss=10.0, Device=cuda\n",
            "2025-10-23 22:36:05,634 - VAETrainer_Loader - INFO - SemiSupMIWAE model created and state loaded successfully.\n",
            "INFO:VAETrainer_Loader:SemiSupMIWAE model created and state loaded successfully.\n",
            "2025-10-23 22:36:05,636 - VAETrainer - INFO - VAETrainer initialized for SemiSupMIWAE. Device: cuda. Optimizer: adam. LR: 0.0003. ReconLoss(X): 'mse'. Alpha(Y Loss): 10.0. LossType(Y): 'gaussian_nll'\n",
            "INFO:VAETrainer:VAETrainer initialized for SemiSupMIWAE. Device: cuda. Optimizer: adam. LR: 0.0003. ReconLoss(X): 'mse'. Alpha(Y Loss): 10.0. LossType(Y): 'gaussian_nll'\n",
            "2025-10-23 22:36:05,638 - VAETrainer_Loader - INFO - VAETrainer instance created and state loaded for model from vae_results/best_model.pth.\n",
            "INFO:VAETrainer_Loader:VAETrainer instance created and state loaded for model from vae_results/best_model.pth.\n",
            "2025-10-23 22:36:05,649 - VAETrainer - INFO - Predicting target (Y) mean and log-variance for 12271 samples...\n",
            "INFO:VAETrainer:Predicting target (Y) mean and log-variance for 12271 samples...\n",
            "2025-10-23 22:36:05,715 - MainRealEstateAnalysis_MIWAE - INFO - [Fold 3] Stored test predictions for 12271 rows.\n",
            "INFO:MainRealEstateAnalysis_MIWAE:[Fold 3] Stored test predictions for 12271 rows.\n",
            "2025-10-23 22:36:05,727 - MainRealEstateAnalysis_MIWAE - INFO - [Fold 4/10] sizes → Train=98170 (80.0%), Val=12271 (10.0%), Test=12271 (10.0%)\n",
            "INFO:MainRealEstateAnalysis_MIWAE:[Fold 4/10] sizes → Train=98170 (80.0%), Val=12271 (10.0%), Test=12271 (10.0%)\n",
            "2025-10-23 22:36:05,728 - __main__.create_vae_from_artifacts - INFO - Creating VAE model. Target column for supervision: log_sale_price.\n",
            "2025-10-23 22:36:05,728 - __main__.create_vae_from_artifacts - INFO - Creating VAE model. Target column for supervision: log_sale_price.\n",
            "INFO:__main__.create_vae_from_artifacts:Creating VAE model. Target column for supervision: log_sale_price.\n",
            "2025-10-23 22:36:05,729 - __main__.create_vae_from_artifacts - INFO - Number of input features for VAE (X): 42 (Features: ['lotarea', 'assess_total', 'exempt_total', 'assessland', 'exemptland']...)\n",
            "2025-10-23 22:36:05,729 - __main__.create_vae_from_artifacts - INFO - Number of input features for VAE (X): 42 (Features: ['lotarea', 'assess_total', 'exempt_total', 'assessland', 'exemptland']...)\n",
            "INFO:__main__.create_vae_from_artifacts:Number of input features for VAE (X): 42 (Features: ['lotarea', 'assess_total', 'exempt_total', 'assessland', 'exemptland']...)\n",
            "2025-10-23 22:36:05,730 - __main__.create_vae_from_artifacts - INFO - Semi-supervised mode: Target 'log_sale_price' (y_dim=1). Alpha for price loss: 10.0.\n",
            "2025-10-23 22:36:05,730 - __main__.create_vae_from_artifacts - INFO - Semi-supervised mode: Target 'log_sale_price' (y_dim=1). Alpha for price loss: 10.0.\n",
            "INFO:__main__.create_vae_from_artifacts:Semi-supervised mode: Target 'log_sale_price' (y_dim=1). Alpha for price loss: 10.0.\n",
            "2025-10-23 22:36:05,731 - __main__.create_vae_from_artifacts - INFO - Using overridden latent_dim: 3\n",
            "2025-10-23 22:36:05,731 - __main__.create_vae_from_artifacts - INFO - Using overridden latent_dim: 3\n",
            "INFO:__main__.create_vae_from_artifacts:Using overridden latent_dim: 3\n",
            "2025-10-23 22:36:05,734 - __main__.create_vae_from_artifacts - INFO - Final effective Latent Dimension: 3\n",
            "2025-10-23 22:36:05,734 - __main__.create_vae_from_artifacts - INFO - Final effective Latent Dimension: 3\n",
            "INFO:__main__.create_vae_from_artifacts:Final effective Latent Dimension: 3\n",
            "2025-10-23 22:36:05,735 - __main__.create_vae_from_artifacts - INFO - Encoder layers: [21], Decoder layers: [21]\n",
            "2025-10-23 22:36:05,735 - __main__.create_vae_from_artifacts - INFO - Encoder layers: [21], Decoder layers: [21]\n",
            "INFO:__main__.create_vae_from_artifacts:Encoder layers: [21], Decoder layers: [21]\n",
            "2025-10-23 22:36:05,737 - __main__.create_vae_from_artifacts - INFO - Using determined prior: Type=student_t_mixture, K=2 (from overrides or artifact inference).\n",
            "2025-10-23 22:36:05,737 - __main__.create_vae_from_artifacts - INFO - Using determined prior: Type=student_t_mixture, K=2 (from overrides or artifact inference).\n",
            "INFO:__main__.create_vae_from_artifacts:Using determined prior: Type=student_t_mixture, K=2 (from overrides or artifact inference).\n",
            "2025-10-23 22:36:05,738 - __main__.create_vae_from_artifacts - INFO - Initializing parameters for student_t_mixture with K=2 components...\n",
            "2025-10-23 22:36:05,738 - __main__.create_vae_from_artifacts - INFO - Initializing parameters for student_t_mixture with K=2 components...\n",
            "INFO:__main__.create_vae_from_artifacts:Initializing parameters for student_t_mixture with K=2 components...\n",
            "2025-10-23 22:36:05,740 - __main__.create_vae_from_artifacts - WARNING - DP-GMM artifacts are unsuitable or insufficient for initializing mixture prior parameters. Default parameters (e.g., zeros for means/logits, identity for covariances) will be used by the VAE.\n",
            "2025-10-23 22:36:05,740 - __main__.create_vae_from_artifacts - WARNING - DP-GMM artifacts are unsuitable or insufficient for initializing mixture prior parameters. Default parameters (e.g., zeros for means/logits, identity for covariances) will be used by the VAE.\n",
            "WARNING:__main__.create_vae_from_artifacts:DP-GMM artifacts are unsuitable or insufficient for initializing mixture prior parameters. Default parameters (e.g., zeros for means/logits, identity for covariances) will be used by the VAE.\n",
            "2025-10-23 22:36:05,741 - __main__.create_vae_from_artifacts - INFO - Setting Student-t df to 4.0 for prior.\n",
            "2025-10-23 22:36:05,741 - __main__.create_vae_from_artifacts - INFO - Setting Student-t df to 4.0 for prior.\n",
            "INFO:__main__.create_vae_from_artifacts:Setting Student-t df to 4.0 for prior.\n",
            "2025-10-23 22:36:05,743 - __main__.create_vae_from_artifacts - INFO - Instantiating SemiSupMIWAE model...\n",
            "2025-10-23 22:36:05,743 - __main__.create_vae_from_artifacts - INFO - Instantiating SemiSupMIWAE model...\n",
            "INFO:__main__.create_vae_from_artifacts:Instantiating SemiSupMIWAE model...\n",
            "2025-10-23 22:36:05,744 - __main__.create_vae_from_artifacts - INFO - Price head layers for SemiSupMIWAE: [8, 4]\n",
            "2025-10-23 22:36:05,744 - __main__.create_vae_from_artifacts - INFO - Price head layers for SemiSupMIWAE: [8, 4]\n",
            "INFO:__main__.create_vae_from_artifacts:Price head layers for SemiSupMIWAE: [8, 4]\n",
            "2025-10-23 22:36:05,752 - SemiSupMIWAE(X:42,Y:1,Lat:3) - INFO - SemiSupMIWAE Initialized: InputXDim=42, TargetYDim=1, LatentDim=3, AlphaPriceLoss=10.0, Device=cuda\n",
            "INFO:SemiSupMIWAE(X:42,Y:1,Lat:3):SemiSupMIWAE Initialized: InputXDim=42, TargetYDim=1, LatentDim=3, AlphaPriceLoss=10.0, Device=cuda\n",
            "2025-10-23 22:36:05,753 - __main__.create_vae_from_artifacts - INFO - SemiSupMIWAE model created successfully on device: cuda.\n",
            "2025-10-23 22:36:05,753 - __main__.create_vae_from_artifacts - INFO - SemiSupMIWAE model created successfully on device: cuda.\n",
            "INFO:__main__.create_vae_from_artifacts:SemiSupMIWAE model created successfully on device: cuda.\n",
            "2025-10-23 22:36:05,756 - VAETrainer - INFO - VAETrainer initialized for SemiSupMIWAE. Device: cuda. Optimizer: adam. LR: 0.0003. ReconLoss(X): 'mse'. Alpha(Y Loss): 10.0. LossType(Y): 'gaussian_nll'\n",
            "INFO:VAETrainer:VAETrainer initialized for SemiSupMIWAE. Device: cuda. Optimizer: adam. LR: 0.0003. ReconLoss(X): 'mse'. Alpha(Y Loss): 10.0. LossType(Y): 'gaussian_nll'\n",
            "2025-10-23 22:36:05,757 - VAETrainer - INFO - Starting training: Epochs=100, BatchSize=512, KLD_weight=1.0, KLD_AnnealEpochs=30, Alpha(Y_Loss)=10.0.\n",
            "INFO:VAETrainer:Starting training: Epochs=100, BatchSize=512, KLD_weight=1.0, KLD_AnnealEpochs=30, Alpha(Y_Loss)=10.0.\n",
            "2025-10-23 22:36:05,758 - VAETrainer - INFO - Train loader: 98170 samples, 192 batches. Drop last: False\n",
            "INFO:VAETrainer:Train loader: 98170 samples, 192 batches. Drop last: False\n",
            "2025-10-23 22:36:05,759 - VAETrainer - INFO - Validation loader: 12271 samples, 24 batches.\n",
            "INFO:VAETrainer:Validation loader: 12271 samples, 24 batches.\n",
            "2025-10-23 22:36:07,967 - VAETrainer - INFO - Epoch [1/100] Train TotalLoss: 513.5764, (ReconX: 1.0867, KLD: 0.4484*0.03, LossY: 51.2475*10.00), Time: 1.8s | Val TotalLoss: 390.0295, (ReconX: 1.0116, KLD: 0.4950, LossY: 38.8523)\n",
            "INFO:VAETrainer:Epoch [1/100] Train TotalLoss: 513.5764, (ReconX: 1.0867, KLD: 0.4484*0.03, LossY: 51.2475*10.00), Time: 1.8s | Val TotalLoss: 390.0295, (ReconX: 1.0116, KLD: 0.4950, LossY: 38.8523)\n",
            "2025-10-23 22:36:07,969 - VAETrainer - INFO - Saving model and trainer state to vae_results/best_model.pth...\n",
            "INFO:VAETrainer:Saving model and trainer state to vae_results/best_model.pth...\n",
            "2025-10-23 22:36:07,983 - VAETrainer - INFO - Successfully saved model and trainer state to vae_results/best_model.pth\n",
            "INFO:VAETrainer:Successfully saved model and trainer state to vae_results/best_model.pth\n",
            "2025-10-23 22:36:10,159 - VAETrainer - INFO - Epoch [2/100] Train TotalLoss: 342.3241, (ReconX: 0.9659, KLD: 0.5424*0.07, LossY: 34.1322*10.00), Time: 1.8s | Val TotalLoss: 290.3619, (ReconX: 0.9361, KLD: 0.5995, LossY: 28.8826)\n",
            "INFO:VAETrainer:Epoch [2/100] Train TotalLoss: 342.3241, (ReconX: 0.9659, KLD: 0.5424*0.07, LossY: 34.1322*10.00), Time: 1.8s | Val TotalLoss: 290.3619, (ReconX: 0.9361, KLD: 0.5995, LossY: 28.8826)\n",
            "2025-10-23 22:36:10,160 - VAETrainer - INFO - Saving model and trainer state to vae_results/best_model.pth...\n",
            "INFO:VAETrainer:Saving model and trainer state to vae_results/best_model.pth...\n",
            "2025-10-23 22:36:10,174 - VAETrainer - INFO - Successfully saved model and trainer state to vae_results/best_model.pth\n",
            "INFO:VAETrainer:Successfully saved model and trainer state to vae_results/best_model.pth\n",
            "2025-10-23 22:36:12,328 - VAETrainer - INFO - Epoch [3/100] Train TotalLoss: 258.5351, (ReconX: 0.9199, KLD: 0.6011*0.10, LossY: 25.7555*10.00), Time: 1.8s | Val TotalLoss: 219.7535, (ReconX: 0.9115, KLD: 0.6375, LossY: 21.8204)\n",
            "INFO:VAETrainer:Epoch [3/100] Train TotalLoss: 258.5351, (ReconX: 0.9199, KLD: 0.6011*0.10, LossY: 25.7555*10.00), Time: 1.8s | Val TotalLoss: 219.7535, (ReconX: 0.9115, KLD: 0.6375, LossY: 21.8204)\n",
            "2025-10-23 22:36:12,329 - VAETrainer - INFO - Saving model and trainer state to vae_results/best_model.pth...\n",
            "INFO:VAETrainer:Saving model and trainer state to vae_results/best_model.pth...\n",
            "2025-10-23 22:36:12,342 - VAETrainer - INFO - Successfully saved model and trainer state to vae_results/best_model.pth\n",
            "INFO:VAETrainer:Successfully saved model and trainer state to vae_results/best_model.pth\n",
            "2025-10-23 22:36:14,472 - VAETrainer - INFO - Epoch [4/100] Train TotalLoss: 196.7057, (ReconX: 0.9059, KLD: 0.6190*0.13, LossY: 19.5717*10.00), Time: 1.7s | Val TotalLoss: 166.3744, (ReconX: 0.9038, KLD: 0.6468, LossY: 16.4824)\n",
            "INFO:VAETrainer:Epoch [4/100] Train TotalLoss: 196.7057, (ReconX: 0.9059, KLD: 0.6190*0.13, LossY: 19.5717*10.00), Time: 1.7s | Val TotalLoss: 166.3744, (ReconX: 0.9038, KLD: 0.6468, LossY: 16.4824)\n",
            "2025-10-23 22:36:14,473 - VAETrainer - INFO - Saving model and trainer state to vae_results/best_model.pth...\n",
            "INFO:VAETrainer:Saving model and trainer state to vae_results/best_model.pth...\n",
            "2025-10-23 22:36:14,486 - VAETrainer - INFO - Successfully saved model and trainer state to vae_results/best_model.pth\n",
            "INFO:VAETrainer:Successfully saved model and trainer state to vae_results/best_model.pth\n",
            "2025-10-23 22:36:16,530 - VAETrainer - INFO - Epoch [5/100] Train TotalLoss: 151.2277, (ReconX: 0.9015, KLD: 0.6459*0.17, LossY: 15.0219*10.00), Time: 1.7s | Val TotalLoss: 129.1601, (ReconX: 0.9006, KLD: 0.6743, LossY: 12.7585)\n",
            "INFO:VAETrainer:Epoch [5/100] Train TotalLoss: 151.2277, (ReconX: 0.9015, KLD: 0.6459*0.17, LossY: 15.0219*10.00), Time: 1.7s | Val TotalLoss: 129.1601, (ReconX: 0.9006, KLD: 0.6743, LossY: 12.7585)\n",
            "2025-10-23 22:36:16,532 - VAETrainer - INFO - Saving model and trainer state to vae_results/best_model.pth...\n",
            "INFO:VAETrainer:Saving model and trainer state to vae_results/best_model.pth...\n",
            "2025-10-23 22:36:16,547 - VAETrainer - INFO - Successfully saved model and trainer state to vae_results/best_model.pth\n",
            "INFO:VAETrainer:Successfully saved model and trainer state to vae_results/best_model.pth\n",
            "2025-10-23 22:36:18,715 - VAETrainer - INFO - Epoch [6/100] Train TotalLoss: 120.3133, (ReconX: 0.8998, KLD: 0.6617*0.20, LossY: 11.9281*10.00), Time: 1.8s | Val TotalLoss: 105.8249, (ReconX: 0.8991, KLD: 0.6775, LossY: 10.4248)\n",
            "INFO:VAETrainer:Epoch [6/100] Train TotalLoss: 120.3133, (ReconX: 0.8998, KLD: 0.6617*0.20, LossY: 11.9281*10.00), Time: 1.8s | Val TotalLoss: 105.8249, (ReconX: 0.8991, KLD: 0.6775, LossY: 10.4248)\n",
            "2025-10-23 22:36:18,717 - VAETrainer - INFO - Saving model and trainer state to vae_results/best_model.pth...\n",
            "INFO:VAETrainer:Saving model and trainer state to vae_results/best_model.pth...\n",
            "2025-10-23 22:36:18,730 - VAETrainer - INFO - Successfully saved model and trainer state to vae_results/best_model.pth\n",
            "INFO:VAETrainer:Successfully saved model and trainer state to vae_results/best_model.pth\n",
            "2025-10-23 22:36:20,892 - VAETrainer - INFO - Epoch [7/100] Train TotalLoss: 99.3349, (ReconX: 0.8987, KLD: 0.6607*0.23, LossY: 9.8282*10.00), Time: 1.8s | Val TotalLoss: 89.0638, (ReconX: 0.9006, KLD: 0.6593, LossY: 8.7504)\n",
            "INFO:VAETrainer:Epoch [7/100] Train TotalLoss: 99.3349, (ReconX: 0.8987, KLD: 0.6607*0.23, LossY: 9.8282*10.00), Time: 1.8s | Val TotalLoss: 89.0638, (ReconX: 0.9006, KLD: 0.6593, LossY: 8.7504)\n",
            "2025-10-23 22:36:20,894 - VAETrainer - INFO - Saving model and trainer state to vae_results/best_model.pth...\n",
            "INFO:VAETrainer:Saving model and trainer state to vae_results/best_model.pth...\n",
            "2025-10-23 22:36:20,907 - VAETrainer - INFO - Successfully saved model and trainer state to vae_results/best_model.pth\n",
            "INFO:VAETrainer:Successfully saved model and trainer state to vae_results/best_model.pth\n",
            "2025-10-23 22:36:23,064 - VAETrainer - INFO - Epoch [8/100] Train TotalLoss: 84.3265, (ReconX: 0.8989, KLD: 0.6537*0.27, LossY: 8.3253*10.00), Time: 1.8s | Val TotalLoss: 77.7458, (ReconX: 0.9015, KLD: 0.6608, LossY: 7.6183)\n",
            "INFO:VAETrainer:Epoch [8/100] Train TotalLoss: 84.3265, (ReconX: 0.8989, KLD: 0.6537*0.27, LossY: 8.3253*10.00), Time: 1.8s | Val TotalLoss: 77.7458, (ReconX: 0.9015, KLD: 0.6608, LossY: 7.6183)\n",
            "2025-10-23 22:36:23,066 - VAETrainer - INFO - Saving model and trainer state to vae_results/best_model.pth...\n",
            "INFO:VAETrainer:Saving model and trainer state to vae_results/best_model.pth...\n",
            "2025-10-23 22:36:23,079 - VAETrainer - INFO - Successfully saved model and trainer state to vae_results/best_model.pth\n",
            "INFO:VAETrainer:Successfully saved model and trainer state to vae_results/best_model.pth\n",
            "2025-10-23 22:36:25,194 - VAETrainer - INFO - Epoch [9/100] Train TotalLoss: 72.7267, (ReconX: 0.8997, KLD: 0.6455*0.30, LossY: 7.1633*10.00), Time: 1.7s | Val TotalLoss: 67.8075, (ReconX: 0.9016, KLD: 0.6467, LossY: 6.6259)\n",
            "INFO:VAETrainer:Epoch [9/100] Train TotalLoss: 72.7267, (ReconX: 0.8997, KLD: 0.6455*0.30, LossY: 7.1633*10.00), Time: 1.7s | Val TotalLoss: 67.8075, (ReconX: 0.9016, KLD: 0.6467, LossY: 6.6259)\n",
            "2025-10-23 22:36:25,195 - VAETrainer - INFO - Saving model and trainer state to vae_results/best_model.pth...\n",
            "INFO:VAETrainer:Saving model and trainer state to vae_results/best_model.pth...\n",
            "2025-10-23 22:36:25,208 - VAETrainer - INFO - Successfully saved model and trainer state to vae_results/best_model.pth\n",
            "INFO:VAETrainer:Successfully saved model and trainer state to vae_results/best_model.pth\n",
            "2025-10-23 22:36:27,269 - VAETrainer - INFO - Epoch [10/100] Train TotalLoss: 63.7627, (ReconX: 0.9007, KLD: 0.6373*0.33, LossY: 6.2650*10.00), Time: 1.7s | Val TotalLoss: 59.4146, (ReconX: 0.9027, KLD: 0.6357, LossY: 5.7876)\n",
            "INFO:VAETrainer:Epoch [10/100] Train TotalLoss: 63.7627, (ReconX: 0.9007, KLD: 0.6373*0.33, LossY: 6.2650*10.00), Time: 1.7s | Val TotalLoss: 59.4146, (ReconX: 0.9027, KLD: 0.6357, LossY: 5.7876)\n",
            "2025-10-23 22:36:27,271 - VAETrainer - INFO - Saving model and trainer state to vae_results/best_model.pth...\n",
            "INFO:VAETrainer:Saving model and trainer state to vae_results/best_model.pth...\n",
            "2025-10-23 22:36:27,287 - VAETrainer - INFO - Successfully saved model and trainer state to vae_results/best_model.pth\n",
            "INFO:VAETrainer:Successfully saved model and trainer state to vae_results/best_model.pth\n",
            "2025-10-23 22:36:29,451 - VAETrainer - INFO - Epoch [11/100] Train TotalLoss: 56.7738, (ReconX: 0.9012, KLD: 0.6263*0.37, LossY: 5.5643*10.00), Time: 1.8s | Val TotalLoss: 53.4325, (ReconX: 0.9047, KLD: 0.6366, LossY: 5.1891)\n",
            "INFO:VAETrainer:Epoch [11/100] Train TotalLoss: 56.7738, (ReconX: 0.9012, KLD: 0.6263*0.37, LossY: 5.5643*10.00), Time: 1.8s | Val TotalLoss: 53.4325, (ReconX: 0.9047, KLD: 0.6366, LossY: 5.1891)\n",
            "2025-10-23 22:36:29,452 - VAETrainer - INFO - Saving model and trainer state to vae_results/best_model.pth...\n",
            "INFO:VAETrainer:Saving model and trainer state to vae_results/best_model.pth...\n",
            "2025-10-23 22:36:29,465 - VAETrainer - INFO - Successfully saved model and trainer state to vae_results/best_model.pth\n",
            "INFO:VAETrainer:Successfully saved model and trainer state to vae_results/best_model.pth\n",
            "2025-10-23 22:36:31,619 - VAETrainer - INFO - Epoch [12/100] Train TotalLoss: 51.1787, (ReconX: 0.9022, KLD: 0.6162*0.40, LossY: 5.0030*10.00), Time: 1.8s | Val TotalLoss: 48.4917, (ReconX: 0.9045, KLD: 0.6238, LossY: 4.6963)\n",
            "INFO:VAETrainer:Epoch [12/100] Train TotalLoss: 51.1787, (ReconX: 0.9022, KLD: 0.6162*0.40, LossY: 5.0030*10.00), Time: 1.8s | Val TotalLoss: 48.4917, (ReconX: 0.9045, KLD: 0.6238, LossY: 4.6963)\n",
            "2025-10-23 22:36:31,621 - VAETrainer - INFO - Saving model and trainer state to vae_results/best_model.pth...\n",
            "INFO:VAETrainer:Saving model and trainer state to vae_results/best_model.pth...\n",
            "2025-10-23 22:36:31,635 - VAETrainer - INFO - Successfully saved model and trainer state to vae_results/best_model.pth\n",
            "INFO:VAETrainer:Successfully saved model and trainer state to vae_results/best_model.pth\n",
            "2025-10-23 22:36:33,794 - VAETrainer - INFO - Epoch [13/100] Train TotalLoss: 46.7998, (ReconX: 0.9029, KLD: 0.6033*0.43, LossY: 4.5635*10.00), Time: 1.8s | Val TotalLoss: 45.4085, (ReconX: 0.9057, KLD: 0.6086, LossY: 4.3894)\n",
            "INFO:VAETrainer:Epoch [13/100] Train TotalLoss: 46.7998, (ReconX: 0.9029, KLD: 0.6033*0.43, LossY: 4.5635*10.00), Time: 1.8s | Val TotalLoss: 45.4085, (ReconX: 0.9057, KLD: 0.6086, LossY: 4.3894)\n",
            "2025-10-23 22:36:33,797 - VAETrainer - INFO - Saving model and trainer state to vae_results/best_model.pth...\n",
            "INFO:VAETrainer:Saving model and trainer state to vae_results/best_model.pth...\n",
            "2025-10-23 22:36:33,809 - VAETrainer - INFO - Successfully saved model and trainer state to vae_results/best_model.pth\n",
            "INFO:VAETrainer:Successfully saved model and trainer state to vae_results/best_model.pth\n",
            "2025-10-23 22:36:35,956 - VAETrainer - INFO - Epoch [14/100] Train TotalLoss: 43.1148, (ReconX: 0.9034, KLD: 0.5883*0.47, LossY: 4.1937*10.00), Time: 1.8s | Val TotalLoss: 41.8289, (ReconX: 0.9082, KLD: 0.5838, LossY: 4.0337)\n",
            "INFO:VAETrainer:Epoch [14/100] Train TotalLoss: 43.1148, (ReconX: 0.9034, KLD: 0.5883*0.47, LossY: 4.1937*10.00), Time: 1.8s | Val TotalLoss: 41.8289, (ReconX: 0.9082, KLD: 0.5838, LossY: 4.0337)\n",
            "2025-10-23 22:36:35,957 - VAETrainer - INFO - Saving model and trainer state to vae_results/best_model.pth...\n",
            "INFO:VAETrainer:Saving model and trainer state to vae_results/best_model.pth...\n",
            "2025-10-23 22:36:35,971 - VAETrainer - INFO - Successfully saved model and trainer state to vae_results/best_model.pth\n",
            "INFO:VAETrainer:Successfully saved model and trainer state to vae_results/best_model.pth\n",
            "2025-10-23 22:36:38,197 - VAETrainer - INFO - Epoch [15/100] Train TotalLoss: 40.1027, (ReconX: 0.9050, KLD: 0.5702*0.50, LossY: 3.8913*10.00), Time: 1.9s | Val TotalLoss: 38.6756, (ReconX: 0.9077, KLD: 0.5651, LossY: 3.7203)\n",
            "INFO:VAETrainer:Epoch [15/100] Train TotalLoss: 40.1027, (ReconX: 0.9050, KLD: 0.5702*0.50, LossY: 3.8913*10.00), Time: 1.9s | Val TotalLoss: 38.6756, (ReconX: 0.9077, KLD: 0.5651, LossY: 3.7203)\n",
            "2025-10-23 22:36:38,200 - VAETrainer - INFO - Saving model and trainer state to vae_results/best_model.pth...\n",
            "INFO:VAETrainer:Saving model and trainer state to vae_results/best_model.pth...\n",
            "2025-10-23 22:36:38,214 - VAETrainer - INFO - Successfully saved model and trainer state to vae_results/best_model.pth\n",
            "INFO:VAETrainer:Successfully saved model and trainer state to vae_results/best_model.pth\n",
            "2025-10-23 22:36:40,368 - VAETrainer - INFO - Epoch [16/100] Train TotalLoss: 37.4835, (ReconX: 0.9060, KLD: 0.5513*0.53, LossY: 3.6284*10.00), Time: 1.8s | Val TotalLoss: 36.4121, (ReconX: 0.9100, KLD: 0.5535, LossY: 3.4949)\n",
            "INFO:VAETrainer:Epoch [16/100] Train TotalLoss: 37.4835, (ReconX: 0.9060, KLD: 0.5513*0.53, LossY: 3.6284*10.00), Time: 1.8s | Val TotalLoss: 36.4121, (ReconX: 0.9100, KLD: 0.5535, LossY: 3.4949)\n",
            "2025-10-23 22:36:40,370 - VAETrainer - INFO - Saving model and trainer state to vae_results/best_model.pth...\n",
            "INFO:VAETrainer:Saving model and trainer state to vae_results/best_model.pth...\n",
            "2025-10-23 22:36:40,385 - VAETrainer - INFO - Successfully saved model and trainer state to vae_results/best_model.pth\n",
            "INFO:VAETrainer:Successfully saved model and trainer state to vae_results/best_model.pth\n",
            "2025-10-23 22:36:42,568 - VAETrainer - INFO - Epoch [17/100] Train TotalLoss: 35.1750, (ReconX: 0.9081, KLD: 0.5310*0.57, LossY: 3.3966*10.00), Time: 1.8s | Val TotalLoss: 34.3100, (ReconX: 0.9095, KLD: 0.5306, LossY: 3.2870)\n",
            "INFO:VAETrainer:Epoch [17/100] Train TotalLoss: 35.1750, (ReconX: 0.9081, KLD: 0.5310*0.57, LossY: 3.3966*10.00), Time: 1.8s | Val TotalLoss: 34.3100, (ReconX: 0.9095, KLD: 0.5306, LossY: 3.2870)\n",
            "2025-10-23 22:36:42,570 - VAETrainer - INFO - Saving model and trainer state to vae_results/best_model.pth...\n",
            "INFO:VAETrainer:Saving model and trainer state to vae_results/best_model.pth...\n",
            "2025-10-23 22:36:42,583 - VAETrainer - INFO - Successfully saved model and trainer state to vae_results/best_model.pth\n",
            "INFO:VAETrainer:Successfully saved model and trainer state to vae_results/best_model.pth\n",
            "2025-10-23 22:36:44,724 - VAETrainer - INFO - Epoch [18/100] Train TotalLoss: 33.1728, (ReconX: 0.9105, KLD: 0.5092*0.60, LossY: 3.1957*10.00), Time: 1.8s | Val TotalLoss: 32.5128, (ReconX: 0.9123, KLD: 0.5054, LossY: 3.1095)\n",
            "INFO:VAETrainer:Epoch [18/100] Train TotalLoss: 33.1728, (ReconX: 0.9105, KLD: 0.5092*0.60, LossY: 3.1957*10.00), Time: 1.8s | Val TotalLoss: 32.5128, (ReconX: 0.9123, KLD: 0.5054, LossY: 3.1095)\n",
            "2025-10-23 22:36:44,725 - VAETrainer - INFO - Saving model and trainer state to vae_results/best_model.pth...\n",
            "INFO:VAETrainer:Saving model and trainer state to vae_results/best_model.pth...\n",
            "2025-10-23 22:36:44,738 - VAETrainer - INFO - Successfully saved model and trainer state to vae_results/best_model.pth\n",
            "INFO:VAETrainer:Successfully saved model and trainer state to vae_results/best_model.pth\n",
            "2025-10-23 22:36:46,966 - VAETrainer - INFO - Epoch [19/100] Train TotalLoss: 31.5257, (ReconX: 0.9119, KLD: 0.4878*0.63, LossY: 3.0305*10.00), Time: 1.8s | Val TotalLoss: 31.0241, (ReconX: 0.9163, KLD: 0.4818, LossY: 2.9626)\n",
            "INFO:VAETrainer:Epoch [19/100] Train TotalLoss: 31.5257, (ReconX: 0.9119, KLD: 0.4878*0.63, LossY: 3.0305*10.00), Time: 1.8s | Val TotalLoss: 31.0241, (ReconX: 0.9163, KLD: 0.4818, LossY: 2.9626)\n",
            "2025-10-23 22:36:46,968 - VAETrainer - INFO - Saving model and trainer state to vae_results/best_model.pth...\n",
            "INFO:VAETrainer:Saving model and trainer state to vae_results/best_model.pth...\n",
            "2025-10-23 22:36:46,982 - VAETrainer - INFO - Successfully saved model and trainer state to vae_results/best_model.pth\n",
            "INFO:VAETrainer:Successfully saved model and trainer state to vae_results/best_model.pth\n",
            "2025-10-23 22:36:49,142 - VAETrainer - INFO - Epoch [20/100] Train TotalLoss: 30.2992, (ReconX: 0.9149, KLD: 0.4645*0.67, LossY: 2.9075*10.00), Time: 1.8s | Val TotalLoss: 29.8091, (ReconX: 0.9179, KLD: 0.4590, LossY: 2.8432)\n",
            "INFO:VAETrainer:Epoch [20/100] Train TotalLoss: 30.2992, (ReconX: 0.9149, KLD: 0.4645*0.67, LossY: 2.9075*10.00), Time: 1.8s | Val TotalLoss: 29.8091, (ReconX: 0.9179, KLD: 0.4590, LossY: 2.8432)\n",
            "2025-10-23 22:36:49,144 - VAETrainer - INFO - Saving model and trainer state to vae_results/best_model.pth...\n",
            "INFO:VAETrainer:Saving model and trainer state to vae_results/best_model.pth...\n",
            "2025-10-23 22:36:49,157 - VAETrainer - INFO - Successfully saved model and trainer state to vae_results/best_model.pth\n",
            "INFO:VAETrainer:Successfully saved model and trainer state to vae_results/best_model.pth\n",
            "2025-10-23 22:36:51,304 - VAETrainer - INFO - Epoch [21/100] Train TotalLoss: 29.3008, (ReconX: 0.9179, KLD: 0.4407*0.70, LossY: 2.8075*10.00), Time: 1.8s | Val TotalLoss: 29.1211, (ReconX: 0.9203, KLD: 0.4329, LossY: 2.7768)\n",
            "INFO:VAETrainer:Epoch [21/100] Train TotalLoss: 29.3008, (ReconX: 0.9179, KLD: 0.4407*0.70, LossY: 2.8075*10.00), Time: 1.8s | Val TotalLoss: 29.1211, (ReconX: 0.9203, KLD: 0.4329, LossY: 2.7768)\n",
            "2025-10-23 22:36:51,306 - VAETrainer - INFO - Saving model and trainer state to vae_results/best_model.pth...\n",
            "INFO:VAETrainer:Saving model and trainer state to vae_results/best_model.pth...\n",
            "2025-10-23 22:36:51,320 - VAETrainer - INFO - Successfully saved model and trainer state to vae_results/best_model.pth\n",
            "INFO:VAETrainer:Successfully saved model and trainer state to vae_results/best_model.pth\n",
            "2025-10-23 22:36:53,381 - VAETrainer - INFO - Epoch [22/100] Train TotalLoss: 28.5061, (ReconX: 0.9213, KLD: 0.4131*0.73, LossY: 2.7282*10.00), Time: 1.7s | Val TotalLoss: 28.3525, (ReconX: 0.9253, KLD: 0.4078, LossY: 2.7019)\n",
            "INFO:VAETrainer:Epoch [22/100] Train TotalLoss: 28.5061, (ReconX: 0.9213, KLD: 0.4131*0.73, LossY: 2.7282*10.00), Time: 1.7s | Val TotalLoss: 28.3525, (ReconX: 0.9253, KLD: 0.4078, LossY: 2.7019)\n",
            "2025-10-23 22:36:53,383 - VAETrainer - INFO - Saving model and trainer state to vae_results/best_model.pth...\n",
            "INFO:VAETrainer:Saving model and trainer state to vae_results/best_model.pth...\n",
            "2025-10-23 22:36:53,396 - VAETrainer - INFO - Successfully saved model and trainer state to vae_results/best_model.pth\n",
            "INFO:VAETrainer:Successfully saved model and trainer state to vae_results/best_model.pth\n",
            "2025-10-23 22:36:55,534 - VAETrainer - INFO - Epoch [23/100] Train TotalLoss: 27.8531, (ReconX: 0.9248, KLD: 0.3861*0.77, LossY: 2.6632*10.00), Time: 1.8s | Val TotalLoss: 27.7367, (ReconX: 0.9296, KLD: 0.3742, LossY: 2.6433)\n",
            "INFO:VAETrainer:Epoch [23/100] Train TotalLoss: 27.8531, (ReconX: 0.9248, KLD: 0.3861*0.77, LossY: 2.6632*10.00), Time: 1.8s | Val TotalLoss: 27.7367, (ReconX: 0.9296, KLD: 0.3742, LossY: 2.6433)\n",
            "2025-10-23 22:36:55,536 - VAETrainer - INFO - Saving model and trainer state to vae_results/best_model.pth...\n",
            "INFO:VAETrainer:Saving model and trainer state to vae_results/best_model.pth...\n",
            "2025-10-23 22:36:55,549 - VAETrainer - INFO - Successfully saved model and trainer state to vae_results/best_model.pth\n",
            "INFO:VAETrainer:Successfully saved model and trainer state to vae_results/best_model.pth\n",
            "2025-10-23 22:36:57,655 - VAETrainer - INFO - Epoch [24/100] Train TotalLoss: 27.3085, (ReconX: 0.9289, KLD: 0.3577*0.80, LossY: 2.6093*10.00), Time: 1.7s | Val TotalLoss: 27.2285, (ReconX: 0.9328, KLD: 0.3487, LossY: 2.5947)\n",
            "INFO:VAETrainer:Epoch [24/100] Train TotalLoss: 27.3085, (ReconX: 0.9289, KLD: 0.3577*0.80, LossY: 2.6093*10.00), Time: 1.7s | Val TotalLoss: 27.2285, (ReconX: 0.9328, KLD: 0.3487, LossY: 2.5947)\n",
            "2025-10-23 22:36:57,657 - VAETrainer - INFO - Saving model and trainer state to vae_results/best_model.pth...\n",
            "INFO:VAETrainer:Saving model and trainer state to vae_results/best_model.pth...\n",
            "2025-10-23 22:36:57,671 - VAETrainer - INFO - Successfully saved model and trainer state to vae_results/best_model.pth\n",
            "INFO:VAETrainer:Successfully saved model and trainer state to vae_results/best_model.pth\n",
            "2025-10-23 22:36:59,833 - VAETrainer - INFO - Epoch [25/100] Train TotalLoss: 26.7720, (ReconX: 0.9330, KLD: 0.3290*0.83, LossY: 2.5565*10.00), Time: 1.8s | Val TotalLoss: 26.5638, (ReconX: 0.9376, KLD: 0.3174, LossY: 2.5309)\n",
            "INFO:VAETrainer:Epoch [25/100] Train TotalLoss: 26.7720, (ReconX: 0.9330, KLD: 0.3290*0.83, LossY: 2.5565*10.00), Time: 1.8s | Val TotalLoss: 26.5638, (ReconX: 0.9376, KLD: 0.3174, LossY: 2.5309)\n",
            "2025-10-23 22:36:59,835 - VAETrainer - INFO - Saving model and trainer state to vae_results/best_model.pth...\n",
            "INFO:VAETrainer:Saving model and trainer state to vae_results/best_model.pth...\n",
            "2025-10-23 22:36:59,850 - VAETrainer - INFO - Successfully saved model and trainer state to vae_results/best_model.pth\n",
            "INFO:VAETrainer:Successfully saved model and trainer state to vae_results/best_model.pth\n",
            "2025-10-23 22:37:02,110 - VAETrainer - INFO - Epoch [26/100] Train TotalLoss: 26.2360, (ReconX: 0.9381, KLD: 0.2978*0.87, LossY: 2.5040*10.00), Time: 1.9s | Val TotalLoss: 25.9686, (ReconX: 0.9427, KLD: 0.2847, LossY: 2.4741)\n",
            "INFO:VAETrainer:Epoch [26/100] Train TotalLoss: 26.2360, (ReconX: 0.9381, KLD: 0.2978*0.87, LossY: 2.5040*10.00), Time: 1.9s | Val TotalLoss: 25.9686, (ReconX: 0.9427, KLD: 0.2847, LossY: 2.4741)\n",
            "2025-10-23 22:37:02,111 - VAETrainer - INFO - Saving model and trainer state to vae_results/best_model.pth...\n",
            "INFO:VAETrainer:Saving model and trainer state to vae_results/best_model.pth...\n",
            "2025-10-23 22:37:02,124 - VAETrainer - INFO - Successfully saved model and trainer state to vae_results/best_model.pth\n",
            "INFO:VAETrainer:Successfully saved model and trainer state to vae_results/best_model.pth\n",
            "2025-10-23 22:37:04,196 - VAETrainer - INFO - Epoch [27/100] Train TotalLoss: 25.5045, (ReconX: 0.9428, KLD: 0.2666*0.90, LossY: 2.4322*10.00), Time: 1.7s | Val TotalLoss: 25.3936, (ReconX: 0.9499, KLD: 0.2583, LossY: 2.4185)\n",
            "INFO:VAETrainer:Epoch [27/100] Train TotalLoss: 25.5045, (ReconX: 0.9428, KLD: 0.2666*0.90, LossY: 2.4322*10.00), Time: 1.7s | Val TotalLoss: 25.3936, (ReconX: 0.9499, KLD: 0.2583, LossY: 2.4185)\n",
            "2025-10-23 22:37:04,198 - VAETrainer - INFO - Saving model and trainer state to vae_results/best_model.pth...\n",
            "INFO:VAETrainer:Saving model and trainer state to vae_results/best_model.pth...\n",
            "2025-10-23 22:37:04,211 - VAETrainer - INFO - Successfully saved model and trainer state to vae_results/best_model.pth\n",
            "INFO:VAETrainer:Successfully saved model and trainer state to vae_results/best_model.pth\n",
            "2025-10-23 22:37:06,331 - VAETrainer - INFO - Epoch [28/100] Train TotalLoss: 24.5558, (ReconX: 0.9471, KLD: 0.2401*0.93, LossY: 2.3385*10.00), Time: 1.8s | Val TotalLoss: 24.0944, (ReconX: 0.9492, KLD: 0.2263, LossY: 2.2919)\n",
            "INFO:VAETrainer:Epoch [28/100] Train TotalLoss: 24.5558, (ReconX: 0.9471, KLD: 0.2401*0.93, LossY: 2.3385*10.00), Time: 1.8s | Val TotalLoss: 24.0944, (ReconX: 0.9492, KLD: 0.2263, LossY: 2.2919)\n",
            "2025-10-23 22:37:06,333 - VAETrainer - INFO - Saving model and trainer state to vae_results/best_model.pth...\n",
            "INFO:VAETrainer:Saving model and trainer state to vae_results/best_model.pth...\n",
            "2025-10-23 22:37:06,345 - VAETrainer - INFO - Successfully saved model and trainer state to vae_results/best_model.pth\n",
            "INFO:VAETrainer:Successfully saved model and trainer state to vae_results/best_model.pth\n",
            "2025-10-23 22:37:08,504 - VAETrainer - INFO - Epoch [29/100] Train TotalLoss: 23.5548, (ReconX: 0.9525, KLD: 0.2101*0.97, LossY: 2.2399*10.00), Time: 1.8s | Val TotalLoss: 22.8329, (ReconX: 0.9579, KLD: 0.1944, LossY: 2.1681)\n",
            "INFO:VAETrainer:Epoch [29/100] Train TotalLoss: 23.5548, (ReconX: 0.9525, KLD: 0.2101*0.97, LossY: 2.2399*10.00), Time: 1.8s | Val TotalLoss: 22.8329, (ReconX: 0.9579, KLD: 0.1944, LossY: 2.1681)\n",
            "2025-10-23 22:37:08,505 - VAETrainer - INFO - Saving model and trainer state to vae_results/best_model.pth...\n",
            "INFO:VAETrainer:Saving model and trainer state to vae_results/best_model.pth...\n",
            "2025-10-23 22:37:08,519 - VAETrainer - INFO - Successfully saved model and trainer state to vae_results/best_model.pth\n",
            "INFO:VAETrainer:Successfully saved model and trainer state to vae_results/best_model.pth\n",
            "2025-10-23 22:37:10,628 - VAETrainer - INFO - Epoch [30/100] Train TotalLoss: 22.1329, (ReconX: 0.9605, KLD: 0.1715*1.00, LossY: 2.1001*10.00), Time: 1.7s | Val TotalLoss: 21.1088, (ReconX: 0.9665, KLD: 0.1554, LossY: 1.9987)\n",
            "INFO:VAETrainer:Epoch [30/100] Train TotalLoss: 22.1329, (ReconX: 0.9605, KLD: 0.1715*1.00, LossY: 2.1001*10.00), Time: 1.7s | Val TotalLoss: 21.1088, (ReconX: 0.9665, KLD: 0.1554, LossY: 1.9987)\n",
            "2025-10-23 22:37:10,630 - VAETrainer - INFO - Saving model and trainer state to vae_results/best_model.pth...\n",
            "INFO:VAETrainer:Saving model and trainer state to vae_results/best_model.pth...\n",
            "2025-10-23 22:37:10,644 - VAETrainer - INFO - Successfully saved model and trainer state to vae_results/best_model.pth\n",
            "INFO:VAETrainer:Successfully saved model and trainer state to vae_results/best_model.pth\n",
            "2025-10-23 22:37:12,818 - VAETrainer - INFO - Epoch [31/100] Train TotalLoss: 20.9486, (ReconX: 0.9673, KLD: 0.1407*1.00, LossY: 1.9841*10.00), Time: 1.8s | Val TotalLoss: 20.0937, (ReconX: 0.9727, KLD: 0.1340, LossY: 1.8987)\n",
            "INFO:VAETrainer:Epoch [31/100] Train TotalLoss: 20.9486, (ReconX: 0.9673, KLD: 0.1407*1.00, LossY: 1.9841*10.00), Time: 1.8s | Val TotalLoss: 20.0937, (ReconX: 0.9727, KLD: 0.1340, LossY: 1.8987)\n",
            "2025-10-23 22:37:12,820 - VAETrainer - INFO - Saving model and trainer state to vae_results/best_model.pth...\n",
            "INFO:VAETrainer:Saving model and trainer state to vae_results/best_model.pth...\n",
            "2025-10-23 22:37:12,834 - VAETrainer - INFO - Successfully saved model and trainer state to vae_results/best_model.pth\n",
            "INFO:VAETrainer:Successfully saved model and trainer state to vae_results/best_model.pth\n",
            "2025-10-23 22:37:14,966 - VAETrainer - INFO - Epoch [32/100] Train TotalLoss: 20.4244, (ReconX: 0.9708, KLD: 0.1244*1.00, LossY: 1.9329*10.00), Time: 1.8s | Val TotalLoss: 19.5136, (ReconX: 0.9767, KLD: 0.1182, LossY: 1.8419)\n",
            "INFO:VAETrainer:Epoch [32/100] Train TotalLoss: 20.4244, (ReconX: 0.9708, KLD: 0.1244*1.00, LossY: 1.9329*10.00), Time: 1.8s | Val TotalLoss: 19.5136, (ReconX: 0.9767, KLD: 0.1182, LossY: 1.8419)\n",
            "2025-10-23 22:37:14,968 - VAETrainer - INFO - Saving model and trainer state to vae_results/best_model.pth...\n",
            "INFO:VAETrainer:Saving model and trainer state to vae_results/best_model.pth...\n",
            "2025-10-23 22:37:14,981 - VAETrainer - INFO - Successfully saved model and trainer state to vae_results/best_model.pth\n",
            "INFO:VAETrainer:Successfully saved model and trainer state to vae_results/best_model.pth\n",
            "2025-10-23 22:37:17,050 - VAETrainer - INFO - Epoch [33/100] Train TotalLoss: 19.7731, (ReconX: 0.9731, KLD: 0.1116*1.00, LossY: 1.8688*10.00), Time: 1.7s | Val TotalLoss: 18.8857, (ReconX: 0.9781, KLD: 0.1084, LossY: 1.7799)\n",
            "INFO:VAETrainer:Epoch [33/100] Train TotalLoss: 19.7731, (ReconX: 0.9731, KLD: 0.1116*1.00, LossY: 1.8688*10.00), Time: 1.7s | Val TotalLoss: 18.8857, (ReconX: 0.9781, KLD: 0.1084, LossY: 1.7799)\n",
            "2025-10-23 22:37:17,052 - VAETrainer - INFO - Saving model and trainer state to vae_results/best_model.pth...\n",
            "INFO:VAETrainer:Saving model and trainer state to vae_results/best_model.pth...\n",
            "2025-10-23 22:37:17,065 - VAETrainer - INFO - Successfully saved model and trainer state to vae_results/best_model.pth\n",
            "INFO:VAETrainer:Successfully saved model and trainer state to vae_results/best_model.pth\n",
            "2025-10-23 22:37:19,265 - VAETrainer - INFO - Epoch [34/100] Train TotalLoss: 19.4691, (ReconX: 0.9756, KLD: 0.1053*1.00, LossY: 1.8388*10.00), Time: 1.8s | Val TotalLoss: 18.7160, (ReconX: 0.9797, KLD: 0.1033, LossY: 1.7633)\n",
            "INFO:VAETrainer:Epoch [34/100] Train TotalLoss: 19.4691, (ReconX: 0.9756, KLD: 0.1053*1.00, LossY: 1.8388*10.00), Time: 1.8s | Val TotalLoss: 18.7160, (ReconX: 0.9797, KLD: 0.1033, LossY: 1.7633)\n",
            "2025-10-23 22:37:19,266 - VAETrainer - INFO - Saving model and trainer state to vae_results/best_model.pth...\n",
            "INFO:VAETrainer:Saving model and trainer state to vae_results/best_model.pth...\n",
            "2025-10-23 22:37:19,281 - VAETrainer - INFO - Successfully saved model and trainer state to vae_results/best_model.pth\n",
            "INFO:VAETrainer:Successfully saved model and trainer state to vae_results/best_model.pth\n",
            "2025-10-23 22:37:21,349 - VAETrainer - INFO - Epoch [35/100] Train TotalLoss: 19.2998, (ReconX: 0.9763, KLD: 0.0984*1.00, LossY: 1.8225*10.00), Time: 1.7s | Val TotalLoss: 18.4780, (ReconX: 0.9806, KLD: 0.0982, LossY: 1.7399)\n",
            "INFO:VAETrainer:Epoch [35/100] Train TotalLoss: 19.2998, (ReconX: 0.9763, KLD: 0.0984*1.00, LossY: 1.8225*10.00), Time: 1.7s | Val TotalLoss: 18.4780, (ReconX: 0.9806, KLD: 0.0982, LossY: 1.7399)\n",
            "2025-10-23 22:37:21,351 - VAETrainer - INFO - Saving model and trainer state to vae_results/best_model.pth...\n",
            "INFO:VAETrainer:Saving model and trainer state to vae_results/best_model.pth...\n",
            "2025-10-23 22:37:21,364 - VAETrainer - INFO - Successfully saved model and trainer state to vae_results/best_model.pth\n",
            "INFO:VAETrainer:Successfully saved model and trainer state to vae_results/best_model.pth\n",
            "2025-10-23 22:37:23,509 - VAETrainer - INFO - Epoch [36/100] Train TotalLoss: 18.9893, (ReconX: 0.9778, KLD: 0.0940*1.00, LossY: 1.7917*10.00), Time: 1.8s | Val TotalLoss: 18.2627, (ReconX: 0.9822, KLD: 0.0912, LossY: 1.7189)\n",
            "INFO:VAETrainer:Epoch [36/100] Train TotalLoss: 18.9893, (ReconX: 0.9778, KLD: 0.0940*1.00, LossY: 1.7917*10.00), Time: 1.8s | Val TotalLoss: 18.2627, (ReconX: 0.9822, KLD: 0.0912, LossY: 1.7189)\n",
            "2025-10-23 22:37:23,511 - VAETrainer - INFO - Saving model and trainer state to vae_results/best_model.pth...\n",
            "INFO:VAETrainer:Saving model and trainer state to vae_results/best_model.pth...\n",
            "2025-10-23 22:37:23,527 - VAETrainer - INFO - Successfully saved model and trainer state to vae_results/best_model.pth\n",
            "INFO:VAETrainer:Successfully saved model and trainer state to vae_results/best_model.pth\n",
            "2025-10-23 22:37:25,665 - VAETrainer - INFO - Epoch [37/100] Train TotalLoss: 18.8271, (ReconX: 0.9792, KLD: 0.0877*1.00, LossY: 1.7760*10.00), Time: 1.8s | Val TotalLoss: 18.0131, (ReconX: 0.9811, KLD: 0.0876, LossY: 1.6944)\n",
            "INFO:VAETrainer:Epoch [37/100] Train TotalLoss: 18.8271, (ReconX: 0.9792, KLD: 0.0877*1.00, LossY: 1.7760*10.00), Time: 1.8s | Val TotalLoss: 18.0131, (ReconX: 0.9811, KLD: 0.0876, LossY: 1.6944)\n",
            "2025-10-23 22:37:25,667 - VAETrainer - INFO - Saving model and trainer state to vae_results/best_model.pth...\n",
            "INFO:VAETrainer:Saving model and trainer state to vae_results/best_model.pth...\n",
            "2025-10-23 22:37:25,683 - VAETrainer - INFO - Successfully saved model and trainer state to vae_results/best_model.pth\n",
            "INFO:VAETrainer:Successfully saved model and trainer state to vae_results/best_model.pth\n",
            "2025-10-23 22:37:27,767 - VAETrainer - INFO - Epoch [38/100] Train TotalLoss: 18.7680, (ReconX: 0.9797, KLD: 0.0848*1.00, LossY: 1.7703*10.00), Time: 1.7s | Val TotalLoss: 17.9147, (ReconX: 0.9840, KLD: 0.0852, LossY: 1.6846)\n",
            "INFO:VAETrainer:Epoch [38/100] Train TotalLoss: 18.7680, (ReconX: 0.9797, KLD: 0.0848*1.00, LossY: 1.7703*10.00), Time: 1.7s | Val TotalLoss: 17.9147, (ReconX: 0.9840, KLD: 0.0852, LossY: 1.6846)\n",
            "2025-10-23 22:37:27,768 - VAETrainer - INFO - Saving model and trainer state to vae_results/best_model.pth...\n",
            "INFO:VAETrainer:Saving model and trainer state to vae_results/best_model.pth...\n",
            "2025-10-23 22:37:27,784 - VAETrainer - INFO - Successfully saved model and trainer state to vae_results/best_model.pth\n",
            "INFO:VAETrainer:Successfully saved model and trainer state to vae_results/best_model.pth\n",
            "2025-10-23 22:37:29,898 - VAETrainer - INFO - Epoch [39/100] Train TotalLoss: 18.5817, (ReconX: 0.9803, KLD: 0.0815*1.00, LossY: 1.7520*10.00), Time: 1.7s | Val TotalLoss: 17.7884, (ReconX: 0.9839, KLD: 0.0798, LossY: 1.6725)\n",
            "INFO:VAETrainer:Epoch [39/100] Train TotalLoss: 18.5817, (ReconX: 0.9803, KLD: 0.0815*1.00, LossY: 1.7520*10.00), Time: 1.7s | Val TotalLoss: 17.7884, (ReconX: 0.9839, KLD: 0.0798, LossY: 1.6725)\n",
            "2025-10-23 22:37:29,900 - VAETrainer - INFO - Saving model and trainer state to vae_results/best_model.pth...\n",
            "INFO:VAETrainer:Saving model and trainer state to vae_results/best_model.pth...\n",
            "2025-10-23 22:37:29,913 - VAETrainer - INFO - Successfully saved model and trainer state to vae_results/best_model.pth\n",
            "INFO:VAETrainer:Successfully saved model and trainer state to vae_results/best_model.pth\n",
            "2025-10-23 22:37:32,022 - VAETrainer - INFO - Epoch [40/100] Train TotalLoss: 18.4697, (ReconX: 0.9818, KLD: 0.0780*1.00, LossY: 1.7410*10.00), Time: 1.7s | Val TotalLoss: 17.6167, (ReconX: 0.9853, KLD: 0.0786, LossY: 1.6553)\n",
            "INFO:VAETrainer:Epoch [40/100] Train TotalLoss: 18.4697, (ReconX: 0.9818, KLD: 0.0780*1.00, LossY: 1.7410*10.00), Time: 1.7s | Val TotalLoss: 17.6167, (ReconX: 0.9853, KLD: 0.0786, LossY: 1.6553)\n",
            "2025-10-23 22:37:32,025 - VAETrainer - INFO - Saving model and trainer state to vae_results/best_model.pth...\n",
            "INFO:VAETrainer:Saving model and trainer state to vae_results/best_model.pth...\n",
            "2025-10-23 22:37:32,038 - VAETrainer - INFO - Successfully saved model and trainer state to vae_results/best_model.pth\n",
            "INFO:VAETrainer:Successfully saved model and trainer state to vae_results/best_model.pth\n",
            "2025-10-23 22:37:34,103 - VAETrainer - INFO - Epoch [41/100] Train TotalLoss: 18.4691, (ReconX: 0.9822, KLD: 0.0756*1.00, LossY: 1.7411*10.00), Time: 1.7s | Val TotalLoss: 17.4492, (ReconX: 0.9848, KLD: 0.0756, LossY: 1.6389)\n",
            "INFO:VAETrainer:Epoch [41/100] Train TotalLoss: 18.4691, (ReconX: 0.9822, KLD: 0.0756*1.00, LossY: 1.7411*10.00), Time: 1.7s | Val TotalLoss: 17.4492, (ReconX: 0.9848, KLD: 0.0756, LossY: 1.6389)\n",
            "2025-10-23 22:37:34,105 - VAETrainer - INFO - Saving model and trainer state to vae_results/best_model.pth...\n",
            "INFO:VAETrainer:Saving model and trainer state to vae_results/best_model.pth...\n",
            "2025-10-23 22:37:34,118 - VAETrainer - INFO - Successfully saved model and trainer state to vae_results/best_model.pth\n",
            "INFO:VAETrainer:Successfully saved model and trainer state to vae_results/best_model.pth\n",
            "2025-10-23 22:37:36,324 - VAETrainer - INFO - Epoch [42/100] Train TotalLoss: 18.2409, (ReconX: 0.9827, KLD: 0.0725*1.00, LossY: 1.7186*10.00), Time: 1.8s | Val TotalLoss: 17.3315, (ReconX: 0.9843, KLD: 0.0714, LossY: 1.6276)\n",
            "INFO:VAETrainer:Epoch [42/100] Train TotalLoss: 18.2409, (ReconX: 0.9827, KLD: 0.0725*1.00, LossY: 1.7186*10.00), Time: 1.8s | Val TotalLoss: 17.3315, (ReconX: 0.9843, KLD: 0.0714, LossY: 1.6276)\n",
            "2025-10-23 22:37:36,325 - VAETrainer - INFO - Saving model and trainer state to vae_results/best_model.pth...\n",
            "INFO:VAETrainer:Saving model and trainer state to vae_results/best_model.pth...\n",
            "2025-10-23 22:37:36,339 - VAETrainer - INFO - Successfully saved model and trainer state to vae_results/best_model.pth\n",
            "INFO:VAETrainer:Successfully saved model and trainer state to vae_results/best_model.pth\n",
            "2025-10-23 22:37:38,511 - VAETrainer - INFO - Epoch [43/100] Train TotalLoss: 18.2061, (ReconX: 0.9837, KLD: 0.0705*1.00, LossY: 1.7152*10.00), Time: 1.8s | Val TotalLoss: 17.3090, (ReconX: 0.9877, KLD: 0.0691, LossY: 1.6252)\n",
            "INFO:VAETrainer:Epoch [43/100] Train TotalLoss: 18.2061, (ReconX: 0.9837, KLD: 0.0705*1.00, LossY: 1.7152*10.00), Time: 1.8s | Val TotalLoss: 17.3090, (ReconX: 0.9877, KLD: 0.0691, LossY: 1.6252)\n",
            "2025-10-23 22:37:38,513 - VAETrainer - INFO - Saving model and trainer state to vae_results/best_model.pth...\n",
            "INFO:VAETrainer:Saving model and trainer state to vae_results/best_model.pth...\n",
            "2025-10-23 22:37:38,526 - VAETrainer - INFO - Successfully saved model and trainer state to vae_results/best_model.pth\n",
            "INFO:VAETrainer:Successfully saved model and trainer state to vae_results/best_model.pth\n",
            "2025-10-23 22:37:40,686 - VAETrainer - INFO - Epoch [44/100] Train TotalLoss: 18.1757, (ReconX: 0.9834, KLD: 0.0683*1.00, LossY: 1.7124*10.00), Time: 1.8s | Val TotalLoss: 17.7090, (ReconX: 0.9872, KLD: 0.0685, LossY: 1.6653)\n",
            "INFO:VAETrainer:Epoch [44/100] Train TotalLoss: 18.1757, (ReconX: 0.9834, KLD: 0.0683*1.00, LossY: 1.7124*10.00), Time: 1.8s | Val TotalLoss: 17.7090, (ReconX: 0.9872, KLD: 0.0685, LossY: 1.6653)\n",
            "2025-10-23 22:37:42,858 - VAETrainer - INFO - Epoch [45/100] Train TotalLoss: 17.9346, (ReconX: 0.9843, KLD: 0.0662*1.00, LossY: 1.6884*10.00), Time: 1.8s | Val TotalLoss: 16.9748, (ReconX: 0.9899, KLD: 0.0651, LossY: 1.5920)\n",
            "INFO:VAETrainer:Epoch [45/100] Train TotalLoss: 17.9346, (ReconX: 0.9843, KLD: 0.0662*1.00, LossY: 1.6884*10.00), Time: 1.8s | Val TotalLoss: 16.9748, (ReconX: 0.9899, KLD: 0.0651, LossY: 1.5920)\n",
            "2025-10-23 22:37:42,860 - VAETrainer - INFO - Saving model and trainer state to vae_results/best_model.pth...\n",
            "INFO:VAETrainer:Saving model and trainer state to vae_results/best_model.pth...\n",
            "2025-10-23 22:37:42,874 - VAETrainer - INFO - Successfully saved model and trainer state to vae_results/best_model.pth\n",
            "INFO:VAETrainer:Successfully saved model and trainer state to vae_results/best_model.pth\n",
            "2025-10-23 22:37:45,073 - VAETrainer - INFO - Epoch [46/100] Train TotalLoss: 17.8044, (ReconX: 0.9852, KLD: 0.0645*1.00, LossY: 1.6755*10.00), Time: 1.8s | Val TotalLoss: 16.9573, (ReconX: 0.9880, KLD: 0.0638, LossY: 1.5905)\n",
            "INFO:VAETrainer:Epoch [46/100] Train TotalLoss: 17.8044, (ReconX: 0.9852, KLD: 0.0645*1.00, LossY: 1.6755*10.00), Time: 1.8s | Val TotalLoss: 16.9573, (ReconX: 0.9880, KLD: 0.0638, LossY: 1.5905)\n",
            "2025-10-23 22:37:45,075 - VAETrainer - INFO - Saving model and trainer state to vae_results/best_model.pth...\n",
            "INFO:VAETrainer:Saving model and trainer state to vae_results/best_model.pth...\n",
            "2025-10-23 22:37:45,092 - VAETrainer - INFO - Successfully saved model and trainer state to vae_results/best_model.pth\n",
            "INFO:VAETrainer:Successfully saved model and trainer state to vae_results/best_model.pth\n",
            "2025-10-23 22:37:47,242 - VAETrainer - INFO - Epoch [47/100] Train TotalLoss: 17.8218, (ReconX: 0.9853, KLD: 0.0625*1.00, LossY: 1.6774*10.00), Time: 1.8s | Val TotalLoss: 17.0531, (ReconX: 0.9889, KLD: 0.0645, LossY: 1.6000)\n",
            "INFO:VAETrainer:Epoch [47/100] Train TotalLoss: 17.8218, (ReconX: 0.9853, KLD: 0.0625*1.00, LossY: 1.6774*10.00), Time: 1.8s | Val TotalLoss: 17.0531, (ReconX: 0.9889, KLD: 0.0645, LossY: 1.6000)\n",
            "2025-10-23 22:37:49,401 - VAETrainer - INFO - Epoch [48/100] Train TotalLoss: 17.9146, (ReconX: 0.9852, KLD: 0.0627*1.00, LossY: 1.6867*10.00), Time: 1.8s | Val TotalLoss: 16.7470, (ReconX: 0.9887, KLD: 0.0631, LossY: 1.5695)\n",
            "INFO:VAETrainer:Epoch [48/100] Train TotalLoss: 17.9146, (ReconX: 0.9852, KLD: 0.0627*1.00, LossY: 1.6867*10.00), Time: 1.8s | Val TotalLoss: 16.7470, (ReconX: 0.9887, KLD: 0.0631, LossY: 1.5695)\n",
            "2025-10-23 22:37:49,403 - VAETrainer - INFO - Saving model and trainer state to vae_results/best_model.pth...\n",
            "INFO:VAETrainer:Saving model and trainer state to vae_results/best_model.pth...\n",
            "2025-10-23 22:37:49,416 - VAETrainer - INFO - Successfully saved model and trainer state to vae_results/best_model.pth\n",
            "INFO:VAETrainer:Successfully saved model and trainer state to vae_results/best_model.pth\n",
            "2025-10-23 22:37:51,596 - VAETrainer - INFO - Epoch [49/100] Train TotalLoss: 17.7059, (ReconX: 0.9854, KLD: 0.0612*1.00, LossY: 1.6659*10.00), Time: 1.8s | Val TotalLoss: 16.8765, (ReconX: 0.9890, KLD: 0.0610, LossY: 1.5826)\n",
            "INFO:VAETrainer:Epoch [49/100] Train TotalLoss: 17.7059, (ReconX: 0.9854, KLD: 0.0612*1.00, LossY: 1.6659*10.00), Time: 1.8s | Val TotalLoss: 16.8765, (ReconX: 0.9890, KLD: 0.0610, LossY: 1.5826)\n",
            "2025-10-23 22:37:53,663 - VAETrainer - INFO - Epoch [50/100] Train TotalLoss: 18.0199, (ReconX: 0.9854, KLD: 0.0619*1.00, LossY: 1.6973*10.00), Time: 1.7s | Val TotalLoss: 17.0352, (ReconX: 0.9888, KLD: 0.0607, LossY: 1.5986)\n",
            "INFO:VAETrainer:Epoch [50/100] Train TotalLoss: 18.0199, (ReconX: 0.9854, KLD: 0.0619*1.00, LossY: 1.6973*10.00), Time: 1.7s | Val TotalLoss: 17.0352, (ReconX: 0.9888, KLD: 0.0607, LossY: 1.5986)\n",
            "2025-10-23 22:37:55,783 - VAETrainer - INFO - Epoch [51/100] Train TotalLoss: 17.5023, (ReconX: 0.9858, KLD: 0.0608*1.00, LossY: 1.6456*10.00), Time: 1.7s | Val TotalLoss: 16.4843, (ReconX: 0.9892, KLD: 0.0601, LossY: 1.5435)\n",
            "INFO:VAETrainer:Epoch [51/100] Train TotalLoss: 17.5023, (ReconX: 0.9858, KLD: 0.0608*1.00, LossY: 1.6456*10.00), Time: 1.7s | Val TotalLoss: 16.4843, (ReconX: 0.9892, KLD: 0.0601, LossY: 1.5435)\n",
            "2025-10-23 22:37:55,784 - VAETrainer - INFO - Saving model and trainer state to vae_results/best_model.pth...\n",
            "INFO:VAETrainer:Saving model and trainer state to vae_results/best_model.pth...\n",
            "2025-10-23 22:37:55,798 - VAETrainer - INFO - Successfully saved model and trainer state to vae_results/best_model.pth\n",
            "INFO:VAETrainer:Successfully saved model and trainer state to vae_results/best_model.pth\n",
            "2025-10-23 22:37:57,884 - VAETrainer - INFO - Epoch [52/100] Train TotalLoss: 17.5068, (ReconX: 0.9860, KLD: 0.0599*1.00, LossY: 1.6461*10.00), Time: 1.7s | Val TotalLoss: 16.6389, (ReconX: 0.9889, KLD: 0.0601, LossY: 1.5590)\n",
            "INFO:VAETrainer:Epoch [52/100] Train TotalLoss: 17.5068, (ReconX: 0.9860, KLD: 0.0599*1.00, LossY: 1.6461*10.00), Time: 1.7s | Val TotalLoss: 16.6389, (ReconX: 0.9889, KLD: 0.0601, LossY: 1.5590)\n",
            "2025-10-23 22:37:59,996 - VAETrainer - INFO - Epoch [53/100] Train TotalLoss: 17.3077, (ReconX: 0.9859, KLD: 0.0588*1.00, LossY: 1.6263*10.00), Time: 1.7s | Val TotalLoss: 16.5408, (ReconX: 0.9896, KLD: 0.0597, LossY: 1.5491)\n",
            "INFO:VAETrainer:Epoch [53/100] Train TotalLoss: 17.3077, (ReconX: 0.9859, KLD: 0.0588*1.00, LossY: 1.6263*10.00), Time: 1.7s | Val TotalLoss: 16.5408, (ReconX: 0.9896, KLD: 0.0597, LossY: 1.5491)\n",
            "2025-10-23 22:38:02,244 - VAETrainer - INFO - Epoch [54/100] Train TotalLoss: 17.2940, (ReconX: 0.9862, KLD: 0.0585*1.00, LossY: 1.6249*10.00), Time: 1.9s | Val TotalLoss: 16.6715, (ReconX: 0.9912, KLD: 0.0564, LossY: 1.5624)\n",
            "INFO:VAETrainer:Epoch [54/100] Train TotalLoss: 17.2940, (ReconX: 0.9862, KLD: 0.0585*1.00, LossY: 1.6249*10.00), Time: 1.9s | Val TotalLoss: 16.6715, (ReconX: 0.9912, KLD: 0.0564, LossY: 1.5624)\n",
            "2025-10-23 22:38:04,294 - VAETrainer - INFO - Epoch [55/100] Train TotalLoss: 17.3489, (ReconX: 0.9862, KLD: 0.0575*1.00, LossY: 1.6305*10.00), Time: 1.7s | Val TotalLoss: 16.2239, (ReconX: 0.9914, KLD: 0.0571, LossY: 1.5175)\n",
            "INFO:VAETrainer:Epoch [55/100] Train TotalLoss: 17.3489, (ReconX: 0.9862, KLD: 0.0575*1.00, LossY: 1.6305*10.00), Time: 1.7s | Val TotalLoss: 16.2239, (ReconX: 0.9914, KLD: 0.0571, LossY: 1.5175)\n",
            "2025-10-23 22:38:04,296 - VAETrainer - INFO - Saving model and trainer state to vae_results/best_model.pth...\n",
            "INFO:VAETrainer:Saving model and trainer state to vae_results/best_model.pth...\n",
            "2025-10-23 22:38:04,309 - VAETrainer - INFO - Successfully saved model and trainer state to vae_results/best_model.pth\n",
            "INFO:VAETrainer:Successfully saved model and trainer state to vae_results/best_model.pth\n",
            "2025-10-23 22:38:06,408 - VAETrainer - INFO - Epoch [56/100] Train TotalLoss: 17.2555, (ReconX: 0.9862, KLD: 0.0574*1.00, LossY: 1.6212*10.00), Time: 1.7s | Val TotalLoss: 16.2814, (ReconX: 0.9901, KLD: 0.0571, LossY: 1.5234)\n",
            "INFO:VAETrainer:Epoch [56/100] Train TotalLoss: 17.2555, (ReconX: 0.9862, KLD: 0.0574*1.00, LossY: 1.6212*10.00), Time: 1.7s | Val TotalLoss: 16.2814, (ReconX: 0.9901, KLD: 0.0571, LossY: 1.5234)\n",
            "2025-10-23 22:38:08,562 - VAETrainer - INFO - Epoch [57/100] Train TotalLoss: 16.9554, (ReconX: 0.9866, KLD: 0.0564*1.00, LossY: 1.5912*10.00), Time: 1.8s | Val TotalLoss: 16.1585, (ReconX: 0.9909, KLD: 0.0566, LossY: 1.5111)\n",
            "INFO:VAETrainer:Epoch [57/100] Train TotalLoss: 16.9554, (ReconX: 0.9866, KLD: 0.0564*1.00, LossY: 1.5912*10.00), Time: 1.8s | Val TotalLoss: 16.1585, (ReconX: 0.9909, KLD: 0.0566, LossY: 1.5111)\n",
            "2025-10-23 22:38:08,563 - VAETrainer - INFO - Saving model and trainer state to vae_results/best_model.pth...\n",
            "INFO:VAETrainer:Saving model and trainer state to vae_results/best_model.pth...\n",
            "2025-10-23 22:38:08,577 - VAETrainer - INFO - Successfully saved model and trainer state to vae_results/best_model.pth\n",
            "INFO:VAETrainer:Successfully saved model and trainer state to vae_results/best_model.pth\n",
            "2025-10-23 22:38:10,710 - VAETrainer - INFO - Epoch [58/100] Train TotalLoss: 16.8929, (ReconX: 0.9867, KLD: 0.0557*1.00, LossY: 1.5850*10.00), Time: 1.8s | Val TotalLoss: 16.1668, (ReconX: 0.9898, KLD: 0.0566, LossY: 1.5120)\n",
            "INFO:VAETrainer:Epoch [58/100] Train TotalLoss: 16.8929, (ReconX: 0.9867, KLD: 0.0557*1.00, LossY: 1.5850*10.00), Time: 1.8s | Val TotalLoss: 16.1668, (ReconX: 0.9898, KLD: 0.0566, LossY: 1.5120)\n",
            "2025-10-23 22:38:12,862 - VAETrainer - INFO - Epoch [59/100] Train TotalLoss: 16.8914, (ReconX: 0.9876, KLD: 0.0545*1.00, LossY: 1.5849*10.00), Time: 1.8s | Val TotalLoss: 15.9844, (ReconX: 0.9915, KLD: 0.0549, LossY: 1.4938)\n",
            "INFO:VAETrainer:Epoch [59/100] Train TotalLoss: 16.8914, (ReconX: 0.9876, KLD: 0.0545*1.00, LossY: 1.5849*10.00), Time: 1.8s | Val TotalLoss: 15.9844, (ReconX: 0.9915, KLD: 0.0549, LossY: 1.4938)\n",
            "2025-10-23 22:38:12,864 - VAETrainer - INFO - Saving model and trainer state to vae_results/best_model.pth...\n",
            "INFO:VAETrainer:Saving model and trainer state to vae_results/best_model.pth...\n",
            "2025-10-23 22:38:12,880 - VAETrainer - INFO - Successfully saved model and trainer state to vae_results/best_model.pth\n",
            "INFO:VAETrainer:Successfully saved model and trainer state to vae_results/best_model.pth\n",
            "2025-10-23 22:38:15,089 - VAETrainer - INFO - Epoch [60/100] Train TotalLoss: 16.7665, (ReconX: 0.9867, KLD: 0.0547*1.00, LossY: 1.5725*10.00), Time: 1.8s | Val TotalLoss: 15.6859, (ReconX: 0.9899, KLD: 0.0551, LossY: 1.4641)\n",
            "INFO:VAETrainer:Epoch [60/100] Train TotalLoss: 16.7665, (ReconX: 0.9867, KLD: 0.0547*1.00, LossY: 1.5725*10.00), Time: 1.8s | Val TotalLoss: 15.6859, (ReconX: 0.9899, KLD: 0.0551, LossY: 1.4641)\n",
            "2025-10-23 22:38:15,091 - VAETrainer - INFO - Saving model and trainer state to vae_results/best_model.pth...\n",
            "INFO:VAETrainer:Saving model and trainer state to vae_results/best_model.pth...\n",
            "2025-10-23 22:38:15,105 - VAETrainer - INFO - Successfully saved model and trainer state to vae_results/best_model.pth\n",
            "INFO:VAETrainer:Successfully saved model and trainer state to vae_results/best_model.pth\n",
            "2025-10-23 22:38:17,341 - VAETrainer - INFO - Epoch [61/100] Train TotalLoss: 16.8314, (ReconX: 0.9865, KLD: 0.0550*1.00, LossY: 1.5790*10.00), Time: 1.9s | Val TotalLoss: 15.5114, (ReconX: 0.9909, KLD: 0.0563, LossY: 1.4464)\n",
            "INFO:VAETrainer:Epoch [61/100] Train TotalLoss: 16.8314, (ReconX: 0.9865, KLD: 0.0550*1.00, LossY: 1.5790*10.00), Time: 1.9s | Val TotalLoss: 15.5114, (ReconX: 0.9909, KLD: 0.0563, LossY: 1.4464)\n",
            "2025-10-23 22:38:17,344 - VAETrainer - INFO - Saving model and trainer state to vae_results/best_model.pth...\n",
            "INFO:VAETrainer:Saving model and trainer state to vae_results/best_model.pth...\n",
            "2025-10-23 22:38:17,357 - VAETrainer - INFO - Successfully saved model and trainer state to vae_results/best_model.pth\n",
            "INFO:VAETrainer:Successfully saved model and trainer state to vae_results/best_model.pth\n",
            "2025-10-23 22:38:19,512 - VAETrainer - INFO - Epoch [62/100] Train TotalLoss: 16.6779, (ReconX: 0.9867, KLD: 0.0547*1.00, LossY: 1.5637*10.00), Time: 1.8s | Val TotalLoss: 15.8600, (ReconX: 0.9902, KLD: 0.0558, LossY: 1.4814)\n",
            "INFO:VAETrainer:Epoch [62/100] Train TotalLoss: 16.6779, (ReconX: 0.9867, KLD: 0.0547*1.00, LossY: 1.5637*10.00), Time: 1.8s | Val TotalLoss: 15.8600, (ReconX: 0.9902, KLD: 0.0558, LossY: 1.4814)\n",
            "2025-10-23 22:38:21,653 - VAETrainer - INFO - Epoch [63/100] Train TotalLoss: 16.7339, (ReconX: 0.9875, KLD: 0.0540*1.00, LossY: 1.5692*10.00), Time: 1.8s | Val TotalLoss: 15.5485, (ReconX: 0.9907, KLD: 0.0548, LossY: 1.4503)\n",
            "INFO:VAETrainer:Epoch [63/100] Train TotalLoss: 16.7339, (ReconX: 0.9875, KLD: 0.0540*1.00, LossY: 1.5692*10.00), Time: 1.8s | Val TotalLoss: 15.5485, (ReconX: 0.9907, KLD: 0.0548, LossY: 1.4503)\n",
            "2025-10-23 22:38:23,775 - VAETrainer - INFO - Epoch [64/100] Train TotalLoss: 16.5493, (ReconX: 0.9877, KLD: 0.0534*1.00, LossY: 1.5508*10.00), Time: 1.7s | Val TotalLoss: 15.8683, (ReconX: 0.9912, KLD: 0.0542, LossY: 1.4823)\n",
            "INFO:VAETrainer:Epoch [64/100] Train TotalLoss: 16.5493, (ReconX: 0.9877, KLD: 0.0534*1.00, LossY: 1.5508*10.00), Time: 1.7s | Val TotalLoss: 15.8683, (ReconX: 0.9912, KLD: 0.0542, LossY: 1.4823)\n",
            "2025-10-23 22:38:25,965 - VAETrainer - INFO - Epoch [65/100] Train TotalLoss: 16.4820, (ReconX: 0.9876, KLD: 0.0536*1.00, LossY: 1.5441*10.00), Time: 1.8s | Val TotalLoss: 15.8433, (ReconX: 0.9916, KLD: 0.0542, LossY: 1.4798)\n",
            "INFO:VAETrainer:Epoch [65/100] Train TotalLoss: 16.4820, (ReconX: 0.9876, KLD: 0.0536*1.00, LossY: 1.5441*10.00), Time: 1.8s | Val TotalLoss: 15.8433, (ReconX: 0.9916, KLD: 0.0542, LossY: 1.4798)\n",
            "2025-10-23 22:38:28,149 - VAETrainer - INFO - Epoch [66/100] Train TotalLoss: 16.3538, (ReconX: 0.9874, KLD: 0.0541*1.00, LossY: 1.5312*10.00), Time: 1.8s | Val TotalLoss: 15.4320, (ReconX: 0.9904, KLD: 0.0529, LossY: 1.4389)\n",
            "INFO:VAETrainer:Epoch [66/100] Train TotalLoss: 16.3538, (ReconX: 0.9874, KLD: 0.0541*1.00, LossY: 1.5312*10.00), Time: 1.8s | Val TotalLoss: 15.4320, (ReconX: 0.9904, KLD: 0.0529, LossY: 1.4389)\n",
            "2025-10-23 22:38:28,151 - VAETrainer - INFO - Saving model and trainer state to vae_results/best_model.pth...\n",
            "INFO:VAETrainer:Saving model and trainer state to vae_results/best_model.pth...\n",
            "2025-10-23 22:38:28,164 - VAETrainer - INFO - Successfully saved model and trainer state to vae_results/best_model.pth\n",
            "INFO:VAETrainer:Successfully saved model and trainer state to vae_results/best_model.pth\n",
            "2025-10-23 22:38:30,335 - VAETrainer - INFO - Epoch [67/100] Train TotalLoss: 16.4287, (ReconX: 0.9875, KLD: 0.0532*1.00, LossY: 1.5388*10.00), Time: 1.8s | Val TotalLoss: 15.5240, (ReconX: 0.9905, KLD: 0.0535, LossY: 1.4480)\n",
            "INFO:VAETrainer:Epoch [67/100] Train TotalLoss: 16.4287, (ReconX: 0.9875, KLD: 0.0532*1.00, LossY: 1.5388*10.00), Time: 1.8s | Val TotalLoss: 15.5240, (ReconX: 0.9905, KLD: 0.0535, LossY: 1.4480)\n",
            "2025-10-23 22:38:32,410 - VAETrainer - INFO - Epoch [68/100] Train TotalLoss: 16.6565, (ReconX: 0.9872, KLD: 0.0544*1.00, LossY: 1.5615*10.00), Time: 1.7s | Val TotalLoss: 15.2134, (ReconX: 0.9906, KLD: 0.0540, LossY: 1.4169)\n",
            "INFO:VAETrainer:Epoch [68/100] Train TotalLoss: 16.6565, (ReconX: 0.9872, KLD: 0.0544*1.00, LossY: 1.5615*10.00), Time: 1.7s | Val TotalLoss: 15.2134, (ReconX: 0.9906, KLD: 0.0540, LossY: 1.4169)\n",
            "2025-10-23 22:38:32,412 - VAETrainer - INFO - Saving model and trainer state to vae_results/best_model.pth...\n",
            "INFO:VAETrainer:Saving model and trainer state to vae_results/best_model.pth...\n",
            "2025-10-23 22:38:32,426 - VAETrainer - INFO - Successfully saved model and trainer state to vae_results/best_model.pth\n",
            "INFO:VAETrainer:Successfully saved model and trainer state to vae_results/best_model.pth\n",
            "2025-10-23 22:38:34,544 - VAETrainer - INFO - Epoch [69/100] Train TotalLoss: 16.3417, (ReconX: 0.9867, KLD: 0.0531*1.00, LossY: 1.5302*10.00), Time: 1.7s | Val TotalLoss: 15.5948, (ReconX: 0.9889, KLD: 0.0560, LossY: 1.4550)\n",
            "INFO:VAETrainer:Epoch [69/100] Train TotalLoss: 16.3417, (ReconX: 0.9867, KLD: 0.0531*1.00, LossY: 1.5302*10.00), Time: 1.7s | Val TotalLoss: 15.5948, (ReconX: 0.9889, KLD: 0.0560, LossY: 1.4550)\n",
            "2025-10-23 22:38:36,654 - VAETrainer - INFO - Epoch [70/100] Train TotalLoss: 16.2613, (ReconX: 0.9870, KLD: 0.0531*1.00, LossY: 1.5221*10.00), Time: 1.7s | Val TotalLoss: 15.1147, (ReconX: 0.9903, KLD: 0.0526, LossY: 1.4072)\n",
            "INFO:VAETrainer:Epoch [70/100] Train TotalLoss: 16.2613, (ReconX: 0.9870, KLD: 0.0531*1.00, LossY: 1.5221*10.00), Time: 1.7s | Val TotalLoss: 15.1147, (ReconX: 0.9903, KLD: 0.0526, LossY: 1.4072)\n",
            "2025-10-23 22:38:36,656 - VAETrainer - INFO - Saving model and trainer state to vae_results/best_model.pth...\n",
            "INFO:VAETrainer:Saving model and trainer state to vae_results/best_model.pth...\n",
            "2025-10-23 22:38:36,670 - VAETrainer - INFO - Successfully saved model and trainer state to vae_results/best_model.pth\n",
            "INFO:VAETrainer:Successfully saved model and trainer state to vae_results/best_model.pth\n",
            "2025-10-23 22:38:38,855 - VAETrainer - INFO - Epoch [71/100] Train TotalLoss: 15.9381, (ReconX: 0.9873, KLD: 0.0519*1.00, LossY: 1.4899*10.00), Time: 1.8s | Val TotalLoss: 15.1922, (ReconX: 0.9901, KLD: 0.0526, LossY: 1.4149)\n",
            "INFO:VAETrainer:Epoch [71/100] Train TotalLoss: 15.9381, (ReconX: 0.9873, KLD: 0.0519*1.00, LossY: 1.4899*10.00), Time: 1.8s | Val TotalLoss: 15.1922, (ReconX: 0.9901, KLD: 0.0526, LossY: 1.4149)\n",
            "2025-10-23 22:38:41,048 - VAETrainer - INFO - Epoch [72/100] Train TotalLoss: 16.0220, (ReconX: 0.9881, KLD: 0.0514*1.00, LossY: 1.4982*10.00), Time: 1.8s | Val TotalLoss: 14.7877, (ReconX: 0.9914, KLD: 0.0515, LossY: 1.3745)\n",
            "INFO:VAETrainer:Epoch [72/100] Train TotalLoss: 16.0220, (ReconX: 0.9881, KLD: 0.0514*1.00, LossY: 1.4982*10.00), Time: 1.8s | Val TotalLoss: 14.7877, (ReconX: 0.9914, KLD: 0.0515, LossY: 1.3745)\n",
            "2025-10-23 22:38:41,050 - VAETrainer - INFO - Saving model and trainer state to vae_results/best_model.pth...\n",
            "INFO:VAETrainer:Saving model and trainer state to vae_results/best_model.pth...\n",
            "2025-10-23 22:38:41,064 - VAETrainer - INFO - Successfully saved model and trainer state to vae_results/best_model.pth\n",
            "INFO:VAETrainer:Successfully saved model and trainer state to vae_results/best_model.pth\n",
            "2025-10-23 22:38:43,282 - VAETrainer - INFO - Epoch [73/100] Train TotalLoss: 16.0420, (ReconX: 0.9877, KLD: 0.0520*1.00, LossY: 1.5002*10.00), Time: 1.8s | Val TotalLoss: 15.3742, (ReconX: 0.9922, KLD: 0.0517, LossY: 1.4330)\n",
            "INFO:VAETrainer:Epoch [73/100] Train TotalLoss: 16.0420, (ReconX: 0.9877, KLD: 0.0520*1.00, LossY: 1.5002*10.00), Time: 1.8s | Val TotalLoss: 15.3742, (ReconX: 0.9922, KLD: 0.0517, LossY: 1.4330)\n",
            "2025-10-23 22:38:45,463 - VAETrainer - INFO - Epoch [74/100] Train TotalLoss: 15.8517, (ReconX: 0.9876, KLD: 0.0514*1.00, LossY: 1.4813*10.00), Time: 1.8s | Val TotalLoss: 14.9627, (ReconX: 0.9915, KLD: 0.0514, LossY: 1.3920)\n",
            "INFO:VAETrainer:Epoch [74/100] Train TotalLoss: 15.8517, (ReconX: 0.9876, KLD: 0.0514*1.00, LossY: 1.4813*10.00), Time: 1.8s | Val TotalLoss: 14.9627, (ReconX: 0.9915, KLD: 0.0514, LossY: 1.3920)\n",
            "2025-10-23 22:38:47,633 - VAETrainer - INFO - Epoch [75/100] Train TotalLoss: 16.0697, (ReconX: 0.9875, KLD: 0.0507*1.00, LossY: 1.5031*10.00), Time: 1.8s | Val TotalLoss: 14.9300, (ReconX: 0.9907, KLD: 0.0502, LossY: 1.3889)\n",
            "INFO:VAETrainer:Epoch [75/100] Train TotalLoss: 16.0697, (ReconX: 0.9875, KLD: 0.0507*1.00, LossY: 1.5031*10.00), Time: 1.8s | Val TotalLoss: 14.9300, (ReconX: 0.9907, KLD: 0.0502, LossY: 1.3889)\n",
            "2025-10-23 22:38:49,749 - VAETrainer - INFO - Epoch [76/100] Train TotalLoss: 15.8467, (ReconX: 0.9884, KLD: 0.0507*1.00, LossY: 1.4808*10.00), Time: 1.7s | Val TotalLoss: 15.4245, (ReconX: 0.9916, KLD: 0.0521, LossY: 1.4381)\n",
            "INFO:VAETrainer:Epoch [76/100] Train TotalLoss: 15.8467, (ReconX: 0.9884, KLD: 0.0507*1.00, LossY: 1.4808*10.00), Time: 1.7s | Val TotalLoss: 15.4245, (ReconX: 0.9916, KLD: 0.0521, LossY: 1.4381)\n",
            "2025-10-23 22:38:51,885 - VAETrainer - INFO - Epoch [77/100] Train TotalLoss: 15.7244, (ReconX: 0.9880, KLD: 0.0505*1.00, LossY: 1.4686*10.00), Time: 1.8s | Val TotalLoss: 14.7575, (ReconX: 0.9912, KLD: 0.0510, LossY: 1.3715)\n",
            "INFO:VAETrainer:Epoch [77/100] Train TotalLoss: 15.7244, (ReconX: 0.9880, KLD: 0.0505*1.00, LossY: 1.4686*10.00), Time: 1.8s | Val TotalLoss: 14.7575, (ReconX: 0.9912, KLD: 0.0510, LossY: 1.3715)\n",
            "2025-10-23 22:38:51,887 - VAETrainer - INFO - Saving model and trainer state to vae_results/best_model.pth...\n",
            "INFO:VAETrainer:Saving model and trainer state to vae_results/best_model.pth...\n",
            "2025-10-23 22:38:51,901 - VAETrainer - INFO - Successfully saved model and trainer state to vae_results/best_model.pth\n",
            "INFO:VAETrainer:Successfully saved model and trainer state to vae_results/best_model.pth\n",
            "2025-10-23 22:38:53,975 - VAETrainer - INFO - Epoch [78/100] Train TotalLoss: 15.8475, (ReconX: 0.9884, KLD: 0.0505*1.00, LossY: 1.4809*10.00), Time: 1.7s | Val TotalLoss: 14.6452, (ReconX: 0.9918, KLD: 0.0504, LossY: 1.3603)\n",
            "INFO:VAETrainer:Epoch [78/100] Train TotalLoss: 15.8475, (ReconX: 0.9884, KLD: 0.0505*1.00, LossY: 1.4809*10.00), Time: 1.7s | Val TotalLoss: 14.6452, (ReconX: 0.9918, KLD: 0.0504, LossY: 1.3603)\n",
            "2025-10-23 22:38:53,977 - VAETrainer - INFO - Saving model and trainer state to vae_results/best_model.pth...\n",
            "INFO:VAETrainer:Saving model and trainer state to vae_results/best_model.pth...\n",
            "2025-10-23 22:38:53,991 - VAETrainer - INFO - Successfully saved model and trainer state to vae_results/best_model.pth\n",
            "INFO:VAETrainer:Successfully saved model and trainer state to vae_results/best_model.pth\n",
            "2025-10-23 22:38:56,058 - VAETrainer - INFO - Epoch [79/100] Train TotalLoss: 15.7367, (ReconX: 0.9878, KLD: 0.0497*1.00, LossY: 1.4699*10.00), Time: 1.7s | Val TotalLoss: 14.8106, (ReconX: 0.9927, KLD: 0.0501, LossY: 1.3768)\n",
            "INFO:VAETrainer:Epoch [79/100] Train TotalLoss: 15.7367, (ReconX: 0.9878, KLD: 0.0497*1.00, LossY: 1.4699*10.00), Time: 1.7s | Val TotalLoss: 14.8106, (ReconX: 0.9927, KLD: 0.0501, LossY: 1.3768)\n",
            "2025-10-23 22:38:58,108 - VAETrainer - INFO - Epoch [80/100] Train TotalLoss: 15.6369, (ReconX: 0.9885, KLD: 0.0496*1.00, LossY: 1.4599*10.00), Time: 1.7s | Val TotalLoss: 15.3852, (ReconX: 0.9919, KLD: 0.0508, LossY: 1.4343)\n",
            "INFO:VAETrainer:Epoch [80/100] Train TotalLoss: 15.6369, (ReconX: 0.9885, KLD: 0.0496*1.00, LossY: 1.4599*10.00), Time: 1.7s | Val TotalLoss: 15.3852, (ReconX: 0.9919, KLD: 0.0508, LossY: 1.4343)\n",
            "2025-10-23 22:39:00,225 - VAETrainer - INFO - Epoch [81/100] Train TotalLoss: 15.7402, (ReconX: 0.9881, KLD: 0.0503*1.00, LossY: 1.4702*10.00), Time: 1.7s | Val TotalLoss: 14.7076, (ReconX: 0.9930, KLD: 0.0510, LossY: 1.3664)\n",
            "INFO:VAETrainer:Epoch [81/100] Train TotalLoss: 15.7402, (ReconX: 0.9881, KLD: 0.0503*1.00, LossY: 1.4702*10.00), Time: 1.7s | Val TotalLoss: 14.7076, (ReconX: 0.9930, KLD: 0.0510, LossY: 1.3664)\n",
            "2025-10-23 22:39:02,405 - VAETrainer - INFO - Epoch [82/100] Train TotalLoss: 15.7408, (ReconX: 0.9884, KLD: 0.0504*1.00, LossY: 1.4702*10.00), Time: 1.8s | Val TotalLoss: 14.7257, (ReconX: 0.9903, KLD: 0.0506, LossY: 1.3685)\n",
            "INFO:VAETrainer:Epoch [82/100] Train TotalLoss: 15.7408, (ReconX: 0.9884, KLD: 0.0504*1.00, LossY: 1.4702*10.00), Time: 1.8s | Val TotalLoss: 14.7257, (ReconX: 0.9903, KLD: 0.0506, LossY: 1.3685)\n",
            "2025-10-23 22:39:04,569 - VAETrainer - INFO - Epoch [83/100] Train TotalLoss: 15.8357, (ReconX: 0.9886, KLD: 0.0500*1.00, LossY: 1.4797*10.00), Time: 1.8s | Val TotalLoss: 14.6592, (ReconX: 0.9929, KLD: 0.0493, LossY: 1.3617)\n",
            "INFO:VAETrainer:Epoch [83/100] Train TotalLoss: 15.8357, (ReconX: 0.9886, KLD: 0.0500*1.00, LossY: 1.4797*10.00), Time: 1.8s | Val TotalLoss: 14.6592, (ReconX: 0.9929, KLD: 0.0493, LossY: 1.3617)\n",
            "2025-10-23 22:39:06,644 - VAETrainer - INFO - Epoch [84/100] Train TotalLoss: 15.4716, (ReconX: 0.9884, KLD: 0.0494*1.00, LossY: 1.4434*10.00), Time: 1.7s | Val TotalLoss: 14.2496, (ReconX: 0.9916, KLD: 0.0497, LossY: 1.3208)\n",
            "INFO:VAETrainer:Epoch [84/100] Train TotalLoss: 15.4716, (ReconX: 0.9884, KLD: 0.0494*1.00, LossY: 1.4434*10.00), Time: 1.7s | Val TotalLoss: 14.2496, (ReconX: 0.9916, KLD: 0.0497, LossY: 1.3208)\n",
            "2025-10-23 22:39:06,646 - VAETrainer - INFO - Saving model and trainer state to vae_results/best_model.pth...\n",
            "INFO:VAETrainer:Saving model and trainer state to vae_results/best_model.pth...\n",
            "2025-10-23 22:39:06,659 - VAETrainer - INFO - Successfully saved model and trainer state to vae_results/best_model.pth\n",
            "INFO:VAETrainer:Successfully saved model and trainer state to vae_results/best_model.pth\n",
            "2025-10-23 22:39:08,880 - VAETrainer - INFO - Epoch [85/100] Train TotalLoss: 15.5992, (ReconX: 0.9884, KLD: 0.0495*1.00, LossY: 1.4561*10.00), Time: 1.9s | Val TotalLoss: 14.1970, (ReconX: 0.9918, KLD: 0.0499, LossY: 1.3155)\n",
            "INFO:VAETrainer:Epoch [85/100] Train TotalLoss: 15.5992, (ReconX: 0.9884, KLD: 0.0495*1.00, LossY: 1.4561*10.00), Time: 1.9s | Val TotalLoss: 14.1970, (ReconX: 0.9918, KLD: 0.0499, LossY: 1.3155)\n",
            "2025-10-23 22:39:08,882 - VAETrainer - INFO - Saving model and trainer state to vae_results/best_model.pth...\n",
            "INFO:VAETrainer:Saving model and trainer state to vae_results/best_model.pth...\n",
            "2025-10-23 22:39:08,895 - VAETrainer - INFO - Successfully saved model and trainer state to vae_results/best_model.pth\n",
            "INFO:VAETrainer:Successfully saved model and trainer state to vae_results/best_model.pth\n",
            "2025-10-23 22:39:11,038 - VAETrainer - INFO - Epoch [86/100] Train TotalLoss: 15.5382, (ReconX: 0.9888, KLD: 0.0488*1.00, LossY: 1.4501*10.00), Time: 1.8s | Val TotalLoss: 14.6382, (ReconX: 0.9917, KLD: 0.0508, LossY: 1.3596)\n",
            "INFO:VAETrainer:Epoch [86/100] Train TotalLoss: 15.5382, (ReconX: 0.9888, KLD: 0.0488*1.00, LossY: 1.4501*10.00), Time: 1.8s | Val TotalLoss: 14.6382, (ReconX: 0.9917, KLD: 0.0508, LossY: 1.3596)\n",
            "2025-10-23 22:39:13,195 - VAETrainer - INFO - Epoch [87/100] Train TotalLoss: 15.3601, (ReconX: 0.9879, KLD: 0.0488*1.00, LossY: 1.4323*10.00), Time: 1.8s | Val TotalLoss: 14.2356, (ReconX: 0.9922, KLD: 0.0491, LossY: 1.3194)\n",
            "INFO:VAETrainer:Epoch [87/100] Train TotalLoss: 15.3601, (ReconX: 0.9879, KLD: 0.0488*1.00, LossY: 1.4323*10.00), Time: 1.8s | Val TotalLoss: 14.2356, (ReconX: 0.9922, KLD: 0.0491, LossY: 1.3194)\n",
            "2025-10-23 22:39:15,465 - VAETrainer - INFO - Epoch [88/100] Train TotalLoss: 15.8753, (ReconX: 0.9886, KLD: 0.0483*1.00, LossY: 1.4838*10.00), Time: 1.9s | Val TotalLoss: 14.1332, (ReconX: 0.9918, KLD: 0.0489, LossY: 1.3092)\n",
            "INFO:VAETrainer:Epoch [88/100] Train TotalLoss: 15.8753, (ReconX: 0.9886, KLD: 0.0483*1.00, LossY: 1.4838*10.00), Time: 1.9s | Val TotalLoss: 14.1332, (ReconX: 0.9918, KLD: 0.0489, LossY: 1.3092)\n",
            "2025-10-23 22:39:15,468 - VAETrainer - INFO - Saving model and trainer state to vae_results/best_model.pth...\n",
            "INFO:VAETrainer:Saving model and trainer state to vae_results/best_model.pth...\n",
            "2025-10-23 22:39:15,481 - VAETrainer - INFO - Successfully saved model and trainer state to vae_results/best_model.pth\n",
            "INFO:VAETrainer:Successfully saved model and trainer state to vae_results/best_model.pth\n",
            "2025-10-23 22:39:17,652 - VAETrainer - INFO - Epoch [89/100] Train TotalLoss: 15.3389, (ReconX: 0.9885, KLD: 0.0480*1.00, LossY: 1.4302*10.00), Time: 1.8s | Val TotalLoss: 15.2433, (ReconX: 0.9915, KLD: 0.0487, LossY: 1.4203)\n",
            "INFO:VAETrainer:Epoch [89/100] Train TotalLoss: 15.3389, (ReconX: 0.9885, KLD: 0.0480*1.00, LossY: 1.4302*10.00), Time: 1.8s | Val TotalLoss: 15.2433, (ReconX: 0.9915, KLD: 0.0487, LossY: 1.4203)\n",
            "2025-10-23 22:39:19,797 - VAETrainer - INFO - Epoch [90/100] Train TotalLoss: 15.7636, (ReconX: 0.9887, KLD: 0.0483*1.00, LossY: 1.4727*10.00), Time: 1.8s | Val TotalLoss: 14.7755, (ReconX: 0.9918, KLD: 0.0492, LossY: 1.3735)\n",
            "INFO:VAETrainer:Epoch [90/100] Train TotalLoss: 15.7636, (ReconX: 0.9887, KLD: 0.0483*1.00, LossY: 1.4727*10.00), Time: 1.8s | Val TotalLoss: 14.7755, (ReconX: 0.9918, KLD: 0.0492, LossY: 1.3735)\n",
            "2025-10-23 22:39:21,947 - VAETrainer - INFO - Epoch [91/100] Train TotalLoss: 15.0757, (ReconX: 0.9889, KLD: 0.0481*1.00, LossY: 1.4039*10.00), Time: 1.8s | Val TotalLoss: 14.5017, (ReconX: 0.9921, KLD: 0.0485, LossY: 1.3461)\n",
            "INFO:VAETrainer:Epoch [91/100] Train TotalLoss: 15.0757, (ReconX: 0.9889, KLD: 0.0481*1.00, LossY: 1.4039*10.00), Time: 1.8s | Val TotalLoss: 14.5017, (ReconX: 0.9921, KLD: 0.0485, LossY: 1.3461)\n",
            "2025-10-23 22:39:24,092 - VAETrainer - INFO - Epoch [92/100] Train TotalLoss: 15.7596, (ReconX: 0.9884, KLD: 0.0487*1.00, LossY: 1.4722*10.00), Time: 1.8s | Val TotalLoss: 14.3784, (ReconX: 0.9913, KLD: 0.0496, LossY: 1.3338)\n",
            "INFO:VAETrainer:Epoch [92/100] Train TotalLoss: 15.7596, (ReconX: 0.9884, KLD: 0.0487*1.00, LossY: 1.4722*10.00), Time: 1.8s | Val TotalLoss: 14.3784, (ReconX: 0.9913, KLD: 0.0496, LossY: 1.3338)\n",
            "2025-10-23 22:39:26,208 - VAETrainer - INFO - Epoch [93/100] Train TotalLoss: 15.1946, (ReconX: 0.9884, KLD: 0.0485*1.00, LossY: 1.4158*10.00), Time: 1.7s | Val TotalLoss: 14.3219, (ReconX: 0.9925, KLD: 0.0487, LossY: 1.3281)\n",
            "INFO:VAETrainer:Epoch [93/100] Train TotalLoss: 15.1946, (ReconX: 0.9884, KLD: 0.0485*1.00, LossY: 1.4158*10.00), Time: 1.7s | Val TotalLoss: 14.3219, (ReconX: 0.9925, KLD: 0.0487, LossY: 1.3281)\n",
            "2025-10-23 22:39:28,401 - VAETrainer - INFO - Epoch [94/100] Train TotalLoss: 15.3953, (ReconX: 0.9890, KLD: 0.0478*1.00, LossY: 1.4359*10.00), Time: 1.8s | Val TotalLoss: 13.9944, (ReconX: 0.9926, KLD: 0.0477, LossY: 1.2954)\n",
            "INFO:VAETrainer:Epoch [94/100] Train TotalLoss: 15.3953, (ReconX: 0.9890, KLD: 0.0478*1.00, LossY: 1.4359*10.00), Time: 1.8s | Val TotalLoss: 13.9944, (ReconX: 0.9926, KLD: 0.0477, LossY: 1.2954)\n",
            "2025-10-23 22:39:28,403 - VAETrainer - INFO - Saving model and trainer state to vae_results/best_model.pth...\n",
            "INFO:VAETrainer:Saving model and trainer state to vae_results/best_model.pth...\n",
            "2025-10-23 22:39:28,416 - VAETrainer - INFO - Successfully saved model and trainer state to vae_results/best_model.pth\n",
            "INFO:VAETrainer:Successfully saved model and trainer state to vae_results/best_model.pth\n",
            "2025-10-23 22:39:30,598 - VAETrainer - INFO - Epoch [95/100] Train TotalLoss: 15.2258, (ReconX: 0.9889, KLD: 0.0480*1.00, LossY: 1.4189*10.00), Time: 1.8s | Val TotalLoss: 14.4338, (ReconX: 0.9930, KLD: 0.0463, LossY: 1.3395)\n",
            "INFO:VAETrainer:Epoch [95/100] Train TotalLoss: 15.2258, (ReconX: 0.9889, KLD: 0.0480*1.00, LossY: 1.4189*10.00), Time: 1.8s | Val TotalLoss: 14.4338, (ReconX: 0.9930, KLD: 0.0463, LossY: 1.3395)\n",
            "2025-10-23 22:39:32,716 - VAETrainer - INFO - Epoch [96/100] Train TotalLoss: 15.7014, (ReconX: 0.9887, KLD: 0.0477*1.00, LossY: 1.4665*10.00), Time: 1.7s | Val TotalLoss: 14.4491, (ReconX: 0.9934, KLD: 0.0474, LossY: 1.3408)\n",
            "INFO:VAETrainer:Epoch [96/100] Train TotalLoss: 15.7014, (ReconX: 0.9887, KLD: 0.0477*1.00, LossY: 1.4665*10.00), Time: 1.7s | Val TotalLoss: 14.4491, (ReconX: 0.9934, KLD: 0.0474, LossY: 1.3408)\n",
            "2025-10-23 22:39:34,807 - VAETrainer - INFO - Epoch [97/100] Train TotalLoss: 14.9320, (ReconX: 0.9887, KLD: 0.0473*1.00, LossY: 1.3896*10.00), Time: 1.7s | Val TotalLoss: 14.0184, (ReconX: 0.9917, KLD: 0.0488, LossY: 1.2978)\n",
            "INFO:VAETrainer:Epoch [97/100] Train TotalLoss: 14.9320, (ReconX: 0.9887, KLD: 0.0473*1.00, LossY: 1.3896*10.00), Time: 1.7s | Val TotalLoss: 14.0184, (ReconX: 0.9917, KLD: 0.0488, LossY: 1.2978)\n",
            "2025-10-23 22:39:36,917 - VAETrainer - INFO - Epoch [98/100] Train TotalLoss: 15.0722, (ReconX: 0.9886, KLD: 0.0479*1.00, LossY: 1.4036*10.00), Time: 1.7s | Val TotalLoss: 14.0283, (ReconX: 0.9911, KLD: 0.0473, LossY: 1.2990)\n",
            "INFO:VAETrainer:Epoch [98/100] Train TotalLoss: 15.0722, (ReconX: 0.9886, KLD: 0.0479*1.00, LossY: 1.4036*10.00), Time: 1.7s | Val TotalLoss: 14.0283, (ReconX: 0.9911, KLD: 0.0473, LossY: 1.2990)\n",
            "2025-10-23 22:39:39,075 - VAETrainer - INFO - Epoch [99/100] Train TotalLoss: 15.0639, (ReconX: 0.9896, KLD: 0.0468*1.00, LossY: 1.4027*10.00), Time: 1.8s | Val TotalLoss: 13.9222, (ReconX: 0.9926, KLD: 0.0477, LossY: 1.2882)\n",
            "INFO:VAETrainer:Epoch [99/100] Train TotalLoss: 15.0639, (ReconX: 0.9896, KLD: 0.0468*1.00, LossY: 1.4027*10.00), Time: 1.8s | Val TotalLoss: 13.9222, (ReconX: 0.9926, KLD: 0.0477, LossY: 1.2882)\n",
            "2025-10-23 22:39:39,077 - VAETrainer - INFO - Saving model and trainer state to vae_results/best_model.pth...\n",
            "INFO:VAETrainer:Saving model and trainer state to vae_results/best_model.pth...\n",
            "2025-10-23 22:39:39,091 - VAETrainer - INFO - Successfully saved model and trainer state to vae_results/best_model.pth\n",
            "INFO:VAETrainer:Successfully saved model and trainer state to vae_results/best_model.pth\n",
            "2025-10-23 22:39:41,226 - VAETrainer - INFO - Epoch [100/100] Train TotalLoss: 16.5626, (ReconX: 0.9869, KLD: 0.0548*1.00, LossY: 1.5521*10.00), Time: 1.8s | Val TotalLoss: 14.1436, (ReconX: 0.9899, KLD: 0.0526, LossY: 1.3101)\n",
            "INFO:VAETrainer:Epoch [100/100] Train TotalLoss: 16.5626, (ReconX: 0.9869, KLD: 0.0548*1.00, LossY: 1.5521*10.00), Time: 1.8s | Val TotalLoss: 14.1436, (ReconX: 0.9899, KLD: 0.0526, LossY: 1.3101)\n",
            "2025-10-23 22:39:41,228 - VAETrainer - INFO - Training loop finished.\n",
            "INFO:VAETrainer:Training loop finished.\n",
            "2025-10-23 22:39:41,229 - VAETrainer - INFO - Loading best model state from 'vae_results/best_model.pth' (Val Loss: 13.9222).\n",
            "INFO:VAETrainer:Loading best model state from 'vae_results/best_model.pth' (Val Loss: 13.9222).\n",
            "2025-10-23 22:39:41,231 - VAETrainer_Loader - INFO - Loading VAE model and trainer state from vae_results/best_model.pth to device cuda...\n",
            "INFO:VAETrainer_Loader:Loading VAE model and trainer state from vae_results/best_model.pth to device cuda...\n",
            "2025-10-23 22:39:41,270 - SemiSupMIWAE(X:42,Y:1,Lat:3) - INFO - SemiSupMIWAE Initialized: InputXDim=42, TargetYDim=1, LatentDim=3, AlphaPriceLoss=10.0, Device=cuda\n",
            "INFO:SemiSupMIWAE(X:42,Y:1,Lat:3):SemiSupMIWAE Initialized: InputXDim=42, TargetYDim=1, LatentDim=3, AlphaPriceLoss=10.0, Device=cuda\n",
            "2025-10-23 22:39:41,274 - VAETrainer_Loader - INFO - SemiSupMIWAE model created and state loaded successfully.\n",
            "INFO:VAETrainer_Loader:SemiSupMIWAE model created and state loaded successfully.\n",
            "2025-10-23 22:39:41,275 - VAETrainer - INFO - VAETrainer initialized for SemiSupMIWAE. Device: cuda. Optimizer: adam. LR: 0.0003. ReconLoss(X): 'mse'. Alpha(Y Loss): 10.0. LossType(Y): 'gaussian_nll'\n",
            "INFO:VAETrainer:VAETrainer initialized for SemiSupMIWAE. Device: cuda. Optimizer: adam. LR: 0.0003. ReconLoss(X): 'mse'. Alpha(Y Loss): 10.0. LossType(Y): 'gaussian_nll'\n",
            "2025-10-23 22:39:41,277 - VAETrainer_Loader - INFO - VAETrainer instance created and state loaded for model from vae_results/best_model.pth.\n",
            "INFO:VAETrainer_Loader:VAETrainer instance created and state loaded for model from vae_results/best_model.pth.\n",
            "2025-10-23 22:39:41,286 - VAETrainer - INFO - Predicting target (Y) mean and log-variance for 12271 samples...\n",
            "INFO:VAETrainer:Predicting target (Y) mean and log-variance for 12271 samples...\n",
            "2025-10-23 22:39:41,350 - MainRealEstateAnalysis_MIWAE - INFO - [Fold 4] Stored test predictions for 12271 rows.\n",
            "INFO:MainRealEstateAnalysis_MIWAE:[Fold 4] Stored test predictions for 12271 rows.\n",
            "2025-10-23 22:39:41,363 - MainRealEstateAnalysis_MIWAE - INFO - [Fold 5/10] sizes → Train=98170 (80.0%), Val=12271 (10.0%), Test=12271 (10.0%)\n",
            "INFO:MainRealEstateAnalysis_MIWAE:[Fold 5/10] sizes → Train=98170 (80.0%), Val=12271 (10.0%), Test=12271 (10.0%)\n",
            "2025-10-23 22:39:41,365 - __main__.create_vae_from_artifacts - INFO - Creating VAE model. Target column for supervision: log_sale_price.\n",
            "2025-10-23 22:39:41,365 - __main__.create_vae_from_artifacts - INFO - Creating VAE model. Target column for supervision: log_sale_price.\n",
            "INFO:__main__.create_vae_from_artifacts:Creating VAE model. Target column for supervision: log_sale_price.\n",
            "2025-10-23 22:39:41,366 - __main__.create_vae_from_artifacts - INFO - Number of input features for VAE (X): 42 (Features: ['lotarea', 'assess_total', 'exempt_total', 'assessland', 'exemptland']...)\n",
            "2025-10-23 22:39:41,366 - __main__.create_vae_from_artifacts - INFO - Number of input features for VAE (X): 42 (Features: ['lotarea', 'assess_total', 'exempt_total', 'assessland', 'exemptland']...)\n",
            "INFO:__main__.create_vae_from_artifacts:Number of input features for VAE (X): 42 (Features: ['lotarea', 'assess_total', 'exempt_total', 'assessland', 'exemptland']...)\n",
            "2025-10-23 22:39:41,368 - __main__.create_vae_from_artifacts - INFO - Semi-supervised mode: Target 'log_sale_price' (y_dim=1). Alpha for price loss: 10.0.\n",
            "2025-10-23 22:39:41,368 - __main__.create_vae_from_artifacts - INFO - Semi-supervised mode: Target 'log_sale_price' (y_dim=1). Alpha for price loss: 10.0.\n",
            "INFO:__main__.create_vae_from_artifacts:Semi-supervised mode: Target 'log_sale_price' (y_dim=1). Alpha for price loss: 10.0.\n",
            "2025-10-23 22:39:41,369 - __main__.create_vae_from_artifacts - INFO - Using overridden latent_dim: 3\n",
            "2025-10-23 22:39:41,369 - __main__.create_vae_from_artifacts - INFO - Using overridden latent_dim: 3\n",
            "INFO:__main__.create_vae_from_artifacts:Using overridden latent_dim: 3\n",
            "2025-10-23 22:39:41,371 - __main__.create_vae_from_artifacts - INFO - Final effective Latent Dimension: 3\n",
            "2025-10-23 22:39:41,371 - __main__.create_vae_from_artifacts - INFO - Final effective Latent Dimension: 3\n",
            "INFO:__main__.create_vae_from_artifacts:Final effective Latent Dimension: 3\n",
            "2025-10-23 22:39:41,372 - __main__.create_vae_from_artifacts - INFO - Encoder layers: [21], Decoder layers: [21]\n",
            "2025-10-23 22:39:41,372 - __main__.create_vae_from_artifacts - INFO - Encoder layers: [21], Decoder layers: [21]\n",
            "INFO:__main__.create_vae_from_artifacts:Encoder layers: [21], Decoder layers: [21]\n",
            "2025-10-23 22:39:41,374 - __main__.create_vae_from_artifacts - INFO - Using determined prior: Type=student_t_mixture, K=2 (from overrides or artifact inference).\n",
            "2025-10-23 22:39:41,374 - __main__.create_vae_from_artifacts - INFO - Using determined prior: Type=student_t_mixture, K=2 (from overrides or artifact inference).\n",
            "INFO:__main__.create_vae_from_artifacts:Using determined prior: Type=student_t_mixture, K=2 (from overrides or artifact inference).\n",
            "2025-10-23 22:39:41,375 - __main__.create_vae_from_artifacts - INFO - Initializing parameters for student_t_mixture with K=2 components...\n",
            "2025-10-23 22:39:41,375 - __main__.create_vae_from_artifacts - INFO - Initializing parameters for student_t_mixture with K=2 components...\n",
            "INFO:__main__.create_vae_from_artifacts:Initializing parameters for student_t_mixture with K=2 components...\n",
            "2025-10-23 22:39:41,378 - __main__.create_vae_from_artifacts - WARNING - DP-GMM artifacts are unsuitable or insufficient for initializing mixture prior parameters. Default parameters (e.g., zeros for means/logits, identity for covariances) will be used by the VAE.\n",
            "2025-10-23 22:39:41,378 - __main__.create_vae_from_artifacts - WARNING - DP-GMM artifacts are unsuitable or insufficient for initializing mixture prior parameters. Default parameters (e.g., zeros for means/logits, identity for covariances) will be used by the VAE.\n",
            "WARNING:__main__.create_vae_from_artifacts:DP-GMM artifacts are unsuitable or insufficient for initializing mixture prior parameters. Default parameters (e.g., zeros for means/logits, identity for covariances) will be used by the VAE.\n",
            "2025-10-23 22:39:41,379 - __main__.create_vae_from_artifacts - INFO - Setting Student-t df to 4.0 for prior.\n",
            "2025-10-23 22:39:41,379 - __main__.create_vae_from_artifacts - INFO - Setting Student-t df to 4.0 for prior.\n",
            "INFO:__main__.create_vae_from_artifacts:Setting Student-t df to 4.0 for prior.\n",
            "2025-10-23 22:39:41,380 - __main__.create_vae_from_artifacts - INFO - Instantiating SemiSupMIWAE model...\n",
            "2025-10-23 22:39:41,380 - __main__.create_vae_from_artifacts - INFO - Instantiating SemiSupMIWAE model...\n",
            "INFO:__main__.create_vae_from_artifacts:Instantiating SemiSupMIWAE model...\n",
            "2025-10-23 22:39:41,381 - __main__.create_vae_from_artifacts - INFO - Price head layers for SemiSupMIWAE: [8, 4]\n",
            "2025-10-23 22:39:41,381 - __main__.create_vae_from_artifacts - INFO - Price head layers for SemiSupMIWAE: [8, 4]\n",
            "INFO:__main__.create_vae_from_artifacts:Price head layers for SemiSupMIWAE: [8, 4]\n",
            "2025-10-23 22:39:41,389 - SemiSupMIWAE(X:42,Y:1,Lat:3) - INFO - SemiSupMIWAE Initialized: InputXDim=42, TargetYDim=1, LatentDim=3, AlphaPriceLoss=10.0, Device=cuda\n",
            "INFO:SemiSupMIWAE(X:42,Y:1,Lat:3):SemiSupMIWAE Initialized: InputXDim=42, TargetYDim=1, LatentDim=3, AlphaPriceLoss=10.0, Device=cuda\n",
            "2025-10-23 22:39:41,389 - __main__.create_vae_from_artifacts - INFO - SemiSupMIWAE model created successfully on device: cuda.\n",
            "2025-10-23 22:39:41,389 - __main__.create_vae_from_artifacts - INFO - SemiSupMIWAE model created successfully on device: cuda.\n",
            "INFO:__main__.create_vae_from_artifacts:SemiSupMIWAE model created successfully on device: cuda.\n",
            "2025-10-23 22:39:41,392 - VAETrainer - INFO - VAETrainer initialized for SemiSupMIWAE. Device: cuda. Optimizer: adam. LR: 0.0003. ReconLoss(X): 'mse'. Alpha(Y Loss): 10.0. LossType(Y): 'gaussian_nll'\n",
            "INFO:VAETrainer:VAETrainer initialized for SemiSupMIWAE. Device: cuda. Optimizer: adam. LR: 0.0003. ReconLoss(X): 'mse'. Alpha(Y Loss): 10.0. LossType(Y): 'gaussian_nll'\n",
            "2025-10-23 22:39:41,393 - VAETrainer - INFO - Starting training: Epochs=100, BatchSize=512, KLD_weight=1.0, KLD_AnnealEpochs=30, Alpha(Y_Loss)=10.0.\n",
            "INFO:VAETrainer:Starting training: Epochs=100, BatchSize=512, KLD_weight=1.0, KLD_AnnealEpochs=30, Alpha(Y_Loss)=10.0.\n",
            "2025-10-23 22:39:41,394 - VAETrainer - INFO - Train loader: 98170 samples, 192 batches. Drop last: False\n",
            "INFO:VAETrainer:Train loader: 98170 samples, 192 batches. Drop last: False\n",
            "2025-10-23 22:39:41,395 - VAETrainer - INFO - Validation loader: 12271 samples, 24 batches.\n",
            "INFO:VAETrainer:Validation loader: 12271 samples, 24 batches.\n",
            "2025-10-23 22:39:43,515 - VAETrainer - INFO - Epoch [1/100] Train TotalLoss: 667.5987, (ReconX: 1.0797, KLD: 0.3423*0.03, LossY: 66.6508*10.00), Time: 1.7s | Val TotalLoss: 518.7842, (ReconX: 1.0271, KLD: 0.3479, LossY: 51.7409)\n",
            "INFO:VAETrainer:Epoch [1/100] Train TotalLoss: 667.5987, (ReconX: 1.0797, KLD: 0.3423*0.03, LossY: 66.6508*10.00), Time: 1.7s | Val TotalLoss: 518.7842, (ReconX: 1.0271, KLD: 0.3479, LossY: 51.7409)\n",
            "2025-10-23 22:39:43,517 - VAETrainer - INFO - Saving model and trainer state to vae_results/best_model.pth...\n",
            "INFO:VAETrainer:Saving model and trainer state to vae_results/best_model.pth...\n",
            "2025-10-23 22:39:43,530 - VAETrainer - INFO - Successfully saved model and trainer state to vae_results/best_model.pth\n",
            "INFO:VAETrainer:Successfully saved model and trainer state to vae_results/best_model.pth\n",
            "2025-10-23 22:39:45,595 - VAETrainer - INFO - Epoch [2/100] Train TotalLoss: 457.7676, (ReconX: 0.9878, KLD: 0.3617*0.07, LossY: 45.6756*10.00), Time: 1.7s | Val TotalLoss: 400.0452, (ReconX: 0.9696, KLD: 0.4074, LossY: 39.8668)\n",
            "INFO:VAETrainer:Epoch [2/100] Train TotalLoss: 457.7676, (ReconX: 0.9878, KLD: 0.3617*0.07, LossY: 45.6756*10.00), Time: 1.7s | Val TotalLoss: 400.0452, (ReconX: 0.9696, KLD: 0.4074, LossY: 39.8668)\n",
            "2025-10-23 22:39:45,596 - VAETrainer - INFO - Saving model and trainer state to vae_results/best_model.pth...\n",
            "INFO:VAETrainer:Saving model and trainer state to vae_results/best_model.pth...\n",
            "2025-10-23 22:39:45,609 - VAETrainer - INFO - Successfully saved model and trainer state to vae_results/best_model.pth\n",
            "INFO:VAETrainer:Successfully saved model and trainer state to vae_results/best_model.pth\n",
            "2025-10-23 22:39:47,694 - VAETrainer - INFO - Epoch [3/100] Train TotalLoss: 368.1013, (ReconX: 0.9476, KLD: 0.4151*0.10, LossY: 36.7112*10.00), Time: 1.7s | Val TotalLoss: 330.9982, (ReconX: 0.9466, KLD: 0.4350, LossY: 32.9617)\n",
            "INFO:VAETrainer:Epoch [3/100] Train TotalLoss: 368.1013, (ReconX: 0.9476, KLD: 0.4151*0.10, LossY: 36.7112*10.00), Time: 1.7s | Val TotalLoss: 330.9982, (ReconX: 0.9466, KLD: 0.4350, LossY: 32.9617)\n",
            "2025-10-23 22:39:47,696 - VAETrainer - INFO - Saving model and trainer state to vae_results/best_model.pth...\n",
            "INFO:VAETrainer:Saving model and trainer state to vae_results/best_model.pth...\n",
            "2025-10-23 22:39:47,708 - VAETrainer - INFO - Successfully saved model and trainer state to vae_results/best_model.pth\n",
            "INFO:VAETrainer:Successfully saved model and trainer state to vae_results/best_model.pth\n",
            "2025-10-23 22:39:49,858 - VAETrainer - INFO - Epoch [4/100] Train TotalLoss: 305.8295, (ReconX: 0.9346, KLD: 0.4163*0.13, LossY: 30.4839*10.00), Time: 1.8s | Val TotalLoss: 274.0164, (ReconX: 0.9416, KLD: 0.4266, LossY: 27.2648)\n",
            "INFO:VAETrainer:Epoch [4/100] Train TotalLoss: 305.8295, (ReconX: 0.9346, KLD: 0.4163*0.13, LossY: 30.4839*10.00), Time: 1.8s | Val TotalLoss: 274.0164, (ReconX: 0.9416, KLD: 0.4266, LossY: 27.2648)\n",
            "2025-10-23 22:39:49,859 - VAETrainer - INFO - Saving model and trainer state to vae_results/best_model.pth...\n",
            "INFO:VAETrainer:Saving model and trainer state to vae_results/best_model.pth...\n",
            "2025-10-23 22:39:49,873 - VAETrainer - INFO - Successfully saved model and trainer state to vae_results/best_model.pth\n",
            "INFO:VAETrainer:Successfully saved model and trainer state to vae_results/best_model.pth\n",
            "2025-10-23 22:39:52,081 - VAETrainer - INFO - Epoch [5/100] Train TotalLoss: 254.4992, (ReconX: 0.9305, KLD: 0.4108*0.17, LossY: 25.3500*10.00), Time: 1.8s | Val TotalLoss: 229.0648, (ReconX: 0.9389, KLD: 0.4165, LossY: 22.7709)\n",
            "INFO:VAETrainer:Epoch [5/100] Train TotalLoss: 254.4992, (ReconX: 0.9305, KLD: 0.4108*0.17, LossY: 25.3500*10.00), Time: 1.8s | Val TotalLoss: 229.0648, (ReconX: 0.9389, KLD: 0.4165, LossY: 22.7709)\n",
            "2025-10-23 22:39:52,083 - VAETrainer - INFO - Saving model and trainer state to vae_results/best_model.pth...\n",
            "INFO:VAETrainer:Saving model and trainer state to vae_results/best_model.pth...\n",
            "2025-10-23 22:39:52,095 - VAETrainer - INFO - Successfully saved model and trainer state to vae_results/best_model.pth\n",
            "INFO:VAETrainer:Successfully saved model and trainer state to vae_results/best_model.pth\n",
            "2025-10-23 22:39:54,221 - VAETrainer - INFO - Epoch [6/100] Train TotalLoss: 214.3834, (ReconX: 0.9288, KLD: 0.4152*0.20, LossY: 21.3371*10.00), Time: 1.7s | Val TotalLoss: 193.9036, (ReconX: 0.9346, KLD: 0.4293, LossY: 19.2540)\n",
            "INFO:VAETrainer:Epoch [6/100] Train TotalLoss: 214.3834, (ReconX: 0.9288, KLD: 0.4152*0.20, LossY: 21.3371*10.00), Time: 1.7s | Val TotalLoss: 193.9036, (ReconX: 0.9346, KLD: 0.4293, LossY: 19.2540)\n",
            "2025-10-23 22:39:54,224 - VAETrainer - INFO - Saving model and trainer state to vae_results/best_model.pth...\n",
            "INFO:VAETrainer:Saving model and trainer state to vae_results/best_model.pth...\n",
            "2025-10-23 22:39:54,238 - VAETrainer - INFO - Successfully saved model and trainer state to vae_results/best_model.pth\n",
            "INFO:VAETrainer:Successfully saved model and trainer state to vae_results/best_model.pth\n",
            "2025-10-23 22:39:56,431 - VAETrainer - INFO - Epoch [7/100] Train TotalLoss: 182.3559, (ReconX: 0.9265, KLD: 0.4194*0.23, LossY: 18.1332*10.00), Time: 1.8s | Val TotalLoss: 167.5795, (ReconX: 0.9340, KLD: 0.4247, LossY: 16.6221)\n",
            "INFO:VAETrainer:Epoch [7/100] Train TotalLoss: 182.3559, (ReconX: 0.9265, KLD: 0.4194*0.23, LossY: 18.1332*10.00), Time: 1.8s | Val TotalLoss: 167.5795, (ReconX: 0.9340, KLD: 0.4247, LossY: 16.6221)\n",
            "2025-10-23 22:39:56,433 - VAETrainer - INFO - Saving model and trainer state to vae_results/best_model.pth...\n",
            "INFO:VAETrainer:Saving model and trainer state to vae_results/best_model.pth...\n",
            "2025-10-23 22:39:56,446 - VAETrainer - INFO - Successfully saved model and trainer state to vae_results/best_model.pth\n",
            "INFO:VAETrainer:Successfully saved model and trainer state to vae_results/best_model.pth\n",
            "2025-10-23 22:39:58,539 - VAETrainer - INFO - Epoch [8/100] Train TotalLoss: 155.9518, (ReconX: 0.9259, KLD: 0.4215*0.27, LossY: 15.4913*10.00), Time: 1.7s | Val TotalLoss: 143.5562, (ReconX: 0.9334, KLD: 0.4260, LossY: 14.2197)\n",
            "INFO:VAETrainer:Epoch [8/100] Train TotalLoss: 155.9518, (ReconX: 0.9259, KLD: 0.4215*0.27, LossY: 15.4913*10.00), Time: 1.7s | Val TotalLoss: 143.5562, (ReconX: 0.9334, KLD: 0.4260, LossY: 14.2197)\n",
            "2025-10-23 22:39:58,541 - VAETrainer - INFO - Saving model and trainer state to vae_results/best_model.pth...\n",
            "INFO:VAETrainer:Saving model and trainer state to vae_results/best_model.pth...\n",
            "2025-10-23 22:39:58,553 - VAETrainer - INFO - Successfully saved model and trainer state to vae_results/best_model.pth\n",
            "INFO:VAETrainer:Successfully saved model and trainer state to vae_results/best_model.pth\n",
            "2025-10-23 22:40:00,613 - VAETrainer - INFO - Epoch [9/100] Train TotalLoss: 134.3872, (ReconX: 0.9259, KLD: 0.4204*0.30, LossY: 13.3335*10.00), Time: 1.7s | Val TotalLoss: 125.4097, (ReconX: 0.9345, KLD: 0.4203, LossY: 12.4055)\n",
            "INFO:VAETrainer:Epoch [9/100] Train TotalLoss: 134.3872, (ReconX: 0.9259, KLD: 0.4204*0.30, LossY: 13.3335*10.00), Time: 1.7s | Val TotalLoss: 125.4097, (ReconX: 0.9345, KLD: 0.4203, LossY: 12.4055)\n",
            "2025-10-23 22:40:00,615 - VAETrainer - INFO - Saving model and trainer state to vae_results/best_model.pth...\n",
            "INFO:VAETrainer:Saving model and trainer state to vae_results/best_model.pth...\n",
            "2025-10-23 22:40:00,627 - VAETrainer - INFO - Successfully saved model and trainer state to vae_results/best_model.pth\n",
            "INFO:VAETrainer:Successfully saved model and trainer state to vae_results/best_model.pth\n",
            "2025-10-23 22:40:02,785 - VAETrainer - INFO - Epoch [10/100] Train TotalLoss: 116.1472, (ReconX: 0.9258, KLD: 0.4186*0.33, LossY: 11.5082*10.00), Time: 1.8s | Val TotalLoss: 110.7441, (ReconX: 0.9353, KLD: 0.4144, LossY: 10.9395)\n",
            "INFO:VAETrainer:Epoch [10/100] Train TotalLoss: 116.1472, (ReconX: 0.9258, KLD: 0.4186*0.33, LossY: 11.5082*10.00), Time: 1.8s | Val TotalLoss: 110.7441, (ReconX: 0.9353, KLD: 0.4144, LossY: 10.9395)\n",
            "2025-10-23 22:40:02,787 - VAETrainer - INFO - Saving model and trainer state to vae_results/best_model.pth...\n",
            "INFO:VAETrainer:Saving model and trainer state to vae_results/best_model.pth...\n",
            "2025-10-23 22:40:02,800 - VAETrainer - INFO - Successfully saved model and trainer state to vae_results/best_model.pth\n",
            "INFO:VAETrainer:Successfully saved model and trainer state to vae_results/best_model.pth\n",
            "2025-10-23 22:40:05,053 - VAETrainer - INFO - Epoch [11/100] Train TotalLoss: 100.9441, (ReconX: 0.9273, KLD: 0.4166*0.37, LossY: 9.9864*10.00), Time: 1.9s | Val TotalLoss: 97.2615, (ReconX: 0.9358, KLD: 0.4115, LossY: 9.5914)\n",
            "INFO:VAETrainer:Epoch [11/100] Train TotalLoss: 100.9441, (ReconX: 0.9273, KLD: 0.4166*0.37, LossY: 9.9864*10.00), Time: 1.9s | Val TotalLoss: 97.2615, (ReconX: 0.9358, KLD: 0.4115, LossY: 9.5914)\n",
            "2025-10-23 22:40:05,055 - VAETrainer - INFO - Saving model and trainer state to vae_results/best_model.pth...\n",
            "INFO:VAETrainer:Saving model and trainer state to vae_results/best_model.pth...\n",
            "2025-10-23 22:40:05,068 - VAETrainer - INFO - Successfully saved model and trainer state to vae_results/best_model.pth\n",
            "INFO:VAETrainer:Successfully saved model and trainer state to vae_results/best_model.pth\n",
            "2025-10-23 22:40:07,259 - VAETrainer - INFO - Epoch [12/100] Train TotalLoss: 88.4136, (ReconX: 0.9274, KLD: 0.4113*0.40, LossY: 8.7322*10.00), Time: 1.8s | Val TotalLoss: 84.9352, (ReconX: 0.9367, KLD: 0.4107, LossY: 8.3588)\n",
            "INFO:VAETrainer:Epoch [12/100] Train TotalLoss: 88.4136, (ReconX: 0.9274, KLD: 0.4113*0.40, LossY: 8.7322*10.00), Time: 1.8s | Val TotalLoss: 84.9352, (ReconX: 0.9367, KLD: 0.4107, LossY: 8.3588)\n",
            "2025-10-23 22:40:07,261 - VAETrainer - INFO - Saving model and trainer state to vae_results/best_model.pth...\n",
            "INFO:VAETrainer:Saving model and trainer state to vae_results/best_model.pth...\n",
            "2025-10-23 22:40:07,274 - VAETrainer - INFO - Successfully saved model and trainer state to vae_results/best_model.pth\n",
            "INFO:VAETrainer:Successfully saved model and trainer state to vae_results/best_model.pth\n",
            "2025-10-23 22:40:09,457 - VAETrainer - INFO - Epoch [13/100] Train TotalLoss: 78.2549, (ReconX: 0.9281, KLD: 0.4082*0.43, LossY: 7.7150*10.00), Time: 1.8s | Val TotalLoss: 73.9922, (ReconX: 0.9372, KLD: 0.4168, LossY: 7.2638)\n",
            "INFO:VAETrainer:Epoch [13/100] Train TotalLoss: 78.2549, (ReconX: 0.9281, KLD: 0.4082*0.43, LossY: 7.7150*10.00), Time: 1.8s | Val TotalLoss: 73.9922, (ReconX: 0.9372, KLD: 0.4168, LossY: 7.2638)\n",
            "2025-10-23 22:40:09,459 - VAETrainer - INFO - Saving model and trainer state to vae_results/best_model.pth...\n",
            "INFO:VAETrainer:Saving model and trainer state to vae_results/best_model.pth...\n",
            "2025-10-23 22:40:09,471 - VAETrainer - INFO - Successfully saved model and trainer state to vae_results/best_model.pth\n",
            "INFO:VAETrainer:Successfully saved model and trainer state to vae_results/best_model.pth\n",
            "2025-10-23 22:40:11,553 - VAETrainer - INFO - Epoch [14/100] Train TotalLoss: 70.6820, (ReconX: 0.9275, KLD: 0.4147*0.47, LossY: 6.9561*10.00), Time: 1.7s | Val TotalLoss: 67.0725, (ReconX: 0.9353, KLD: 0.4195, LossY: 6.5718)\n",
            "INFO:VAETrainer:Epoch [14/100] Train TotalLoss: 70.6820, (ReconX: 0.9275, KLD: 0.4147*0.47, LossY: 6.9561*10.00), Time: 1.7s | Val TotalLoss: 67.0725, (ReconX: 0.9353, KLD: 0.4195, LossY: 6.5718)\n",
            "2025-10-23 22:40:11,555 - VAETrainer - INFO - Saving model and trainer state to vae_results/best_model.pth...\n",
            "INFO:VAETrainer:Saving model and trainer state to vae_results/best_model.pth...\n",
            "2025-10-23 22:40:11,568 - VAETrainer - INFO - Successfully saved model and trainer state to vae_results/best_model.pth\n",
            "INFO:VAETrainer:Successfully saved model and trainer state to vae_results/best_model.pth\n",
            "2025-10-23 22:40:13,715 - VAETrainer - INFO - Epoch [15/100] Train TotalLoss: 64.2622, (ReconX: 0.9270, KLD: 0.4163*0.50, LossY: 6.3127*10.00), Time: 1.8s | Val TotalLoss: 60.5258, (ReconX: 0.9353, KLD: 0.4151, LossY: 5.9175)\n",
            "INFO:VAETrainer:Epoch [15/100] Train TotalLoss: 64.2622, (ReconX: 0.9270, KLD: 0.4163*0.50, LossY: 6.3127*10.00), Time: 1.8s | Val TotalLoss: 60.5258, (ReconX: 0.9353, KLD: 0.4151, LossY: 5.9175)\n",
            "2025-10-23 22:40:13,717 - VAETrainer - INFO - Saving model and trainer state to vae_results/best_model.pth...\n",
            "INFO:VAETrainer:Saving model and trainer state to vae_results/best_model.pth...\n",
            "2025-10-23 22:40:13,730 - VAETrainer - INFO - Successfully saved model and trainer state to vae_results/best_model.pth\n",
            "INFO:VAETrainer:Successfully saved model and trainer state to vae_results/best_model.pth\n",
            "2025-10-23 22:40:15,870 - VAETrainer - INFO - Epoch [16/100] Train TotalLoss: 59.2392, (ReconX: 0.9266, KLD: 0.4149*0.53, LossY: 5.8091*10.00), Time: 1.8s | Val TotalLoss: 55.7839, (ReconX: 0.9368, KLD: 0.4177, LossY: 5.4429)\n",
            "INFO:VAETrainer:Epoch [16/100] Train TotalLoss: 59.2392, (ReconX: 0.9266, KLD: 0.4149*0.53, LossY: 5.8091*10.00), Time: 1.8s | Val TotalLoss: 55.7839, (ReconX: 0.9368, KLD: 0.4177, LossY: 5.4429)\n",
            "2025-10-23 22:40:15,871 - VAETrainer - INFO - Saving model and trainer state to vae_results/best_model.pth...\n",
            "INFO:VAETrainer:Saving model and trainer state to vae_results/best_model.pth...\n",
            "2025-10-23 22:40:15,884 - VAETrainer - INFO - Successfully saved model and trainer state to vae_results/best_model.pth\n",
            "INFO:VAETrainer:Successfully saved model and trainer state to vae_results/best_model.pth\n",
            "2025-10-23 22:40:18,054 - VAETrainer - INFO - Epoch [17/100] Train TotalLoss: 54.8023, (ReconX: 0.9285, KLD: 0.4104*0.57, LossY: 5.3641*10.00), Time: 1.8s | Val TotalLoss: 53.4820, (ReconX: 0.9372, KLD: 0.4113, LossY: 5.2134)\n",
            "INFO:VAETrainer:Epoch [17/100] Train TotalLoss: 54.8023, (ReconX: 0.9285, KLD: 0.4104*0.57, LossY: 5.3641*10.00), Time: 1.8s | Val TotalLoss: 53.4820, (ReconX: 0.9372, KLD: 0.4113, LossY: 5.2134)\n",
            "2025-10-23 22:40:18,055 - VAETrainer - INFO - Saving model and trainer state to vae_results/best_model.pth...\n",
            "INFO:VAETrainer:Saving model and trainer state to vae_results/best_model.pth...\n",
            "2025-10-23 22:40:18,068 - VAETrainer - INFO - Successfully saved model and trainer state to vae_results/best_model.pth\n",
            "INFO:VAETrainer:Successfully saved model and trainer state to vae_results/best_model.pth\n",
            "2025-10-23 22:40:20,178 - VAETrainer - INFO - Epoch [18/100] Train TotalLoss: 51.6714, (ReconX: 0.9282, KLD: 0.4075*0.60, LossY: 5.0499*10.00), Time: 1.7s | Val TotalLoss: 49.6423, (ReconX: 0.9365, KLD: 0.4074, LossY: 4.8298)\n",
            "INFO:VAETrainer:Epoch [18/100] Train TotalLoss: 51.6714, (ReconX: 0.9282, KLD: 0.4075*0.60, LossY: 5.0499*10.00), Time: 1.7s | Val TotalLoss: 49.6423, (ReconX: 0.9365, KLD: 0.4074, LossY: 4.8298)\n",
            "2025-10-23 22:40:20,179 - VAETrainer - INFO - Saving model and trainer state to vae_results/best_model.pth...\n",
            "INFO:VAETrainer:Saving model and trainer state to vae_results/best_model.pth...\n",
            "2025-10-23 22:40:20,195 - VAETrainer - INFO - Successfully saved model and trainer state to vae_results/best_model.pth\n",
            "INFO:VAETrainer:Successfully saved model and trainer state to vae_results/best_model.pth\n",
            "2025-10-23 22:40:22,283 - VAETrainer - INFO - Epoch [19/100] Train TotalLoss: 48.6285, (ReconX: 0.9286, KLD: 0.3999*0.63, LossY: 4.7447*10.00), Time: 1.7s | Val TotalLoss: 46.9489, (ReconX: 0.9381, KLD: 0.4030, LossY: 4.5608)\n",
            "INFO:VAETrainer:Epoch [19/100] Train TotalLoss: 48.6285, (ReconX: 0.9286, KLD: 0.3999*0.63, LossY: 4.7447*10.00), Time: 1.7s | Val TotalLoss: 46.9489, (ReconX: 0.9381, KLD: 0.4030, LossY: 4.5608)\n",
            "2025-10-23 22:40:22,285 - VAETrainer - INFO - Saving model and trainer state to vae_results/best_model.pth...\n",
            "INFO:VAETrainer:Saving model and trainer state to vae_results/best_model.pth...\n",
            "2025-10-23 22:40:22,298 - VAETrainer - INFO - Successfully saved model and trainer state to vae_results/best_model.pth\n",
            "INFO:VAETrainer:Successfully saved model and trainer state to vae_results/best_model.pth\n",
            "2025-10-23 22:40:24,506 - VAETrainer - INFO - Epoch [20/100] Train TotalLoss: 45.9416, (ReconX: 0.9290, KLD: 0.3950*0.67, LossY: 4.4749*10.00), Time: 1.8s | Val TotalLoss: 44.1116, (ReconX: 0.9383, KLD: 0.3948, LossY: 4.2778)\n",
            "INFO:VAETrainer:Epoch [20/100] Train TotalLoss: 45.9416, (ReconX: 0.9290, KLD: 0.3950*0.67, LossY: 4.4749*10.00), Time: 1.8s | Val TotalLoss: 44.1116, (ReconX: 0.9383, KLD: 0.3948, LossY: 4.2778)\n",
            "2025-10-23 22:40:24,508 - VAETrainer - INFO - Saving model and trainer state to vae_results/best_model.pth...\n",
            "INFO:VAETrainer:Saving model and trainer state to vae_results/best_model.pth...\n",
            "2025-10-23 22:40:24,521 - VAETrainer - INFO - Successfully saved model and trainer state to vae_results/best_model.pth\n",
            "INFO:VAETrainer:Successfully saved model and trainer state to vae_results/best_model.pth\n",
            "2025-10-23 22:40:26,632 - VAETrainer - INFO - Epoch [21/100] Train TotalLoss: 43.9083, (ReconX: 0.9306, KLD: 0.3894*0.70, LossY: 4.2705*10.00), Time: 1.7s | Val TotalLoss: 42.6738, (ReconX: 0.9377, KLD: 0.3920, LossY: 4.1344)\n",
            "INFO:VAETrainer:Epoch [21/100] Train TotalLoss: 43.9083, (ReconX: 0.9306, KLD: 0.3894*0.70, LossY: 4.2705*10.00), Time: 1.7s | Val TotalLoss: 42.6738, (ReconX: 0.9377, KLD: 0.3920, LossY: 4.1344)\n",
            "2025-10-23 22:40:26,633 - VAETrainer - INFO - Saving model and trainer state to vae_results/best_model.pth...\n",
            "INFO:VAETrainer:Saving model and trainer state to vae_results/best_model.pth...\n",
            "2025-10-23 22:40:26,652 - VAETrainer - INFO - Successfully saved model and trainer state to vae_results/best_model.pth\n",
            "INFO:VAETrainer:Successfully saved model and trainer state to vae_results/best_model.pth\n",
            "2025-10-23 22:40:28,858 - VAETrainer - INFO - Epoch [22/100] Train TotalLoss: 42.1942, (ReconX: 0.9307, KLD: 0.3822*0.73, LossY: 4.0983*10.00), Time: 1.8s | Val TotalLoss: 39.6388, (ReconX: 0.9404, KLD: 0.3867, LossY: 3.8312)\n",
            "INFO:VAETrainer:Epoch [22/100] Train TotalLoss: 42.1942, (ReconX: 0.9307, KLD: 0.3822*0.73, LossY: 4.0983*10.00), Time: 1.8s | Val TotalLoss: 39.6388, (ReconX: 0.9404, KLD: 0.3867, LossY: 3.8312)\n",
            "2025-10-23 22:40:28,861 - VAETrainer - INFO - Saving model and trainer state to vae_results/best_model.pth...\n",
            "INFO:VAETrainer:Saving model and trainer state to vae_results/best_model.pth...\n",
            "2025-10-23 22:40:28,874 - VAETrainer - INFO - Successfully saved model and trainer state to vae_results/best_model.pth\n",
            "INFO:VAETrainer:Successfully saved model and trainer state to vae_results/best_model.pth\n",
            "2025-10-23 22:40:31,046 - VAETrainer - INFO - Epoch [23/100] Train TotalLoss: 40.6356, (ReconX: 0.9320, KLD: 0.3730*0.77, LossY: 3.9418*10.00), Time: 1.8s | Val TotalLoss: 39.1040, (ReconX: 0.9391, KLD: 0.3718, LossY: 3.7793)\n",
            "INFO:VAETrainer:Epoch [23/100] Train TotalLoss: 40.6356, (ReconX: 0.9320, KLD: 0.3730*0.77, LossY: 3.9418*10.00), Time: 1.8s | Val TotalLoss: 39.1040, (ReconX: 0.9391, KLD: 0.3718, LossY: 3.7793)\n",
            "2025-10-23 22:40:31,047 - VAETrainer - INFO - Saving model and trainer state to vae_results/best_model.pth...\n",
            "INFO:VAETrainer:Saving model and trainer state to vae_results/best_model.pth...\n",
            "2025-10-23 22:40:31,062 - VAETrainer - INFO - Successfully saved model and trainer state to vae_results/best_model.pth\n",
            "INFO:VAETrainer:Successfully saved model and trainer state to vae_results/best_model.pth\n",
            "2025-10-23 22:40:33,217 - VAETrainer - INFO - Epoch [24/100] Train TotalLoss: 39.1352, (ReconX: 0.9333, KLD: 0.3627*0.80, LossY: 3.7912*10.00), Time: 1.8s | Val TotalLoss: 38.0207, (ReconX: 0.9435, KLD: 0.3628, LossY: 3.6714)\n",
            "INFO:VAETrainer:Epoch [24/100] Train TotalLoss: 39.1352, (ReconX: 0.9333, KLD: 0.3627*0.80, LossY: 3.7912*10.00), Time: 1.8s | Val TotalLoss: 38.0207, (ReconX: 0.9435, KLD: 0.3628, LossY: 3.6714)\n",
            "2025-10-23 22:40:33,218 - VAETrainer - INFO - Saving model and trainer state to vae_results/best_model.pth...\n",
            "INFO:VAETrainer:Saving model and trainer state to vae_results/best_model.pth...\n",
            "2025-10-23 22:40:33,231 - VAETrainer - INFO - Successfully saved model and trainer state to vae_results/best_model.pth\n",
            "INFO:VAETrainer:Successfully saved model and trainer state to vae_results/best_model.pth\n",
            "2025-10-23 22:40:35,354 - VAETrainer - INFO - Epoch [25/100] Train TotalLoss: 38.2849, (ReconX: 0.9340, KLD: 0.3527*0.83, LossY: 3.7057*10.00), Time: 1.7s | Val TotalLoss: 37.5948, (ReconX: 0.9440, KLD: 0.3486, LossY: 3.6302)\n",
            "INFO:VAETrainer:Epoch [25/100] Train TotalLoss: 38.2849, (ReconX: 0.9340, KLD: 0.3527*0.83, LossY: 3.7057*10.00), Time: 1.7s | Val TotalLoss: 37.5948, (ReconX: 0.9440, KLD: 0.3486, LossY: 3.6302)\n",
            "2025-10-23 22:40:35,356 - VAETrainer - INFO - Saving model and trainer state to vae_results/best_model.pth...\n",
            "INFO:VAETrainer:Saving model and trainer state to vae_results/best_model.pth...\n",
            "2025-10-23 22:40:35,369 - VAETrainer - INFO - Successfully saved model and trainer state to vae_results/best_model.pth\n",
            "INFO:VAETrainer:Successfully saved model and trainer state to vae_results/best_model.pth\n",
            "2025-10-23 22:40:37,531 - VAETrainer - INFO - Epoch [26/100] Train TotalLoss: 37.3133, (ReconX: 0.9356, KLD: 0.3432*0.87, LossY: 3.6080*10.00), Time: 1.8s | Val TotalLoss: 36.4705, (ReconX: 0.9440, KLD: 0.3419, LossY: 3.5185)\n",
            "INFO:VAETrainer:Epoch [26/100] Train TotalLoss: 37.3133, (ReconX: 0.9356, KLD: 0.3432*0.87, LossY: 3.6080*10.00), Time: 1.8s | Val TotalLoss: 36.4705, (ReconX: 0.9440, KLD: 0.3419, LossY: 3.5185)\n",
            "2025-10-23 22:40:37,533 - VAETrainer - INFO - Saving model and trainer state to vae_results/best_model.pth...\n",
            "INFO:VAETrainer:Saving model and trainer state to vae_results/best_model.pth...\n",
            "2025-10-23 22:40:37,546 - VAETrainer - INFO - Successfully saved model and trainer state to vae_results/best_model.pth\n",
            "INFO:VAETrainer:Successfully saved model and trainer state to vae_results/best_model.pth\n",
            "2025-10-23 22:40:39,694 - VAETrainer - INFO - Epoch [27/100] Train TotalLoss: 36.6775, (ReconX: 0.9368, KLD: 0.3338*0.90, LossY: 3.5440*10.00), Time: 1.8s | Val TotalLoss: 35.7345, (ReconX: 0.9463, KLD: 0.3317, LossY: 3.4457)\n",
            "INFO:VAETrainer:Epoch [27/100] Train TotalLoss: 36.6775, (ReconX: 0.9368, KLD: 0.3338*0.90, LossY: 3.5440*10.00), Time: 1.8s | Val TotalLoss: 35.7345, (ReconX: 0.9463, KLD: 0.3317, LossY: 3.4457)\n",
            "2025-10-23 22:40:39,696 - VAETrainer - INFO - Saving model and trainer state to vae_results/best_model.pth...\n",
            "INFO:VAETrainer:Saving model and trainer state to vae_results/best_model.pth...\n",
            "2025-10-23 22:40:39,709 - VAETrainer - INFO - Successfully saved model and trainer state to vae_results/best_model.pth\n",
            "INFO:VAETrainer:Successfully saved model and trainer state to vae_results/best_model.pth\n",
            "2025-10-23 22:40:41,904 - VAETrainer - INFO - Epoch [28/100] Train TotalLoss: 35.7968, (ReconX: 0.9379, KLD: 0.3237*0.93, LossY: 3.4557*10.00), Time: 1.8s | Val TotalLoss: 35.2314, (ReconX: 0.9459, KLD: 0.3214, LossY: 3.3964)\n",
            "INFO:VAETrainer:Epoch [28/100] Train TotalLoss: 35.7968, (ReconX: 0.9379, KLD: 0.3237*0.93, LossY: 3.4557*10.00), Time: 1.8s | Val TotalLoss: 35.2314, (ReconX: 0.9459, KLD: 0.3214, LossY: 3.3964)\n",
            "2025-10-23 22:40:41,905 - VAETrainer - INFO - Saving model and trainer state to vae_results/best_model.pth...\n",
            "INFO:VAETrainer:Saving model and trainer state to vae_results/best_model.pth...\n",
            "2025-10-23 22:40:41,925 - VAETrainer - INFO - Successfully saved model and trainer state to vae_results/best_model.pth\n",
            "INFO:VAETrainer:Successfully saved model and trainer state to vae_results/best_model.pth\n",
            "2025-10-23 22:40:44,026 - VAETrainer - INFO - Epoch [29/100] Train TotalLoss: 35.1600, (ReconX: 0.9387, KLD: 0.3123*0.97, LossY: 3.3919*10.00), Time: 1.7s | Val TotalLoss: 34.7278, (ReconX: 0.9492, KLD: 0.3123, LossY: 3.3466)\n",
            "INFO:VAETrainer:Epoch [29/100] Train TotalLoss: 35.1600, (ReconX: 0.9387, KLD: 0.3123*0.97, LossY: 3.3919*10.00), Time: 1.7s | Val TotalLoss: 34.7278, (ReconX: 0.9492, KLD: 0.3123, LossY: 3.3466)\n",
            "2025-10-23 22:40:44,028 - VAETrainer - INFO - Saving model and trainer state to vae_results/best_model.pth...\n",
            "INFO:VAETrainer:Saving model and trainer state to vae_results/best_model.pth...\n",
            "2025-10-23 22:40:44,045 - VAETrainer - INFO - Successfully saved model and trainer state to vae_results/best_model.pth\n",
            "INFO:VAETrainer:Successfully saved model and trainer state to vae_results/best_model.pth\n",
            "2025-10-23 22:40:46,209 - VAETrainer - INFO - Epoch [30/100] Train TotalLoss: 34.7834, (ReconX: 0.9408, KLD: 0.3019*1.00, LossY: 3.3541*10.00), Time: 1.8s | Val TotalLoss: 34.1905, (ReconX: 0.9509, KLD: 0.2949, LossY: 3.2945)\n",
            "INFO:VAETrainer:Epoch [30/100] Train TotalLoss: 34.7834, (ReconX: 0.9408, KLD: 0.3019*1.00, LossY: 3.3541*10.00), Time: 1.8s | Val TotalLoss: 34.1905, (ReconX: 0.9509, KLD: 0.2949, LossY: 3.2945)\n",
            "2025-10-23 22:40:46,211 - VAETrainer - INFO - Saving model and trainer state to vae_results/best_model.pth...\n",
            "INFO:VAETrainer:Saving model and trainer state to vae_results/best_model.pth...\n",
            "2025-10-23 22:40:46,224 - VAETrainer - INFO - Successfully saved model and trainer state to vae_results/best_model.pth\n",
            "INFO:VAETrainer:Successfully saved model and trainer state to vae_results/best_model.pth\n",
            "2025-10-23 22:40:48,302 - VAETrainer - INFO - Epoch [31/100] Train TotalLoss: 34.1542, (ReconX: 0.9424, KLD: 0.2890*1.00, LossY: 3.2923*10.00), Time: 1.7s | Val TotalLoss: 33.5777, (ReconX: 0.9525, KLD: 0.2856, LossY: 3.2340)\n",
            "INFO:VAETrainer:Epoch [31/100] Train TotalLoss: 34.1542, (ReconX: 0.9424, KLD: 0.2890*1.00, LossY: 3.2923*10.00), Time: 1.7s | Val TotalLoss: 33.5777, (ReconX: 0.9525, KLD: 0.2856, LossY: 3.2340)\n",
            "2025-10-23 22:40:48,304 - VAETrainer - INFO - Saving model and trainer state to vae_results/best_model.pth...\n",
            "INFO:VAETrainer:Saving model and trainer state to vae_results/best_model.pth...\n",
            "2025-10-23 22:40:48,317 - VAETrainer - INFO - Successfully saved model and trainer state to vae_results/best_model.pth\n",
            "INFO:VAETrainer:Successfully saved model and trainer state to vae_results/best_model.pth\n",
            "2025-10-23 22:40:50,511 - VAETrainer - INFO - Epoch [32/100] Train TotalLoss: 33.8714, (ReconX: 0.9433, KLD: 0.2808*1.00, LossY: 3.2647*10.00), Time: 1.8s | Val TotalLoss: 33.1552, (ReconX: 0.9534, KLD: 0.2775, LossY: 3.1924)\n",
            "INFO:VAETrainer:Epoch [32/100] Train TotalLoss: 33.8714, (ReconX: 0.9433, KLD: 0.2808*1.00, LossY: 3.2647*10.00), Time: 1.8s | Val TotalLoss: 33.1552, (ReconX: 0.9534, KLD: 0.2775, LossY: 3.1924)\n",
            "2025-10-23 22:40:50,513 - VAETrainer - INFO - Saving model and trainer state to vae_results/best_model.pth...\n",
            "INFO:VAETrainer:Saving model and trainer state to vae_results/best_model.pth...\n",
            "2025-10-23 22:40:50,529 - VAETrainer - INFO - Successfully saved model and trainer state to vae_results/best_model.pth\n",
            "INFO:VAETrainer:Successfully saved model and trainer state to vae_results/best_model.pth\n",
            "2025-10-23 22:40:52,680 - VAETrainer - INFO - Epoch [33/100] Train TotalLoss: 33.5126, (ReconX: 0.9440, KLD: 0.2727*1.00, LossY: 3.2296*10.00), Time: 1.7s | Val TotalLoss: 32.7903, (ReconX: 0.9538, KLD: 0.2703, LossY: 3.1566)\n",
            "INFO:VAETrainer:Epoch [33/100] Train TotalLoss: 33.5126, (ReconX: 0.9440, KLD: 0.2727*1.00, LossY: 3.2296*10.00), Time: 1.7s | Val TotalLoss: 32.7903, (ReconX: 0.9538, KLD: 0.2703, LossY: 3.1566)\n",
            "2025-10-23 22:40:52,682 - VAETrainer - INFO - Saving model and trainer state to vae_results/best_model.pth...\n",
            "INFO:VAETrainer:Saving model and trainer state to vae_results/best_model.pth...\n",
            "2025-10-23 22:40:52,697 - VAETrainer - INFO - Successfully saved model and trainer state to vae_results/best_model.pth\n",
            "INFO:VAETrainer:Successfully saved model and trainer state to vae_results/best_model.pth\n",
            "2025-10-23 22:40:54,814 - VAETrainer - INFO - Epoch [34/100] Train TotalLoss: 33.1817, (ReconX: 0.9463, KLD: 0.2626*1.00, LossY: 3.1973*10.00), Time: 1.7s | Val TotalLoss: 32.5046, (ReconX: 0.9543, KLD: 0.2610, LossY: 3.1289)\n",
            "INFO:VAETrainer:Epoch [34/100] Train TotalLoss: 33.1817, (ReconX: 0.9463, KLD: 0.2626*1.00, LossY: 3.1973*10.00), Time: 1.7s | Val TotalLoss: 32.5046, (ReconX: 0.9543, KLD: 0.2610, LossY: 3.1289)\n",
            "2025-10-23 22:40:54,816 - VAETrainer - INFO - Saving model and trainer state to vae_results/best_model.pth...\n",
            "INFO:VAETrainer:Saving model and trainer state to vae_results/best_model.pth...\n",
            "2025-10-23 22:40:54,829 - VAETrainer - INFO - Successfully saved model and trainer state to vae_results/best_model.pth\n",
            "INFO:VAETrainer:Successfully saved model and trainer state to vae_results/best_model.pth\n",
            "2025-10-23 22:40:56,973 - VAETrainer - INFO - Epoch [35/100] Train TotalLoss: 32.8102, (ReconX: 0.9473, KLD: 0.2516*1.00, LossY: 3.1611*10.00), Time: 1.8s | Val TotalLoss: 32.3796, (ReconX: 0.9576, KLD: 0.2470, LossY: 3.1175)\n",
            "INFO:VAETrainer:Epoch [35/100] Train TotalLoss: 32.8102, (ReconX: 0.9473, KLD: 0.2516*1.00, LossY: 3.1611*10.00), Time: 1.8s | Val TotalLoss: 32.3796, (ReconX: 0.9576, KLD: 0.2470, LossY: 3.1175)\n",
            "2025-10-23 22:40:56,974 - VAETrainer - INFO - Saving model and trainer state to vae_results/best_model.pth...\n",
            "INFO:VAETrainer:Saving model and trainer state to vae_results/best_model.pth...\n",
            "2025-10-23 22:40:56,988 - VAETrainer - INFO - Successfully saved model and trainer state to vae_results/best_model.pth\n",
            "INFO:VAETrainer:Successfully saved model and trainer state to vae_results/best_model.pth\n",
            "2025-10-23 22:40:59,087 - VAETrainer - INFO - Epoch [36/100] Train TotalLoss: 32.4197, (ReconX: 0.9491, KLD: 0.2425*1.00, LossY: 3.1228*10.00), Time: 1.7s | Val TotalLoss: 31.9903, (ReconX: 0.9602, KLD: 0.2388, LossY: 3.0791)\n",
            "INFO:VAETrainer:Epoch [36/100] Train TotalLoss: 32.4197, (ReconX: 0.9491, KLD: 0.2425*1.00, LossY: 3.1228*10.00), Time: 1.7s | Val TotalLoss: 31.9903, (ReconX: 0.9602, KLD: 0.2388, LossY: 3.0791)\n",
            "2025-10-23 22:40:59,089 - VAETrainer - INFO - Saving model and trainer state to vae_results/best_model.pth...\n",
            "INFO:VAETrainer:Saving model and trainer state to vae_results/best_model.pth...\n",
            "2025-10-23 22:40:59,102 - VAETrainer - INFO - Successfully saved model and trainer state to vae_results/best_model.pth\n",
            "INFO:VAETrainer:Successfully saved model and trainer state to vae_results/best_model.pth\n",
            "2025-10-23 22:41:01,317 - VAETrainer - INFO - Epoch [37/100] Train TotalLoss: 32.0751, (ReconX: 0.9497, KLD: 0.2322*1.00, LossY: 3.0893*10.00), Time: 1.8s | Val TotalLoss: 31.4819, (ReconX: 0.9602, KLD: 0.2300, LossY: 3.0292)\n",
            "INFO:VAETrainer:Epoch [37/100] Train TotalLoss: 32.0751, (ReconX: 0.9497, KLD: 0.2322*1.00, LossY: 3.0893*10.00), Time: 1.8s | Val TotalLoss: 31.4819, (ReconX: 0.9602, KLD: 0.2300, LossY: 3.0292)\n",
            "2025-10-23 22:41:01,319 - VAETrainer - INFO - Saving model and trainer state to vae_results/best_model.pth...\n",
            "INFO:VAETrainer:Saving model and trainer state to vae_results/best_model.pth...\n",
            "2025-10-23 22:41:01,332 - VAETrainer - INFO - Successfully saved model and trainer state to vae_results/best_model.pth\n",
            "INFO:VAETrainer:Successfully saved model and trainer state to vae_results/best_model.pth\n",
            "2025-10-23 22:41:03,486 - VAETrainer - INFO - Epoch [38/100] Train TotalLoss: 31.5219, (ReconX: 0.9515, KLD: 0.2235*1.00, LossY: 3.0347*10.00), Time: 1.8s | Val TotalLoss: 30.5804, (ReconX: 0.9613, KLD: 0.2216, LossY: 2.9397)\n",
            "INFO:VAETrainer:Epoch [38/100] Train TotalLoss: 31.5219, (ReconX: 0.9515, KLD: 0.2235*1.00, LossY: 3.0347*10.00), Time: 1.8s | Val TotalLoss: 30.5804, (ReconX: 0.9613, KLD: 0.2216, LossY: 2.9397)\n",
            "2025-10-23 22:41:03,488 - VAETrainer - INFO - Saving model and trainer state to vae_results/best_model.pth...\n",
            "INFO:VAETrainer:Saving model and trainer state to vae_results/best_model.pth...\n",
            "2025-10-23 22:41:03,501 - VAETrainer - INFO - Successfully saved model and trainer state to vae_results/best_model.pth\n",
            "INFO:VAETrainer:Successfully saved model and trainer state to vae_results/best_model.pth\n",
            "2025-10-23 22:41:05,616 - VAETrainer - INFO - Epoch [39/100] Train TotalLoss: 30.7796, (ReconX: 0.9529, KLD: 0.2149*1.00, LossY: 2.9612*10.00), Time: 1.7s | Val TotalLoss: 29.8281, (ReconX: 0.9625, KLD: 0.2149, LossY: 2.8651)\n",
            "INFO:VAETrainer:Epoch [39/100] Train TotalLoss: 30.7796, (ReconX: 0.9529, KLD: 0.2149*1.00, LossY: 2.9612*10.00), Time: 1.7s | Val TotalLoss: 29.8281, (ReconX: 0.9625, KLD: 0.2149, LossY: 2.8651)\n",
            "2025-10-23 22:41:05,618 - VAETrainer - INFO - Saving model and trainer state to vae_results/best_model.pth...\n",
            "INFO:VAETrainer:Saving model and trainer state to vae_results/best_model.pth...\n",
            "2025-10-23 22:41:05,631 - VAETrainer - INFO - Successfully saved model and trainer state to vae_results/best_model.pth\n",
            "INFO:VAETrainer:Successfully saved model and trainer state to vae_results/best_model.pth\n",
            "2025-10-23 22:41:07,832 - VAETrainer - INFO - Epoch [40/100] Train TotalLoss: 29.9872, (ReconX: 0.9538, KLD: 0.2059*1.00, LossY: 2.8827*10.00), Time: 1.8s | Val TotalLoss: 28.9716, (ReconX: 0.9633, KLD: 0.2072, LossY: 2.7801)\n",
            "INFO:VAETrainer:Epoch [40/100] Train TotalLoss: 29.9872, (ReconX: 0.9538, KLD: 0.2059*1.00, LossY: 2.8827*10.00), Time: 1.8s | Val TotalLoss: 28.9716, (ReconX: 0.9633, KLD: 0.2072, LossY: 2.7801)\n",
            "2025-10-23 22:41:07,834 - VAETrainer - INFO - Saving model and trainer state to vae_results/best_model.pth...\n",
            "INFO:VAETrainer:Saving model and trainer state to vae_results/best_model.pth...\n",
            "2025-10-23 22:41:07,848 - VAETrainer - INFO - Successfully saved model and trainer state to vae_results/best_model.pth\n",
            "INFO:VAETrainer:Successfully saved model and trainer state to vae_results/best_model.pth\n",
            "2025-10-23 22:41:10,063 - VAETrainer - INFO - Epoch [41/100] Train TotalLoss: 29.2845, (ReconX: 0.9552, KLD: 0.1981*1.00, LossY: 2.8131*10.00), Time: 1.8s | Val TotalLoss: 28.5885, (ReconX: 0.9655, KLD: 0.1961, LossY: 2.7427)\n",
            "INFO:VAETrainer:Epoch [41/100] Train TotalLoss: 29.2845, (ReconX: 0.9552, KLD: 0.1981*1.00, LossY: 2.8131*10.00), Time: 1.8s | Val TotalLoss: 28.5885, (ReconX: 0.9655, KLD: 0.1961, LossY: 2.7427)\n",
            "2025-10-23 22:41:10,064 - VAETrainer - INFO - Saving model and trainer state to vae_results/best_model.pth...\n",
            "INFO:VAETrainer:Saving model and trainer state to vae_results/best_model.pth...\n",
            "2025-10-23 22:41:10,081 - VAETrainer - INFO - Successfully saved model and trainer state to vae_results/best_model.pth\n",
            "INFO:VAETrainer:Successfully saved model and trainer state to vae_results/best_model.pth\n",
            "2025-10-23 22:41:12,257 - VAETrainer - INFO - Epoch [42/100] Train TotalLoss: 28.5866, (ReconX: 0.9556, KLD: 0.1947*1.00, LossY: 2.7436*10.00), Time: 1.8s | Val TotalLoss: 27.8322, (ReconX: 0.9651, KLD: 0.1955, LossY: 2.6672)\n",
            "INFO:VAETrainer:Epoch [42/100] Train TotalLoss: 28.5866, (ReconX: 0.9556, KLD: 0.1947*1.00, LossY: 2.7436*10.00), Time: 1.8s | Val TotalLoss: 27.8322, (ReconX: 0.9651, KLD: 0.1955, LossY: 2.6672)\n",
            "2025-10-23 22:41:12,259 - VAETrainer - INFO - Saving model and trainer state to vae_results/best_model.pth...\n",
            "INFO:VAETrainer:Saving model and trainer state to vae_results/best_model.pth...\n",
            "2025-10-23 22:41:12,272 - VAETrainer - INFO - Successfully saved model and trainer state to vae_results/best_model.pth\n",
            "INFO:VAETrainer:Successfully saved model and trainer state to vae_results/best_model.pth\n",
            "2025-10-23 22:41:14,415 - VAETrainer - INFO - Epoch [43/100] Train TotalLoss: 27.9179, (ReconX: 0.9564, KLD: 0.1941*1.00, LossY: 2.6767*10.00), Time: 1.8s | Val TotalLoss: 26.9249, (ReconX: 0.9644, KLD: 0.1951, LossY: 2.5765)\n",
            "INFO:VAETrainer:Epoch [43/100] Train TotalLoss: 27.9179, (ReconX: 0.9564, KLD: 0.1941*1.00, LossY: 2.6767*10.00), Time: 1.8s | Val TotalLoss: 26.9249, (ReconX: 0.9644, KLD: 0.1951, LossY: 2.5765)\n",
            "2025-10-23 22:41:14,417 - VAETrainer - INFO - Saving model and trainer state to vae_results/best_model.pth...\n",
            "INFO:VAETrainer:Saving model and trainer state to vae_results/best_model.pth...\n",
            "2025-10-23 22:41:14,430 - VAETrainer - INFO - Successfully saved model and trainer state to vae_results/best_model.pth\n",
            "INFO:VAETrainer:Successfully saved model and trainer state to vae_results/best_model.pth\n",
            "2025-10-23 22:41:16,582 - VAETrainer - INFO - Epoch [44/100] Train TotalLoss: 27.2230, (ReconX: 0.9563, KLD: 0.1935*1.00, LossY: 2.6073*10.00), Time: 1.8s | Val TotalLoss: 26.7022, (ReconX: 0.9664, KLD: 0.1932, LossY: 2.5543)\n",
            "INFO:VAETrainer:Epoch [44/100] Train TotalLoss: 27.2230, (ReconX: 0.9563, KLD: 0.1935*1.00, LossY: 2.6073*10.00), Time: 1.8s | Val TotalLoss: 26.7022, (ReconX: 0.9664, KLD: 0.1932, LossY: 2.5543)\n",
            "2025-10-23 22:41:16,584 - VAETrainer - INFO - Saving model and trainer state to vae_results/best_model.pth...\n",
            "INFO:VAETrainer:Saving model and trainer state to vae_results/best_model.pth...\n",
            "2025-10-23 22:41:16,599 - VAETrainer - INFO - Successfully saved model and trainer state to vae_results/best_model.pth\n",
            "INFO:VAETrainer:Successfully saved model and trainer state to vae_results/best_model.pth\n",
            "2025-10-23 22:41:18,887 - VAETrainer - INFO - Epoch [45/100] Train TotalLoss: 26.4363, (ReconX: 0.9563, KLD: 0.1928*1.00, LossY: 2.5287*10.00), Time: 1.9s | Val TotalLoss: 26.1574, (ReconX: 0.9655, KLD: 0.1895, LossY: 2.5002)\n",
            "INFO:VAETrainer:Epoch [45/100] Train TotalLoss: 26.4363, (ReconX: 0.9563, KLD: 0.1928*1.00, LossY: 2.5287*10.00), Time: 1.9s | Val TotalLoss: 26.1574, (ReconX: 0.9655, KLD: 0.1895, LossY: 2.5002)\n",
            "2025-10-23 22:41:18,888 - VAETrainer - INFO - Saving model and trainer state to vae_results/best_model.pth...\n",
            "INFO:VAETrainer:Saving model and trainer state to vae_results/best_model.pth...\n",
            "2025-10-23 22:41:18,902 - VAETrainer - INFO - Successfully saved model and trainer state to vae_results/best_model.pth\n",
            "INFO:VAETrainer:Successfully saved model and trainer state to vae_results/best_model.pth\n",
            "2025-10-23 22:41:21,073 - VAETrainer - INFO - Epoch [46/100] Train TotalLoss: 25.6025, (ReconX: 0.9570, KLD: 0.1895*1.00, LossY: 2.4456*10.00), Time: 1.8s | Val TotalLoss: 24.5963, (ReconX: 0.9670, KLD: 0.1874, LossY: 2.3442)\n",
            "INFO:VAETrainer:Epoch [46/100] Train TotalLoss: 25.6025, (ReconX: 0.9570, KLD: 0.1895*1.00, LossY: 2.4456*10.00), Time: 1.8s | Val TotalLoss: 24.5963, (ReconX: 0.9670, KLD: 0.1874, LossY: 2.3442)\n",
            "2025-10-23 22:41:21,075 - VAETrainer - INFO - Saving model and trainer state to vae_results/best_model.pth...\n",
            "INFO:VAETrainer:Saving model and trainer state to vae_results/best_model.pth...\n",
            "2025-10-23 22:41:21,089 - VAETrainer - INFO - Successfully saved model and trainer state to vae_results/best_model.pth\n",
            "INFO:VAETrainer:Successfully saved model and trainer state to vae_results/best_model.pth\n",
            "2025-10-23 22:41:23,198 - VAETrainer - INFO - Epoch [47/100] Train TotalLoss: 24.5134, (ReconX: 0.9579, KLD: 0.1853*1.00, LossY: 2.3370*10.00), Time: 1.7s | Val TotalLoss: 23.7316, (ReconX: 0.9675, KLD: 0.1844, LossY: 2.2580)\n",
            "INFO:VAETrainer:Epoch [47/100] Train TotalLoss: 24.5134, (ReconX: 0.9579, KLD: 0.1853*1.00, LossY: 2.3370*10.00), Time: 1.7s | Val TotalLoss: 23.7316, (ReconX: 0.9675, KLD: 0.1844, LossY: 2.2580)\n",
            "2025-10-23 22:41:23,199 - VAETrainer - INFO - Saving model and trainer state to vae_results/best_model.pth...\n",
            "INFO:VAETrainer:Saving model and trainer state to vae_results/best_model.pth...\n",
            "2025-10-23 22:41:23,214 - VAETrainer - INFO - Successfully saved model and trainer state to vae_results/best_model.pth\n",
            "INFO:VAETrainer:Successfully saved model and trainer state to vae_results/best_model.pth\n",
            "2025-10-23 22:41:25,272 - VAETrainer - INFO - Epoch [48/100] Train TotalLoss: 23.0523, (ReconX: 0.9580, KLD: 0.1823*1.00, LossY: 2.1912*10.00), Time: 1.7s | Val TotalLoss: 22.1831, (ReconX: 0.9700, KLD: 0.1809, LossY: 2.1032)\n",
            "INFO:VAETrainer:Epoch [48/100] Train TotalLoss: 23.0523, (ReconX: 0.9580, KLD: 0.1823*1.00, LossY: 2.1912*10.00), Time: 1.7s | Val TotalLoss: 22.1831, (ReconX: 0.9700, KLD: 0.1809, LossY: 2.1032)\n",
            "2025-10-23 22:41:25,274 - VAETrainer - INFO - Saving model and trainer state to vae_results/best_model.pth...\n",
            "INFO:VAETrainer:Saving model and trainer state to vae_results/best_model.pth...\n",
            "2025-10-23 22:41:25,288 - VAETrainer - INFO - Successfully saved model and trainer state to vae_results/best_model.pth\n",
            "INFO:VAETrainer:Successfully saved model and trainer state to vae_results/best_model.pth\n",
            "2025-10-23 22:41:27,378 - VAETrainer - INFO - Epoch [49/100] Train TotalLoss: 21.5577, (ReconX: 0.9600, KLD: 0.1780*1.00, LossY: 2.0420*10.00), Time: 1.7s | Val TotalLoss: 20.7424, (ReconX: 0.9718, KLD: 0.1757, LossY: 1.9595)\n",
            "INFO:VAETrainer:Epoch [49/100] Train TotalLoss: 21.5577, (ReconX: 0.9600, KLD: 0.1780*1.00, LossY: 2.0420*10.00), Time: 1.7s | Val TotalLoss: 20.7424, (ReconX: 0.9718, KLD: 0.1757, LossY: 1.9595)\n",
            "2025-10-23 22:41:27,380 - VAETrainer - INFO - Saving model and trainer state to vae_results/best_model.pth...\n",
            "INFO:VAETrainer:Saving model and trainer state to vae_results/best_model.pth...\n",
            "2025-10-23 22:41:27,397 - VAETrainer - INFO - Successfully saved model and trainer state to vae_results/best_model.pth\n",
            "INFO:VAETrainer:Successfully saved model and trainer state to vae_results/best_model.pth\n",
            "2025-10-23 22:41:29,553 - VAETrainer - INFO - Epoch [50/100] Train TotalLoss: 20.7524, (ReconX: 0.9611, KLD: 0.1696*1.00, LossY: 1.9622*10.00), Time: 1.8s | Val TotalLoss: 20.1033, (ReconX: 0.9723, KLD: 0.1676, LossY: 1.8963)\n",
            "INFO:VAETrainer:Epoch [50/100] Train TotalLoss: 20.7524, (ReconX: 0.9611, KLD: 0.1696*1.00, LossY: 1.9622*10.00), Time: 1.8s | Val TotalLoss: 20.1033, (ReconX: 0.9723, KLD: 0.1676, LossY: 1.8963)\n",
            "2025-10-23 22:41:29,555 - VAETrainer - INFO - Saving model and trainer state to vae_results/best_model.pth...\n",
            "INFO:VAETrainer:Saving model and trainer state to vae_results/best_model.pth...\n",
            "2025-10-23 22:41:29,572 - VAETrainer - INFO - Successfully saved model and trainer state to vae_results/best_model.pth\n",
            "INFO:VAETrainer:Successfully saved model and trainer state to vae_results/best_model.pth\n",
            "2025-10-23 22:41:31,800 - VAETrainer - INFO - Epoch [51/100] Train TotalLoss: 20.2804, (ReconX: 0.9638, KLD: 0.1630*1.00, LossY: 1.9154*10.00), Time: 1.8s | Val TotalLoss: 19.8375, (ReconX: 0.9718, KLD: 0.1593, LossY: 1.8706)\n",
            "INFO:VAETrainer:Epoch [51/100] Train TotalLoss: 20.2804, (ReconX: 0.9638, KLD: 0.1630*1.00, LossY: 1.9154*10.00), Time: 1.8s | Val TotalLoss: 19.8375, (ReconX: 0.9718, KLD: 0.1593, LossY: 1.8706)\n",
            "2025-10-23 22:41:31,802 - VAETrainer - INFO - Saving model and trainer state to vae_results/best_model.pth...\n",
            "INFO:VAETrainer:Saving model and trainer state to vae_results/best_model.pth...\n",
            "2025-10-23 22:41:31,816 - VAETrainer - INFO - Successfully saved model and trainer state to vae_results/best_model.pth\n",
            "INFO:VAETrainer:Successfully saved model and trainer state to vae_results/best_model.pth\n",
            "2025-10-23 22:41:33,997 - VAETrainer - INFO - Epoch [52/100] Train TotalLoss: 20.0374, (ReconX: 0.9641, KLD: 0.1561*1.00, LossY: 1.8917*10.00), Time: 1.8s | Val TotalLoss: 20.1062, (ReconX: 0.9726, KLD: 0.1543, LossY: 1.8979)\n",
            "INFO:VAETrainer:Epoch [52/100] Train TotalLoss: 20.0374, (ReconX: 0.9641, KLD: 0.1561*1.00, LossY: 1.8917*10.00), Time: 1.8s | Val TotalLoss: 20.1062, (ReconX: 0.9726, KLD: 0.1543, LossY: 1.8979)\n",
            "2025-10-23 22:41:36,141 - VAETrainer - INFO - Epoch [53/100] Train TotalLoss: 19.8237, (ReconX: 0.9664, KLD: 0.1481*1.00, LossY: 1.8709*10.00), Time: 1.8s | Val TotalLoss: 19.5876, (ReconX: 0.9769, KLD: 0.1458, LossY: 1.8465)\n",
            "INFO:VAETrainer:Epoch [53/100] Train TotalLoss: 19.8237, (ReconX: 0.9664, KLD: 0.1481*1.00, LossY: 1.8709*10.00), Time: 1.8s | Val TotalLoss: 19.5876, (ReconX: 0.9769, KLD: 0.1458, LossY: 1.8465)\n",
            "2025-10-23 22:41:36,143 - VAETrainer - INFO - Saving model and trainer state to vae_results/best_model.pth...\n",
            "INFO:VAETrainer:Saving model and trainer state to vae_results/best_model.pth...\n",
            "2025-10-23 22:41:36,156 - VAETrainer - INFO - Successfully saved model and trainer state to vae_results/best_model.pth\n",
            "INFO:VAETrainer:Successfully saved model and trainer state to vae_results/best_model.pth\n",
            "2025-10-23 22:41:38,259 - VAETrainer - INFO - Epoch [54/100] Train TotalLoss: 19.6278, (ReconX: 0.9677, KLD: 0.1419*1.00, LossY: 1.8518*10.00), Time: 1.7s | Val TotalLoss: 19.0417, (ReconX: 0.9775, KLD: 0.1396, LossY: 1.7925)\n",
            "INFO:VAETrainer:Epoch [54/100] Train TotalLoss: 19.6278, (ReconX: 0.9677, KLD: 0.1419*1.00, LossY: 1.8518*10.00), Time: 1.7s | Val TotalLoss: 19.0417, (ReconX: 0.9775, KLD: 0.1396, LossY: 1.7925)\n",
            "2025-10-23 22:41:38,261 - VAETrainer - INFO - Saving model and trainer state to vae_results/best_model.pth...\n",
            "INFO:VAETrainer:Saving model and trainer state to vae_results/best_model.pth...\n",
            "2025-10-23 22:41:38,274 - VAETrainer - INFO - Successfully saved model and trainer state to vae_results/best_model.pth\n",
            "INFO:VAETrainer:Successfully saved model and trainer state to vae_results/best_model.pth\n",
            "2025-10-23 22:41:40,385 - VAETrainer - INFO - Epoch [55/100] Train TotalLoss: 19.4464, (ReconX: 0.9684, KLD: 0.1367*1.00, LossY: 1.8341*10.00), Time: 1.7s | Val TotalLoss: 18.9201, (ReconX: 0.9787, KLD: 0.1335, LossY: 1.7808)\n",
            "INFO:VAETrainer:Epoch [55/100] Train TotalLoss: 19.4464, (ReconX: 0.9684, KLD: 0.1367*1.00, LossY: 1.8341*10.00), Time: 1.7s | Val TotalLoss: 18.9201, (ReconX: 0.9787, KLD: 0.1335, LossY: 1.7808)\n",
            "2025-10-23 22:41:40,387 - VAETrainer - INFO - Saving model and trainer state to vae_results/best_model.pth...\n",
            "INFO:VAETrainer:Saving model and trainer state to vae_results/best_model.pth...\n",
            "2025-10-23 22:41:40,400 - VAETrainer - INFO - Successfully saved model and trainer state to vae_results/best_model.pth\n",
            "INFO:VAETrainer:Successfully saved model and trainer state to vae_results/best_model.pth\n",
            "2025-10-23 22:41:42,566 - VAETrainer - INFO - Epoch [56/100] Train TotalLoss: 19.4406, (ReconX: 0.9697, KLD: 0.1308*1.00, LossY: 1.8340*10.00), Time: 1.8s | Val TotalLoss: 19.1023, (ReconX: 0.9790, KLD: 0.1270, LossY: 1.7996)\n",
            "INFO:VAETrainer:Epoch [56/100] Train TotalLoss: 19.4406, (ReconX: 0.9697, KLD: 0.1308*1.00, LossY: 1.8340*10.00), Time: 1.8s | Val TotalLoss: 19.1023, (ReconX: 0.9790, KLD: 0.1270, LossY: 1.7996)\n",
            "2025-10-23 22:41:44,742 - VAETrainer - INFO - Epoch [57/100] Train TotalLoss: 19.3119, (ReconX: 0.9710, KLD: 0.1246*1.00, LossY: 1.8216*10.00), Time: 1.8s | Val TotalLoss: 18.6814, (ReconX: 0.9815, KLD: 0.1234, LossY: 1.7576)\n",
            "INFO:VAETrainer:Epoch [57/100] Train TotalLoss: 19.3119, (ReconX: 0.9710, KLD: 0.1246*1.00, LossY: 1.8216*10.00), Time: 1.8s | Val TotalLoss: 18.6814, (ReconX: 0.9815, KLD: 0.1234, LossY: 1.7576)\n",
            "2025-10-23 22:41:44,744 - VAETrainer - INFO - Saving model and trainer state to vae_results/best_model.pth...\n",
            "INFO:VAETrainer:Saving model and trainer state to vae_results/best_model.pth...\n",
            "2025-10-23 22:41:44,758 - VAETrainer - INFO - Successfully saved model and trainer state to vae_results/best_model.pth\n",
            "INFO:VAETrainer:Successfully saved model and trainer state to vae_results/best_model.pth\n",
            "2025-10-23 22:41:46,963 - VAETrainer - INFO - Epoch [58/100] Train TotalLoss: 19.1204, (ReconX: 0.9719, KLD: 0.1195*1.00, LossY: 1.8029*10.00), Time: 1.8s | Val TotalLoss: 18.5589, (ReconX: 0.9846, KLD: 0.1186, LossY: 1.7456)\n",
            "INFO:VAETrainer:Epoch [58/100] Train TotalLoss: 19.1204, (ReconX: 0.9719, KLD: 0.1195*1.00, LossY: 1.8029*10.00), Time: 1.8s | Val TotalLoss: 18.5589, (ReconX: 0.9846, KLD: 0.1186, LossY: 1.7456)\n",
            "2025-10-23 22:41:46,965 - VAETrainer - INFO - Saving model and trainer state to vae_results/best_model.pth...\n",
            "INFO:VAETrainer:Saving model and trainer state to vae_results/best_model.pth...\n",
            "2025-10-23 22:41:46,978 - VAETrainer - INFO - Successfully saved model and trainer state to vae_results/best_model.pth\n",
            "INFO:VAETrainer:Successfully saved model and trainer state to vae_results/best_model.pth\n",
            "2025-10-23 22:41:49,103 - VAETrainer - INFO - Epoch [59/100] Train TotalLoss: 19.0584, (ReconX: 0.9725, KLD: 0.1153*1.00, LossY: 1.7971*10.00), Time: 1.7s | Val TotalLoss: 18.3246, (ReconX: 0.9825, KLD: 0.1133, LossY: 1.7229)\n",
            "INFO:VAETrainer:Epoch [59/100] Train TotalLoss: 19.0584, (ReconX: 0.9725, KLD: 0.1153*1.00, LossY: 1.7971*10.00), Time: 1.7s | Val TotalLoss: 18.3246, (ReconX: 0.9825, KLD: 0.1133, LossY: 1.7229)\n",
            "2025-10-23 22:41:49,104 - VAETrainer - INFO - Saving model and trainer state to vae_results/best_model.pth...\n",
            "INFO:VAETrainer:Saving model and trainer state to vae_results/best_model.pth...\n",
            "2025-10-23 22:41:49,117 - VAETrainer - INFO - Successfully saved model and trainer state to vae_results/best_model.pth\n",
            "INFO:VAETrainer:Successfully saved model and trainer state to vae_results/best_model.pth\n",
            "2025-10-23 22:41:51,227 - VAETrainer - INFO - Epoch [60/100] Train TotalLoss: 18.7984, (ReconX: 0.9733, KLD: 0.1110*1.00, LossY: 1.7714*10.00), Time: 1.7s | Val TotalLoss: 18.2091, (ReconX: 0.9844, KLD: 0.1075, LossY: 1.7117)\n",
            "INFO:VAETrainer:Epoch [60/100] Train TotalLoss: 18.7984, (ReconX: 0.9733, KLD: 0.1110*1.00, LossY: 1.7714*10.00), Time: 1.7s | Val TotalLoss: 18.2091, (ReconX: 0.9844, KLD: 0.1075, LossY: 1.7117)\n",
            "2025-10-23 22:41:51,229 - VAETrainer - INFO - Saving model and trainer state to vae_results/best_model.pth...\n",
            "INFO:VAETrainer:Saving model and trainer state to vae_results/best_model.pth...\n",
            "2025-10-23 22:41:51,244 - VAETrainer - INFO - Successfully saved model and trainer state to vae_results/best_model.pth\n",
            "INFO:VAETrainer:Successfully saved model and trainer state to vae_results/best_model.pth\n",
            "2025-10-23 22:41:53,482 - VAETrainer - INFO - Epoch [61/100] Train TotalLoss: 18.7714, (ReconX: 0.9745, KLD: 0.1063*1.00, LossY: 1.7691*10.00), Time: 1.9s | Val TotalLoss: 18.3882, (ReconX: 0.9866, KLD: 0.1024, LossY: 1.7299)\n",
            "INFO:VAETrainer:Epoch [61/100] Train TotalLoss: 18.7714, (ReconX: 0.9745, KLD: 0.1063*1.00, LossY: 1.7691*10.00), Time: 1.9s | Val TotalLoss: 18.3882, (ReconX: 0.9866, KLD: 0.1024, LossY: 1.7299)\n",
            "2025-10-23 22:41:55,616 - VAETrainer - INFO - Epoch [62/100] Train TotalLoss: 18.5825, (ReconX: 0.9752, KLD: 0.1005*1.00, LossY: 1.7507*10.00), Time: 1.7s | Val TotalLoss: 17.8903, (ReconX: 0.9872, KLD: 0.0982, LossY: 1.6805)\n",
            "INFO:VAETrainer:Epoch [62/100] Train TotalLoss: 18.5825, (ReconX: 0.9752, KLD: 0.1005*1.00, LossY: 1.7507*10.00), Time: 1.7s | Val TotalLoss: 17.8903, (ReconX: 0.9872, KLD: 0.0982, LossY: 1.6805)\n",
            "2025-10-23 22:41:55,618 - VAETrainer - INFO - Saving model and trainer state to vae_results/best_model.pth...\n",
            "INFO:VAETrainer:Saving model and trainer state to vae_results/best_model.pth...\n",
            "2025-10-23 22:41:55,631 - VAETrainer - INFO - Successfully saved model and trainer state to vae_results/best_model.pth\n",
            "INFO:VAETrainer:Successfully saved model and trainer state to vae_results/best_model.pth\n",
            "2025-10-23 22:41:57,784 - VAETrainer - INFO - Epoch [63/100] Train TotalLoss: 18.4541, (ReconX: 0.9766, KLD: 0.0975*1.00, LossY: 1.7380*10.00), Time: 1.8s | Val TotalLoss: 17.9219, (ReconX: 0.9858, KLD: 0.0936, LossY: 1.6842)\n",
            "INFO:VAETrainer:Epoch [63/100] Train TotalLoss: 18.4541, (ReconX: 0.9766, KLD: 0.0975*1.00, LossY: 1.7380*10.00), Time: 1.8s | Val TotalLoss: 17.9219, (ReconX: 0.9858, KLD: 0.0936, LossY: 1.6842)\n",
            "2025-10-23 22:41:59,919 - VAETrainer - INFO - Epoch [64/100] Train TotalLoss: 18.2084, (ReconX: 0.9770, KLD: 0.0950*1.00, LossY: 1.7136*10.00), Time: 1.8s | Val TotalLoss: 17.5627, (ReconX: 0.9867, KLD: 0.0938, LossY: 1.6482)\n",
            "INFO:VAETrainer:Epoch [64/100] Train TotalLoss: 18.2084, (ReconX: 0.9770, KLD: 0.0950*1.00, LossY: 1.7136*10.00), Time: 1.8s | Val TotalLoss: 17.5627, (ReconX: 0.9867, KLD: 0.0938, LossY: 1.6482)\n",
            "2025-10-23 22:41:59,921 - VAETrainer - INFO - Saving model and trainer state to vae_results/best_model.pth...\n",
            "INFO:VAETrainer:Saving model and trainer state to vae_results/best_model.pth...\n",
            "2025-10-23 22:41:59,934 - VAETrainer - INFO - Successfully saved model and trainer state to vae_results/best_model.pth\n",
            "INFO:VAETrainer:Successfully saved model and trainer state to vae_results/best_model.pth\n",
            "2025-10-23 22:42:02,045 - VAETrainer - INFO - Epoch [65/100] Train TotalLoss: 18.2277, (ReconX: 0.9772, KLD: 0.0920*1.00, LossY: 1.7158*10.00), Time: 1.7s | Val TotalLoss: 17.5258, (ReconX: 0.9883, KLD: 0.0904, LossY: 1.6447)\n",
            "INFO:VAETrainer:Epoch [65/100] Train TotalLoss: 18.2277, (ReconX: 0.9772, KLD: 0.0920*1.00, LossY: 1.7158*10.00), Time: 1.7s | Val TotalLoss: 17.5258, (ReconX: 0.9883, KLD: 0.0904, LossY: 1.6447)\n",
            "2025-10-23 22:42:02,047 - VAETrainer - INFO - Saving model and trainer state to vae_results/best_model.pth...\n",
            "INFO:VAETrainer:Saving model and trainer state to vae_results/best_model.pth...\n",
            "2025-10-23 22:42:02,060 - VAETrainer - INFO - Successfully saved model and trainer state to vae_results/best_model.pth\n",
            "INFO:VAETrainer:Successfully saved model and trainer state to vae_results/best_model.pth\n",
            "2025-10-23 22:42:04,262 - VAETrainer - INFO - Epoch [66/100] Train TotalLoss: 17.8913, (ReconX: 0.9775, KLD: 0.0903*1.00, LossY: 1.6824*10.00), Time: 1.8s | Val TotalLoss: 17.3673, (ReconX: 0.9888, KLD: 0.0871, LossY: 1.6291)\n",
            "INFO:VAETrainer:Epoch [66/100] Train TotalLoss: 17.8913, (ReconX: 0.9775, KLD: 0.0903*1.00, LossY: 1.6824*10.00), Time: 1.8s | Val TotalLoss: 17.3673, (ReconX: 0.9888, KLD: 0.0871, LossY: 1.6291)\n",
            "2025-10-23 22:42:04,264 - VAETrainer - INFO - Saving model and trainer state to vae_results/best_model.pth...\n",
            "INFO:VAETrainer:Saving model and trainer state to vae_results/best_model.pth...\n",
            "2025-10-23 22:42:04,277 - VAETrainer - INFO - Successfully saved model and trainer state to vae_results/best_model.pth\n",
            "INFO:VAETrainer:Successfully saved model and trainer state to vae_results/best_model.pth\n",
            "2025-10-23 22:42:06,387 - VAETrainer - INFO - Epoch [67/100] Train TotalLoss: 17.6980, (ReconX: 0.9787, KLD: 0.0872*1.00, LossY: 1.6632*10.00), Time: 1.7s | Val TotalLoss: 17.2461, (ReconX: 0.9877, KLD: 0.0868, LossY: 1.6172)\n",
            "INFO:VAETrainer:Epoch [67/100] Train TotalLoss: 17.6980, (ReconX: 0.9787, KLD: 0.0872*1.00, LossY: 1.6632*10.00), Time: 1.7s | Val TotalLoss: 17.2461, (ReconX: 0.9877, KLD: 0.0868, LossY: 1.6172)\n",
            "2025-10-23 22:42:06,388 - VAETrainer - INFO - Saving model and trainer state to vae_results/best_model.pth...\n",
            "INFO:VAETrainer:Saving model and trainer state to vae_results/best_model.pth...\n",
            "2025-10-23 22:42:06,402 - VAETrainer - INFO - Successfully saved model and trainer state to vae_results/best_model.pth\n",
            "INFO:VAETrainer:Successfully saved model and trainer state to vae_results/best_model.pth\n",
            "2025-10-23 22:42:08,625 - VAETrainer - INFO - Epoch [68/100] Train TotalLoss: 17.7447, (ReconX: 0.9788, KLD: 0.0856*1.00, LossY: 1.6680*10.00), Time: 1.8s | Val TotalLoss: 16.9563, (ReconX: 0.9898, KLD: 0.0845, LossY: 1.5882)\n",
            "INFO:VAETrainer:Epoch [68/100] Train TotalLoss: 17.7447, (ReconX: 0.9788, KLD: 0.0856*1.00, LossY: 1.6680*10.00), Time: 1.8s | Val TotalLoss: 16.9563, (ReconX: 0.9898, KLD: 0.0845, LossY: 1.5882)\n",
            "2025-10-23 22:42:08,626 - VAETrainer - INFO - Saving model and trainer state to vae_results/best_model.pth...\n",
            "INFO:VAETrainer:Saving model and trainer state to vae_results/best_model.pth...\n",
            "2025-10-23 22:42:08,641 - VAETrainer - INFO - Successfully saved model and trainer state to vae_results/best_model.pth\n",
            "INFO:VAETrainer:Successfully saved model and trainer state to vae_results/best_model.pth\n",
            "2025-10-23 22:42:10,744 - VAETrainer - INFO - Epoch [69/100] Train TotalLoss: 17.5740, (ReconX: 0.9797, KLD: 0.0831*1.00, LossY: 1.6511*10.00), Time: 1.7s | Val TotalLoss: 16.9442, (ReconX: 0.9916, KLD: 0.0788, LossY: 1.5874)\n",
            "INFO:VAETrainer:Epoch [69/100] Train TotalLoss: 17.5740, (ReconX: 0.9797, KLD: 0.0831*1.00, LossY: 1.6511*10.00), Time: 1.7s | Val TotalLoss: 16.9442, (ReconX: 0.9916, KLD: 0.0788, LossY: 1.5874)\n",
            "2025-10-23 22:42:10,746 - VAETrainer - INFO - Saving model and trainer state to vae_results/best_model.pth...\n",
            "INFO:VAETrainer:Saving model and trainer state to vae_results/best_model.pth...\n",
            "2025-10-23 22:42:10,759 - VAETrainer - INFO - Successfully saved model and trainer state to vae_results/best_model.pth\n",
            "INFO:VAETrainer:Successfully saved model and trainer state to vae_results/best_model.pth\n",
            "2025-10-23 22:42:12,901 - VAETrainer - INFO - Epoch [70/100] Train TotalLoss: 17.4743, (ReconX: 0.9805, KLD: 0.0796*1.00, LossY: 1.6414*10.00), Time: 1.7s | Val TotalLoss: 16.8735, (ReconX: 0.9911, KLD: 0.0785, LossY: 1.5804)\n",
            "INFO:VAETrainer:Epoch [70/100] Train TotalLoss: 17.4743, (ReconX: 0.9805, KLD: 0.0796*1.00, LossY: 1.6414*10.00), Time: 1.7s | Val TotalLoss: 16.8735, (ReconX: 0.9911, KLD: 0.0785, LossY: 1.5804)\n",
            "2025-10-23 22:42:12,902 - VAETrainer - INFO - Saving model and trainer state to vae_results/best_model.pth...\n",
            "INFO:VAETrainer:Saving model and trainer state to vae_results/best_model.pth...\n",
            "2025-10-23 22:42:12,916 - VAETrainer - INFO - Successfully saved model and trainer state to vae_results/best_model.pth\n",
            "INFO:VAETrainer:Successfully saved model and trainer state to vae_results/best_model.pth\n",
            "2025-10-23 22:42:15,088 - VAETrainer - INFO - Epoch [71/100] Train TotalLoss: 17.2057, (ReconX: 0.9807, KLD: 0.0776*1.00, LossY: 1.6147*10.00), Time: 1.8s | Val TotalLoss: 16.7622, (ReconX: 0.9913, KLD: 0.0774, LossY: 1.5694)\n",
            "INFO:VAETrainer:Epoch [71/100] Train TotalLoss: 17.2057, (ReconX: 0.9807, KLD: 0.0776*1.00, LossY: 1.6147*10.00), Time: 1.8s | Val TotalLoss: 16.7622, (ReconX: 0.9913, KLD: 0.0774, LossY: 1.5694)\n",
            "2025-10-23 22:42:15,090 - VAETrainer - INFO - Saving model and trainer state to vae_results/best_model.pth...\n",
            "INFO:VAETrainer:Saving model and trainer state to vae_results/best_model.pth...\n",
            "2025-10-23 22:42:15,103 - VAETrainer - INFO - Successfully saved model and trainer state to vae_results/best_model.pth\n",
            "INFO:VAETrainer:Successfully saved model and trainer state to vae_results/best_model.pth\n",
            "2025-10-23 22:42:17,247 - VAETrainer - INFO - Epoch [72/100] Train TotalLoss: 17.2353, (ReconX: 0.9812, KLD: 0.0766*1.00, LossY: 1.6177*10.00), Time: 1.8s | Val TotalLoss: 16.6036, (ReconX: 0.9923, KLD: 0.0741, LossY: 1.5537)\n",
            "INFO:VAETrainer:Epoch [72/100] Train TotalLoss: 17.2353, (ReconX: 0.9812, KLD: 0.0766*1.00, LossY: 1.6177*10.00), Time: 1.8s | Val TotalLoss: 16.6036, (ReconX: 0.9923, KLD: 0.0741, LossY: 1.5537)\n",
            "2025-10-23 22:42:17,249 - VAETrainer - INFO - Saving model and trainer state to vae_results/best_model.pth...\n",
            "INFO:VAETrainer:Saving model and trainer state to vae_results/best_model.pth...\n",
            "2025-10-23 22:42:17,261 - VAETrainer - INFO - Successfully saved model and trainer state to vae_results/best_model.pth\n",
            "INFO:VAETrainer:Successfully saved model and trainer state to vae_results/best_model.pth\n",
            "2025-10-23 22:42:19,483 - VAETrainer - INFO - Epoch [73/100] Train TotalLoss: 17.1154, (ReconX: 0.9818, KLD: 0.0735*1.00, LossY: 1.6060*10.00), Time: 1.8s | Val TotalLoss: 16.5386, (ReconX: 0.9910, KLD: 0.0728, LossY: 1.5475)\n",
            "INFO:VAETrainer:Epoch [73/100] Train TotalLoss: 17.1154, (ReconX: 0.9818, KLD: 0.0735*1.00, LossY: 1.6060*10.00), Time: 1.8s | Val TotalLoss: 16.5386, (ReconX: 0.9910, KLD: 0.0728, LossY: 1.5475)\n",
            "2025-10-23 22:42:19,485 - VAETrainer - INFO - Saving model and trainer state to vae_results/best_model.pth...\n",
            "INFO:VAETrainer:Saving model and trainer state to vae_results/best_model.pth...\n",
            "2025-10-23 22:42:19,498 - VAETrainer - INFO - Successfully saved model and trainer state to vae_results/best_model.pth\n",
            "INFO:VAETrainer:Successfully saved model and trainer state to vae_results/best_model.pth\n",
            "2025-10-23 22:42:21,779 - VAETrainer - INFO - Epoch [74/100] Train TotalLoss: 16.8895, (ReconX: 0.9817, KLD: 0.0735*1.00, LossY: 1.5834*10.00), Time: 1.9s | Val TotalLoss: 16.2091, (ReconX: 0.9920, KLD: 0.0718, LossY: 1.5145)\n",
            "INFO:VAETrainer:Epoch [74/100] Train TotalLoss: 16.8895, (ReconX: 0.9817, KLD: 0.0735*1.00, LossY: 1.5834*10.00), Time: 1.9s | Val TotalLoss: 16.2091, (ReconX: 0.9920, KLD: 0.0718, LossY: 1.5145)\n",
            "2025-10-23 22:42:21,781 - VAETrainer - INFO - Saving model and trainer state to vae_results/best_model.pth...\n",
            "INFO:VAETrainer:Saving model and trainer state to vae_results/best_model.pth...\n",
            "2025-10-23 22:42:21,795 - VAETrainer - INFO - Successfully saved model and trainer state to vae_results/best_model.pth\n",
            "INFO:VAETrainer:Successfully saved model and trainer state to vae_results/best_model.pth\n",
            "2025-10-23 22:42:23,934 - VAETrainer - INFO - Epoch [75/100] Train TotalLoss: 16.8637, (ReconX: 0.9823, KLD: 0.0716*1.00, LossY: 1.5810*10.00), Time: 1.8s | Val TotalLoss: 16.5361, (ReconX: 0.9940, KLD: 0.0709, LossY: 1.5471)\n",
            "INFO:VAETrainer:Epoch [75/100] Train TotalLoss: 16.8637, (ReconX: 0.9823, KLD: 0.0716*1.00, LossY: 1.5810*10.00), Time: 1.8s | Val TotalLoss: 16.5361, (ReconX: 0.9940, KLD: 0.0709, LossY: 1.5471)\n",
            "2025-10-23 22:42:26,077 - VAETrainer - INFO - Epoch [76/100] Train TotalLoss: 16.7791, (ReconX: 0.9820, KLD: 0.0706*1.00, LossY: 1.5726*10.00), Time: 1.8s | Val TotalLoss: 16.3398, (ReconX: 0.9929, KLD: 0.0685, LossY: 1.5278)\n",
            "INFO:VAETrainer:Epoch [76/100] Train TotalLoss: 16.7791, (ReconX: 0.9820, KLD: 0.0706*1.00, LossY: 1.5726*10.00), Time: 1.8s | Val TotalLoss: 16.3398, (ReconX: 0.9929, KLD: 0.0685, LossY: 1.5278)\n",
            "2025-10-23 22:42:28,195 - VAETrainer - INFO - Epoch [77/100] Train TotalLoss: 16.6922, (ReconX: 0.9821, KLD: 0.0694*1.00, LossY: 1.5641*10.00), Time: 1.7s | Val TotalLoss: 16.1864, (ReconX: 0.9924, KLD: 0.0685, LossY: 1.5126)\n",
            "INFO:VAETrainer:Epoch [77/100] Train TotalLoss: 16.6922, (ReconX: 0.9821, KLD: 0.0694*1.00, LossY: 1.5641*10.00), Time: 1.7s | Val TotalLoss: 16.1864, (ReconX: 0.9924, KLD: 0.0685, LossY: 1.5126)\n",
            "2025-10-23 22:42:28,196 - VAETrainer - INFO - Saving model and trainer state to vae_results/best_model.pth...\n",
            "INFO:VAETrainer:Saving model and trainer state to vae_results/best_model.pth...\n",
            "2025-10-23 22:42:28,209 - VAETrainer - INFO - Successfully saved model and trainer state to vae_results/best_model.pth\n",
            "INFO:VAETrainer:Successfully saved model and trainer state to vae_results/best_model.pth\n",
            "2025-10-23 22:42:30,376 - VAETrainer - INFO - Epoch [78/100] Train TotalLoss: 16.5540, (ReconX: 0.9827, KLD: 0.0679*1.00, LossY: 1.5503*10.00), Time: 1.8s | Val TotalLoss: 16.3104, (ReconX: 0.9943, KLD: 0.0680, LossY: 1.5248)\n",
            "INFO:VAETrainer:Epoch [78/100] Train TotalLoss: 16.5540, (ReconX: 0.9827, KLD: 0.0679*1.00, LossY: 1.5503*10.00), Time: 1.8s | Val TotalLoss: 16.3104, (ReconX: 0.9943, KLD: 0.0680, LossY: 1.5248)\n",
            "2025-10-23 22:42:32,573 - VAETrainer - INFO - Epoch [79/100] Train TotalLoss: 16.5776, (ReconX: 0.9826, KLD: 0.0670*1.00, LossY: 1.5528*10.00), Time: 1.8s | Val TotalLoss: 16.0857, (ReconX: 0.9933, KLD: 0.0651, LossY: 1.5027)\n",
            "INFO:VAETrainer:Epoch [79/100] Train TotalLoss: 16.5776, (ReconX: 0.9826, KLD: 0.0670*1.00, LossY: 1.5528*10.00), Time: 1.8s | Val TotalLoss: 16.0857, (ReconX: 0.9933, KLD: 0.0651, LossY: 1.5027)\n",
            "2025-10-23 22:42:32,574 - VAETrainer - INFO - Saving model and trainer state to vae_results/best_model.pth...\n",
            "INFO:VAETrainer:Saving model and trainer state to vae_results/best_model.pth...\n",
            "2025-10-23 22:42:32,588 - VAETrainer - INFO - Successfully saved model and trainer state to vae_results/best_model.pth\n",
            "INFO:VAETrainer:Successfully saved model and trainer state to vae_results/best_model.pth\n",
            "2025-10-23 22:42:34,703 - VAETrainer - INFO - Epoch [80/100] Train TotalLoss: 16.3918, (ReconX: 0.9837, KLD: 0.0662*1.00, LossY: 1.5342*10.00), Time: 1.7s | Val TotalLoss: 15.8604, (ReconX: 0.9943, KLD: 0.0657, LossY: 1.4800)\n",
            "INFO:VAETrainer:Epoch [80/100] Train TotalLoss: 16.3918, (ReconX: 0.9837, KLD: 0.0662*1.00, LossY: 1.5342*10.00), Time: 1.7s | Val TotalLoss: 15.8604, (ReconX: 0.9943, KLD: 0.0657, LossY: 1.4800)\n",
            "2025-10-23 22:42:34,704 - VAETrainer - INFO - Saving model and trainer state to vae_results/best_model.pth...\n",
            "INFO:VAETrainer:Saving model and trainer state to vae_results/best_model.pth...\n",
            "2025-10-23 22:42:34,718 - VAETrainer - INFO - Successfully saved model and trainer state to vae_results/best_model.pth\n",
            "INFO:VAETrainer:Successfully saved model and trainer state to vae_results/best_model.pth\n",
            "2025-10-23 22:42:36,844 - VAETrainer - INFO - Epoch [81/100] Train TotalLoss: 16.2849, (ReconX: 0.9829, KLD: 0.0667*1.00, LossY: 1.5235*10.00), Time: 1.8s | Val TotalLoss: 15.4365, (ReconX: 0.9937, KLD: 0.0671, LossY: 1.4376)\n",
            "INFO:VAETrainer:Epoch [81/100] Train TotalLoss: 16.2849, (ReconX: 0.9829, KLD: 0.0667*1.00, LossY: 1.5235*10.00), Time: 1.8s | Val TotalLoss: 15.4365, (ReconX: 0.9937, KLD: 0.0671, LossY: 1.4376)\n",
            "2025-10-23 22:42:36,846 - VAETrainer - INFO - Saving model and trainer state to vae_results/best_model.pth...\n",
            "INFO:VAETrainer:Saving model and trainer state to vae_results/best_model.pth...\n",
            "2025-10-23 22:42:36,861 - VAETrainer - INFO - Successfully saved model and trainer state to vae_results/best_model.pth\n",
            "INFO:VAETrainer:Successfully saved model and trainer state to vae_results/best_model.pth\n",
            "2025-10-23 22:42:38,936 - VAETrainer - INFO - Epoch [82/100] Train TotalLoss: 16.3496, (ReconX: 0.9834, KLD: 0.0664*1.00, LossY: 1.5300*10.00), Time: 1.7s | Val TotalLoss: 15.6607, (ReconX: 0.9942, KLD: 0.0650, LossY: 1.4601)\n",
            "INFO:VAETrainer:Epoch [82/100] Train TotalLoss: 16.3496, (ReconX: 0.9834, KLD: 0.0664*1.00, LossY: 1.5300*10.00), Time: 1.7s | Val TotalLoss: 15.6607, (ReconX: 0.9942, KLD: 0.0650, LossY: 1.4601)\n",
            "2025-10-23 22:42:41,046 - VAETrainer - INFO - Epoch [83/100] Train TotalLoss: 16.3821, (ReconX: 0.9839, KLD: 0.0642*1.00, LossY: 1.5334*10.00), Time: 1.7s | Val TotalLoss: 15.4252, (ReconX: 0.9937, KLD: 0.0627, LossY: 1.4369)\n",
            "INFO:VAETrainer:Epoch [83/100] Train TotalLoss: 16.3821, (ReconX: 0.9839, KLD: 0.0642*1.00, LossY: 1.5334*10.00), Time: 1.7s | Val TotalLoss: 15.4252, (ReconX: 0.9937, KLD: 0.0627, LossY: 1.4369)\n",
            "2025-10-23 22:42:41,048 - VAETrainer - INFO - Saving model and trainer state to vae_results/best_model.pth...\n",
            "INFO:VAETrainer:Saving model and trainer state to vae_results/best_model.pth...\n",
            "2025-10-23 22:42:41,063 - VAETrainer - INFO - Successfully saved model and trainer state to vae_results/best_model.pth\n",
            "INFO:VAETrainer:Successfully saved model and trainer state to vae_results/best_model.pth\n",
            "2025-10-23 22:42:43,314 - VAETrainer - INFO - Epoch [84/100] Train TotalLoss: 16.2119, (ReconX: 0.9842, KLD: 0.0638*1.00, LossY: 1.5164*10.00), Time: 1.9s | Val TotalLoss: 15.9780, (ReconX: 0.9940, KLD: 0.0644, LossY: 1.4920)\n",
            "INFO:VAETrainer:Epoch [84/100] Train TotalLoss: 16.2119, (ReconX: 0.9842, KLD: 0.0638*1.00, LossY: 1.5164*10.00), Time: 1.9s | Val TotalLoss: 15.9780, (ReconX: 0.9940, KLD: 0.0644, LossY: 1.4920)\n",
            "2025-10-23 22:42:45,444 - VAETrainer - INFO - Epoch [85/100] Train TotalLoss: 16.1339, (ReconX: 0.9836, KLD: 0.0636*1.00, LossY: 1.5087*10.00), Time: 1.7s | Val TotalLoss: 15.5946, (ReconX: 0.9929, KLD: 0.0630, LossY: 1.4539)\n",
            "INFO:VAETrainer:Epoch [85/100] Train TotalLoss: 16.1339, (ReconX: 0.9836, KLD: 0.0636*1.00, LossY: 1.5087*10.00), Time: 1.7s | Val TotalLoss: 15.5946, (ReconX: 0.9929, KLD: 0.0630, LossY: 1.4539)\n",
            "2025-10-23 22:42:47,579 - VAETrainer - INFO - Epoch [86/100] Train TotalLoss: 15.9255, (ReconX: 0.9836, KLD: 0.0633*1.00, LossY: 1.4879*10.00), Time: 1.7s | Val TotalLoss: 16.3856, (ReconX: 0.9942, KLD: 0.0623, LossY: 1.5329)\n",
            "INFO:VAETrainer:Epoch [86/100] Train TotalLoss: 15.9255, (ReconX: 0.9836, KLD: 0.0633*1.00, LossY: 1.4879*10.00), Time: 1.7s | Val TotalLoss: 16.3856, (ReconX: 0.9942, KLD: 0.0623, LossY: 1.5329)\n",
            "2025-10-23 22:42:49,702 - VAETrainer - INFO - Epoch [87/100] Train TotalLoss: 15.7672, (ReconX: 0.9842, KLD: 0.0627*1.00, LossY: 1.4720*10.00), Time: 1.7s | Val TotalLoss: 15.1746, (ReconX: 0.9950, KLD: 0.0617, LossY: 1.4118)\n",
            "INFO:VAETrainer:Epoch [87/100] Train TotalLoss: 15.7672, (ReconX: 0.9842, KLD: 0.0627*1.00, LossY: 1.4720*10.00), Time: 1.7s | Val TotalLoss: 15.1746, (ReconX: 0.9950, KLD: 0.0617, LossY: 1.4118)\n",
            "2025-10-23 22:42:49,704 - VAETrainer - INFO - Saving model and trainer state to vae_results/best_model.pth...\n",
            "INFO:VAETrainer:Saving model and trainer state to vae_results/best_model.pth...\n",
            "2025-10-23 22:42:49,718 - VAETrainer - INFO - Successfully saved model and trainer state to vae_results/best_model.pth\n",
            "INFO:VAETrainer:Successfully saved model and trainer state to vae_results/best_model.pth\n",
            "2025-10-23 22:42:51,864 - VAETrainer - INFO - Epoch [88/100] Train TotalLoss: 16.0754, (ReconX: 0.9838, KLD: 0.0611*1.00, LossY: 1.5030*10.00), Time: 1.8s | Val TotalLoss: 15.6881, (ReconX: 0.9954, KLD: 0.0608, LossY: 1.4632)\n",
            "INFO:VAETrainer:Epoch [88/100] Train TotalLoss: 16.0754, (ReconX: 0.9838, KLD: 0.0611*1.00, LossY: 1.5030*10.00), Time: 1.8s | Val TotalLoss: 15.6881, (ReconX: 0.9954, KLD: 0.0608, LossY: 1.4632)\n",
            "2025-10-23 22:42:54,059 - VAETrainer - INFO - Epoch [89/100] Train TotalLoss: 15.7484, (ReconX: 0.9844, KLD: 0.0610*1.00, LossY: 1.4703*10.00), Time: 1.8s | Val TotalLoss: 15.3694, (ReconX: 0.9933, KLD: 0.0622, LossY: 1.4314)\n",
            "INFO:VAETrainer:Epoch [89/100] Train TotalLoss: 15.7484, (ReconX: 0.9844, KLD: 0.0610*1.00, LossY: 1.4703*10.00), Time: 1.8s | Val TotalLoss: 15.3694, (ReconX: 0.9933, KLD: 0.0622, LossY: 1.4314)\n",
            "2025-10-23 22:42:56,190 - VAETrainer - INFO - Epoch [90/100] Train TotalLoss: 15.7471, (ReconX: 0.9843, KLD: 0.0615*1.00, LossY: 1.4701*10.00), Time: 1.7s | Val TotalLoss: 15.1862, (ReconX: 0.9946, KLD: 0.0617, LossY: 1.4130)\n",
            "INFO:VAETrainer:Epoch [90/100] Train TotalLoss: 15.7471, (ReconX: 0.9843, KLD: 0.0615*1.00, LossY: 1.4701*10.00), Time: 1.7s | Val TotalLoss: 15.1862, (ReconX: 0.9946, KLD: 0.0617, LossY: 1.4130)\n",
            "2025-10-23 22:42:58,372 - VAETrainer - INFO - Epoch [91/100] Train TotalLoss: 15.5859, (ReconX: 0.9845, KLD: 0.0613*1.00, LossY: 1.4540*10.00), Time: 1.8s | Val TotalLoss: 14.8816, (ReconX: 0.9953, KLD: 0.0613, LossY: 1.3825)\n",
            "INFO:VAETrainer:Epoch [91/100] Train TotalLoss: 15.5859, (ReconX: 0.9845, KLD: 0.0613*1.00, LossY: 1.4540*10.00), Time: 1.8s | Val TotalLoss: 14.8816, (ReconX: 0.9953, KLD: 0.0613, LossY: 1.3825)\n",
            "2025-10-23 22:42:58,374 - VAETrainer - INFO - Saving model and trainer state to vae_results/best_model.pth...\n",
            "INFO:VAETrainer:Saving model and trainer state to vae_results/best_model.pth...\n",
            "2025-10-23 22:42:58,389 - VAETrainer - INFO - Successfully saved model and trainer state to vae_results/best_model.pth\n",
            "INFO:VAETrainer:Successfully saved model and trainer state to vae_results/best_model.pth\n",
            "2025-10-23 22:43:00,458 - VAETrainer - INFO - Epoch [92/100] Train TotalLoss: 15.5917, (ReconX: 0.9841, KLD: 0.0609*1.00, LossY: 1.4547*10.00), Time: 1.7s | Val TotalLoss: 15.6670, (ReconX: 0.9943, KLD: 0.0606, LossY: 1.4612)\n",
            "INFO:VAETrainer:Epoch [92/100] Train TotalLoss: 15.5917, (ReconX: 0.9841, KLD: 0.0609*1.00, LossY: 1.4547*10.00), Time: 1.7s | Val TotalLoss: 15.6670, (ReconX: 0.9943, KLD: 0.0606, LossY: 1.4612)\n",
            "2025-10-23 22:43:02,557 - VAETrainer - INFO - Epoch [93/100] Train TotalLoss: 15.4190, (ReconX: 0.9851, KLD: 0.0601*1.00, LossY: 1.4374*10.00), Time: 1.7s | Val TotalLoss: 14.7710, (ReconX: 0.9957, KLD: 0.0600, LossY: 1.3715)\n",
            "INFO:VAETrainer:Epoch [93/100] Train TotalLoss: 15.4190, (ReconX: 0.9851, KLD: 0.0601*1.00, LossY: 1.4374*10.00), Time: 1.7s | Val TotalLoss: 14.7710, (ReconX: 0.9957, KLD: 0.0600, LossY: 1.3715)\n",
            "2025-10-23 22:43:02,559 - VAETrainer - INFO - Saving model and trainer state to vae_results/best_model.pth...\n",
            "INFO:VAETrainer:Saving model and trainer state to vae_results/best_model.pth...\n",
            "2025-10-23 22:43:02,573 - VAETrainer - INFO - Successfully saved model and trainer state to vae_results/best_model.pth\n",
            "INFO:VAETrainer:Successfully saved model and trainer state to vae_results/best_model.pth\n",
            "2025-10-23 22:43:04,739 - VAETrainer - INFO - Epoch [94/100] Train TotalLoss: 15.5009, (ReconX: 0.9845, KLD: 0.0591*1.00, LossY: 1.4457*10.00), Time: 1.8s | Val TotalLoss: 14.7512, (ReconX: 0.9963, KLD: 0.0604, LossY: 1.3695)\n",
            "INFO:VAETrainer:Epoch [94/100] Train TotalLoss: 15.5009, (ReconX: 0.9845, KLD: 0.0591*1.00, LossY: 1.4457*10.00), Time: 1.8s | Val TotalLoss: 14.7512, (ReconX: 0.9963, KLD: 0.0604, LossY: 1.3695)\n",
            "2025-10-23 22:43:04,741 - VAETrainer - INFO - Saving model and trainer state to vae_results/best_model.pth...\n",
            "INFO:VAETrainer:Saving model and trainer state to vae_results/best_model.pth...\n",
            "2025-10-23 22:43:04,754 - VAETrainer - INFO - Successfully saved model and trainer state to vae_results/best_model.pth\n",
            "INFO:VAETrainer:Successfully saved model and trainer state to vae_results/best_model.pth\n",
            "2025-10-23 22:43:06,924 - VAETrainer - INFO - Epoch [95/100] Train TotalLoss: 15.3784, (ReconX: 0.9856, KLD: 0.0592*1.00, LossY: 1.4334*10.00), Time: 1.8s | Val TotalLoss: 14.6333, (ReconX: 0.9948, KLD: 0.0584, LossY: 1.3580)\n",
            "INFO:VAETrainer:Epoch [95/100] Train TotalLoss: 15.3784, (ReconX: 0.9856, KLD: 0.0592*1.00, LossY: 1.4334*10.00), Time: 1.8s | Val TotalLoss: 14.6333, (ReconX: 0.9948, KLD: 0.0584, LossY: 1.3580)\n",
            "2025-10-23 22:43:06,925 - VAETrainer - INFO - Saving model and trainer state to vae_results/best_model.pth...\n",
            "INFO:VAETrainer:Saving model and trainer state to vae_results/best_model.pth...\n",
            "2025-10-23 22:43:06,939 - VAETrainer - INFO - Successfully saved model and trainer state to vae_results/best_model.pth\n",
            "INFO:VAETrainer:Successfully saved model and trainer state to vae_results/best_model.pth\n",
            "2025-10-23 22:43:09,065 - VAETrainer - INFO - Epoch [96/100] Train TotalLoss: 15.3821, (ReconX: 0.9853, KLD: 0.0574*1.00, LossY: 1.4339*10.00), Time: 1.7s | Val TotalLoss: 16.3670, (ReconX: 0.9948, KLD: 0.0569, LossY: 1.5315)\n",
            "INFO:VAETrainer:Epoch [96/100] Train TotalLoss: 15.3821, (ReconX: 0.9853, KLD: 0.0574*1.00, LossY: 1.4339*10.00), Time: 1.7s | Val TotalLoss: 16.3670, (ReconX: 0.9948, KLD: 0.0569, LossY: 1.5315)\n",
            "2025-10-23 22:43:11,262 - VAETrainer - INFO - Epoch [97/100] Train TotalLoss: 15.5541, (ReconX: 0.9853, KLD: 0.0581*1.00, LossY: 1.4511*10.00), Time: 1.8s | Val TotalLoss: 16.3908, (ReconX: 0.9948, KLD: 0.0578, LossY: 1.5338)\n",
            "INFO:VAETrainer:Epoch [97/100] Train TotalLoss: 15.5541, (ReconX: 0.9853, KLD: 0.0581*1.00, LossY: 1.4511*10.00), Time: 1.8s | Val TotalLoss: 16.3908, (ReconX: 0.9948, KLD: 0.0578, LossY: 1.5338)\n",
            "2025-10-23 22:43:13,313 - VAETrainer - INFO - Epoch [98/100] Train TotalLoss: 15.2601, (ReconX: 0.9853, KLD: 0.0577*1.00, LossY: 1.4217*10.00), Time: 1.7s | Val TotalLoss: 14.5589, (ReconX: 0.9959, KLD: 0.0572, LossY: 1.3506)\n",
            "INFO:VAETrainer:Epoch [98/100] Train TotalLoss: 15.2601, (ReconX: 0.9853, KLD: 0.0577*1.00, LossY: 1.4217*10.00), Time: 1.7s | Val TotalLoss: 14.5589, (ReconX: 0.9959, KLD: 0.0572, LossY: 1.3506)\n",
            "2025-10-23 22:43:13,315 - VAETrainer - INFO - Saving model and trainer state to vae_results/best_model.pth...\n",
            "INFO:VAETrainer:Saving model and trainer state to vae_results/best_model.pth...\n",
            "2025-10-23 22:43:13,329 - VAETrainer - INFO - Successfully saved model and trainer state to vae_results/best_model.pth\n",
            "INFO:VAETrainer:Successfully saved model and trainer state to vae_results/best_model.pth\n",
            "2025-10-23 22:43:15,434 - VAETrainer - INFO - Epoch [99/100] Train TotalLoss: 15.2699, (ReconX: 0.9854, KLD: 0.0567*1.00, LossY: 1.4228*10.00), Time: 1.7s | Val TotalLoss: 14.8205, (ReconX: 0.9961, KLD: 0.0584, LossY: 1.3766)\n",
            "INFO:VAETrainer:Epoch [99/100] Train TotalLoss: 15.2699, (ReconX: 0.9854, KLD: 0.0567*1.00, LossY: 1.4228*10.00), Time: 1.7s | Val TotalLoss: 14.8205, (ReconX: 0.9961, KLD: 0.0584, LossY: 1.3766)\n",
            "2025-10-23 22:43:17,555 - VAETrainer - INFO - Epoch [100/100] Train TotalLoss: 15.2695, (ReconX: 0.9850, KLD: 0.0572*1.00, LossY: 1.4227*10.00), Time: 1.7s | Val TotalLoss: 14.4785, (ReconX: 0.9953, KLD: 0.0575, LossY: 1.3426)\n",
            "INFO:VAETrainer:Epoch [100/100] Train TotalLoss: 15.2695, (ReconX: 0.9850, KLD: 0.0572*1.00, LossY: 1.4227*10.00), Time: 1.7s | Val TotalLoss: 14.4785, (ReconX: 0.9953, KLD: 0.0575, LossY: 1.3426)\n",
            "2025-10-23 22:43:17,557 - VAETrainer - INFO - Saving model and trainer state to vae_results/best_model.pth...\n",
            "INFO:VAETrainer:Saving model and trainer state to vae_results/best_model.pth...\n",
            "2025-10-23 22:43:17,570 - VAETrainer - INFO - Successfully saved model and trainer state to vae_results/best_model.pth\n",
            "INFO:VAETrainer:Successfully saved model and trainer state to vae_results/best_model.pth\n",
            "2025-10-23 22:43:17,572 - VAETrainer - INFO - Training loop finished.\n",
            "INFO:VAETrainer:Training loop finished.\n",
            "2025-10-23 22:43:17,573 - VAETrainer - INFO - Loading best model state from 'vae_results/best_model.pth' (Val Loss: 14.4785).\n",
            "INFO:VAETrainer:Loading best model state from 'vae_results/best_model.pth' (Val Loss: 14.4785).\n",
            "2025-10-23 22:43:17,574 - VAETrainer_Loader - INFO - Loading VAE model and trainer state from vae_results/best_model.pth to device cuda...\n",
            "INFO:VAETrainer_Loader:Loading VAE model and trainer state from vae_results/best_model.pth to device cuda...\n",
            "2025-10-23 22:43:17,620 - SemiSupMIWAE(X:42,Y:1,Lat:3) - INFO - SemiSupMIWAE Initialized: InputXDim=42, TargetYDim=1, LatentDim=3, AlphaPriceLoss=10.0, Device=cuda\n",
            "INFO:SemiSupMIWAE(X:42,Y:1,Lat:3):SemiSupMIWAE Initialized: InputXDim=42, TargetYDim=1, LatentDim=3, AlphaPriceLoss=10.0, Device=cuda\n",
            "2025-10-23 22:43:17,624 - VAETrainer_Loader - INFO - SemiSupMIWAE model created and state loaded successfully.\n",
            "INFO:VAETrainer_Loader:SemiSupMIWAE model created and state loaded successfully.\n",
            "2025-10-23 22:43:17,626 - VAETrainer - INFO - VAETrainer initialized for SemiSupMIWAE. Device: cuda. Optimizer: adam. LR: 0.0003. ReconLoss(X): 'mse'. Alpha(Y Loss): 10.0. LossType(Y): 'gaussian_nll'\n",
            "INFO:VAETrainer:VAETrainer initialized for SemiSupMIWAE. Device: cuda. Optimizer: adam. LR: 0.0003. ReconLoss(X): 'mse'. Alpha(Y Loss): 10.0. LossType(Y): 'gaussian_nll'\n",
            "2025-10-23 22:43:17,628 - VAETrainer_Loader - INFO - VAETrainer instance created and state loaded for model from vae_results/best_model.pth.\n",
            "INFO:VAETrainer_Loader:VAETrainer instance created and state loaded for model from vae_results/best_model.pth.\n",
            "2025-10-23 22:43:17,637 - VAETrainer - INFO - Predicting target (Y) mean and log-variance for 12271 samples...\n",
            "INFO:VAETrainer:Predicting target (Y) mean and log-variance for 12271 samples...\n",
            "2025-10-23 22:43:17,709 - MainRealEstateAnalysis_MIWAE - INFO - [Fold 5] Stored test predictions for 12271 rows.\n",
            "INFO:MainRealEstateAnalysis_MIWAE:[Fold 5] Stored test predictions for 12271 rows.\n",
            "2025-10-23 22:43:17,723 - MainRealEstateAnalysis_MIWAE - INFO - [Fold 6/10] sizes → Train=98170 (80.0%), Val=12271 (10.0%), Test=12271 (10.0%)\n",
            "INFO:MainRealEstateAnalysis_MIWAE:[Fold 6/10] sizes → Train=98170 (80.0%), Val=12271 (10.0%), Test=12271 (10.0%)\n",
            "2025-10-23 22:43:17,725 - __main__.create_vae_from_artifacts - INFO - Creating VAE model. Target column for supervision: log_sale_price.\n",
            "2025-10-23 22:43:17,725 - __main__.create_vae_from_artifacts - INFO - Creating VAE model. Target column for supervision: log_sale_price.\n",
            "INFO:__main__.create_vae_from_artifacts:Creating VAE model. Target column for supervision: log_sale_price.\n",
            "2025-10-23 22:43:17,726 - __main__.create_vae_from_artifacts - INFO - Number of input features for VAE (X): 42 (Features: ['lotarea', 'assess_total', 'exempt_total', 'assessland', 'exemptland']...)\n",
            "2025-10-23 22:43:17,726 - __main__.create_vae_from_artifacts - INFO - Number of input features for VAE (X): 42 (Features: ['lotarea', 'assess_total', 'exempt_total', 'assessland', 'exemptland']...)\n",
            "INFO:__main__.create_vae_from_artifacts:Number of input features for VAE (X): 42 (Features: ['lotarea', 'assess_total', 'exempt_total', 'assessland', 'exemptland']...)\n",
            "2025-10-23 22:43:17,727 - __main__.create_vae_from_artifacts - INFO - Semi-supervised mode: Target 'log_sale_price' (y_dim=1). Alpha for price loss: 10.0.\n",
            "2025-10-23 22:43:17,727 - __main__.create_vae_from_artifacts - INFO - Semi-supervised mode: Target 'log_sale_price' (y_dim=1). Alpha for price loss: 10.0.\n",
            "INFO:__main__.create_vae_from_artifacts:Semi-supervised mode: Target 'log_sale_price' (y_dim=1). Alpha for price loss: 10.0.\n",
            "2025-10-23 22:43:17,729 - __main__.create_vae_from_artifacts - INFO - Using overridden latent_dim: 3\n",
            "2025-10-23 22:43:17,729 - __main__.create_vae_from_artifacts - INFO - Using overridden latent_dim: 3\n",
            "INFO:__main__.create_vae_from_artifacts:Using overridden latent_dim: 3\n",
            "2025-10-23 22:43:17,730 - __main__.create_vae_from_artifacts - INFO - Final effective Latent Dimension: 3\n",
            "2025-10-23 22:43:17,730 - __main__.create_vae_from_artifacts - INFO - Final effective Latent Dimension: 3\n",
            "INFO:__main__.create_vae_from_artifacts:Final effective Latent Dimension: 3\n",
            "2025-10-23 22:43:17,731 - __main__.create_vae_from_artifacts - INFO - Encoder layers: [21], Decoder layers: [21]\n",
            "2025-10-23 22:43:17,731 - __main__.create_vae_from_artifacts - INFO - Encoder layers: [21], Decoder layers: [21]\n",
            "INFO:__main__.create_vae_from_artifacts:Encoder layers: [21], Decoder layers: [21]\n",
            "2025-10-23 22:43:17,735 - __main__.create_vae_from_artifacts - INFO - Using determined prior: Type=student_t_mixture, K=2 (from overrides or artifact inference).\n",
            "2025-10-23 22:43:17,735 - __main__.create_vae_from_artifacts - INFO - Using determined prior: Type=student_t_mixture, K=2 (from overrides or artifact inference).\n",
            "INFO:__main__.create_vae_from_artifacts:Using determined prior: Type=student_t_mixture, K=2 (from overrides or artifact inference).\n",
            "2025-10-23 22:43:17,736 - __main__.create_vae_from_artifacts - INFO - Initializing parameters for student_t_mixture with K=2 components...\n",
            "2025-10-23 22:43:17,736 - __main__.create_vae_from_artifacts - INFO - Initializing parameters for student_t_mixture with K=2 components...\n",
            "INFO:__main__.create_vae_from_artifacts:Initializing parameters for student_t_mixture with K=2 components...\n",
            "2025-10-23 22:43:17,737 - __main__.create_vae_from_artifacts - WARNING - DP-GMM artifacts are unsuitable or insufficient for initializing mixture prior parameters. Default parameters (e.g., zeros for means/logits, identity for covariances) will be used by the VAE.\n",
            "2025-10-23 22:43:17,737 - __main__.create_vae_from_artifacts - WARNING - DP-GMM artifacts are unsuitable or insufficient for initializing mixture prior parameters. Default parameters (e.g., zeros for means/logits, identity for covariances) will be used by the VAE.\n",
            "WARNING:__main__.create_vae_from_artifacts:DP-GMM artifacts are unsuitable or insufficient for initializing mixture prior parameters. Default parameters (e.g., zeros for means/logits, identity for covariances) will be used by the VAE.\n",
            "2025-10-23 22:43:17,738 - __main__.create_vae_from_artifacts - INFO - Setting Student-t df to 4.0 for prior.\n",
            "2025-10-23 22:43:17,738 - __main__.create_vae_from_artifacts - INFO - Setting Student-t df to 4.0 for prior.\n",
            "INFO:__main__.create_vae_from_artifacts:Setting Student-t df to 4.0 for prior.\n",
            "2025-10-23 22:43:17,739 - __main__.create_vae_from_artifacts - INFO - Instantiating SemiSupMIWAE model...\n",
            "2025-10-23 22:43:17,739 - __main__.create_vae_from_artifacts - INFO - Instantiating SemiSupMIWAE model...\n",
            "INFO:__main__.create_vae_from_artifacts:Instantiating SemiSupMIWAE model...\n",
            "2025-10-23 22:43:17,741 - __main__.create_vae_from_artifacts - INFO - Price head layers for SemiSupMIWAE: [8, 4]\n",
            "2025-10-23 22:43:17,741 - __main__.create_vae_from_artifacts - INFO - Price head layers for SemiSupMIWAE: [8, 4]\n",
            "INFO:__main__.create_vae_from_artifacts:Price head layers for SemiSupMIWAE: [8, 4]\n",
            "2025-10-23 22:43:17,748 - SemiSupMIWAE(X:42,Y:1,Lat:3) - INFO - SemiSupMIWAE Initialized: InputXDim=42, TargetYDim=1, LatentDim=3, AlphaPriceLoss=10.0, Device=cuda\n",
            "INFO:SemiSupMIWAE(X:42,Y:1,Lat:3):SemiSupMIWAE Initialized: InputXDim=42, TargetYDim=1, LatentDim=3, AlphaPriceLoss=10.0, Device=cuda\n",
            "2025-10-23 22:43:17,749 - __main__.create_vae_from_artifacts - INFO - SemiSupMIWAE model created successfully on device: cuda.\n",
            "2025-10-23 22:43:17,749 - __main__.create_vae_from_artifacts - INFO - SemiSupMIWAE model created successfully on device: cuda.\n",
            "INFO:__main__.create_vae_from_artifacts:SemiSupMIWAE model created successfully on device: cuda.\n",
            "2025-10-23 22:43:17,752 - VAETrainer - INFO - VAETrainer initialized for SemiSupMIWAE. Device: cuda. Optimizer: adam. LR: 0.0003. ReconLoss(X): 'mse'. Alpha(Y Loss): 10.0. LossType(Y): 'gaussian_nll'\n",
            "INFO:VAETrainer:VAETrainer initialized for SemiSupMIWAE. Device: cuda. Optimizer: adam. LR: 0.0003. ReconLoss(X): 'mse'. Alpha(Y Loss): 10.0. LossType(Y): 'gaussian_nll'\n",
            "2025-10-23 22:43:17,753 - VAETrainer - INFO - Starting training: Epochs=100, BatchSize=512, KLD_weight=1.0, KLD_AnnealEpochs=30, Alpha(Y_Loss)=10.0.\n",
            "INFO:VAETrainer:Starting training: Epochs=100, BatchSize=512, KLD_weight=1.0, KLD_AnnealEpochs=30, Alpha(Y_Loss)=10.0.\n",
            "2025-10-23 22:43:17,754 - VAETrainer - INFO - Train loader: 98170 samples, 192 batches. Drop last: False\n",
            "INFO:VAETrainer:Train loader: 98170 samples, 192 batches. Drop last: False\n",
            "2025-10-23 22:43:17,755 - VAETrainer - INFO - Validation loader: 12271 samples, 24 batches.\n",
            "INFO:VAETrainer:Validation loader: 12271 samples, 24 batches.\n",
            "2025-10-23 22:43:19,869 - VAETrainer - INFO - Epoch [1/100] Train TotalLoss: 535.0432, (ReconX: 1.0881, KLD: 0.3487*0.03, LossY: 53.3944*10.00), Time: 1.7s | Val TotalLoss: 418.5473, (ReconX: 1.0217, KLD: 0.4217, LossY: 41.7104)\n",
            "INFO:VAETrainer:Epoch [1/100] Train TotalLoss: 535.0432, (ReconX: 1.0881, KLD: 0.3487*0.03, LossY: 53.3944*10.00), Time: 1.7s | Val TotalLoss: 418.5473, (ReconX: 1.0217, KLD: 0.4217, LossY: 41.7104)\n",
            "2025-10-23 22:43:19,871 - VAETrainer - INFO - Saving model and trainer state to vae_results/best_model.pth...\n",
            "INFO:VAETrainer:Saving model and trainer state to vae_results/best_model.pth...\n",
            "2025-10-23 22:43:19,884 - VAETrainer - INFO - Successfully saved model and trainer state to vae_results/best_model.pth\n",
            "INFO:VAETrainer:Successfully saved model and trainer state to vae_results/best_model.pth\n",
            "2025-10-23 22:43:22,085 - VAETrainer - INFO - Epoch [2/100] Train TotalLoss: 375.7315, (ReconX: 0.9863, KLD: 0.4564*0.07, LossY: 37.4715*10.00), Time: 1.8s | Val TotalLoss: 339.1406, (ReconX: 0.9643, KLD: 0.4903, LossY: 33.7686)\n",
            "INFO:VAETrainer:Epoch [2/100] Train TotalLoss: 375.7315, (ReconX: 0.9863, KLD: 0.4564*0.07, LossY: 37.4715*10.00), Time: 1.8s | Val TotalLoss: 339.1406, (ReconX: 0.9643, KLD: 0.4903, LossY: 33.7686)\n",
            "2025-10-23 22:43:22,087 - VAETrainer - INFO - Saving model and trainer state to vae_results/best_model.pth...\n",
            "INFO:VAETrainer:Saving model and trainer state to vae_results/best_model.pth...\n",
            "2025-10-23 22:43:22,100 - VAETrainer - INFO - Successfully saved model and trainer state to vae_results/best_model.pth\n",
            "INFO:VAETrainer:Successfully saved model and trainer state to vae_results/best_model.pth\n",
            "2025-10-23 22:43:24,229 - VAETrainer - INFO - Epoch [3/100] Train TotalLoss: 314.8005, (ReconX: 0.9457, KLD: 0.5019*0.10, LossY: 31.3805*10.00), Time: 1.7s | Val TotalLoss: 291.6993, (ReconX: 0.9344, KLD: 0.5300, LossY: 29.0235)\n",
            "INFO:VAETrainer:Epoch [3/100] Train TotalLoss: 314.8005, (ReconX: 0.9457, KLD: 0.5019*0.10, LossY: 31.3805*10.00), Time: 1.7s | Val TotalLoss: 291.6993, (ReconX: 0.9344, KLD: 0.5300, LossY: 29.0235)\n",
            "2025-10-23 22:43:24,230 - VAETrainer - INFO - Saving model and trainer state to vae_results/best_model.pth...\n",
            "INFO:VAETrainer:Saving model and trainer state to vae_results/best_model.pth...\n",
            "2025-10-23 22:43:24,243 - VAETrainer - INFO - Successfully saved model and trainer state to vae_results/best_model.pth\n",
            "INFO:VAETrainer:Successfully saved model and trainer state to vae_results/best_model.pth\n",
            "2025-10-23 22:43:26,403 - VAETrainer - INFO - Epoch [4/100] Train TotalLoss: 268.5638, (ReconX: 0.9260, KLD: 0.5146*0.13, LossY: 26.7569*10.00), Time: 1.8s | Val TotalLoss: 251.1598, (ReconX: 0.9224, KLD: 0.5280, LossY: 24.9709)\n",
            "INFO:VAETrainer:Epoch [4/100] Train TotalLoss: 268.5638, (ReconX: 0.9260, KLD: 0.5146*0.13, LossY: 26.7569*10.00), Time: 1.8s | Val TotalLoss: 251.1598, (ReconX: 0.9224, KLD: 0.5280, LossY: 24.9709)\n",
            "2025-10-23 22:43:26,404 - VAETrainer - INFO - Saving model and trainer state to vae_results/best_model.pth...\n",
            "INFO:VAETrainer:Saving model and trainer state to vae_results/best_model.pth...\n",
            "2025-10-23 22:43:26,417 - VAETrainer - INFO - Successfully saved model and trainer state to vae_results/best_model.pth\n",
            "INFO:VAETrainer:Successfully saved model and trainer state to vae_results/best_model.pth\n",
            "2025-10-23 22:43:28,512 - VAETrainer - INFO - Epoch [5/100] Train TotalLoss: 229.9215, (ReconX: 0.9193, KLD: 0.5160*0.17, LossY: 22.8916*10.00), Time: 1.7s | Val TotalLoss: 214.5997, (ReconX: 0.9192, KLD: 0.5227, LossY: 21.3158)\n",
            "INFO:VAETrainer:Epoch [5/100] Train TotalLoss: 229.9215, (ReconX: 0.9193, KLD: 0.5160*0.17, LossY: 22.8916*10.00), Time: 1.7s | Val TotalLoss: 214.5997, (ReconX: 0.9192, KLD: 0.5227, LossY: 21.3158)\n",
            "2025-10-23 22:43:28,514 - VAETrainer - INFO - Saving model and trainer state to vae_results/best_model.pth...\n",
            "INFO:VAETrainer:Saving model and trainer state to vae_results/best_model.pth...\n",
            "2025-10-23 22:43:28,526 - VAETrainer - INFO - Successfully saved model and trainer state to vae_results/best_model.pth\n",
            "INFO:VAETrainer:Successfully saved model and trainer state to vae_results/best_model.pth\n",
            "2025-10-23 22:43:30,655 - VAETrainer - INFO - Epoch [6/100] Train TotalLoss: 197.8239, (ReconX: 0.9169, KLD: 0.5144*0.20, LossY: 19.6804*10.00), Time: 1.8s | Val TotalLoss: 189.1592, (ReconX: 0.9171, KLD: 0.5186, LossY: 18.7724)\n",
            "INFO:VAETrainer:Epoch [6/100] Train TotalLoss: 197.8239, (ReconX: 0.9169, KLD: 0.5144*0.20, LossY: 19.6804*10.00), Time: 1.8s | Val TotalLoss: 189.1592, (ReconX: 0.9171, KLD: 0.5186, LossY: 18.7724)\n",
            "2025-10-23 22:43:30,657 - VAETrainer - INFO - Saving model and trainer state to vae_results/best_model.pth...\n",
            "INFO:VAETrainer:Saving model and trainer state to vae_results/best_model.pth...\n",
            "2025-10-23 22:43:30,670 - VAETrainer - INFO - Successfully saved model and trainer state to vae_results/best_model.pth\n",
            "INFO:VAETrainer:Successfully saved model and trainer state to vae_results/best_model.pth\n",
            "2025-10-23 22:43:32,723 - VAETrainer - INFO - Epoch [7/100] Train TotalLoss: 171.2821, (ReconX: 0.9156, KLD: 0.5117*0.23, LossY: 17.0247*10.00), Time: 1.7s | Val TotalLoss: 162.0081, (ReconX: 0.9151, KLD: 0.5183, LossY: 16.0575)\n",
            "INFO:VAETrainer:Epoch [7/100] Train TotalLoss: 171.2821, (ReconX: 0.9156, KLD: 0.5117*0.23, LossY: 17.0247*10.00), Time: 1.7s | Val TotalLoss: 162.0081, (ReconX: 0.9151, KLD: 0.5183, LossY: 16.0575)\n",
            "2025-10-23 22:43:32,725 - VAETrainer - INFO - Saving model and trainer state to vae_results/best_model.pth...\n",
            "INFO:VAETrainer:Saving model and trainer state to vae_results/best_model.pth...\n",
            "2025-10-23 22:43:32,742 - VAETrainer - INFO - Successfully saved model and trainer state to vae_results/best_model.pth\n",
            "INFO:VAETrainer:Successfully saved model and trainer state to vae_results/best_model.pth\n",
            "2025-10-23 22:43:34,925 - VAETrainer - INFO - Epoch [8/100] Train TotalLoss: 149.3249, (ReconX: 0.9147, KLD: 0.5127*0.27, LossY: 14.8273*10.00), Time: 1.8s | Val TotalLoss: 141.8576, (ReconX: 0.9158, KLD: 0.5123, LossY: 14.0429)\n",
            "INFO:VAETrainer:Epoch [8/100] Train TotalLoss: 149.3249, (ReconX: 0.9147, KLD: 0.5127*0.27, LossY: 14.8273*10.00), Time: 1.8s | Val TotalLoss: 141.8576, (ReconX: 0.9158, KLD: 0.5123, LossY: 14.0429)\n",
            "2025-10-23 22:43:34,927 - VAETrainer - INFO - Saving model and trainer state to vae_results/best_model.pth...\n",
            "INFO:VAETrainer:Saving model and trainer state to vae_results/best_model.pth...\n",
            "2025-10-23 22:43:34,940 - VAETrainer - INFO - Successfully saved model and trainer state to vae_results/best_model.pth\n",
            "INFO:VAETrainer:Successfully saved model and trainer state to vae_results/best_model.pth\n",
            "2025-10-23 22:43:37,117 - VAETrainer - INFO - Epoch [9/100] Train TotalLoss: 130.9947, (ReconX: 0.9136, KLD: 0.5131*0.30, LossY: 12.9927*10.00), Time: 1.8s | Val TotalLoss: 126.7247, (ReconX: 0.9154, KLD: 0.5182, LossY: 12.5291)\n",
            "INFO:VAETrainer:Epoch [9/100] Train TotalLoss: 130.9947, (ReconX: 0.9136, KLD: 0.5131*0.30, LossY: 12.9927*10.00), Time: 1.8s | Val TotalLoss: 126.7247, (ReconX: 0.9154, KLD: 0.5182, LossY: 12.5291)\n",
            "2025-10-23 22:43:37,119 - VAETrainer - INFO - Saving model and trainer state to vae_results/best_model.pth...\n",
            "INFO:VAETrainer:Saving model and trainer state to vae_results/best_model.pth...\n",
            "2025-10-23 22:43:37,133 - VAETrainer - INFO - Successfully saved model and trainer state to vae_results/best_model.pth\n",
            "INFO:VAETrainer:Successfully saved model and trainer state to vae_results/best_model.pth\n",
            "2025-10-23 22:43:39,264 - VAETrainer - INFO - Epoch [10/100] Train TotalLoss: 115.8447, (ReconX: 0.9132, KLD: 0.5176*0.33, LossY: 11.4759*10.00), Time: 1.8s | Val TotalLoss: 111.7867, (ReconX: 0.9133, KLD: 0.5325, LossY: 11.0341)\n",
            "INFO:VAETrainer:Epoch [10/100] Train TotalLoss: 115.8447, (ReconX: 0.9132, KLD: 0.5176*0.33, LossY: 11.4759*10.00), Time: 1.8s | Val TotalLoss: 111.7867, (ReconX: 0.9133, KLD: 0.5325, LossY: 11.0341)\n",
            "2025-10-23 22:43:39,266 - VAETrainer - INFO - Saving model and trainer state to vae_results/best_model.pth...\n",
            "INFO:VAETrainer:Saving model and trainer state to vae_results/best_model.pth...\n",
            "2025-10-23 22:43:39,279 - VAETrainer - INFO - Successfully saved model and trainer state to vae_results/best_model.pth\n",
            "INFO:VAETrainer:Successfully saved model and trainer state to vae_results/best_model.pth\n",
            "2025-10-23 22:43:41,379 - VAETrainer - INFO - Epoch [11/100] Train TotalLoss: 103.1679, (ReconX: 0.9129, KLD: 0.5232*0.37, LossY: 10.2063*10.00), Time: 1.7s | Val TotalLoss: 101.3968, (ReconX: 0.9154, KLD: 0.5249, LossY: 9.9956)\n",
            "INFO:VAETrainer:Epoch [11/100] Train TotalLoss: 103.1679, (ReconX: 0.9129, KLD: 0.5232*0.37, LossY: 10.2063*10.00), Time: 1.7s | Val TotalLoss: 101.3968, (ReconX: 0.9154, KLD: 0.5249, LossY: 9.9956)\n",
            "2025-10-23 22:43:41,381 - VAETrainer - INFO - Saving model and trainer state to vae_results/best_model.pth...\n",
            "INFO:VAETrainer:Saving model and trainer state to vae_results/best_model.pth...\n",
            "2025-10-23 22:43:41,393 - VAETrainer - INFO - Successfully saved model and trainer state to vae_results/best_model.pth\n",
            "INFO:VAETrainer:Successfully saved model and trainer state to vae_results/best_model.pth\n",
            "2025-10-23 22:43:43,450 - VAETrainer - INFO - Epoch [12/100] Train TotalLoss: 92.3786, (ReconX: 0.9145, KLD: 0.5275*0.40, LossY: 9.1253*10.00), Time: 1.7s | Val TotalLoss: 89.9312, (ReconX: 0.9146, KLD: 0.5195, LossY: 8.8497)\n",
            "INFO:VAETrainer:Epoch [12/100] Train TotalLoss: 92.3786, (ReconX: 0.9145, KLD: 0.5275*0.40, LossY: 9.1253*10.00), Time: 1.7s | Val TotalLoss: 89.9312, (ReconX: 0.9146, KLD: 0.5195, LossY: 8.8497)\n",
            "2025-10-23 22:43:43,452 - VAETrainer - INFO - Saving model and trainer state to vae_results/best_model.pth...\n",
            "INFO:VAETrainer:Saving model and trainer state to vae_results/best_model.pth...\n",
            "2025-10-23 22:43:43,466 - VAETrainer - INFO - Successfully saved model and trainer state to vae_results/best_model.pth\n",
            "INFO:VAETrainer:Successfully saved model and trainer state to vae_results/best_model.pth\n",
            "2025-10-23 22:43:45,652 - VAETrainer - INFO - Epoch [13/100] Train TotalLoss: 83.5215, (ReconX: 0.9133, KLD: 0.5317*0.43, LossY: 8.2378*10.00), Time: 1.8s | Val TotalLoss: 82.5521, (ReconX: 0.9154, KLD: 0.5368, LossY: 8.1100)\n",
            "INFO:VAETrainer:Epoch [13/100] Train TotalLoss: 83.5215, (ReconX: 0.9133, KLD: 0.5317*0.43, LossY: 8.2378*10.00), Time: 1.8s | Val TotalLoss: 82.5521, (ReconX: 0.9154, KLD: 0.5368, LossY: 8.1100)\n",
            "2025-10-23 22:43:45,654 - VAETrainer - INFO - Saving model and trainer state to vae_results/best_model.pth...\n",
            "INFO:VAETrainer:Saving model and trainer state to vae_results/best_model.pth...\n",
            "2025-10-23 22:43:45,667 - VAETrainer - INFO - Successfully saved model and trainer state to vae_results/best_model.pth\n",
            "INFO:VAETrainer:Successfully saved model and trainer state to vae_results/best_model.pth\n",
            "2025-10-23 22:43:47,848 - VAETrainer - INFO - Epoch [14/100] Train TotalLoss: 75.9485, (ReconX: 0.9140, KLD: 0.5341*0.47, LossY: 7.4785*10.00), Time: 1.8s | Val TotalLoss: 73.2196, (ReconX: 0.9167, KLD: 0.5344, LossY: 7.1768)\n",
            "INFO:VAETrainer:Epoch [14/100] Train TotalLoss: 75.9485, (ReconX: 0.9140, KLD: 0.5341*0.47, LossY: 7.4785*10.00), Time: 1.8s | Val TotalLoss: 73.2196, (ReconX: 0.9167, KLD: 0.5344, LossY: 7.1768)\n",
            "2025-10-23 22:43:47,849 - VAETrainer - INFO - Saving model and trainer state to vae_results/best_model.pth...\n",
            "INFO:VAETrainer:Saving model and trainer state to vae_results/best_model.pth...\n",
            "2025-10-23 22:43:47,862 - VAETrainer - INFO - Successfully saved model and trainer state to vae_results/best_model.pth\n",
            "INFO:VAETrainer:Successfully saved model and trainer state to vae_results/best_model.pth\n",
            "2025-10-23 22:43:50,093 - VAETrainer - INFO - Epoch [15/100] Train TotalLoss: 69.6415, (ReconX: 0.9140, KLD: 0.5340*0.50, LossY: 6.8460*10.00), Time: 1.9s | Val TotalLoss: 68.8321, (ReconX: 0.9165, KLD: 0.5427, LossY: 6.7373)\n",
            "INFO:VAETrainer:Epoch [15/100] Train TotalLoss: 69.6415, (ReconX: 0.9140, KLD: 0.5340*0.50, LossY: 6.8460*10.00), Time: 1.9s | Val TotalLoss: 68.8321, (ReconX: 0.9165, KLD: 0.5427, LossY: 6.7373)\n",
            "2025-10-23 22:43:50,095 - VAETrainer - INFO - Saving model and trainer state to vae_results/best_model.pth...\n",
            "INFO:VAETrainer:Saving model and trainer state to vae_results/best_model.pth...\n",
            "2025-10-23 22:43:50,108 - VAETrainer - INFO - Successfully saved model and trainer state to vae_results/best_model.pth\n",
            "INFO:VAETrainer:Successfully saved model and trainer state to vae_results/best_model.pth\n",
            "2025-10-23 22:43:52,216 - VAETrainer - INFO - Epoch [16/100] Train TotalLoss: 64.2656, (ReconX: 0.9146, KLD: 0.5278*0.53, LossY: 6.3069*10.00), Time: 1.7s | Val TotalLoss: 63.2991, (ReconX: 0.9154, KLD: 0.5391, LossY: 6.1845)\n",
            "INFO:VAETrainer:Epoch [16/100] Train TotalLoss: 64.2656, (ReconX: 0.9146, KLD: 0.5278*0.53, LossY: 6.3069*10.00), Time: 1.7s | Val TotalLoss: 63.2991, (ReconX: 0.9154, KLD: 0.5391, LossY: 6.1845)\n",
            "2025-10-23 22:43:52,218 - VAETrainer - INFO - Saving model and trainer state to vae_results/best_model.pth...\n",
            "INFO:VAETrainer:Saving model and trainer state to vae_results/best_model.pth...\n",
            "2025-10-23 22:43:52,231 - VAETrainer - INFO - Successfully saved model and trainer state to vae_results/best_model.pth\n",
            "INFO:VAETrainer:Successfully saved model and trainer state to vae_results/best_model.pth\n",
            "2025-10-23 22:43:54,316 - VAETrainer - INFO - Epoch [17/100] Train TotalLoss: 59.7816, (ReconX: 0.9148, KLD: 0.5255*0.57, LossY: 5.8569*10.00), Time: 1.7s | Val TotalLoss: 58.8857, (ReconX: 0.9114, KLD: 0.5303, LossY: 5.7444)\n",
            "INFO:VAETrainer:Epoch [17/100] Train TotalLoss: 59.7816, (ReconX: 0.9148, KLD: 0.5255*0.57, LossY: 5.8569*10.00), Time: 1.7s | Val TotalLoss: 58.8857, (ReconX: 0.9114, KLD: 0.5303, LossY: 5.7444)\n",
            "2025-10-23 22:43:54,318 - VAETrainer - INFO - Saving model and trainer state to vae_results/best_model.pth...\n",
            "INFO:VAETrainer:Saving model and trainer state to vae_results/best_model.pth...\n",
            "2025-10-23 22:43:54,332 - VAETrainer - INFO - Successfully saved model and trainer state to vae_results/best_model.pth\n",
            "INFO:VAETrainer:Successfully saved model and trainer state to vae_results/best_model.pth\n",
            "2025-10-23 22:43:56,440 - VAETrainer - INFO - Epoch [18/100] Train TotalLoss: 55.9414, (ReconX: 0.9150, KLD: 0.5196*0.60, LossY: 5.4715*10.00), Time: 1.7s | Val TotalLoss: 54.6701, (ReconX: 0.9168, KLD: 0.5105, LossY: 5.3243)\n",
            "INFO:VAETrainer:Epoch [18/100] Train TotalLoss: 55.9414, (ReconX: 0.9150, KLD: 0.5196*0.60, LossY: 5.4715*10.00), Time: 1.7s | Val TotalLoss: 54.6701, (ReconX: 0.9168, KLD: 0.5105, LossY: 5.3243)\n",
            "2025-10-23 22:43:56,443 - VAETrainer - INFO - Saving model and trainer state to vae_results/best_model.pth...\n",
            "INFO:VAETrainer:Saving model and trainer state to vae_results/best_model.pth...\n",
            "2025-10-23 22:43:56,457 - VAETrainer - INFO - Successfully saved model and trainer state to vae_results/best_model.pth\n",
            "INFO:VAETrainer:Successfully saved model and trainer state to vae_results/best_model.pth\n",
            "2025-10-23 22:43:58,530 - VAETrainer - INFO - Epoch [19/100] Train TotalLoss: 52.7621, (ReconX: 0.9161, KLD: 0.5080*0.63, LossY: 5.1524*10.00), Time: 1.7s | Val TotalLoss: 52.5047, (ReconX: 0.9184, KLD: 0.5104, LossY: 5.1076)\n",
            "INFO:VAETrainer:Epoch [19/100] Train TotalLoss: 52.7621, (ReconX: 0.9161, KLD: 0.5080*0.63, LossY: 5.1524*10.00), Time: 1.7s | Val TotalLoss: 52.5047, (ReconX: 0.9184, KLD: 0.5104, LossY: 5.1076)\n",
            "2025-10-23 22:43:58,531 - VAETrainer - INFO - Saving model and trainer state to vae_results/best_model.pth...\n",
            "INFO:VAETrainer:Saving model and trainer state to vae_results/best_model.pth...\n",
            "2025-10-23 22:43:58,545 - VAETrainer - INFO - Successfully saved model and trainer state to vae_results/best_model.pth\n",
            "INFO:VAETrainer:Successfully saved model and trainer state to vae_results/best_model.pth\n",
            "2025-10-23 22:44:00,756 - VAETrainer - INFO - Epoch [20/100] Train TotalLoss: 50.0807, (ReconX: 0.9177, KLD: 0.4939*0.67, LossY: 4.8834*10.00), Time: 1.8s | Val TotalLoss: 49.3136, (ReconX: 0.9197, KLD: 0.4846, LossY: 4.7909)\n",
            "INFO:VAETrainer:Epoch [20/100] Train TotalLoss: 50.0807, (ReconX: 0.9177, KLD: 0.4939*0.67, LossY: 4.8834*10.00), Time: 1.8s | Val TotalLoss: 49.3136, (ReconX: 0.9197, KLD: 0.4846, LossY: 4.7909)\n",
            "2025-10-23 22:44:00,758 - VAETrainer - INFO - Saving model and trainer state to vae_results/best_model.pth...\n",
            "INFO:VAETrainer:Saving model and trainer state to vae_results/best_model.pth...\n",
            "2025-10-23 22:44:00,771 - VAETrainer - INFO - Successfully saved model and trainer state to vae_results/best_model.pth\n",
            "INFO:VAETrainer:Successfully saved model and trainer state to vae_results/best_model.pth\n",
            "2025-10-23 22:44:02,995 - VAETrainer - INFO - Epoch [21/100] Train TotalLoss: 47.8231, (ReconX: 0.9191, KLD: 0.4745*0.70, LossY: 4.6572*10.00), Time: 1.8s | Val TotalLoss: 47.7258, (ReconX: 0.9225, KLD: 0.4606, LossY: 4.6343)\n",
            "INFO:VAETrainer:Epoch [21/100] Train TotalLoss: 47.8231, (ReconX: 0.9191, KLD: 0.4745*0.70, LossY: 4.6572*10.00), Time: 1.8s | Val TotalLoss: 47.7258, (ReconX: 0.9225, KLD: 0.4606, LossY: 4.6343)\n",
            "2025-10-23 22:44:02,997 - VAETrainer - INFO - Saving model and trainer state to vae_results/best_model.pth...\n",
            "INFO:VAETrainer:Saving model and trainer state to vae_results/best_model.pth...\n",
            "2025-10-23 22:44:03,010 - VAETrainer - INFO - Successfully saved model and trainer state to vae_results/best_model.pth\n",
            "INFO:VAETrainer:Successfully saved model and trainer state to vae_results/best_model.pth\n",
            "2025-10-23 22:44:05,117 - VAETrainer - INFO - Epoch [22/100] Train TotalLoss: 45.8711, (ReconX: 0.9215, KLD: 0.4538*0.73, LossY: 4.4617*10.00), Time: 1.7s | Val TotalLoss: 45.4639, (ReconX: 0.9239, KLD: 0.4417, LossY: 4.4098)\n",
            "INFO:VAETrainer:Epoch [22/100] Train TotalLoss: 45.8711, (ReconX: 0.9215, KLD: 0.4538*0.73, LossY: 4.4617*10.00), Time: 1.7s | Val TotalLoss: 45.4639, (ReconX: 0.9239, KLD: 0.4417, LossY: 4.4098)\n",
            "2025-10-23 22:44:05,119 - VAETrainer - INFO - Saving model and trainer state to vae_results/best_model.pth...\n",
            "INFO:VAETrainer:Saving model and trainer state to vae_results/best_model.pth...\n",
            "2025-10-23 22:44:05,132 - VAETrainer - INFO - Successfully saved model and trainer state to vae_results/best_model.pth\n",
            "INFO:VAETrainer:Successfully saved model and trainer state to vae_results/best_model.pth\n",
            "2025-10-23 22:44:07,312 - VAETrainer - INFO - Epoch [23/100] Train TotalLoss: 44.1633, (ReconX: 0.9245, KLD: 0.4295*0.77, LossY: 4.2909*10.00), Time: 1.8s | Val TotalLoss: 44.2547, (ReconX: 0.9277, KLD: 0.4213, LossY: 4.2906)\n",
            "INFO:VAETrainer:Epoch [23/100] Train TotalLoss: 44.1633, (ReconX: 0.9245, KLD: 0.4295*0.77, LossY: 4.2909*10.00), Time: 1.8s | Val TotalLoss: 44.2547, (ReconX: 0.9277, KLD: 0.4213, LossY: 4.2906)\n",
            "2025-10-23 22:44:07,313 - VAETrainer - INFO - Saving model and trainer state to vae_results/best_model.pth...\n",
            "INFO:VAETrainer:Saving model and trainer state to vae_results/best_model.pth...\n",
            "2025-10-23 22:44:07,327 - VAETrainer - INFO - Successfully saved model and trainer state to vae_results/best_model.pth\n",
            "INFO:VAETrainer:Successfully saved model and trainer state to vae_results/best_model.pth\n",
            "2025-10-23 22:44:09,506 - VAETrainer - INFO - Epoch [24/100] Train TotalLoss: 42.7626, (ReconX: 0.9271, KLD: 0.4068*0.80, LossY: 4.1510*10.00), Time: 1.8s | Val TotalLoss: 42.6935, (ReconX: 0.9306, KLD: 0.3941, LossY: 4.1369)\n",
            "INFO:VAETrainer:Epoch [24/100] Train TotalLoss: 42.7626, (ReconX: 0.9271, KLD: 0.4068*0.80, LossY: 4.1510*10.00), Time: 1.8s | Val TotalLoss: 42.6935, (ReconX: 0.9306, KLD: 0.3941, LossY: 4.1369)\n",
            "2025-10-23 22:44:09,508 - VAETrainer - INFO - Saving model and trainer state to vae_results/best_model.pth...\n",
            "INFO:VAETrainer:Saving model and trainer state to vae_results/best_model.pth...\n",
            "2025-10-23 22:44:09,521 - VAETrainer - INFO - Successfully saved model and trainer state to vae_results/best_model.pth\n",
            "INFO:VAETrainer:Successfully saved model and trainer state to vae_results/best_model.pth\n",
            "2025-10-23 22:44:11,692 - VAETrainer - INFO - Epoch [25/100] Train TotalLoss: 41.6592, (ReconX: 0.9301, KLD: 0.3829*0.83, LossY: 4.0410*10.00), Time: 1.8s | Val TotalLoss: 41.4788, (ReconX: 0.9320, KLD: 0.3737, LossY: 4.0173)\n",
            "INFO:VAETrainer:Epoch [25/100] Train TotalLoss: 41.6592, (ReconX: 0.9301, KLD: 0.3829*0.83, LossY: 4.0410*10.00), Time: 1.8s | Val TotalLoss: 41.4788, (ReconX: 0.9320, KLD: 0.3737, LossY: 4.0173)\n",
            "2025-10-23 22:44:11,694 - VAETrainer - INFO - Saving model and trainer state to vae_results/best_model.pth...\n",
            "INFO:VAETrainer:Saving model and trainer state to vae_results/best_model.pth...\n",
            "2025-10-23 22:44:11,706 - VAETrainer - INFO - Successfully saved model and trainer state to vae_results/best_model.pth\n",
            "INFO:VAETrainer:Successfully saved model and trainer state to vae_results/best_model.pth\n",
            "2025-10-23 22:44:13,888 - VAETrainer - INFO - Epoch [26/100] Train TotalLoss: 40.6552, (ReconX: 0.9323, KLD: 0.3578*0.87, LossY: 3.9413*10.00), Time: 1.8s | Val TotalLoss: 40.7249, (ReconX: 0.9370, KLD: 0.3392, LossY: 3.9449)\n",
            "INFO:VAETrainer:Epoch [26/100] Train TotalLoss: 40.6552, (ReconX: 0.9323, KLD: 0.3578*0.87, LossY: 3.9413*10.00), Time: 1.8s | Val TotalLoss: 40.7249, (ReconX: 0.9370, KLD: 0.3392, LossY: 3.9449)\n",
            "2025-10-23 22:44:13,889 - VAETrainer - INFO - Saving model and trainer state to vae_results/best_model.pth...\n",
            "INFO:VAETrainer:Saving model and trainer state to vae_results/best_model.pth...\n",
            "2025-10-23 22:44:13,903 - VAETrainer - INFO - Successfully saved model and trainer state to vae_results/best_model.pth\n",
            "INFO:VAETrainer:Successfully saved model and trainer state to vae_results/best_model.pth\n",
            "2025-10-23 22:44:16,110 - VAETrainer - INFO - Epoch [27/100] Train TotalLoss: 39.8527, (ReconX: 0.9361, KLD: 0.3298*0.90, LossY: 3.8620*10.00), Time: 1.8s | Val TotalLoss: 39.6261, (ReconX: 0.9398, KLD: 0.3141, LossY: 3.8372)\n",
            "INFO:VAETrainer:Epoch [27/100] Train TotalLoss: 39.8527, (ReconX: 0.9361, KLD: 0.3298*0.90, LossY: 3.8620*10.00), Time: 1.8s | Val TotalLoss: 39.6261, (ReconX: 0.9398, KLD: 0.3141, LossY: 3.8372)\n",
            "2025-10-23 22:44:16,111 - VAETrainer - INFO - Saving model and trainer state to vae_results/best_model.pth...\n",
            "INFO:VAETrainer:Saving model and trainer state to vae_results/best_model.pth...\n",
            "2025-10-23 22:44:16,128 - VAETrainer - INFO - Successfully saved model and trainer state to vae_results/best_model.pth\n",
            "INFO:VAETrainer:Successfully saved model and trainer state to vae_results/best_model.pth\n",
            "2025-10-23 22:44:18,280 - VAETrainer - INFO - Epoch [28/100] Train TotalLoss: 39.1675, (ReconX: 0.9406, KLD: 0.3024*0.93, LossY: 3.7945*10.00), Time: 1.8s | Val TotalLoss: 39.4900, (ReconX: 0.9442, KLD: 0.2902, LossY: 3.8256)\n",
            "INFO:VAETrainer:Epoch [28/100] Train TotalLoss: 39.1675, (ReconX: 0.9406, KLD: 0.3024*0.93, LossY: 3.7945*10.00), Time: 1.8s | Val TotalLoss: 39.4900, (ReconX: 0.9442, KLD: 0.2902, LossY: 3.8256)\n",
            "2025-10-23 22:44:18,281 - VAETrainer - INFO - Saving model and trainer state to vae_results/best_model.pth...\n",
            "INFO:VAETrainer:Saving model and trainer state to vae_results/best_model.pth...\n",
            "2025-10-23 22:44:18,296 - VAETrainer - INFO - Successfully saved model and trainer state to vae_results/best_model.pth\n",
            "INFO:VAETrainer:Successfully saved model and trainer state to vae_results/best_model.pth\n",
            "2025-10-23 22:44:20,398 - VAETrainer - INFO - Epoch [29/100] Train TotalLoss: 38.5693, (ReconX: 0.9429, KLD: 0.2763*0.97, LossY: 3.7359*10.00), Time: 1.7s | Val TotalLoss: 38.4137, (ReconX: 0.9486, KLD: 0.2632, LossY: 3.7202)\n",
            "INFO:VAETrainer:Epoch [29/100] Train TotalLoss: 38.5693, (ReconX: 0.9429, KLD: 0.2763*0.97, LossY: 3.7359*10.00), Time: 1.7s | Val TotalLoss: 38.4137, (ReconX: 0.9486, KLD: 0.2632, LossY: 3.7202)\n",
            "2025-10-23 22:44:20,400 - VAETrainer - INFO - Saving model and trainer state to vae_results/best_model.pth...\n",
            "INFO:VAETrainer:Saving model and trainer state to vae_results/best_model.pth...\n",
            "2025-10-23 22:44:20,412 - VAETrainer - INFO - Successfully saved model and trainer state to vae_results/best_model.pth\n",
            "INFO:VAETrainer:Successfully saved model and trainer state to vae_results/best_model.pth\n",
            "2025-10-23 22:44:22,611 - VAETrainer - INFO - Epoch [30/100] Train TotalLoss: 38.0769, (ReconX: 0.9479, KLD: 0.2507*1.00, LossY: 3.6878*10.00), Time: 1.8s | Val TotalLoss: 38.0423, (ReconX: 0.9521, KLD: 0.2423, LossY: 3.6848)\n",
            "INFO:VAETrainer:Epoch [30/100] Train TotalLoss: 38.0769, (ReconX: 0.9479, KLD: 0.2507*1.00, LossY: 3.6878*10.00), Time: 1.8s | Val TotalLoss: 38.0423, (ReconX: 0.9521, KLD: 0.2423, LossY: 3.6848)\n",
            "2025-10-23 22:44:22,613 - VAETrainer - INFO - Saving model and trainer state to vae_results/best_model.pth...\n",
            "INFO:VAETrainer:Saving model and trainer state to vae_results/best_model.pth...\n",
            "2025-10-23 22:44:22,627 - VAETrainer - INFO - Successfully saved model and trainer state to vae_results/best_model.pth\n",
            "INFO:VAETrainer:Successfully saved model and trainer state to vae_results/best_model.pth\n",
            "2025-10-23 22:44:24,794 - VAETrainer - INFO - Epoch [31/100] Train TotalLoss: 37.6856, (ReconX: 0.9518, KLD: 0.2254*1.00, LossY: 3.6508*10.00), Time: 1.8s | Val TotalLoss: 37.7214, (ReconX: 0.9560, KLD: 0.2206, LossY: 3.6545)\n",
            "INFO:VAETrainer:Epoch [31/100] Train TotalLoss: 37.6856, (ReconX: 0.9518, KLD: 0.2254*1.00, LossY: 3.6508*10.00), Time: 1.8s | Val TotalLoss: 37.7214, (ReconX: 0.9560, KLD: 0.2206, LossY: 3.6545)\n",
            "2025-10-23 22:44:24,796 - VAETrainer - INFO - Saving model and trainer state to vae_results/best_model.pth...\n",
            "INFO:VAETrainer:Saving model and trainer state to vae_results/best_model.pth...\n",
            "2025-10-23 22:44:24,809 - VAETrainer - INFO - Successfully saved model and trainer state to vae_results/best_model.pth\n",
            "INFO:VAETrainer:Successfully saved model and trainer state to vae_results/best_model.pth\n",
            "2025-10-23 22:44:26,903 - VAETrainer - INFO - Epoch [32/100] Train TotalLoss: 37.3510, (ReconX: 0.9557, KLD: 0.2056*1.00, LossY: 3.6190*10.00), Time: 1.7s | Val TotalLoss: 37.4670, (ReconX: 0.9577, KLD: 0.1985, LossY: 3.6311)\n",
            "INFO:VAETrainer:Epoch [32/100] Train TotalLoss: 37.3510, (ReconX: 0.9557, KLD: 0.2056*1.00, LossY: 3.6190*10.00), Time: 1.7s | Val TotalLoss: 37.4670, (ReconX: 0.9577, KLD: 0.1985, LossY: 3.6311)\n",
            "2025-10-23 22:44:26,905 - VAETrainer - INFO - Saving model and trainer state to vae_results/best_model.pth...\n",
            "INFO:VAETrainer:Saving model and trainer state to vae_results/best_model.pth...\n",
            "2025-10-23 22:44:26,918 - VAETrainer - INFO - Successfully saved model and trainer state to vae_results/best_model.pth\n",
            "INFO:VAETrainer:Successfully saved model and trainer state to vae_results/best_model.pth\n",
            "2025-10-23 22:44:28,951 - VAETrainer - INFO - Epoch [33/100] Train TotalLoss: 37.0838, (ReconX: 0.9587, KLD: 0.1874*1.00, LossY: 3.5938*10.00), Time: 1.7s | Val TotalLoss: 37.0690, (ReconX: 0.9630, KLD: 0.1809, LossY: 3.5925)\n",
            "INFO:VAETrainer:Epoch [33/100] Train TotalLoss: 37.0838, (ReconX: 0.9587, KLD: 0.1874*1.00, LossY: 3.5938*10.00), Time: 1.7s | Val TotalLoss: 37.0690, (ReconX: 0.9630, KLD: 0.1809, LossY: 3.5925)\n",
            "2025-10-23 22:44:28,952 - VAETrainer - INFO - Saving model and trainer state to vae_results/best_model.pth...\n",
            "INFO:VAETrainer:Saving model and trainer state to vae_results/best_model.pth...\n",
            "2025-10-23 22:44:28,966 - VAETrainer - INFO - Successfully saved model and trainer state to vae_results/best_model.pth\n",
            "INFO:VAETrainer:Successfully saved model and trainer state to vae_results/best_model.pth\n",
            "2025-10-23 22:44:31,150 - VAETrainer - INFO - Epoch [34/100] Train TotalLoss: 36.9064, (ReconX: 0.9615, KLD: 0.1726*1.00, LossY: 3.5772*10.00), Time: 1.8s | Val TotalLoss: 36.6875, (ReconX: 0.9663, KLD: 0.1655, LossY: 3.5556)\n",
            "INFO:VAETrainer:Epoch [34/100] Train TotalLoss: 36.9064, (ReconX: 0.9615, KLD: 0.1726*1.00, LossY: 3.5772*10.00), Time: 1.8s | Val TotalLoss: 36.6875, (ReconX: 0.9663, KLD: 0.1655, LossY: 3.5556)\n",
            "2025-10-23 22:44:31,152 - VAETrainer - INFO - Saving model and trainer state to vae_results/best_model.pth...\n",
            "INFO:VAETrainer:Saving model and trainer state to vae_results/best_model.pth...\n",
            "2025-10-23 22:44:31,166 - VAETrainer - INFO - Successfully saved model and trainer state to vae_results/best_model.pth\n",
            "INFO:VAETrainer:Successfully saved model and trainer state to vae_results/best_model.pth\n",
            "2025-10-23 22:44:33,330 - VAETrainer - INFO - Epoch [35/100] Train TotalLoss: 36.6795, (ReconX: 0.9646, KLD: 0.1582*1.00, LossY: 3.5557*10.00), Time: 1.8s | Val TotalLoss: 36.8368, (ReconX: 0.9654, KLD: 0.1545, LossY: 3.5717)\n",
            "INFO:VAETrainer:Epoch [35/100] Train TotalLoss: 36.6795, (ReconX: 0.9646, KLD: 0.1582*1.00, LossY: 3.5557*10.00), Time: 1.8s | Val TotalLoss: 36.8368, (ReconX: 0.9654, KLD: 0.1545, LossY: 3.5717)\n",
            "2025-10-23 22:44:35,442 - VAETrainer - INFO - Epoch [36/100] Train TotalLoss: 36.5735, (ReconX: 0.9667, KLD: 0.1469*1.00, LossY: 3.5460*10.00), Time: 1.7s | Val TotalLoss: 36.6150, (ReconX: 0.9712, KLD: 0.1367, LossY: 3.5507)\n",
            "INFO:VAETrainer:Epoch [36/100] Train TotalLoss: 36.5735, (ReconX: 0.9667, KLD: 0.1469*1.00, LossY: 3.5460*10.00), Time: 1.7s | Val TotalLoss: 36.6150, (ReconX: 0.9712, KLD: 0.1367, LossY: 3.5507)\n",
            "2025-10-23 22:44:35,444 - VAETrainer - INFO - Saving model and trainer state to vae_results/best_model.pth...\n",
            "INFO:VAETrainer:Saving model and trainer state to vae_results/best_model.pth...\n",
            "2025-10-23 22:44:35,456 - VAETrainer - INFO - Successfully saved model and trainer state to vae_results/best_model.pth\n",
            "INFO:VAETrainer:Successfully saved model and trainer state to vae_results/best_model.pth\n",
            "2025-10-23 22:44:37,599 - VAETrainer - INFO - Epoch [37/100] Train TotalLoss: 36.4835, (ReconX: 0.9698, KLD: 0.1327*1.00, LossY: 3.5381*10.00), Time: 1.8s | Val TotalLoss: 36.3512, (ReconX: 0.9718, KLD: 0.1326, LossY: 3.5247)\n",
            "INFO:VAETrainer:Epoch [37/100] Train TotalLoss: 36.4835, (ReconX: 0.9698, KLD: 0.1327*1.00, LossY: 3.5381*10.00), Time: 1.8s | Val TotalLoss: 36.3512, (ReconX: 0.9718, KLD: 0.1326, LossY: 3.5247)\n",
            "2025-10-23 22:44:37,601 - VAETrainer - INFO - Saving model and trainer state to vae_results/best_model.pth...\n",
            "INFO:VAETrainer:Saving model and trainer state to vae_results/best_model.pth...\n",
            "2025-10-23 22:44:37,614 - VAETrainer - INFO - Successfully saved model and trainer state to vae_results/best_model.pth\n",
            "INFO:VAETrainer:Successfully saved model and trainer state to vae_results/best_model.pth\n",
            "2025-10-23 22:44:39,744 - VAETrainer - INFO - Epoch [38/100] Train TotalLoss: 36.3329, (ReconX: 0.9709, KLD: 0.1252*1.00, LossY: 3.5237*10.00), Time: 1.8s | Val TotalLoss: 36.3876, (ReconX: 0.9759, KLD: 0.1200, LossY: 3.5292)\n",
            "INFO:VAETrainer:Epoch [38/100] Train TotalLoss: 36.3329, (ReconX: 0.9709, KLD: 0.1252*1.00, LossY: 3.5237*10.00), Time: 1.8s | Val TotalLoss: 36.3876, (ReconX: 0.9759, KLD: 0.1200, LossY: 3.5292)\n",
            "2025-10-23 22:44:41,810 - VAETrainer - INFO - Epoch [39/100] Train TotalLoss: 36.2841, (ReconX: 0.9721, KLD: 0.1174*1.00, LossY: 3.5195*10.00), Time: 1.7s | Val TotalLoss: 36.3477, (ReconX: 0.9749, KLD: 0.1135, LossY: 3.5259)\n",
            "INFO:VAETrainer:Epoch [39/100] Train TotalLoss: 36.2841, (ReconX: 0.9721, KLD: 0.1174*1.00, LossY: 3.5195*10.00), Time: 1.7s | Val TotalLoss: 36.3477, (ReconX: 0.9749, KLD: 0.1135, LossY: 3.5259)\n",
            "2025-10-23 22:44:41,813 - VAETrainer - INFO - Saving model and trainer state to vae_results/best_model.pth...\n",
            "INFO:VAETrainer:Saving model and trainer state to vae_results/best_model.pth...\n",
            "2025-10-23 22:44:41,826 - VAETrainer - INFO - Successfully saved model and trainer state to vae_results/best_model.pth\n",
            "INFO:VAETrainer:Successfully saved model and trainer state to vae_results/best_model.pth\n",
            "2025-10-23 22:44:43,966 - VAETrainer - INFO - Epoch [40/100] Train TotalLoss: 36.2449, (ReconX: 0.9744, KLD: 0.1104*1.00, LossY: 3.5160*10.00), Time: 1.8s | Val TotalLoss: 36.1910, (ReconX: 0.9757, KLD: 0.1086, LossY: 3.5107)\n",
            "INFO:VAETrainer:Epoch [40/100] Train TotalLoss: 36.2449, (ReconX: 0.9744, KLD: 0.1104*1.00, LossY: 3.5160*10.00), Time: 1.8s | Val TotalLoss: 36.1910, (ReconX: 0.9757, KLD: 0.1086, LossY: 3.5107)\n",
            "2025-10-23 22:44:43,968 - VAETrainer - INFO - Saving model and trainer state to vae_results/best_model.pth...\n",
            "INFO:VAETrainer:Saving model and trainer state to vae_results/best_model.pth...\n",
            "2025-10-23 22:44:43,984 - VAETrainer - INFO - Successfully saved model and trainer state to vae_results/best_model.pth\n",
            "INFO:VAETrainer:Successfully saved model and trainer state to vae_results/best_model.pth\n",
            "2025-10-23 22:44:46,133 - VAETrainer - INFO - Epoch [41/100] Train TotalLoss: 36.1679, (ReconX: 0.9758, KLD: 0.1030*1.00, LossY: 3.5089*10.00), Time: 1.8s | Val TotalLoss: 36.3231, (ReconX: 0.9782, KLD: 0.1041, LossY: 3.5241)\n",
            "INFO:VAETrainer:Epoch [41/100] Train TotalLoss: 36.1679, (ReconX: 0.9758, KLD: 0.1030*1.00, LossY: 3.5089*10.00), Time: 1.8s | Val TotalLoss: 36.3231, (ReconX: 0.9782, KLD: 0.1041, LossY: 3.5241)\n",
            "2025-10-23 22:44:48,325 - VAETrainer - INFO - Epoch [42/100] Train TotalLoss: 36.1116, (ReconX: 0.9769, KLD: 0.0985*1.00, LossY: 3.5036*10.00), Time: 1.8s | Val TotalLoss: 36.1261, (ReconX: 0.9790, KLD: 0.0943, LossY: 3.5053)\n",
            "INFO:VAETrainer:Epoch [42/100] Train TotalLoss: 36.1116, (ReconX: 0.9769, KLD: 0.0985*1.00, LossY: 3.5036*10.00), Time: 1.8s | Val TotalLoss: 36.1261, (ReconX: 0.9790, KLD: 0.0943, LossY: 3.5053)\n",
            "2025-10-23 22:44:48,327 - VAETrainer - INFO - Saving model and trainer state to vae_results/best_model.pth...\n",
            "INFO:VAETrainer:Saving model and trainer state to vae_results/best_model.pth...\n",
            "2025-10-23 22:44:48,341 - VAETrainer - INFO - Successfully saved model and trainer state to vae_results/best_model.pth\n",
            "INFO:VAETrainer:Successfully saved model and trainer state to vae_results/best_model.pth\n",
            "2025-10-23 22:44:50,557 - VAETrainer - INFO - Epoch [43/100] Train TotalLoss: 36.0618, (ReconX: 0.9789, KLD: 0.0921*1.00, LossY: 3.4991*10.00), Time: 1.8s | Val TotalLoss: 36.1422, (ReconX: 0.9807, KLD: 0.0911, LossY: 3.5070)\n",
            "INFO:VAETrainer:Epoch [43/100] Train TotalLoss: 36.0618, (ReconX: 0.9789, KLD: 0.0921*1.00, LossY: 3.4991*10.00), Time: 1.8s | Val TotalLoss: 36.1422, (ReconX: 0.9807, KLD: 0.0911, LossY: 3.5070)\n",
            "2025-10-23 22:44:52,596 - VAETrainer - INFO - Epoch [44/100] Train TotalLoss: 35.9666, (ReconX: 0.9792, KLD: 0.0887*1.00, LossY: 3.4899*10.00), Time: 1.7s | Val TotalLoss: 36.0025, (ReconX: 0.9828, KLD: 0.0877, LossY: 3.4932)\n",
            "INFO:VAETrainer:Epoch [44/100] Train TotalLoss: 35.9666, (ReconX: 0.9792, KLD: 0.0887*1.00, LossY: 3.4899*10.00), Time: 1.7s | Val TotalLoss: 36.0025, (ReconX: 0.9828, KLD: 0.0877, LossY: 3.4932)\n",
            "2025-10-23 22:44:52,598 - VAETrainer - INFO - Saving model and trainer state to vae_results/best_model.pth...\n",
            "INFO:VAETrainer:Saving model and trainer state to vae_results/best_model.pth...\n",
            "2025-10-23 22:44:52,611 - VAETrainer - INFO - Successfully saved model and trainer state to vae_results/best_model.pth\n",
            "INFO:VAETrainer:Successfully saved model and trainer state to vae_results/best_model.pth\n",
            "2025-10-23 22:44:54,811 - VAETrainer - INFO - Epoch [45/100] Train TotalLoss: 35.9527, (ReconX: 0.9798, KLD: 0.0854*1.00, LossY: 3.4887*10.00), Time: 1.8s | Val TotalLoss: 35.9866, (ReconX: 0.9842, KLD: 0.0815, LossY: 3.4921)\n",
            "INFO:VAETrainer:Epoch [45/100] Train TotalLoss: 35.9527, (ReconX: 0.9798, KLD: 0.0854*1.00, LossY: 3.4887*10.00), Time: 1.8s | Val TotalLoss: 35.9866, (ReconX: 0.9842, KLD: 0.0815, LossY: 3.4921)\n",
            "2025-10-23 22:44:54,813 - VAETrainer - INFO - Saving model and trainer state to vae_results/best_model.pth...\n",
            "INFO:VAETrainer:Saving model and trainer state to vae_results/best_model.pth...\n",
            "2025-10-23 22:44:54,826 - VAETrainer - INFO - Successfully saved model and trainer state to vae_results/best_model.pth\n",
            "INFO:VAETrainer:Successfully saved model and trainer state to vae_results/best_model.pth\n",
            "2025-10-23 22:44:56,874 - VAETrainer - INFO - Epoch [46/100] Train TotalLoss: 35.9204, (ReconX: 0.9804, KLD: 0.0813*1.00, LossY: 3.4859*10.00), Time: 1.7s | Val TotalLoss: 35.9221, (ReconX: 0.9854, KLD: 0.0807, LossY: 3.4856)\n",
            "INFO:VAETrainer:Epoch [46/100] Train TotalLoss: 35.9204, (ReconX: 0.9804, KLD: 0.0813*1.00, LossY: 3.4859*10.00), Time: 1.7s | Val TotalLoss: 35.9221, (ReconX: 0.9854, KLD: 0.0807, LossY: 3.4856)\n",
            "2025-10-23 22:44:56,876 - VAETrainer - INFO - Saving model and trainer state to vae_results/best_model.pth...\n",
            "INFO:VAETrainer:Saving model and trainer state to vae_results/best_model.pth...\n",
            "2025-10-23 22:44:56,889 - VAETrainer - INFO - Successfully saved model and trainer state to vae_results/best_model.pth\n",
            "INFO:VAETrainer:Successfully saved model and trainer state to vae_results/best_model.pth\n",
            "2025-10-23 22:44:59,009 - VAETrainer - INFO - Epoch [47/100] Train TotalLoss: 35.8599, (ReconX: 0.9816, KLD: 0.0776*1.00, LossY: 3.4801*10.00), Time: 1.8s | Val TotalLoss: 35.9219, (ReconX: 0.9857, KLD: 0.0769, LossY: 3.4859)\n",
            "INFO:VAETrainer:Epoch [47/100] Train TotalLoss: 35.8599, (ReconX: 0.9816, KLD: 0.0776*1.00, LossY: 3.4801*10.00), Time: 1.8s | Val TotalLoss: 35.9219, (ReconX: 0.9857, KLD: 0.0769, LossY: 3.4859)\n",
            "2025-10-23 22:44:59,011 - VAETrainer - INFO - Saving model and trainer state to vae_results/best_model.pth...\n",
            "INFO:VAETrainer:Saving model and trainer state to vae_results/best_model.pth...\n",
            "2025-10-23 22:44:59,027 - VAETrainer - INFO - Successfully saved model and trainer state to vae_results/best_model.pth\n",
            "INFO:VAETrainer:Successfully saved model and trainer state to vae_results/best_model.pth\n",
            "2025-10-23 22:45:01,187 - VAETrainer - INFO - Epoch [48/100] Train TotalLoss: 35.8029, (ReconX: 0.9823, KLD: 0.0742*1.00, LossY: 3.4746*10.00), Time: 1.8s | Val TotalLoss: 35.8314, (ReconX: 0.9854, KLD: 0.0740, LossY: 3.4772)\n",
            "INFO:VAETrainer:Epoch [48/100] Train TotalLoss: 35.8029, (ReconX: 0.9823, KLD: 0.0742*1.00, LossY: 3.4746*10.00), Time: 1.8s | Val TotalLoss: 35.8314, (ReconX: 0.9854, KLD: 0.0740, LossY: 3.4772)\n",
            "2025-10-23 22:45:01,189 - VAETrainer - INFO - Saving model and trainer state to vae_results/best_model.pth...\n",
            "INFO:VAETrainer:Saving model and trainer state to vae_results/best_model.pth...\n",
            "2025-10-23 22:45:01,202 - VAETrainer - INFO - Successfully saved model and trainer state to vae_results/best_model.pth\n",
            "INFO:VAETrainer:Successfully saved model and trainer state to vae_results/best_model.pth\n",
            "2025-10-23 22:45:03,308 - VAETrainer - INFO - Epoch [49/100] Train TotalLoss: 35.7551, (ReconX: 0.9829, KLD: 0.0721*1.00, LossY: 3.4700*10.00), Time: 1.7s | Val TotalLoss: 35.7190, (ReconX: 0.9842, KLD: 0.0729, LossY: 3.4662)\n",
            "INFO:VAETrainer:Epoch [49/100] Train TotalLoss: 35.7551, (ReconX: 0.9829, KLD: 0.0721*1.00, LossY: 3.4700*10.00), Time: 1.7s | Val TotalLoss: 35.7190, (ReconX: 0.9842, KLD: 0.0729, LossY: 3.4662)\n",
            "2025-10-23 22:45:03,310 - VAETrainer - INFO - Saving model and trainer state to vae_results/best_model.pth...\n",
            "INFO:VAETrainer:Saving model and trainer state to vae_results/best_model.pth...\n",
            "2025-10-23 22:45:03,323 - VAETrainer - INFO - Successfully saved model and trainer state to vae_results/best_model.pth\n",
            "INFO:VAETrainer:Successfully saved model and trainer state to vae_results/best_model.pth\n",
            "2025-10-23 22:45:05,464 - VAETrainer - INFO - Epoch [50/100] Train TotalLoss: 35.7312, (ReconX: 0.9833, KLD: 0.0709*1.00, LossY: 3.4677*10.00), Time: 1.8s | Val TotalLoss: 35.7027, (ReconX: 0.9875, KLD: 0.0670, LossY: 3.4648)\n",
            "INFO:VAETrainer:Epoch [50/100] Train TotalLoss: 35.7312, (ReconX: 0.9833, KLD: 0.0709*1.00, LossY: 3.4677*10.00), Time: 1.8s | Val TotalLoss: 35.7027, (ReconX: 0.9875, KLD: 0.0670, LossY: 3.4648)\n",
            "2025-10-23 22:45:05,466 - VAETrainer - INFO - Saving model and trainer state to vae_results/best_model.pth...\n",
            "INFO:VAETrainer:Saving model and trainer state to vae_results/best_model.pth...\n",
            "2025-10-23 22:45:05,479 - VAETrainer - INFO - Successfully saved model and trainer state to vae_results/best_model.pth\n",
            "INFO:VAETrainer:Successfully saved model and trainer state to vae_results/best_model.pth\n",
            "2025-10-23 22:45:07,546 - VAETrainer - INFO - Epoch [51/100] Train TotalLoss: 35.7153, (ReconX: 0.9835, KLD: 0.0680*1.00, LossY: 3.4664*10.00), Time: 1.7s | Val TotalLoss: 35.7346, (ReconX: 0.9872, KLD: 0.0627, LossY: 3.4685)\n",
            "INFO:VAETrainer:Epoch [51/100] Train TotalLoss: 35.7153, (ReconX: 0.9835, KLD: 0.0680*1.00, LossY: 3.4664*10.00), Time: 1.7s | Val TotalLoss: 35.7346, (ReconX: 0.9872, KLD: 0.0627, LossY: 3.4685)\n",
            "2025-10-23 22:45:09,690 - VAETrainer - INFO - Epoch [52/100] Train TotalLoss: 35.6265, (ReconX: 0.9841, KLD: 0.0653*1.00, LossY: 3.4577*10.00), Time: 1.8s | Val TotalLoss: 35.6306, (ReconX: 0.9879, KLD: 0.0650, LossY: 3.4578)\n",
            "INFO:VAETrainer:Epoch [52/100] Train TotalLoss: 35.6265, (ReconX: 0.9841, KLD: 0.0653*1.00, LossY: 3.4577*10.00), Time: 1.8s | Val TotalLoss: 35.6306, (ReconX: 0.9879, KLD: 0.0650, LossY: 3.4578)\n",
            "2025-10-23 22:45:09,692 - VAETrainer - INFO - Saving model and trainer state to vae_results/best_model.pth...\n",
            "INFO:VAETrainer:Saving model and trainer state to vae_results/best_model.pth...\n",
            "2025-10-23 22:45:09,706 - VAETrainer - INFO - Successfully saved model and trainer state to vae_results/best_model.pth\n",
            "INFO:VAETrainer:Successfully saved model and trainer state to vae_results/best_model.pth\n",
            "2025-10-23 22:45:11,828 - VAETrainer - INFO - Epoch [53/100] Train TotalLoss: 35.5726, (ReconX: 0.9841, KLD: 0.0642*1.00, LossY: 3.4524*10.00), Time: 1.7s | Val TotalLoss: 35.5724, (ReconX: 0.9864, KLD: 0.0653, LossY: 3.4521)\n",
            "INFO:VAETrainer:Epoch [53/100] Train TotalLoss: 35.5726, (ReconX: 0.9841, KLD: 0.0642*1.00, LossY: 3.4524*10.00), Time: 1.7s | Val TotalLoss: 35.5724, (ReconX: 0.9864, KLD: 0.0653, LossY: 3.4521)\n",
            "2025-10-23 22:45:11,830 - VAETrainer - INFO - Saving model and trainer state to vae_results/best_model.pth...\n",
            "INFO:VAETrainer:Saving model and trainer state to vae_results/best_model.pth...\n",
            "2025-10-23 22:45:11,843 - VAETrainer - INFO - Successfully saved model and trainer state to vae_results/best_model.pth\n",
            "INFO:VAETrainer:Successfully saved model and trainer state to vae_results/best_model.pth\n",
            "2025-10-23 22:45:14,014 - VAETrainer - INFO - Epoch [54/100] Train TotalLoss: 35.5019, (ReconX: 0.9847, KLD: 0.0624*1.00, LossY: 3.4455*10.00), Time: 1.8s | Val TotalLoss: 35.5281, (ReconX: 0.9889, KLD: 0.0630, LossY: 3.4476)\n",
            "INFO:VAETrainer:Epoch [54/100] Train TotalLoss: 35.5019, (ReconX: 0.9847, KLD: 0.0624*1.00, LossY: 3.4455*10.00), Time: 1.8s | Val TotalLoss: 35.5281, (ReconX: 0.9889, KLD: 0.0630, LossY: 3.4476)\n",
            "2025-10-23 22:45:14,016 - VAETrainer - INFO - Saving model and trainer state to vae_results/best_model.pth...\n",
            "INFO:VAETrainer:Saving model and trainer state to vae_results/best_model.pth...\n",
            "2025-10-23 22:45:14,034 - VAETrainer - INFO - Successfully saved model and trainer state to vae_results/best_model.pth\n",
            "INFO:VAETrainer:Successfully saved model and trainer state to vae_results/best_model.pth\n",
            "2025-10-23 22:45:16,192 - VAETrainer - INFO - Epoch [55/100] Train TotalLoss: 35.4181, (ReconX: 0.9855, KLD: 0.0609*1.00, LossY: 3.4372*10.00), Time: 1.8s | Val TotalLoss: 35.3785, (ReconX: 0.9885, KLD: 0.0592, LossY: 3.4331)\n",
            "INFO:VAETrainer:Epoch [55/100] Train TotalLoss: 35.4181, (ReconX: 0.9855, KLD: 0.0609*1.00, LossY: 3.4372*10.00), Time: 1.8s | Val TotalLoss: 35.3785, (ReconX: 0.9885, KLD: 0.0592, LossY: 3.4331)\n",
            "2025-10-23 22:45:16,194 - VAETrainer - INFO - Saving model and trainer state to vae_results/best_model.pth...\n",
            "INFO:VAETrainer:Saving model and trainer state to vae_results/best_model.pth...\n",
            "2025-10-23 22:45:16,207 - VAETrainer - INFO - Successfully saved model and trainer state to vae_results/best_model.pth\n",
            "INFO:VAETrainer:Successfully saved model and trainer state to vae_results/best_model.pth\n",
            "2025-10-23 22:45:18,315 - VAETrainer - INFO - Epoch [56/100] Train TotalLoss: 35.3685, (ReconX: 0.9858, KLD: 0.0598*1.00, LossY: 3.4323*10.00), Time: 1.7s | Val TotalLoss: 35.3629, (ReconX: 0.9902, KLD: 0.0565, LossY: 3.4316)\n",
            "INFO:VAETrainer:Epoch [56/100] Train TotalLoss: 35.3685, (ReconX: 0.9858, KLD: 0.0598*1.00, LossY: 3.4323*10.00), Time: 1.7s | Val TotalLoss: 35.3629, (ReconX: 0.9902, KLD: 0.0565, LossY: 3.4316)\n",
            "2025-10-23 22:45:18,317 - VAETrainer - INFO - Saving model and trainer state to vae_results/best_model.pth...\n",
            "INFO:VAETrainer:Saving model and trainer state to vae_results/best_model.pth...\n",
            "2025-10-23 22:45:18,330 - VAETrainer - INFO - Successfully saved model and trainer state to vae_results/best_model.pth\n",
            "INFO:VAETrainer:Successfully saved model and trainer state to vae_results/best_model.pth\n",
            "2025-10-23 22:45:20,375 - VAETrainer - INFO - Epoch [57/100] Train TotalLoss: 35.3429, (ReconX: 0.9862, KLD: 0.0587*1.00, LossY: 3.4298*10.00), Time: 1.7s | Val TotalLoss: 35.3584, (ReconX: 0.9888, KLD: 0.0550, LossY: 3.4315)\n",
            "INFO:VAETrainer:Epoch [57/100] Train TotalLoss: 35.3429, (ReconX: 0.9862, KLD: 0.0587*1.00, LossY: 3.4298*10.00), Time: 1.7s | Val TotalLoss: 35.3584, (ReconX: 0.9888, KLD: 0.0550, LossY: 3.4315)\n",
            "2025-10-23 22:45:20,377 - VAETrainer - INFO - Saving model and trainer state to vae_results/best_model.pth...\n",
            "INFO:VAETrainer:Saving model and trainer state to vae_results/best_model.pth...\n",
            "2025-10-23 22:45:20,390 - VAETrainer - INFO - Successfully saved model and trainer state to vae_results/best_model.pth\n",
            "INFO:VAETrainer:Successfully saved model and trainer state to vae_results/best_model.pth\n",
            "2025-10-23 22:45:22,561 - VAETrainer - INFO - Epoch [58/100] Train TotalLoss: 35.2370, (ReconX: 0.9873, KLD: 0.0562*1.00, LossY: 3.4193*10.00), Time: 1.8s | Val TotalLoss: 35.3103, (ReconX: 0.9903, KLD: 0.0553, LossY: 3.4265)\n",
            "INFO:VAETrainer:Epoch [58/100] Train TotalLoss: 35.2370, (ReconX: 0.9873, KLD: 0.0562*1.00, LossY: 3.4193*10.00), Time: 1.8s | Val TotalLoss: 35.3103, (ReconX: 0.9903, KLD: 0.0553, LossY: 3.4265)\n",
            "2025-10-23 22:45:22,562 - VAETrainer - INFO - Saving model and trainer state to vae_results/best_model.pth...\n",
            "INFO:VAETrainer:Saving model and trainer state to vae_results/best_model.pth...\n",
            "2025-10-23 22:45:22,576 - VAETrainer - INFO - Successfully saved model and trainer state to vae_results/best_model.pth\n",
            "INFO:VAETrainer:Successfully saved model and trainer state to vae_results/best_model.pth\n",
            "2025-10-23 22:45:24,709 - VAETrainer - INFO - Epoch [59/100] Train TotalLoss: 35.2260, (ReconX: 0.9861, KLD: 0.0553*1.00, LossY: 3.4185*10.00), Time: 1.7s | Val TotalLoss: 35.2155, (ReconX: 0.9887, KLD: 0.0536, LossY: 3.4173)\n",
            "INFO:VAETrainer:Epoch [59/100] Train TotalLoss: 35.2260, (ReconX: 0.9861, KLD: 0.0553*1.00, LossY: 3.4185*10.00), Time: 1.7s | Val TotalLoss: 35.2155, (ReconX: 0.9887, KLD: 0.0536, LossY: 3.4173)\n",
            "2025-10-23 22:45:24,711 - VAETrainer - INFO - Saving model and trainer state to vae_results/best_model.pth...\n",
            "INFO:VAETrainer:Saving model and trainer state to vae_results/best_model.pth...\n",
            "2025-10-23 22:45:24,724 - VAETrainer - INFO - Successfully saved model and trainer state to vae_results/best_model.pth\n",
            "INFO:VAETrainer:Successfully saved model and trainer state to vae_results/best_model.pth\n",
            "2025-10-23 22:45:26,947 - VAETrainer - INFO - Epoch [60/100] Train TotalLoss: 35.0971, (ReconX: 0.9874, KLD: 0.0528*1.00, LossY: 3.4057*10.00), Time: 1.8s | Val TotalLoss: 35.1416, (ReconX: 0.9914, KLD: 0.0528, LossY: 3.4097)\n",
            "INFO:VAETrainer:Epoch [60/100] Train TotalLoss: 35.0971, (ReconX: 0.9874, KLD: 0.0528*1.00, LossY: 3.4057*10.00), Time: 1.8s | Val TotalLoss: 35.1416, (ReconX: 0.9914, KLD: 0.0528, LossY: 3.4097)\n",
            "2025-10-23 22:45:26,949 - VAETrainer - INFO - Saving model and trainer state to vae_results/best_model.pth...\n",
            "INFO:VAETrainer:Saving model and trainer state to vae_results/best_model.pth...\n",
            "2025-10-23 22:45:26,962 - VAETrainer - INFO - Successfully saved model and trainer state to vae_results/best_model.pth\n",
            "INFO:VAETrainer:Successfully saved model and trainer state to vae_results/best_model.pth\n",
            "2025-10-23 22:45:29,141 - VAETrainer - INFO - Epoch [61/100] Train TotalLoss: 35.0469, (ReconX: 0.9872, KLD: 0.0517*1.00, LossY: 3.4008*10.00), Time: 1.8s | Val TotalLoss: 35.0626, (ReconX: 0.9899, KLD: 0.0535, LossY: 3.4019)\n",
            "INFO:VAETrainer:Epoch [61/100] Train TotalLoss: 35.0469, (ReconX: 0.9872, KLD: 0.0517*1.00, LossY: 3.4008*10.00), Time: 1.8s | Val TotalLoss: 35.0626, (ReconX: 0.9899, KLD: 0.0535, LossY: 3.4019)\n",
            "2025-10-23 22:45:29,142 - VAETrainer - INFO - Saving model and trainer state to vae_results/best_model.pth...\n",
            "INFO:VAETrainer:Saving model and trainer state to vae_results/best_model.pth...\n",
            "2025-10-23 22:45:29,157 - VAETrainer - INFO - Successfully saved model and trainer state to vae_results/best_model.pth\n",
            "INFO:VAETrainer:Successfully saved model and trainer state to vae_results/best_model.pth\n",
            "2025-10-23 22:45:31,255 - VAETrainer - INFO - Epoch [62/100] Train TotalLoss: 35.0251, (ReconX: 0.9885, KLD: 0.0498*1.00, LossY: 3.3987*10.00), Time: 1.7s | Val TotalLoss: 35.0715, (ReconX: 0.9907, KLD: 0.0495, LossY: 3.4031)\n",
            "INFO:VAETrainer:Epoch [62/100] Train TotalLoss: 35.0251, (ReconX: 0.9885, KLD: 0.0498*1.00, LossY: 3.3987*10.00), Time: 1.7s | Val TotalLoss: 35.0715, (ReconX: 0.9907, KLD: 0.0495, LossY: 3.4031)\n",
            "2025-10-23 22:45:33,372 - VAETrainer - INFO - Epoch [63/100] Train TotalLoss: 34.9269, (ReconX: 0.9886, KLD: 0.0482*1.00, LossY: 3.3890*10.00), Time: 1.7s | Val TotalLoss: 34.9144, (ReconX: 0.9904, KLD: 0.0487, LossY: 3.3875)\n",
            "INFO:VAETrainer:Epoch [63/100] Train TotalLoss: 34.9269, (ReconX: 0.9886, KLD: 0.0482*1.00, LossY: 3.3890*10.00), Time: 1.7s | Val TotalLoss: 34.9144, (ReconX: 0.9904, KLD: 0.0487, LossY: 3.3875)\n",
            "2025-10-23 22:45:33,374 - VAETrainer - INFO - Saving model and trainer state to vae_results/best_model.pth...\n",
            "INFO:VAETrainer:Saving model and trainer state to vae_results/best_model.pth...\n",
            "2025-10-23 22:45:33,387 - VAETrainer - INFO - Successfully saved model and trainer state to vae_results/best_model.pth\n",
            "INFO:VAETrainer:Successfully saved model and trainer state to vae_results/best_model.pth\n",
            "2025-10-23 22:45:35,503 - VAETrainer - INFO - Epoch [64/100] Train TotalLoss: 34.9122, (ReconX: 0.9882, KLD: 0.0472*1.00, LossY: 3.3877*10.00), Time: 1.7s | Val TotalLoss: 34.9084, (ReconX: 0.9911, KLD: 0.0474, LossY: 3.3870)\n",
            "INFO:VAETrainer:Epoch [64/100] Train TotalLoss: 34.9122, (ReconX: 0.9882, KLD: 0.0472*1.00, LossY: 3.3877*10.00), Time: 1.7s | Val TotalLoss: 34.9084, (ReconX: 0.9911, KLD: 0.0474, LossY: 3.3870)\n",
            "2025-10-23 22:45:35,505 - VAETrainer - INFO - Saving model and trainer state to vae_results/best_model.pth...\n",
            "INFO:VAETrainer:Saving model and trainer state to vae_results/best_model.pth...\n",
            "2025-10-23 22:45:35,518 - VAETrainer - INFO - Successfully saved model and trainer state to vae_results/best_model.pth\n",
            "INFO:VAETrainer:Successfully saved model and trainer state to vae_results/best_model.pth\n",
            "2025-10-23 22:45:37,684 - VAETrainer - INFO - Epoch [65/100] Train TotalLoss: 34.8387, (ReconX: 0.9885, KLD: 0.0473*1.00, LossY: 3.3803*10.00), Time: 1.8s | Val TotalLoss: 34.8473, (ReconX: 0.9917, KLD: 0.0469, LossY: 3.3809)\n",
            "INFO:VAETrainer:Epoch [65/100] Train TotalLoss: 34.8387, (ReconX: 0.9885, KLD: 0.0473*1.00, LossY: 3.3803*10.00), Time: 1.8s | Val TotalLoss: 34.8473, (ReconX: 0.9917, KLD: 0.0469, LossY: 3.3809)\n",
            "2025-10-23 22:45:37,686 - VAETrainer - INFO - Saving model and trainer state to vae_results/best_model.pth...\n",
            "INFO:VAETrainer:Saving model and trainer state to vae_results/best_model.pth...\n",
            "2025-10-23 22:45:37,699 - VAETrainer - INFO - Successfully saved model and trainer state to vae_results/best_model.pth\n",
            "INFO:VAETrainer:Successfully saved model and trainer state to vae_results/best_model.pth\n",
            "2025-10-23 22:45:39,856 - VAETrainer - INFO - Epoch [66/100] Train TotalLoss: 34.7965, (ReconX: 0.9893, KLD: 0.0458*1.00, LossY: 3.3761*10.00), Time: 1.8s | Val TotalLoss: 34.7856, (ReconX: 0.9925, KLD: 0.0453, LossY: 3.3748)\n",
            "INFO:VAETrainer:Epoch [66/100] Train TotalLoss: 34.7965, (ReconX: 0.9893, KLD: 0.0458*1.00, LossY: 3.3761*10.00), Time: 1.8s | Val TotalLoss: 34.7856, (ReconX: 0.9925, KLD: 0.0453, LossY: 3.3748)\n",
            "2025-10-23 22:45:39,857 - VAETrainer - INFO - Saving model and trainer state to vae_results/best_model.pth...\n",
            "INFO:VAETrainer:Saving model and trainer state to vae_results/best_model.pth...\n",
            "2025-10-23 22:45:39,870 - VAETrainer - INFO - Successfully saved model and trainer state to vae_results/best_model.pth\n",
            "INFO:VAETrainer:Successfully saved model and trainer state to vae_results/best_model.pth\n",
            "2025-10-23 22:45:42,011 - VAETrainer - INFO - Epoch [67/100] Train TotalLoss: 34.7473, (ReconX: 0.9898, KLD: 0.0440*1.00, LossY: 3.3714*10.00), Time: 1.8s | Val TotalLoss: 34.7961, (ReconX: 0.9903, KLD: 0.0454, LossY: 3.3760)\n",
            "INFO:VAETrainer:Epoch [67/100] Train TotalLoss: 34.7473, (ReconX: 0.9898, KLD: 0.0440*1.00, LossY: 3.3714*10.00), Time: 1.8s | Val TotalLoss: 34.7961, (ReconX: 0.9903, KLD: 0.0454, LossY: 3.3760)\n",
            "2025-10-23 22:45:44,100 - VAETrainer - INFO - Epoch [68/100] Train TotalLoss: 34.7086, (ReconX: 0.9895, KLD: 0.0439*1.00, LossY: 3.3675*10.00), Time: 1.7s | Val TotalLoss: 34.8240, (ReconX: 0.9932, KLD: 0.0427, LossY: 3.3788)\n",
            "INFO:VAETrainer:Epoch [68/100] Train TotalLoss: 34.7086, (ReconX: 0.9895, KLD: 0.0439*1.00, LossY: 3.3675*10.00), Time: 1.7s | Val TotalLoss: 34.8240, (ReconX: 0.9932, KLD: 0.0427, LossY: 3.3788)\n",
            "2025-10-23 22:45:46,294 - VAETrainer - INFO - Epoch [69/100] Train TotalLoss: 34.6070, (ReconX: 0.9901, KLD: 0.0428*1.00, LossY: 3.3574*10.00), Time: 1.8s | Val TotalLoss: 34.7041, (ReconX: 0.9934, KLD: 0.0414, LossY: 3.3669)\n",
            "INFO:VAETrainer:Epoch [69/100] Train TotalLoss: 34.6070, (ReconX: 0.9901, KLD: 0.0428*1.00, LossY: 3.3574*10.00), Time: 1.8s | Val TotalLoss: 34.7041, (ReconX: 0.9934, KLD: 0.0414, LossY: 3.3669)\n",
            "2025-10-23 22:45:46,295 - VAETrainer - INFO - Saving model and trainer state to vae_results/best_model.pth...\n",
            "INFO:VAETrainer:Saving model and trainer state to vae_results/best_model.pth...\n",
            "2025-10-23 22:45:46,309 - VAETrainer - INFO - Successfully saved model and trainer state to vae_results/best_model.pth\n",
            "INFO:VAETrainer:Successfully saved model and trainer state to vae_results/best_model.pth\n",
            "2025-10-23 22:45:48,419 - VAETrainer - INFO - Epoch [70/100] Train TotalLoss: 34.6516, (ReconX: 0.9899, KLD: 0.0428*1.00, LossY: 3.3619*10.00), Time: 1.7s | Val TotalLoss: 34.6228, (ReconX: 0.9912, KLD: 0.0440, LossY: 3.3588)\n",
            "INFO:VAETrainer:Epoch [70/100] Train TotalLoss: 34.6516, (ReconX: 0.9899, KLD: 0.0428*1.00, LossY: 3.3619*10.00), Time: 1.7s | Val TotalLoss: 34.6228, (ReconX: 0.9912, KLD: 0.0440, LossY: 3.3588)\n",
            "2025-10-23 22:45:48,420 - VAETrainer - INFO - Saving model and trainer state to vae_results/best_model.pth...\n",
            "INFO:VAETrainer:Saving model and trainer state to vae_results/best_model.pth...\n",
            "2025-10-23 22:45:48,434 - VAETrainer - INFO - Successfully saved model and trainer state to vae_results/best_model.pth\n",
            "INFO:VAETrainer:Successfully saved model and trainer state to vae_results/best_model.pth\n",
            "2025-10-23 22:45:50,643 - VAETrainer - INFO - Epoch [71/100] Train TotalLoss: 34.5890, (ReconX: 0.9900, KLD: 0.0427*1.00, LossY: 3.3556*10.00), Time: 1.8s | Val TotalLoss: 34.5671, (ReconX: 0.9914, KLD: 0.0430, LossY: 3.3533)\n",
            "INFO:VAETrainer:Epoch [71/100] Train TotalLoss: 34.5890, (ReconX: 0.9900, KLD: 0.0427*1.00, LossY: 3.3556*10.00), Time: 1.8s | Val TotalLoss: 34.5671, (ReconX: 0.9914, KLD: 0.0430, LossY: 3.3533)\n",
            "2025-10-23 22:45:50,644 - VAETrainer - INFO - Saving model and trainer state to vae_results/best_model.pth...\n",
            "INFO:VAETrainer:Saving model and trainer state to vae_results/best_model.pth...\n",
            "2025-10-23 22:45:50,658 - VAETrainer - INFO - Successfully saved model and trainer state to vae_results/best_model.pth\n",
            "INFO:VAETrainer:Successfully saved model and trainer state to vae_results/best_model.pth\n",
            "2025-10-23 22:45:52,848 - VAETrainer - INFO - Epoch [72/100] Train TotalLoss: 34.4675, (ReconX: 0.9900, KLD: 0.0419*1.00, LossY: 3.3436*10.00), Time: 1.8s | Val TotalLoss: 34.5660, (ReconX: 0.9926, KLD: 0.0411, LossY: 3.3532)\n",
            "INFO:VAETrainer:Epoch [72/100] Train TotalLoss: 34.4675, (ReconX: 0.9900, KLD: 0.0419*1.00, LossY: 3.3436*10.00), Time: 1.8s | Val TotalLoss: 34.5660, (ReconX: 0.9926, KLD: 0.0411, LossY: 3.3532)\n",
            "2025-10-23 22:45:52,849 - VAETrainer - INFO - Saving model and trainer state to vae_results/best_model.pth...\n",
            "INFO:VAETrainer:Saving model and trainer state to vae_results/best_model.pth...\n",
            "2025-10-23 22:45:52,863 - VAETrainer - INFO - Successfully saved model and trainer state to vae_results/best_model.pth\n",
            "INFO:VAETrainer:Successfully saved model and trainer state to vae_results/best_model.pth\n",
            "2025-10-23 22:45:54,996 - VAETrainer - INFO - Epoch [73/100] Train TotalLoss: 34.5013, (ReconX: 0.9902, KLD: 0.0418*1.00, LossY: 3.3469*10.00), Time: 1.8s | Val TotalLoss: 34.6333, (ReconX: 0.9935, KLD: 0.0418, LossY: 3.3598)\n",
            "INFO:VAETrainer:Epoch [73/100] Train TotalLoss: 34.5013, (ReconX: 0.9902, KLD: 0.0418*1.00, LossY: 3.3469*10.00), Time: 1.8s | Val TotalLoss: 34.6333, (ReconX: 0.9935, KLD: 0.0418, LossY: 3.3598)\n",
            "2025-10-23 22:45:57,048 - VAETrainer - INFO - Epoch [74/100] Train TotalLoss: 34.4564, (ReconX: 0.9907, KLD: 0.0415*1.00, LossY: 3.3424*10.00), Time: 1.7s | Val TotalLoss: 34.4551, (ReconX: 0.9926, KLD: 0.0415, LossY: 3.3421)\n",
            "INFO:VAETrainer:Epoch [74/100] Train TotalLoss: 34.4564, (ReconX: 0.9907, KLD: 0.0415*1.00, LossY: 3.3424*10.00), Time: 1.7s | Val TotalLoss: 34.4551, (ReconX: 0.9926, KLD: 0.0415, LossY: 3.3421)\n",
            "2025-10-23 22:45:57,050 - VAETrainer - INFO - Saving model and trainer state to vae_results/best_model.pth...\n",
            "INFO:VAETrainer:Saving model and trainer state to vae_results/best_model.pth...\n",
            "2025-10-23 22:45:57,063 - VAETrainer - INFO - Successfully saved model and trainer state to vae_results/best_model.pth\n",
            "INFO:VAETrainer:Successfully saved model and trainer state to vae_results/best_model.pth\n",
            "2025-10-23 22:45:59,287 - VAETrainer - INFO - Epoch [75/100] Train TotalLoss: 34.3760, (ReconX: 0.9904, KLD: 0.0418*1.00, LossY: 3.3344*10.00), Time: 1.9s | Val TotalLoss: 34.3929, (ReconX: 0.9925, KLD: 0.0425, LossY: 3.3358)\n",
            "INFO:VAETrainer:Epoch [75/100] Train TotalLoss: 34.3760, (ReconX: 0.9904, KLD: 0.0418*1.00, LossY: 3.3344*10.00), Time: 1.9s | Val TotalLoss: 34.3929, (ReconX: 0.9925, KLD: 0.0425, LossY: 3.3358)\n",
            "2025-10-23 22:45:59,289 - VAETrainer - INFO - Saving model and trainer state to vae_results/best_model.pth...\n",
            "INFO:VAETrainer:Saving model and trainer state to vae_results/best_model.pth...\n",
            "2025-10-23 22:45:59,302 - VAETrainer - INFO - Successfully saved model and trainer state to vae_results/best_model.pth\n",
            "INFO:VAETrainer:Successfully saved model and trainer state to vae_results/best_model.pth\n",
            "2025-10-23 22:46:01,373 - VAETrainer - INFO - Epoch [76/100] Train TotalLoss: 34.3226, (ReconX: 0.9905, KLD: 0.0420*1.00, LossY: 3.3290*10.00), Time: 1.7s | Val TotalLoss: 34.3462, (ReconX: 0.9937, KLD: 0.0419, LossY: 3.3311)\n",
            "INFO:VAETrainer:Epoch [76/100] Train TotalLoss: 34.3226, (ReconX: 0.9905, KLD: 0.0420*1.00, LossY: 3.3290*10.00), Time: 1.7s | Val TotalLoss: 34.3462, (ReconX: 0.9937, KLD: 0.0419, LossY: 3.3311)\n",
            "2025-10-23 22:46:01,375 - VAETrainer - INFO - Saving model and trainer state to vae_results/best_model.pth...\n",
            "INFO:VAETrainer:Saving model and trainer state to vae_results/best_model.pth...\n",
            "2025-10-23 22:46:01,389 - VAETrainer - INFO - Successfully saved model and trainer state to vae_results/best_model.pth\n",
            "INFO:VAETrainer:Successfully saved model and trainer state to vae_results/best_model.pth\n",
            "2025-10-23 22:46:03,491 - VAETrainer - INFO - Epoch [77/100] Train TotalLoss: 34.2903, (ReconX: 0.9901, KLD: 0.0422*1.00, LossY: 3.3258*10.00), Time: 1.7s | Val TotalLoss: 34.3282, (ReconX: 0.9936, KLD: 0.0397, LossY: 3.3295)\n",
            "INFO:VAETrainer:Epoch [77/100] Train TotalLoss: 34.2903, (ReconX: 0.9901, KLD: 0.0422*1.00, LossY: 3.3258*10.00), Time: 1.7s | Val TotalLoss: 34.3282, (ReconX: 0.9936, KLD: 0.0397, LossY: 3.3295)\n",
            "2025-10-23 22:46:03,493 - VAETrainer - INFO - Saving model and trainer state to vae_results/best_model.pth...\n",
            "INFO:VAETrainer:Saving model and trainer state to vae_results/best_model.pth...\n",
            "2025-10-23 22:46:03,507 - VAETrainer - INFO - Successfully saved model and trainer state to vae_results/best_model.pth\n",
            "INFO:VAETrainer:Successfully saved model and trainer state to vae_results/best_model.pth\n",
            "2025-10-23 22:46:05,568 - VAETrainer - INFO - Epoch [78/100] Train TotalLoss: 34.2258, (ReconX: 0.9902, KLD: 0.0418*1.00, LossY: 3.3194*10.00), Time: 1.7s | Val TotalLoss: 34.3264, (ReconX: 0.9932, KLD: 0.0420, LossY: 3.3291)\n",
            "INFO:VAETrainer:Epoch [78/100] Train TotalLoss: 34.2258, (ReconX: 0.9902, KLD: 0.0418*1.00, LossY: 3.3194*10.00), Time: 1.7s | Val TotalLoss: 34.3264, (ReconX: 0.9932, KLD: 0.0420, LossY: 3.3291)\n",
            "2025-10-23 22:46:05,570 - VAETrainer - INFO - Saving model and trainer state to vae_results/best_model.pth...\n",
            "INFO:VAETrainer:Saving model and trainer state to vae_results/best_model.pth...\n",
            "2025-10-23 22:46:05,583 - VAETrainer - INFO - Successfully saved model and trainer state to vae_results/best_model.pth\n",
            "INFO:VAETrainer:Successfully saved model and trainer state to vae_results/best_model.pth\n",
            "2025-10-23 22:46:07,801 - VAETrainer - INFO - Epoch [79/100] Train TotalLoss: 34.1799, (ReconX: 0.9905, KLD: 0.0418*1.00, LossY: 3.3148*10.00), Time: 1.8s | Val TotalLoss: 34.2191, (ReconX: 0.9921, KLD: 0.0414, LossY: 3.3186)\n",
            "INFO:VAETrainer:Epoch [79/100] Train TotalLoss: 34.1799, (ReconX: 0.9905, KLD: 0.0418*1.00, LossY: 3.3148*10.00), Time: 1.8s | Val TotalLoss: 34.2191, (ReconX: 0.9921, KLD: 0.0414, LossY: 3.3186)\n",
            "2025-10-23 22:46:07,803 - VAETrainer - INFO - Saving model and trainer state to vae_results/best_model.pth...\n",
            "INFO:VAETrainer:Saving model and trainer state to vae_results/best_model.pth...\n",
            "2025-10-23 22:46:07,817 - VAETrainer - INFO - Successfully saved model and trainer state to vae_results/best_model.pth\n",
            "INFO:VAETrainer:Successfully saved model and trainer state to vae_results/best_model.pth\n",
            "2025-10-23 22:46:09,974 - VAETrainer - INFO - Epoch [80/100] Train TotalLoss: 34.1292, (ReconX: 0.9906, KLD: 0.0415*1.00, LossY: 3.3097*10.00), Time: 1.8s | Val TotalLoss: 34.1845, (ReconX: 0.9923, KLD: 0.0407, LossY: 3.3151)\n",
            "INFO:VAETrainer:Epoch [80/100] Train TotalLoss: 34.1292, (ReconX: 0.9906, KLD: 0.0415*1.00, LossY: 3.3097*10.00), Time: 1.8s | Val TotalLoss: 34.1845, (ReconX: 0.9923, KLD: 0.0407, LossY: 3.3151)\n",
            "2025-10-23 22:46:09,976 - VAETrainer - INFO - Saving model and trainer state to vae_results/best_model.pth...\n",
            "INFO:VAETrainer:Saving model and trainer state to vae_results/best_model.pth...\n",
            "2025-10-23 22:46:09,991 - VAETrainer - INFO - Successfully saved model and trainer state to vae_results/best_model.pth\n",
            "INFO:VAETrainer:Successfully saved model and trainer state to vae_results/best_model.pth\n",
            "2025-10-23 22:46:12,136 - VAETrainer - INFO - Epoch [81/100] Train TotalLoss: 34.0752, (ReconX: 0.9903, KLD: 0.0413*1.00, LossY: 3.3044*10.00), Time: 1.8s | Val TotalLoss: 34.2057, (ReconX: 0.9922, KLD: 0.0427, LossY: 3.3171)\n",
            "INFO:VAETrainer:Epoch [81/100] Train TotalLoss: 34.0752, (ReconX: 0.9903, KLD: 0.0413*1.00, LossY: 3.3044*10.00), Time: 1.8s | Val TotalLoss: 34.2057, (ReconX: 0.9922, KLD: 0.0427, LossY: 3.3171)\n",
            "2025-10-23 22:46:14,328 - VAETrainer - INFO - Epoch [82/100] Train TotalLoss: 34.0385, (ReconX: 0.9900, KLD: 0.0421*1.00, LossY: 3.3007*10.00), Time: 1.8s | Val TotalLoss: 34.0710, (ReconX: 0.9931, KLD: 0.0429, LossY: 3.3035)\n",
            "INFO:VAETrainer:Epoch [82/100] Train TotalLoss: 34.0385, (ReconX: 0.9900, KLD: 0.0421*1.00, LossY: 3.3007*10.00), Time: 1.8s | Val TotalLoss: 34.0710, (ReconX: 0.9931, KLD: 0.0429, LossY: 3.3035)\n",
            "2025-10-23 22:46:14,330 - VAETrainer - INFO - Saving model and trainer state to vae_results/best_model.pth...\n",
            "INFO:VAETrainer:Saving model and trainer state to vae_results/best_model.pth...\n",
            "2025-10-23 22:46:14,343 - VAETrainer - INFO - Successfully saved model and trainer state to vae_results/best_model.pth\n",
            "INFO:VAETrainer:Successfully saved model and trainer state to vae_results/best_model.pth\n",
            "2025-10-23 22:46:16,563 - VAETrainer - INFO - Epoch [83/100] Train TotalLoss: 34.0075, (ReconX: 0.9900, KLD: 0.0423*1.00, LossY: 3.2975*10.00), Time: 1.8s | Val TotalLoss: 34.0417, (ReconX: 0.9916, KLD: 0.0426, LossY: 3.3007)\n",
            "INFO:VAETrainer:Epoch [83/100] Train TotalLoss: 34.0075, (ReconX: 0.9900, KLD: 0.0423*1.00, LossY: 3.2975*10.00), Time: 1.8s | Val TotalLoss: 34.0417, (ReconX: 0.9916, KLD: 0.0426, LossY: 3.3007)\n",
            "2025-10-23 22:46:16,565 - VAETrainer - INFO - Saving model and trainer state to vae_results/best_model.pth...\n",
            "INFO:VAETrainer:Saving model and trainer state to vae_results/best_model.pth...\n",
            "2025-10-23 22:46:16,578 - VAETrainer - INFO - Successfully saved model and trainer state to vae_results/best_model.pth\n",
            "INFO:VAETrainer:Successfully saved model and trainer state to vae_results/best_model.pth\n",
            "2025-10-23 22:46:18,745 - VAETrainer - INFO - Epoch [84/100] Train TotalLoss: 33.9516, (ReconX: 0.9897, KLD: 0.0417*1.00, LossY: 3.2920*10.00), Time: 1.8s | Val TotalLoss: 34.0871, (ReconX: 0.9927, KLD: 0.0430, LossY: 3.3051)\n",
            "INFO:VAETrainer:Epoch [84/100] Train TotalLoss: 33.9516, (ReconX: 0.9897, KLD: 0.0417*1.00, LossY: 3.2920*10.00), Time: 1.8s | Val TotalLoss: 34.0871, (ReconX: 0.9927, KLD: 0.0430, LossY: 3.3051)\n",
            "2025-10-23 22:46:20,912 - VAETrainer - INFO - Epoch [85/100] Train TotalLoss: 33.9283, (ReconX: 0.9901, KLD: 0.0423*1.00, LossY: 3.2896*10.00), Time: 1.8s | Val TotalLoss: 33.9764, (ReconX: 0.9921, KLD: 0.0414, LossY: 3.2943)\n",
            "INFO:VAETrainer:Epoch [85/100] Train TotalLoss: 33.9283, (ReconX: 0.9901, KLD: 0.0423*1.00, LossY: 3.2896*10.00), Time: 1.8s | Val TotalLoss: 33.9764, (ReconX: 0.9921, KLD: 0.0414, LossY: 3.2943)\n",
            "2025-10-23 22:46:20,914 - VAETrainer - INFO - Saving model and trainer state to vae_results/best_model.pth...\n",
            "INFO:VAETrainer:Saving model and trainer state to vae_results/best_model.pth...\n",
            "2025-10-23 22:46:20,927 - VAETrainer - INFO - Successfully saved model and trainer state to vae_results/best_model.pth\n",
            "INFO:VAETrainer:Successfully saved model and trainer state to vae_results/best_model.pth\n",
            "2025-10-23 22:46:23,106 - VAETrainer - INFO - Epoch [86/100] Train TotalLoss: 33.8640, (ReconX: 0.9905, KLD: 0.0411*1.00, LossY: 3.2832*10.00), Time: 1.8s | Val TotalLoss: 33.9384, (ReconX: 0.9925, KLD: 0.0413, LossY: 3.2905)\n",
            "INFO:VAETrainer:Epoch [86/100] Train TotalLoss: 33.8640, (ReconX: 0.9905, KLD: 0.0411*1.00, LossY: 3.2832*10.00), Time: 1.8s | Val TotalLoss: 33.9384, (ReconX: 0.9925, KLD: 0.0413, LossY: 3.2905)\n",
            "2025-10-23 22:46:23,107 - VAETrainer - INFO - Saving model and trainer state to vae_results/best_model.pth...\n",
            "INFO:VAETrainer:Saving model and trainer state to vae_results/best_model.pth...\n",
            "2025-10-23 22:46:23,121 - VAETrainer - INFO - Successfully saved model and trainer state to vae_results/best_model.pth\n",
            "INFO:VAETrainer:Successfully saved model and trainer state to vae_results/best_model.pth\n",
            "2025-10-23 22:46:25,245 - VAETrainer - INFO - Epoch [87/100] Train TotalLoss: 33.8165, (ReconX: 0.9905, KLD: 0.0411*1.00, LossY: 3.2785*10.00), Time: 1.7s | Val TotalLoss: 33.9087, (ReconX: 0.9941, KLD: 0.0409, LossY: 3.2874)\n",
            "INFO:VAETrainer:Epoch [87/100] Train TotalLoss: 33.8165, (ReconX: 0.9905, KLD: 0.0411*1.00, LossY: 3.2785*10.00), Time: 1.7s | Val TotalLoss: 33.9087, (ReconX: 0.9941, KLD: 0.0409, LossY: 3.2874)\n",
            "2025-10-23 22:46:25,246 - VAETrainer - INFO - Saving model and trainer state to vae_results/best_model.pth...\n",
            "INFO:VAETrainer:Saving model and trainer state to vae_results/best_model.pth...\n",
            "2025-10-23 22:46:25,260 - VAETrainer - INFO - Successfully saved model and trainer state to vae_results/best_model.pth\n",
            "INFO:VAETrainer:Successfully saved model and trainer state to vae_results/best_model.pth\n",
            "2025-10-23 22:46:27,432 - VAETrainer - INFO - Epoch [88/100] Train TotalLoss: 33.7860, (ReconX: 0.9898, KLD: 0.0419*1.00, LossY: 3.2754*10.00), Time: 1.8s | Val TotalLoss: 33.8608, (ReconX: 0.9922, KLD: 0.0429, LossY: 3.2826)\n",
            "INFO:VAETrainer:Epoch [88/100] Train TotalLoss: 33.7860, (ReconX: 0.9898, KLD: 0.0419*1.00, LossY: 3.2754*10.00), Time: 1.8s | Val TotalLoss: 33.8608, (ReconX: 0.9922, KLD: 0.0429, LossY: 3.2826)\n",
            "2025-10-23 22:46:27,434 - VAETrainer - INFO - Saving model and trainer state to vae_results/best_model.pth...\n",
            "INFO:VAETrainer:Saving model and trainer state to vae_results/best_model.pth...\n",
            "2025-10-23 22:46:27,452 - VAETrainer - INFO - Successfully saved model and trainer state to vae_results/best_model.pth\n",
            "INFO:VAETrainer:Successfully saved model and trainer state to vae_results/best_model.pth\n",
            "2025-10-23 22:46:29,621 - VAETrainer - INFO - Epoch [89/100] Train TotalLoss: 33.7279, (ReconX: 0.9898, KLD: 0.0418*1.00, LossY: 3.2696*10.00), Time: 1.8s | Val TotalLoss: 33.8203, (ReconX: 0.9932, KLD: 0.0407, LossY: 3.2786)\n",
            "INFO:VAETrainer:Epoch [89/100] Train TotalLoss: 33.7279, (ReconX: 0.9898, KLD: 0.0418*1.00, LossY: 3.2696*10.00), Time: 1.8s | Val TotalLoss: 33.8203, (ReconX: 0.9932, KLD: 0.0407, LossY: 3.2786)\n",
            "2025-10-23 22:46:29,623 - VAETrainer - INFO - Saving model and trainer state to vae_results/best_model.pth...\n",
            "INFO:VAETrainer:Saving model and trainer state to vae_results/best_model.pth...\n",
            "2025-10-23 22:46:29,637 - VAETrainer - INFO - Successfully saved model and trainer state to vae_results/best_model.pth\n",
            "INFO:VAETrainer:Successfully saved model and trainer state to vae_results/best_model.pth\n",
            "2025-10-23 22:46:31,721 - VAETrainer - INFO - Epoch [90/100] Train TotalLoss: 33.7107, (ReconX: 0.9903, KLD: 0.0412*1.00, LossY: 3.2679*10.00), Time: 1.7s | Val TotalLoss: 33.8993, (ReconX: 0.9932, KLD: 0.0407, LossY: 3.2865)\n",
            "INFO:VAETrainer:Epoch [90/100] Train TotalLoss: 33.7107, (ReconX: 0.9903, KLD: 0.0412*1.00, LossY: 3.2679*10.00), Time: 1.7s | Val TotalLoss: 33.8993, (ReconX: 0.9932, KLD: 0.0407, LossY: 3.2865)\n",
            "2025-10-23 22:46:33,837 - VAETrainer - INFO - Epoch [91/100] Train TotalLoss: 33.6336, (ReconX: 0.9906, KLD: 0.0416*1.00, LossY: 3.2601*10.00), Time: 1.7s | Val TotalLoss: 33.7610, (ReconX: 0.9932, KLD: 0.0405, LossY: 3.2727)\n",
            "INFO:VAETrainer:Epoch [91/100] Train TotalLoss: 33.6336, (ReconX: 0.9906, KLD: 0.0416*1.00, LossY: 3.2601*10.00), Time: 1.7s | Val TotalLoss: 33.7610, (ReconX: 0.9932, KLD: 0.0405, LossY: 3.2727)\n",
            "2025-10-23 22:46:33,838 - VAETrainer - INFO - Saving model and trainer state to vae_results/best_model.pth...\n",
            "INFO:VAETrainer:Saving model and trainer state to vae_results/best_model.pth...\n",
            "2025-10-23 22:46:33,851 - VAETrainer - INFO - Successfully saved model and trainer state to vae_results/best_model.pth\n",
            "INFO:VAETrainer:Successfully saved model and trainer state to vae_results/best_model.pth\n",
            "2025-10-23 22:46:36,008 - VAETrainer - INFO - Epoch [92/100] Train TotalLoss: 33.6185, (ReconX: 0.9903, KLD: 0.0415*1.00, LossY: 3.2587*10.00), Time: 1.8s | Val TotalLoss: 33.7086, (ReconX: 0.9936, KLD: 0.0404, LossY: 3.2675)\n",
            "INFO:VAETrainer:Epoch [92/100] Train TotalLoss: 33.6185, (ReconX: 0.9903, KLD: 0.0415*1.00, LossY: 3.2587*10.00), Time: 1.8s | Val TotalLoss: 33.7086, (ReconX: 0.9936, KLD: 0.0404, LossY: 3.2675)\n",
            "2025-10-23 22:46:36,010 - VAETrainer - INFO - Saving model and trainer state to vae_results/best_model.pth...\n",
            "INFO:VAETrainer:Saving model and trainer state to vae_results/best_model.pth...\n",
            "2025-10-23 22:46:36,024 - VAETrainer - INFO - Successfully saved model and trainer state to vae_results/best_model.pth\n",
            "INFO:VAETrainer:Successfully saved model and trainer state to vae_results/best_model.pth\n",
            "2025-10-23 22:46:38,137 - VAETrainer - INFO - Epoch [93/100] Train TotalLoss: 33.5387, (ReconX: 0.9906, KLD: 0.0408*1.00, LossY: 3.2507*10.00), Time: 1.7s | Val TotalLoss: 33.6103, (ReconX: 0.9932, KLD: 0.0396, LossY: 3.2578)\n",
            "INFO:VAETrainer:Epoch [93/100] Train TotalLoss: 33.5387, (ReconX: 0.9906, KLD: 0.0408*1.00, LossY: 3.2507*10.00), Time: 1.7s | Val TotalLoss: 33.6103, (ReconX: 0.9932, KLD: 0.0396, LossY: 3.2578)\n",
            "2025-10-23 22:46:38,139 - VAETrainer - INFO - Saving model and trainer state to vae_results/best_model.pth...\n",
            "INFO:VAETrainer:Saving model and trainer state to vae_results/best_model.pth...\n",
            "2025-10-23 22:46:38,154 - VAETrainer - INFO - Successfully saved model and trainer state to vae_results/best_model.pth\n",
            "INFO:VAETrainer:Successfully saved model and trainer state to vae_results/best_model.pth\n",
            "2025-10-23 22:46:40,322 - VAETrainer - INFO - Epoch [94/100] Train TotalLoss: 33.5315, (ReconX: 0.9903, KLD: 0.0412*1.00, LossY: 3.2500*10.00), Time: 1.8s | Val TotalLoss: 33.6629, (ReconX: 0.9933, KLD: 0.0423, LossY: 3.2627)\n",
            "INFO:VAETrainer:Epoch [94/100] Train TotalLoss: 33.5315, (ReconX: 0.9903, KLD: 0.0412*1.00, LossY: 3.2500*10.00), Time: 1.8s | Val TotalLoss: 33.6629, (ReconX: 0.9933, KLD: 0.0423, LossY: 3.2627)\n",
            "2025-10-23 22:46:42,449 - VAETrainer - INFO - Epoch [95/100] Train TotalLoss: 33.4689, (ReconX: 0.9906, KLD: 0.0412*1.00, LossY: 3.2437*10.00), Time: 1.8s | Val TotalLoss: 33.6694, (ReconX: 0.9927, KLD: 0.0416, LossY: 3.2635)\n",
            "INFO:VAETrainer:Epoch [95/100] Train TotalLoss: 33.4689, (ReconX: 0.9906, KLD: 0.0412*1.00, LossY: 3.2437*10.00), Time: 1.8s | Val TotalLoss: 33.6694, (ReconX: 0.9927, KLD: 0.0416, LossY: 3.2635)\n",
            "2025-10-23 22:46:44,614 - VAETrainer - INFO - Epoch [96/100] Train TotalLoss: 33.4764, (ReconX: 0.9904, KLD: 0.0410*1.00, LossY: 3.2445*10.00), Time: 1.8s | Val TotalLoss: 33.6055, (ReconX: 0.9935, KLD: 0.0416, LossY: 3.2570)\n",
            "INFO:VAETrainer:Epoch [96/100] Train TotalLoss: 33.4764, (ReconX: 0.9904, KLD: 0.0410*1.00, LossY: 3.2445*10.00), Time: 1.8s | Val TotalLoss: 33.6055, (ReconX: 0.9935, KLD: 0.0416, LossY: 3.2570)\n",
            "2025-10-23 22:46:44,616 - VAETrainer - INFO - Saving model and trainer state to vae_results/best_model.pth...\n",
            "INFO:VAETrainer:Saving model and trainer state to vae_results/best_model.pth...\n",
            "2025-10-23 22:46:44,629 - VAETrainer - INFO - Successfully saved model and trainer state to vae_results/best_model.pth\n",
            "INFO:VAETrainer:Successfully saved model and trainer state to vae_results/best_model.pth\n",
            "2025-10-23 22:46:46,779 - VAETrainer - INFO - Epoch [97/100] Train TotalLoss: 33.3705, (ReconX: 0.9906, KLD: 0.0402*1.00, LossY: 3.2340*10.00), Time: 1.8s | Val TotalLoss: 33.5872, (ReconX: 0.9938, KLD: 0.0400, LossY: 3.2553)\n",
            "INFO:VAETrainer:Epoch [97/100] Train TotalLoss: 33.3705, (ReconX: 0.9906, KLD: 0.0402*1.00, LossY: 3.2340*10.00), Time: 1.8s | Val TotalLoss: 33.5872, (ReconX: 0.9938, KLD: 0.0400, LossY: 3.2553)\n",
            "2025-10-23 22:46:46,781 - VAETrainer - INFO - Saving model and trainer state to vae_results/best_model.pth...\n",
            "INFO:VAETrainer:Saving model and trainer state to vae_results/best_model.pth...\n",
            "2025-10-23 22:46:46,794 - VAETrainer - INFO - Successfully saved model and trainer state to vae_results/best_model.pth\n",
            "INFO:VAETrainer:Successfully saved model and trainer state to vae_results/best_model.pth\n",
            "2025-10-23 22:46:48,851 - VAETrainer - INFO - Epoch [98/100] Train TotalLoss: 33.3479, (ReconX: 0.9908, KLD: 0.0402*1.00, LossY: 3.2317*10.00), Time: 1.7s | Val TotalLoss: 33.4630, (ReconX: 0.9921, KLD: 0.0410, LossY: 3.2430)\n",
            "INFO:VAETrainer:Epoch [98/100] Train TotalLoss: 33.3479, (ReconX: 0.9908, KLD: 0.0402*1.00, LossY: 3.2317*10.00), Time: 1.7s | Val TotalLoss: 33.4630, (ReconX: 0.9921, KLD: 0.0410, LossY: 3.2430)\n",
            "2025-10-23 22:46:48,853 - VAETrainer - INFO - Saving model and trainer state to vae_results/best_model.pth...\n",
            "INFO:VAETrainer:Saving model and trainer state to vae_results/best_model.pth...\n",
            "2025-10-23 22:46:48,866 - VAETrainer - INFO - Successfully saved model and trainer state to vae_results/best_model.pth\n",
            "INFO:VAETrainer:Successfully saved model and trainer state to vae_results/best_model.pth\n",
            "2025-10-23 22:46:51,201 - VAETrainer - INFO - Epoch [99/100] Train TotalLoss: 33.3223, (ReconX: 0.9907, KLD: 0.0402*1.00, LossY: 3.2291*10.00), Time: 1.9s | Val TotalLoss: 33.4466, (ReconX: 0.9942, KLD: 0.0394, LossY: 3.2413)\n",
            "INFO:VAETrainer:Epoch [99/100] Train TotalLoss: 33.3223, (ReconX: 0.9907, KLD: 0.0402*1.00, LossY: 3.2291*10.00), Time: 1.9s | Val TotalLoss: 33.4466, (ReconX: 0.9942, KLD: 0.0394, LossY: 3.2413)\n",
            "2025-10-23 22:46:51,203 - VAETrainer - INFO - Saving model and trainer state to vae_results/best_model.pth...\n",
            "INFO:VAETrainer:Saving model and trainer state to vae_results/best_model.pth...\n",
            "2025-10-23 22:46:51,217 - VAETrainer - INFO - Successfully saved model and trainer state to vae_results/best_model.pth\n",
            "INFO:VAETrainer:Successfully saved model and trainer state to vae_results/best_model.pth\n",
            "2025-10-23 22:46:53,298 - VAETrainer - INFO - Epoch [100/100] Train TotalLoss: 33.2969, (ReconX: 0.9907, KLD: 0.0405*1.00, LossY: 3.2266*10.00), Time: 1.7s | Val TotalLoss: 33.4471, (ReconX: 0.9930, KLD: 0.0406, LossY: 3.2414)\n",
            "INFO:VAETrainer:Epoch [100/100] Train TotalLoss: 33.2969, (ReconX: 0.9907, KLD: 0.0405*1.00, LossY: 3.2266*10.00), Time: 1.7s | Val TotalLoss: 33.4471, (ReconX: 0.9930, KLD: 0.0406, LossY: 3.2414)\n",
            "2025-10-23 22:46:53,300 - VAETrainer - INFO - Training loop finished.\n",
            "INFO:VAETrainer:Training loop finished.\n",
            "2025-10-23 22:46:53,301 - VAETrainer - INFO - Loading best model state from 'vae_results/best_model.pth' (Val Loss: 33.4466).\n",
            "INFO:VAETrainer:Loading best model state from 'vae_results/best_model.pth' (Val Loss: 33.4466).\n",
            "2025-10-23 22:46:53,303 - VAETrainer_Loader - INFO - Loading VAE model and trainer state from vae_results/best_model.pth to device cuda...\n",
            "INFO:VAETrainer_Loader:Loading VAE model and trainer state from vae_results/best_model.pth to device cuda...\n",
            "2025-10-23 22:46:53,348 - SemiSupMIWAE(X:42,Y:1,Lat:3) - INFO - SemiSupMIWAE Initialized: InputXDim=42, TargetYDim=1, LatentDim=3, AlphaPriceLoss=10.0, Device=cuda\n",
            "INFO:SemiSupMIWAE(X:42,Y:1,Lat:3):SemiSupMIWAE Initialized: InputXDim=42, TargetYDim=1, LatentDim=3, AlphaPriceLoss=10.0, Device=cuda\n",
            "2025-10-23 22:46:53,352 - VAETrainer_Loader - INFO - SemiSupMIWAE model created and state loaded successfully.\n",
            "INFO:VAETrainer_Loader:SemiSupMIWAE model created and state loaded successfully.\n",
            "2025-10-23 22:46:53,355 - VAETrainer - INFO - VAETrainer initialized for SemiSupMIWAE. Device: cuda. Optimizer: adam. LR: 0.0003. ReconLoss(X): 'mse'. Alpha(Y Loss): 10.0. LossType(Y): 'gaussian_nll'\n",
            "INFO:VAETrainer:VAETrainer initialized for SemiSupMIWAE. Device: cuda. Optimizer: adam. LR: 0.0003. ReconLoss(X): 'mse'. Alpha(Y Loss): 10.0. LossType(Y): 'gaussian_nll'\n",
            "2025-10-23 22:46:53,356 - VAETrainer_Loader - INFO - VAETrainer instance created and state loaded for model from vae_results/best_model.pth.\n",
            "INFO:VAETrainer_Loader:VAETrainer instance created and state loaded for model from vae_results/best_model.pth.\n",
            "2025-10-23 22:46:53,367 - VAETrainer - INFO - Predicting target (Y) mean and log-variance for 12271 samples...\n",
            "INFO:VAETrainer:Predicting target (Y) mean and log-variance for 12271 samples...\n",
            "2025-10-23 22:46:53,431 - MainRealEstateAnalysis_MIWAE - INFO - [Fold 6] Stored test predictions for 12271 rows.\n",
            "INFO:MainRealEstateAnalysis_MIWAE:[Fold 6] Stored test predictions for 12271 rows.\n",
            "2025-10-23 22:46:53,444 - MainRealEstateAnalysis_MIWAE - INFO - [Fold 7/10] sizes → Train=98170 (80.0%), Val=12271 (10.0%), Test=12271 (10.0%)\n",
            "INFO:MainRealEstateAnalysis_MIWAE:[Fold 7/10] sizes → Train=98170 (80.0%), Val=12271 (10.0%), Test=12271 (10.0%)\n",
            "2025-10-23 22:46:53,445 - __main__.create_vae_from_artifacts - INFO - Creating VAE model. Target column for supervision: log_sale_price.\n",
            "2025-10-23 22:46:53,445 - __main__.create_vae_from_artifacts - INFO - Creating VAE model. Target column for supervision: log_sale_price.\n",
            "INFO:__main__.create_vae_from_artifacts:Creating VAE model. Target column for supervision: log_sale_price.\n",
            "2025-10-23 22:46:53,446 - __main__.create_vae_from_artifacts - INFO - Number of input features for VAE (X): 42 (Features: ['lotarea', 'assess_total', 'exempt_total', 'assessland', 'exemptland']...)\n",
            "2025-10-23 22:46:53,446 - __main__.create_vae_from_artifacts - INFO - Number of input features for VAE (X): 42 (Features: ['lotarea', 'assess_total', 'exempt_total', 'assessland', 'exemptland']...)\n",
            "INFO:__main__.create_vae_from_artifacts:Number of input features for VAE (X): 42 (Features: ['lotarea', 'assess_total', 'exempt_total', 'assessland', 'exemptland']...)\n",
            "2025-10-23 22:46:53,448 - __main__.create_vae_from_artifacts - INFO - Semi-supervised mode: Target 'log_sale_price' (y_dim=1). Alpha for price loss: 10.0.\n",
            "2025-10-23 22:46:53,448 - __main__.create_vae_from_artifacts - INFO - Semi-supervised mode: Target 'log_sale_price' (y_dim=1). Alpha for price loss: 10.0.\n",
            "INFO:__main__.create_vae_from_artifacts:Semi-supervised mode: Target 'log_sale_price' (y_dim=1). Alpha for price loss: 10.0.\n",
            "2025-10-23 22:46:53,449 - __main__.create_vae_from_artifacts - INFO - Using overridden latent_dim: 3\n",
            "2025-10-23 22:46:53,449 - __main__.create_vae_from_artifacts - INFO - Using overridden latent_dim: 3\n",
            "INFO:__main__.create_vae_from_artifacts:Using overridden latent_dim: 3\n",
            "2025-10-23 22:46:53,450 - __main__.create_vae_from_artifacts - INFO - Final effective Latent Dimension: 3\n",
            "2025-10-23 22:46:53,450 - __main__.create_vae_from_artifacts - INFO - Final effective Latent Dimension: 3\n",
            "INFO:__main__.create_vae_from_artifacts:Final effective Latent Dimension: 3\n",
            "2025-10-23 22:46:53,453 - __main__.create_vae_from_artifacts - INFO - Encoder layers: [21], Decoder layers: [21]\n",
            "2025-10-23 22:46:53,453 - __main__.create_vae_from_artifacts - INFO - Encoder layers: [21], Decoder layers: [21]\n",
            "INFO:__main__.create_vae_from_artifacts:Encoder layers: [21], Decoder layers: [21]\n",
            "2025-10-23 22:46:53,454 - __main__.create_vae_from_artifacts - INFO - Using determined prior: Type=student_t_mixture, K=2 (from overrides or artifact inference).\n",
            "2025-10-23 22:46:53,454 - __main__.create_vae_from_artifacts - INFO - Using determined prior: Type=student_t_mixture, K=2 (from overrides or artifact inference).\n",
            "INFO:__main__.create_vae_from_artifacts:Using determined prior: Type=student_t_mixture, K=2 (from overrides or artifact inference).\n",
            "2025-10-23 22:46:53,455 - __main__.create_vae_from_artifacts - INFO - Initializing parameters for student_t_mixture with K=2 components...\n",
            "2025-10-23 22:46:53,455 - __main__.create_vae_from_artifacts - INFO - Initializing parameters for student_t_mixture with K=2 components...\n",
            "INFO:__main__.create_vae_from_artifacts:Initializing parameters for student_t_mixture with K=2 components...\n",
            "2025-10-23 22:46:53,457 - __main__.create_vae_from_artifacts - WARNING - DP-GMM artifacts are unsuitable or insufficient for initializing mixture prior parameters. Default parameters (e.g., zeros for means/logits, identity for covariances) will be used by the VAE.\n",
            "2025-10-23 22:46:53,457 - __main__.create_vae_from_artifacts - WARNING - DP-GMM artifacts are unsuitable or insufficient for initializing mixture prior parameters. Default parameters (e.g., zeros for means/logits, identity for covariances) will be used by the VAE.\n",
            "WARNING:__main__.create_vae_from_artifacts:DP-GMM artifacts are unsuitable or insufficient for initializing mixture prior parameters. Default parameters (e.g., zeros for means/logits, identity for covariances) will be used by the VAE.\n",
            "2025-10-23 22:46:53,458 - __main__.create_vae_from_artifacts - INFO - Setting Student-t df to 4.0 for prior.\n",
            "2025-10-23 22:46:53,458 - __main__.create_vae_from_artifacts - INFO - Setting Student-t df to 4.0 for prior.\n",
            "INFO:__main__.create_vae_from_artifacts:Setting Student-t df to 4.0 for prior.\n",
            "2025-10-23 22:46:53,459 - __main__.create_vae_from_artifacts - INFO - Instantiating SemiSupMIWAE model...\n",
            "2025-10-23 22:46:53,459 - __main__.create_vae_from_artifacts - INFO - Instantiating SemiSupMIWAE model...\n",
            "INFO:__main__.create_vae_from_artifacts:Instantiating SemiSupMIWAE model...\n",
            "2025-10-23 22:46:53,460 - __main__.create_vae_from_artifacts - INFO - Price head layers for SemiSupMIWAE: [8, 4]\n",
            "2025-10-23 22:46:53,460 - __main__.create_vae_from_artifacts - INFO - Price head layers for SemiSupMIWAE: [8, 4]\n",
            "INFO:__main__.create_vae_from_artifacts:Price head layers for SemiSupMIWAE: [8, 4]\n",
            "2025-10-23 22:46:53,467 - SemiSupMIWAE(X:42,Y:1,Lat:3) - INFO - SemiSupMIWAE Initialized: InputXDim=42, TargetYDim=1, LatentDim=3, AlphaPriceLoss=10.0, Device=cuda\n",
            "INFO:SemiSupMIWAE(X:42,Y:1,Lat:3):SemiSupMIWAE Initialized: InputXDim=42, TargetYDim=1, LatentDim=3, AlphaPriceLoss=10.0, Device=cuda\n",
            "2025-10-23 22:46:53,468 - __main__.create_vae_from_artifacts - INFO - SemiSupMIWAE model created successfully on device: cuda.\n",
            "2025-10-23 22:46:53,468 - __main__.create_vae_from_artifacts - INFO - SemiSupMIWAE model created successfully on device: cuda.\n",
            "INFO:__main__.create_vae_from_artifacts:SemiSupMIWAE model created successfully on device: cuda.\n",
            "2025-10-23 22:46:53,470 - VAETrainer - INFO - VAETrainer initialized for SemiSupMIWAE. Device: cuda. Optimizer: adam. LR: 0.0003. ReconLoss(X): 'mse'. Alpha(Y Loss): 10.0. LossType(Y): 'gaussian_nll'\n",
            "INFO:VAETrainer:VAETrainer initialized for SemiSupMIWAE. Device: cuda. Optimizer: adam. LR: 0.0003. ReconLoss(X): 'mse'. Alpha(Y Loss): 10.0. LossType(Y): 'gaussian_nll'\n",
            "2025-10-23 22:46:53,472 - VAETrainer - INFO - Starting training: Epochs=100, BatchSize=512, KLD_weight=1.0, KLD_AnnealEpochs=30, Alpha(Y_Loss)=10.0.\n",
            "INFO:VAETrainer:Starting training: Epochs=100, BatchSize=512, KLD_weight=1.0, KLD_AnnealEpochs=30, Alpha(Y_Loss)=10.0.\n",
            "2025-10-23 22:46:53,473 - VAETrainer - INFO - Train loader: 98170 samples, 192 batches. Drop last: False\n",
            "INFO:VAETrainer:Train loader: 98170 samples, 192 batches. Drop last: False\n",
            "2025-10-23 22:46:53,474 - VAETrainer - INFO - Validation loader: 12271 samples, 24 batches.\n",
            "INFO:VAETrainer:Validation loader: 12271 samples, 24 batches.\n",
            "2025-10-23 22:46:55,552 - VAETrainer - INFO - Epoch [1/100] Train TotalLoss: 517.2436, (ReconX: 1.0646, KLD: 0.3617*0.03, LossY: 51.6167*10.00), Time: 1.7s | Val TotalLoss: 417.9819, (ReconX: 1.0063, KLD: 0.4074, LossY: 41.6568)\n",
            "INFO:VAETrainer:Epoch [1/100] Train TotalLoss: 517.2436, (ReconX: 1.0646, KLD: 0.3617*0.03, LossY: 51.6167*10.00), Time: 1.7s | Val TotalLoss: 417.9819, (ReconX: 1.0063, KLD: 0.4074, LossY: 41.6568)\n",
            "2025-10-23 22:46:55,554 - VAETrainer - INFO - Saving model and trainer state to vae_results/best_model.pth...\n",
            "INFO:VAETrainer:Saving model and trainer state to vae_results/best_model.pth...\n",
            "2025-10-23 22:46:55,568 - VAETrainer - INFO - Successfully saved model and trainer state to vae_results/best_model.pth\n",
            "INFO:VAETrainer:Successfully saved model and trainer state to vae_results/best_model.pth\n",
            "2025-10-23 22:46:57,722 - VAETrainer - INFO - Epoch [2/100] Train TotalLoss: 362.1512, (ReconX: 0.9626, KLD: 0.4201*0.07, LossY: 36.1161*10.00), Time: 1.8s | Val TotalLoss: 315.0477, (ReconX: 0.9530, KLD: 0.4624, LossY: 31.3632)\n",
            "INFO:VAETrainer:Epoch [2/100] Train TotalLoss: 362.1512, (ReconX: 0.9626, KLD: 0.4201*0.07, LossY: 36.1161*10.00), Time: 1.8s | Val TotalLoss: 315.0477, (ReconX: 0.9530, KLD: 0.4624, LossY: 31.3632)\n",
            "2025-10-23 22:46:57,724 - VAETrainer - INFO - Saving model and trainer state to vae_results/best_model.pth...\n",
            "INFO:VAETrainer:Saving model and trainer state to vae_results/best_model.pth...\n",
            "2025-10-23 22:46:57,739 - VAETrainer - INFO - Successfully saved model and trainer state to vae_results/best_model.pth\n",
            "INFO:VAETrainer:Successfully saved model and trainer state to vae_results/best_model.pth\n",
            "2025-10-23 22:46:59,984 - VAETrainer - INFO - Epoch [3/100] Train TotalLoss: 265.7215, (ReconX: 0.9348, KLD: 0.4323*0.10, LossY: 26.4744*10.00), Time: 1.9s | Val TotalLoss: 220.1079, (ReconX: 0.9424, KLD: 0.4404, LossY: 21.8725)\n",
            "INFO:VAETrainer:Epoch [3/100] Train TotalLoss: 265.7215, (ReconX: 0.9348, KLD: 0.4323*0.10, LossY: 26.4744*10.00), Time: 1.9s | Val TotalLoss: 220.1079, (ReconX: 0.9424, KLD: 0.4404, LossY: 21.8725)\n",
            "2025-10-23 22:46:59,986 - VAETrainer - INFO - Saving model and trainer state to vae_results/best_model.pth...\n",
            "INFO:VAETrainer:Saving model and trainer state to vae_results/best_model.pth...\n",
            "2025-10-23 22:47:00,000 - VAETrainer - INFO - Successfully saved model and trainer state to vae_results/best_model.pth\n",
            "INFO:VAETrainer:Successfully saved model and trainer state to vae_results/best_model.pth\n",
            "2025-10-23 22:47:02,173 - VAETrainer - INFO - Epoch [4/100] Train TotalLoss: 188.2697, (ReconX: 0.9306, KLD: 0.3966*0.13, LossY: 18.7286*10.00), Time: 1.8s | Val TotalLoss: 158.9435, (ReconX: 0.9461, KLD: 0.3827, LossY: 15.7615)\n",
            "INFO:VAETrainer:Epoch [4/100] Train TotalLoss: 188.2697, (ReconX: 0.9306, KLD: 0.3966*0.13, LossY: 18.7286*10.00), Time: 1.8s | Val TotalLoss: 158.9435, (ReconX: 0.9461, KLD: 0.3827, LossY: 15.7615)\n",
            "2025-10-23 22:47:02,175 - VAETrainer - INFO - Saving model and trainer state to vae_results/best_model.pth...\n",
            "INFO:VAETrainer:Saving model and trainer state to vae_results/best_model.pth...\n",
            "2025-10-23 22:47:02,192 - VAETrainer - INFO - Successfully saved model and trainer state to vae_results/best_model.pth\n",
            "INFO:VAETrainer:Successfully saved model and trainer state to vae_results/best_model.pth\n",
            "2025-10-23 22:47:04,451 - VAETrainer - INFO - Epoch [5/100] Train TotalLoss: 136.9739, (ReconX: 0.9339, KLD: 0.3484*0.17, LossY: 13.5982*10.00), Time: 1.9s | Val TotalLoss: 119.2275, (ReconX: 0.9487, KLD: 0.3400, LossY: 11.7939)\n",
            "INFO:VAETrainer:Epoch [5/100] Train TotalLoss: 136.9739, (ReconX: 0.9339, KLD: 0.3484*0.17, LossY: 13.5982*10.00), Time: 1.9s | Val TotalLoss: 119.2275, (ReconX: 0.9487, KLD: 0.3400, LossY: 11.7939)\n",
            "2025-10-23 22:47:04,452 - VAETrainer - INFO - Saving model and trainer state to vae_results/best_model.pth...\n",
            "INFO:VAETrainer:Saving model and trainer state to vae_results/best_model.pth...\n",
            "2025-10-23 22:47:04,466 - VAETrainer - INFO - Successfully saved model and trainer state to vae_results/best_model.pth\n",
            "INFO:VAETrainer:Successfully saved model and trainer state to vae_results/best_model.pth\n",
            "2025-10-23 22:47:06,658 - VAETrainer - INFO - Epoch [6/100] Train TotalLoss: 104.7237, (ReconX: 0.9344, KLD: 0.3233*0.20, LossY: 10.3725*10.00), Time: 1.8s | Val TotalLoss: 94.9351, (ReconX: 0.9478, KLD: 0.3205, LossY: 9.3667)\n",
            "INFO:VAETrainer:Epoch [6/100] Train TotalLoss: 104.7237, (ReconX: 0.9344, KLD: 0.3233*0.20, LossY: 10.3725*10.00), Time: 1.8s | Val TotalLoss: 94.9351, (ReconX: 0.9478, KLD: 0.3205, LossY: 9.3667)\n",
            "2025-10-23 22:47:06,660 - VAETrainer - INFO - Saving model and trainer state to vae_results/best_model.pth...\n",
            "INFO:VAETrainer:Saving model and trainer state to vae_results/best_model.pth...\n",
            "2025-10-23 22:47:06,673 - VAETrainer - INFO - Successfully saved model and trainer state to vae_results/best_model.pth\n",
            "INFO:VAETrainer:Successfully saved model and trainer state to vae_results/best_model.pth\n",
            "2025-10-23 22:47:08,828 - VAETrainer - INFO - Epoch [7/100] Train TotalLoss: 84.2469, (ReconX: 0.9357, KLD: 0.3113*0.23, LossY: 8.3239*10.00), Time: 1.8s | Val TotalLoss: 76.5192, (ReconX: 0.9497, KLD: 0.3141, LossY: 7.5255)\n",
            "INFO:VAETrainer:Epoch [7/100] Train TotalLoss: 84.2469, (ReconX: 0.9357, KLD: 0.3113*0.23, LossY: 8.3239*10.00), Time: 1.8s | Val TotalLoss: 76.5192, (ReconX: 0.9497, KLD: 0.3141, LossY: 7.5255)\n",
            "2025-10-23 22:47:08,830 - VAETrainer - INFO - Saving model and trainer state to vae_results/best_model.pth...\n",
            "INFO:VAETrainer:Saving model and trainer state to vae_results/best_model.pth...\n",
            "2025-10-23 22:47:08,843 - VAETrainer - INFO - Successfully saved model and trainer state to vae_results/best_model.pth\n",
            "INFO:VAETrainer:Successfully saved model and trainer state to vae_results/best_model.pth\n",
            "2025-10-23 22:47:10,976 - VAETrainer - INFO - Epoch [8/100] Train TotalLoss: 70.1418, (ReconX: 0.9354, KLD: 0.3061*0.27, LossY: 6.9125*10.00), Time: 1.8s | Val TotalLoss: 64.7880, (ReconX: 0.9484, KLD: 0.3087, LossY: 6.3531)\n",
            "INFO:VAETrainer:Epoch [8/100] Train TotalLoss: 70.1418, (ReconX: 0.9354, KLD: 0.3061*0.27, LossY: 6.9125*10.00), Time: 1.8s | Val TotalLoss: 64.7880, (ReconX: 0.9484, KLD: 0.3087, LossY: 6.3531)\n",
            "2025-10-23 22:47:10,978 - VAETrainer - INFO - Saving model and trainer state to vae_results/best_model.pth...\n",
            "INFO:VAETrainer:Saving model and trainer state to vae_results/best_model.pth...\n",
            "2025-10-23 22:47:10,991 - VAETrainer - INFO - Successfully saved model and trainer state to vae_results/best_model.pth\n",
            "INFO:VAETrainer:Successfully saved model and trainer state to vae_results/best_model.pth\n",
            "2025-10-23 22:47:13,245 - VAETrainer - INFO - Epoch [9/100] Train TotalLoss: 60.0706, (ReconX: 0.9355, KLD: 0.3056*0.30, LossY: 5.9043*10.00), Time: 1.9s | Val TotalLoss: 56.7270, (ReconX: 0.9493, KLD: 0.3090, LossY: 5.5469)\n",
            "INFO:VAETrainer:Epoch [9/100] Train TotalLoss: 60.0706, (ReconX: 0.9355, KLD: 0.3056*0.30, LossY: 5.9043*10.00), Time: 1.9s | Val TotalLoss: 56.7270, (ReconX: 0.9493, KLD: 0.3090, LossY: 5.5469)\n",
            "2025-10-23 22:47:13,247 - VAETrainer - INFO - Saving model and trainer state to vae_results/best_model.pth...\n",
            "INFO:VAETrainer:Saving model and trainer state to vae_results/best_model.pth...\n",
            "2025-10-23 22:47:13,261 - VAETrainer - INFO - Successfully saved model and trainer state to vae_results/best_model.pth\n",
            "INFO:VAETrainer:Successfully saved model and trainer state to vae_results/best_model.pth\n",
            "2025-10-23 22:47:15,445 - VAETrainer - INFO - Epoch [10/100] Train TotalLoss: 52.7608, (ReconX: 0.9355, KLD: 0.3048*0.33, LossY: 5.1724*10.00), Time: 1.8s | Val TotalLoss: 49.6202, (ReconX: 0.9507, KLD: 0.3094, LossY: 4.8360)\n",
            "INFO:VAETrainer:Epoch [10/100] Train TotalLoss: 52.7608, (ReconX: 0.9355, KLD: 0.3048*0.33, LossY: 5.1724*10.00), Time: 1.8s | Val TotalLoss: 49.6202, (ReconX: 0.9507, KLD: 0.3094, LossY: 4.8360)\n",
            "2025-10-23 22:47:15,447 - VAETrainer - INFO - Saving model and trainer state to vae_results/best_model.pth...\n",
            "INFO:VAETrainer:Saving model and trainer state to vae_results/best_model.pth...\n",
            "2025-10-23 22:47:15,460 - VAETrainer - INFO - Successfully saved model and trainer state to vae_results/best_model.pth\n",
            "INFO:VAETrainer:Successfully saved model and trainer state to vae_results/best_model.pth\n",
            "2025-10-23 22:47:17,641 - VAETrainer - INFO - Epoch [11/100] Train TotalLoss: 47.3986, (ReconX: 0.9352, KLD: 0.3055*0.37, LossY: 4.6351*10.00), Time: 1.8s | Val TotalLoss: 44.7830, (ReconX: 0.9480, KLD: 0.3107, LossY: 4.3524)\n",
            "INFO:VAETrainer:Epoch [11/100] Train TotalLoss: 47.3986, (ReconX: 0.9352, KLD: 0.3055*0.37, LossY: 4.6351*10.00), Time: 1.8s | Val TotalLoss: 44.7830, (ReconX: 0.9480, KLD: 0.3107, LossY: 4.3524)\n",
            "2025-10-23 22:47:17,643 - VAETrainer - INFO - Saving model and trainer state to vae_results/best_model.pth...\n",
            "INFO:VAETrainer:Saving model and trainer state to vae_results/best_model.pth...\n",
            "2025-10-23 22:47:17,656 - VAETrainer - INFO - Successfully saved model and trainer state to vae_results/best_model.pth\n",
            "INFO:VAETrainer:Successfully saved model and trainer state to vae_results/best_model.pth\n",
            "2025-10-23 22:47:19,840 - VAETrainer - INFO - Epoch [12/100] Train TotalLoss: 43.2491, (ReconX: 0.9365, KLD: 0.3043*0.40, LossY: 4.2191*10.00), Time: 1.8s | Val TotalLoss: 42.3179, (ReconX: 0.9492, KLD: 0.3053, LossY: 4.1063)\n",
            "INFO:VAETrainer:Epoch [12/100] Train TotalLoss: 43.2491, (ReconX: 0.9365, KLD: 0.3043*0.40, LossY: 4.2191*10.00), Time: 1.8s | Val TotalLoss: 42.3179, (ReconX: 0.9492, KLD: 0.3053, LossY: 4.1063)\n",
            "2025-10-23 22:47:19,841 - VAETrainer - INFO - Saving model and trainer state to vae_results/best_model.pth...\n",
            "INFO:VAETrainer:Saving model and trainer state to vae_results/best_model.pth...\n",
            "2025-10-23 22:47:19,855 - VAETrainer - INFO - Successfully saved model and trainer state to vae_results/best_model.pth\n",
            "INFO:VAETrainer:Successfully saved model and trainer state to vae_results/best_model.pth\n",
            "2025-10-23 22:47:21,967 - VAETrainer - INFO - Epoch [13/100] Train TotalLoss: 40.1356, (ReconX: 0.9359, KLD: 0.3047*0.43, LossY: 3.9068*10.00), Time: 1.7s | Val TotalLoss: 38.4945, (ReconX: 0.9514, KLD: 0.3075, LossY: 3.7236)\n",
            "INFO:VAETrainer:Epoch [13/100] Train TotalLoss: 40.1356, (ReconX: 0.9359, KLD: 0.3047*0.43, LossY: 3.9068*10.00), Time: 1.7s | Val TotalLoss: 38.4945, (ReconX: 0.9514, KLD: 0.3075, LossY: 3.7236)\n",
            "2025-10-23 22:47:21,969 - VAETrainer - INFO - Saving model and trainer state to vae_results/best_model.pth...\n",
            "INFO:VAETrainer:Saving model and trainer state to vae_results/best_model.pth...\n",
            "2025-10-23 22:47:21,986 - VAETrainer - INFO - Successfully saved model and trainer state to vae_results/best_model.pth\n",
            "INFO:VAETrainer:Successfully saved model and trainer state to vae_results/best_model.pth\n",
            "2025-10-23 22:47:24,097 - VAETrainer - INFO - Epoch [14/100] Train TotalLoss: 37.4231, (ReconX: 0.9355, KLD: 0.3020*0.47, LossY: 3.6347*10.00), Time: 1.7s | Val TotalLoss: 36.7255, (ReconX: 0.9492, KLD: 0.3030, LossY: 3.5473)\n",
            "INFO:VAETrainer:Epoch [14/100] Train TotalLoss: 37.4231, (ReconX: 0.9355, KLD: 0.3020*0.47, LossY: 3.6347*10.00), Time: 1.7s | Val TotalLoss: 36.7255, (ReconX: 0.9492, KLD: 0.3030, LossY: 3.5473)\n",
            "2025-10-23 22:47:24,098 - VAETrainer - INFO - Saving model and trainer state to vae_results/best_model.pth...\n",
            "INFO:VAETrainer:Saving model and trainer state to vae_results/best_model.pth...\n",
            "2025-10-23 22:47:24,111 - VAETrainer - INFO - Successfully saved model and trainer state to vae_results/best_model.pth\n",
            "INFO:VAETrainer:Successfully saved model and trainer state to vae_results/best_model.pth\n",
            "2025-10-23 22:47:26,219 - VAETrainer - INFO - Epoch [15/100] Train TotalLoss: 35.4236, (ReconX: 0.9360, KLD: 0.3033*0.50, LossY: 3.4336*10.00), Time: 1.7s | Val TotalLoss: 34.9116, (ReconX: 0.9504, KLD: 0.3026, LossY: 3.3659)\n",
            "INFO:VAETrainer:Epoch [15/100] Train TotalLoss: 35.4236, (ReconX: 0.9360, KLD: 0.3033*0.50, LossY: 3.4336*10.00), Time: 1.7s | Val TotalLoss: 34.9116, (ReconX: 0.9504, KLD: 0.3026, LossY: 3.3659)\n",
            "2025-10-23 22:47:26,221 - VAETrainer - INFO - Saving model and trainer state to vae_results/best_model.pth...\n",
            "INFO:VAETrainer:Saving model and trainer state to vae_results/best_model.pth...\n",
            "2025-10-23 22:47:26,234 - VAETrainer - INFO - Successfully saved model and trainer state to vae_results/best_model.pth\n",
            "INFO:VAETrainer:Successfully saved model and trainer state to vae_results/best_model.pth\n",
            "2025-10-23 22:47:28,329 - VAETrainer - INFO - Epoch [16/100] Train TotalLoss: 33.4540, (ReconX: 0.9365, KLD: 0.3009*0.53, LossY: 3.2357*10.00), Time: 1.7s | Val TotalLoss: 33.1814, (ReconX: 0.9504, KLD: 0.3006, LossY: 3.1930)\n",
            "INFO:VAETrainer:Epoch [16/100] Train TotalLoss: 33.4540, (ReconX: 0.9365, KLD: 0.3009*0.53, LossY: 3.2357*10.00), Time: 1.7s | Val TotalLoss: 33.1814, (ReconX: 0.9504, KLD: 0.3006, LossY: 3.1930)\n",
            "2025-10-23 22:47:28,332 - VAETrainer - INFO - Saving model and trainer state to vae_results/best_model.pth...\n",
            "INFO:VAETrainer:Saving model and trainer state to vae_results/best_model.pth...\n",
            "2025-10-23 22:47:28,346 - VAETrainer - INFO - Successfully saved model and trainer state to vae_results/best_model.pth\n",
            "INFO:VAETrainer:Successfully saved model and trainer state to vae_results/best_model.pth\n",
            "2025-10-23 22:47:30,561 - VAETrainer - INFO - Epoch [17/100] Train TotalLoss: 31.7966, (ReconX: 0.9364, KLD: 0.2998*0.57, LossY: 3.0690*10.00), Time: 1.8s | Val TotalLoss: 31.3315, (ReconX: 0.9494, KLD: 0.3008, LossY: 3.0081)\n",
            "INFO:VAETrainer:Epoch [17/100] Train TotalLoss: 31.7966, (ReconX: 0.9364, KLD: 0.2998*0.57, LossY: 3.0690*10.00), Time: 1.8s | Val TotalLoss: 31.3315, (ReconX: 0.9494, KLD: 0.3008, LossY: 3.0081)\n",
            "2025-10-23 22:47:30,562 - VAETrainer - INFO - Saving model and trainer state to vae_results/best_model.pth...\n",
            "INFO:VAETrainer:Saving model and trainer state to vae_results/best_model.pth...\n",
            "2025-10-23 22:47:30,579 - VAETrainer - INFO - Successfully saved model and trainer state to vae_results/best_model.pth\n",
            "INFO:VAETrainer:Successfully saved model and trainer state to vae_results/best_model.pth\n",
            "2025-10-23 22:47:32,733 - VAETrainer - INFO - Epoch [18/100] Train TotalLoss: 30.3985, (ReconX: 0.9365, KLD: 0.2937*0.60, LossY: 2.9286*10.00), Time: 1.8s | Val TotalLoss: 29.9788, (ReconX: 0.9512, KLD: 0.2942, LossY: 2.8733)\n",
            "INFO:VAETrainer:Epoch [18/100] Train TotalLoss: 30.3985, (ReconX: 0.9365, KLD: 0.2937*0.60, LossY: 2.9286*10.00), Time: 1.8s | Val TotalLoss: 29.9788, (ReconX: 0.9512, KLD: 0.2942, LossY: 2.8733)\n",
            "2025-10-23 22:47:32,734 - VAETrainer - INFO - Saving model and trainer state to vae_results/best_model.pth...\n",
            "INFO:VAETrainer:Saving model and trainer state to vae_results/best_model.pth...\n",
            "2025-10-23 22:47:32,748 - VAETrainer - INFO - Successfully saved model and trainer state to vae_results/best_model.pth\n",
            "INFO:VAETrainer:Successfully saved model and trainer state to vae_results/best_model.pth\n",
            "2025-10-23 22:47:34,803 - VAETrainer - INFO - Epoch [19/100] Train TotalLoss: 29.3567, (ReconX: 0.9384, KLD: 0.2857*0.63, LossY: 2.8237*10.00), Time: 1.7s | Val TotalLoss: 28.9916, (ReconX: 0.9548, KLD: 0.2819, LossY: 2.7755)\n",
            "INFO:VAETrainer:Epoch [19/100] Train TotalLoss: 29.3567, (ReconX: 0.9384, KLD: 0.2857*0.63, LossY: 2.8237*10.00), Time: 1.7s | Val TotalLoss: 28.9916, (ReconX: 0.9548, KLD: 0.2819, LossY: 2.7755)\n",
            "2025-10-23 22:47:34,805 - VAETrainer - INFO - Saving model and trainer state to vae_results/best_model.pth...\n",
            "INFO:VAETrainer:Saving model and trainer state to vae_results/best_model.pth...\n",
            "2025-10-23 22:47:34,820 - VAETrainer - INFO - Successfully saved model and trainer state to vae_results/best_model.pth\n",
            "INFO:VAETrainer:Successfully saved model and trainer state to vae_results/best_model.pth\n",
            "2025-10-23 22:47:36,968 - VAETrainer - INFO - Epoch [20/100] Train TotalLoss: 28.4510, (ReconX: 0.9407, KLD: 0.2777*0.67, LossY: 2.7325*10.00), Time: 1.8s | Val TotalLoss: 27.9397, (ReconX: 0.9546, KLD: 0.2761, LossY: 2.6709)\n",
            "INFO:VAETrainer:Epoch [20/100] Train TotalLoss: 28.4510, (ReconX: 0.9407, KLD: 0.2777*0.67, LossY: 2.7325*10.00), Time: 1.8s | Val TotalLoss: 27.9397, (ReconX: 0.9546, KLD: 0.2761, LossY: 2.6709)\n",
            "2025-10-23 22:47:36,969 - VAETrainer - INFO - Saving model and trainer state to vae_results/best_model.pth...\n",
            "INFO:VAETrainer:Saving model and trainer state to vae_results/best_model.pth...\n",
            "2025-10-23 22:47:36,983 - VAETrainer - INFO - Successfully saved model and trainer state to vae_results/best_model.pth\n",
            "INFO:VAETrainer:Successfully saved model and trainer state to vae_results/best_model.pth\n",
            "2025-10-23 22:47:39,142 - VAETrainer - INFO - Epoch [21/100] Train TotalLoss: 27.7657, (ReconX: 0.9425, KLD: 0.2671*0.70, LossY: 2.6636*10.00), Time: 1.8s | Val TotalLoss: 27.6235, (ReconX: 0.9594, KLD: 0.2562, LossY: 2.6408)\n",
            "INFO:VAETrainer:Epoch [21/100] Train TotalLoss: 27.7657, (ReconX: 0.9425, KLD: 0.2671*0.70, LossY: 2.6636*10.00), Time: 1.8s | Val TotalLoss: 27.6235, (ReconX: 0.9594, KLD: 0.2562, LossY: 2.6408)\n",
            "2025-10-23 22:47:39,144 - VAETrainer - INFO - Saving model and trainer state to vae_results/best_model.pth...\n",
            "INFO:VAETrainer:Saving model and trainer state to vae_results/best_model.pth...\n",
            "2025-10-23 22:47:39,157 - VAETrainer - INFO - Successfully saved model and trainer state to vae_results/best_model.pth\n",
            "INFO:VAETrainer:Successfully saved model and trainer state to vae_results/best_model.pth\n",
            "2025-10-23 22:47:41,302 - VAETrainer - INFO - Epoch [22/100] Train TotalLoss: 27.1462, (ReconX: 0.9454, KLD: 0.2511*0.73, LossY: 2.6017*10.00), Time: 1.8s | Val TotalLoss: 26.8519, (ReconX: 0.9628, KLD: 0.2465, LossY: 2.5643)\n",
            "INFO:VAETrainer:Epoch [22/100] Train TotalLoss: 27.1462, (ReconX: 0.9454, KLD: 0.2511*0.73, LossY: 2.6017*10.00), Time: 1.8s | Val TotalLoss: 26.8519, (ReconX: 0.9628, KLD: 0.2465, LossY: 2.5643)\n",
            "2025-10-23 22:47:41,304 - VAETrainer - INFO - Saving model and trainer state to vae_results/best_model.pth...\n",
            "INFO:VAETrainer:Saving model and trainer state to vae_results/best_model.pth...\n",
            "2025-10-23 22:47:41,318 - VAETrainer - INFO - Successfully saved model and trainer state to vae_results/best_model.pth\n",
            "INFO:VAETrainer:Successfully saved model and trainer state to vae_results/best_model.pth\n",
            "2025-10-23 22:47:43,424 - VAETrainer - INFO - Epoch [23/100] Train TotalLoss: 26.6877, (ReconX: 0.9482, KLD: 0.2350*0.77, LossY: 2.5559*10.00), Time: 1.7s | Val TotalLoss: 26.3325, (ReconX: 0.9661, KLD: 0.2289, LossY: 2.5137)\n",
            "INFO:VAETrainer:Epoch [23/100] Train TotalLoss: 26.6877, (ReconX: 0.9482, KLD: 0.2350*0.77, LossY: 2.5559*10.00), Time: 1.7s | Val TotalLoss: 26.3325, (ReconX: 0.9661, KLD: 0.2289, LossY: 2.5137)\n",
            "2025-10-23 22:47:43,426 - VAETrainer - INFO - Saving model and trainer state to vae_results/best_model.pth...\n",
            "INFO:VAETrainer:Saving model and trainer state to vae_results/best_model.pth...\n",
            "2025-10-23 22:47:43,439 - VAETrainer - INFO - Successfully saved model and trainer state to vae_results/best_model.pth\n",
            "INFO:VAETrainer:Successfully saved model and trainer state to vae_results/best_model.pth\n",
            "2025-10-23 22:47:45,504 - VAETrainer - INFO - Epoch [24/100] Train TotalLoss: 26.1290, (ReconX: 0.9529, KLD: 0.2180*0.80, LossY: 2.5002*10.00), Time: 1.7s | Val TotalLoss: 25.7275, (ReconX: 0.9667, KLD: 0.2112, LossY: 2.4550)\n",
            "INFO:VAETrainer:Epoch [24/100] Train TotalLoss: 26.1290, (ReconX: 0.9529, KLD: 0.2180*0.80, LossY: 2.5002*10.00), Time: 1.7s | Val TotalLoss: 25.7275, (ReconX: 0.9667, KLD: 0.2112, LossY: 2.4550)\n",
            "2025-10-23 22:47:45,506 - VAETrainer - INFO - Saving model and trainer state to vae_results/best_model.pth...\n",
            "INFO:VAETrainer:Saving model and trainer state to vae_results/best_model.pth...\n",
            "2025-10-23 22:47:45,519 - VAETrainer - INFO - Successfully saved model and trainer state to vae_results/best_model.pth\n",
            "INFO:VAETrainer:Successfully saved model and trainer state to vae_results/best_model.pth\n",
            "2025-10-23 22:47:47,684 - VAETrainer - INFO - Epoch [25/100] Train TotalLoss: 25.4296, (ReconX: 0.9562, KLD: 0.2001*0.83, LossY: 2.4307*10.00), Time: 1.8s | Val TotalLoss: 24.7832, (ReconX: 0.9727, KLD: 0.1925, LossY: 2.3618)\n",
            "INFO:VAETrainer:Epoch [25/100] Train TotalLoss: 25.4296, (ReconX: 0.9562, KLD: 0.2001*0.83, LossY: 2.4307*10.00), Time: 1.8s | Val TotalLoss: 24.7832, (ReconX: 0.9727, KLD: 0.1925, LossY: 2.3618)\n",
            "2025-10-23 22:47:47,686 - VAETrainer - INFO - Saving model and trainer state to vae_results/best_model.pth...\n",
            "INFO:VAETrainer:Saving model and trainer state to vae_results/best_model.pth...\n",
            "2025-10-23 22:47:47,699 - VAETrainer - INFO - Successfully saved model and trainer state to vae_results/best_model.pth\n",
            "INFO:VAETrainer:Successfully saved model and trainer state to vae_results/best_model.pth\n",
            "2025-10-23 22:47:49,882 - VAETrainer - INFO - Epoch [26/100] Train TotalLoss: 24.4890, (ReconX: 0.9602, KLD: 0.1843*0.87, LossY: 2.3369*10.00), Time: 1.8s | Val TotalLoss: 23.6946, (ReconX: 0.9762, KLD: 0.1753, LossY: 2.2543)\n",
            "INFO:VAETrainer:Epoch [26/100] Train TotalLoss: 24.4890, (ReconX: 0.9602, KLD: 0.1843*0.87, LossY: 2.3369*10.00), Time: 1.8s | Val TotalLoss: 23.6946, (ReconX: 0.9762, KLD: 0.1753, LossY: 2.2543)\n",
            "2025-10-23 22:47:49,884 - VAETrainer - INFO - Saving model and trainer state to vae_results/best_model.pth...\n",
            "INFO:VAETrainer:Saving model and trainer state to vae_results/best_model.pth...\n",
            "2025-10-23 22:47:49,899 - VAETrainer - INFO - Successfully saved model and trainer state to vae_results/best_model.pth\n",
            "INFO:VAETrainer:Successfully saved model and trainer state to vae_results/best_model.pth\n",
            "2025-10-23 22:47:51,969 - VAETrainer - INFO - Epoch [27/100] Train TotalLoss: 23.6094, (ReconX: 0.9640, KLD: 0.1672*0.90, LossY: 2.2495*10.00), Time: 1.7s | Val TotalLoss: 22.8055, (ReconX: 0.9803, KLD: 0.1586, LossY: 2.1667)\n",
            "INFO:VAETrainer:Epoch [27/100] Train TotalLoss: 23.6094, (ReconX: 0.9640, KLD: 0.1672*0.90, LossY: 2.2495*10.00), Time: 1.7s | Val TotalLoss: 22.8055, (ReconX: 0.9803, KLD: 0.1586, LossY: 2.1667)\n",
            "2025-10-23 22:47:51,970 - VAETrainer - INFO - Saving model and trainer state to vae_results/best_model.pth...\n",
            "INFO:VAETrainer:Saving model and trainer state to vae_results/best_model.pth...\n",
            "2025-10-23 22:47:51,984 - VAETrainer - INFO - Successfully saved model and trainer state to vae_results/best_model.pth\n",
            "INFO:VAETrainer:Successfully saved model and trainer state to vae_results/best_model.pth\n",
            "2025-10-23 22:47:54,139 - VAETrainer - INFO - Epoch [28/100] Train TotalLoss: 22.6755, (ReconX: 0.9674, KLD: 0.1522*0.93, LossY: 2.1566*10.00), Time: 1.8s | Val TotalLoss: 21.8527, (ReconX: 0.9824, KLD: 0.1472, LossY: 2.0723)\n",
            "INFO:VAETrainer:Epoch [28/100] Train TotalLoss: 22.6755, (ReconX: 0.9674, KLD: 0.1522*0.93, LossY: 2.1566*10.00), Time: 1.8s | Val TotalLoss: 21.8527, (ReconX: 0.9824, KLD: 0.1472, LossY: 2.0723)\n",
            "2025-10-23 22:47:54,140 - VAETrainer - INFO - Saving model and trainer state to vae_results/best_model.pth...\n",
            "INFO:VAETrainer:Saving model and trainer state to vae_results/best_model.pth...\n",
            "2025-10-23 22:47:54,159 - VAETrainer - INFO - Successfully saved model and trainer state to vae_results/best_model.pth\n",
            "INFO:VAETrainer:Successfully saved model and trainer state to vae_results/best_model.pth\n",
            "2025-10-23 22:47:56,299 - VAETrainer - INFO - Epoch [29/100] Train TotalLoss: 21.7479, (ReconX: 0.9696, KLD: 0.1425*0.97, LossY: 2.0641*10.00), Time: 1.7s | Val TotalLoss: 20.9489, (ReconX: 0.9831, KLD: 0.1385, LossY: 1.9827)\n",
            "INFO:VAETrainer:Epoch [29/100] Train TotalLoss: 21.7479, (ReconX: 0.9696, KLD: 0.1425*0.97, LossY: 2.0641*10.00), Time: 1.7s | Val TotalLoss: 20.9489, (ReconX: 0.9831, KLD: 0.1385, LossY: 1.9827)\n",
            "2025-10-23 22:47:56,301 - VAETrainer - INFO - Saving model and trainer state to vae_results/best_model.pth...\n",
            "INFO:VAETrainer:Saving model and trainer state to vae_results/best_model.pth...\n",
            "2025-10-23 22:47:56,315 - VAETrainer - INFO - Successfully saved model and trainer state to vae_results/best_model.pth\n",
            "INFO:VAETrainer:Successfully saved model and trainer state to vae_results/best_model.pth\n",
            "2025-10-23 22:47:58,417 - VAETrainer - INFO - Epoch [30/100] Train TotalLoss: 20.7290, (ReconX: 0.9704, KLD: 0.1367*1.00, LossY: 1.9622*10.00), Time: 1.7s | Val TotalLoss: 19.8993, (ReconX: 0.9870, KLD: 0.1309, LossY: 1.8781)\n",
            "INFO:VAETrainer:Epoch [30/100] Train TotalLoss: 20.7290, (ReconX: 0.9704, KLD: 0.1367*1.00, LossY: 1.9622*10.00), Time: 1.7s | Val TotalLoss: 19.8993, (ReconX: 0.9870, KLD: 0.1309, LossY: 1.8781)\n",
            "2025-10-23 22:47:58,419 - VAETrainer - INFO - Saving model and trainer state to vae_results/best_model.pth...\n",
            "INFO:VAETrainer:Saving model and trainer state to vae_results/best_model.pth...\n",
            "2025-10-23 22:47:58,433 - VAETrainer - INFO - Successfully saved model and trainer state to vae_results/best_model.pth\n",
            "INFO:VAETrainer:Successfully saved model and trainer state to vae_results/best_model.pth\n",
            "2025-10-23 22:48:00,580 - VAETrainer - INFO - Epoch [31/100] Train TotalLoss: 19.9095, (ReconX: 0.9718, KLD: 0.1327*1.00, LossY: 1.8805*10.00), Time: 1.8s | Val TotalLoss: 19.2921, (ReconX: 0.9850, KLD: 0.1334, LossY: 1.8174)\n",
            "INFO:VAETrainer:Epoch [31/100] Train TotalLoss: 19.9095, (ReconX: 0.9718, KLD: 0.1327*1.00, LossY: 1.8805*10.00), Time: 1.8s | Val TotalLoss: 19.2921, (ReconX: 0.9850, KLD: 0.1334, LossY: 1.8174)\n",
            "2025-10-23 22:48:00,581 - VAETrainer - INFO - Saving model and trainer state to vae_results/best_model.pth...\n",
            "INFO:VAETrainer:Saving model and trainer state to vae_results/best_model.pth...\n",
            "2025-10-23 22:48:00,594 - VAETrainer - INFO - Successfully saved model and trainer state to vae_results/best_model.pth\n",
            "INFO:VAETrainer:Successfully saved model and trainer state to vae_results/best_model.pth\n",
            "2025-10-23 22:48:02,655 - VAETrainer - INFO - Epoch [32/100] Train TotalLoss: 19.5275, (ReconX: 0.9722, KLD: 0.1295*1.00, LossY: 1.8426*10.00), Time: 1.7s | Val TotalLoss: 18.8718, (ReconX: 0.9867, KLD: 0.1298, LossY: 1.7755)\n",
            "INFO:VAETrainer:Epoch [32/100] Train TotalLoss: 19.5275, (ReconX: 0.9722, KLD: 0.1295*1.00, LossY: 1.8426*10.00), Time: 1.7s | Val TotalLoss: 18.8718, (ReconX: 0.9867, KLD: 0.1298, LossY: 1.7755)\n",
            "2025-10-23 22:48:02,656 - VAETrainer - INFO - Saving model and trainer state to vae_results/best_model.pth...\n",
            "INFO:VAETrainer:Saving model and trainer state to vae_results/best_model.pth...\n",
            "2025-10-23 22:48:02,669 - VAETrainer - INFO - Successfully saved model and trainer state to vae_results/best_model.pth\n",
            "INFO:VAETrainer:Successfully saved model and trainer state to vae_results/best_model.pth\n",
            "2025-10-23 22:48:04,759 - VAETrainer - INFO - Epoch [33/100] Train TotalLoss: 19.3120, (ReconX: 0.9721, KLD: 0.1283*1.00, LossY: 1.8212*10.00), Time: 1.7s | Val TotalLoss: 18.8210, (ReconX: 0.9849, KLD: 0.1328, LossY: 1.7703)\n",
            "INFO:VAETrainer:Epoch [33/100] Train TotalLoss: 19.3120, (ReconX: 0.9721, KLD: 0.1283*1.00, LossY: 1.8212*10.00), Time: 1.7s | Val TotalLoss: 18.8210, (ReconX: 0.9849, KLD: 0.1328, LossY: 1.7703)\n",
            "2025-10-23 22:48:04,761 - VAETrainer - INFO - Saving model and trainer state to vae_results/best_model.pth...\n",
            "INFO:VAETrainer:Saving model and trainer state to vae_results/best_model.pth...\n",
            "2025-10-23 22:48:04,775 - VAETrainer - INFO - Successfully saved model and trainer state to vae_results/best_model.pth\n",
            "INFO:VAETrainer:Successfully saved model and trainer state to vae_results/best_model.pth\n",
            "2025-10-23 22:48:06,938 - VAETrainer - INFO - Epoch [34/100] Train TotalLoss: 18.9865, (ReconX: 0.9725, KLD: 0.1244*1.00, LossY: 1.7890*10.00), Time: 1.8s | Val TotalLoss: 18.3752, (ReconX: 0.9874, KLD: 0.1219, LossY: 1.7266)\n",
            "INFO:VAETrainer:Epoch [34/100] Train TotalLoss: 18.9865, (ReconX: 0.9725, KLD: 0.1244*1.00, LossY: 1.7890*10.00), Time: 1.8s | Val TotalLoss: 18.3752, (ReconX: 0.9874, KLD: 0.1219, LossY: 1.7266)\n",
            "2025-10-23 22:48:06,940 - VAETrainer - INFO - Saving model and trainer state to vae_results/best_model.pth...\n",
            "INFO:VAETrainer:Saving model and trainer state to vae_results/best_model.pth...\n",
            "2025-10-23 22:48:06,953 - VAETrainer - INFO - Successfully saved model and trainer state to vae_results/best_model.pth\n",
            "INFO:VAETrainer:Successfully saved model and trainer state to vae_results/best_model.pth\n",
            "2025-10-23 22:48:09,096 - VAETrainer - INFO - Epoch [35/100] Train TotalLoss: 18.8004, (ReconX: 0.9728, KLD: 0.1226*1.00, LossY: 1.7705*10.00), Time: 1.8s | Val TotalLoss: 18.1938, (ReconX: 0.9885, KLD: 0.1191, LossY: 1.7086)\n",
            "INFO:VAETrainer:Epoch [35/100] Train TotalLoss: 18.8004, (ReconX: 0.9728, KLD: 0.1226*1.00, LossY: 1.7705*10.00), Time: 1.8s | Val TotalLoss: 18.1938, (ReconX: 0.9885, KLD: 0.1191, LossY: 1.7086)\n",
            "2025-10-23 22:48:09,098 - VAETrainer - INFO - Saving model and trainer state to vae_results/best_model.pth...\n",
            "INFO:VAETrainer:Saving model and trainer state to vae_results/best_model.pth...\n",
            "2025-10-23 22:48:09,111 - VAETrainer - INFO - Successfully saved model and trainer state to vae_results/best_model.pth\n",
            "INFO:VAETrainer:Successfully saved model and trainer state to vae_results/best_model.pth\n",
            "2025-10-23 22:48:11,290 - VAETrainer - INFO - Epoch [36/100] Train TotalLoss: 18.5847, (ReconX: 0.9739, KLD: 0.1211*1.00, LossY: 1.7490*10.00), Time: 1.8s | Val TotalLoss: 18.0309, (ReconX: 0.9874, KLD: 0.1193, LossY: 1.6924)\n",
            "INFO:VAETrainer:Epoch [36/100] Train TotalLoss: 18.5847, (ReconX: 0.9739, KLD: 0.1211*1.00, LossY: 1.7490*10.00), Time: 1.8s | Val TotalLoss: 18.0309, (ReconX: 0.9874, KLD: 0.1193, LossY: 1.6924)\n",
            "2025-10-23 22:48:11,292 - VAETrainer - INFO - Saving model and trainer state to vae_results/best_model.pth...\n",
            "INFO:VAETrainer:Saving model and trainer state to vae_results/best_model.pth...\n",
            "2025-10-23 22:48:11,306 - VAETrainer - INFO - Successfully saved model and trainer state to vae_results/best_model.pth\n",
            "INFO:VAETrainer:Successfully saved model and trainer state to vae_results/best_model.pth\n",
            "2025-10-23 22:48:13,474 - VAETrainer - INFO - Epoch [37/100] Train TotalLoss: 18.4091, (ReconX: 0.9742, KLD: 0.1183*1.00, LossY: 1.7317*10.00), Time: 1.8s | Val TotalLoss: 17.8427, (ReconX: 0.9880, KLD: 0.1132, LossY: 1.6741)\n",
            "INFO:VAETrainer:Epoch [37/100] Train TotalLoss: 18.4091, (ReconX: 0.9742, KLD: 0.1183*1.00, LossY: 1.7317*10.00), Time: 1.8s | Val TotalLoss: 17.8427, (ReconX: 0.9880, KLD: 0.1132, LossY: 1.6741)\n",
            "2025-10-23 22:48:13,475 - VAETrainer - INFO - Saving model and trainer state to vae_results/best_model.pth...\n",
            "INFO:VAETrainer:Saving model and trainer state to vae_results/best_model.pth...\n",
            "2025-10-23 22:48:13,489 - VAETrainer - INFO - Successfully saved model and trainer state to vae_results/best_model.pth\n",
            "INFO:VAETrainer:Successfully saved model and trainer state to vae_results/best_model.pth\n",
            "2025-10-23 22:48:15,640 - VAETrainer - INFO - Epoch [38/100] Train TotalLoss: 18.2478, (ReconX: 0.9739, KLD: 0.1158*1.00, LossY: 1.7158*10.00), Time: 1.8s | Val TotalLoss: 17.6488, (ReconX: 0.9862, KLD: 0.1199, LossY: 1.6543)\n",
            "INFO:VAETrainer:Epoch [38/100] Train TotalLoss: 18.2478, (ReconX: 0.9739, KLD: 0.1158*1.00, LossY: 1.7158*10.00), Time: 1.8s | Val TotalLoss: 17.6488, (ReconX: 0.9862, KLD: 0.1199, LossY: 1.6543)\n",
            "2025-10-23 22:48:15,642 - VAETrainer - INFO - Saving model and trainer state to vae_results/best_model.pth...\n",
            "INFO:VAETrainer:Saving model and trainer state to vae_results/best_model.pth...\n",
            "2025-10-23 22:48:15,655 - VAETrainer - INFO - Successfully saved model and trainer state to vae_results/best_model.pth\n",
            "INFO:VAETrainer:Successfully saved model and trainer state to vae_results/best_model.pth\n",
            "2025-10-23 22:48:17,781 - VAETrainer - INFO - Epoch [39/100] Train TotalLoss: 18.1825, (ReconX: 0.9748, KLD: 0.1131*1.00, LossY: 1.7095*10.00), Time: 1.7s | Val TotalLoss: 17.5793, (ReconX: 0.9906, KLD: 0.1095, LossY: 1.6479)\n",
            "INFO:VAETrainer:Epoch [39/100] Train TotalLoss: 18.1825, (ReconX: 0.9748, KLD: 0.1131*1.00, LossY: 1.7095*10.00), Time: 1.7s | Val TotalLoss: 17.5793, (ReconX: 0.9906, KLD: 0.1095, LossY: 1.6479)\n",
            "2025-10-23 22:48:17,784 - VAETrainer - INFO - Saving model and trainer state to vae_results/best_model.pth...\n",
            "INFO:VAETrainer:Saving model and trainer state to vae_results/best_model.pth...\n",
            "2025-10-23 22:48:17,797 - VAETrainer - INFO - Successfully saved model and trainer state to vae_results/best_model.pth\n",
            "INFO:VAETrainer:Successfully saved model and trainer state to vae_results/best_model.pth\n",
            "2025-10-23 22:48:19,966 - VAETrainer - INFO - Epoch [40/100] Train TotalLoss: 18.0271, (ReconX: 0.9755, KLD: 0.1104*1.00, LossY: 1.6941*10.00), Time: 1.8s | Val TotalLoss: 17.3833, (ReconX: 0.9878, KLD: 0.1127, LossY: 1.6283)\n",
            "INFO:VAETrainer:Epoch [40/100] Train TotalLoss: 18.0271, (ReconX: 0.9755, KLD: 0.1104*1.00, LossY: 1.6941*10.00), Time: 1.8s | Val TotalLoss: 17.3833, (ReconX: 0.9878, KLD: 0.1127, LossY: 1.6283)\n",
            "2025-10-23 22:48:19,968 - VAETrainer - INFO - Saving model and trainer state to vae_results/best_model.pth...\n",
            "INFO:VAETrainer:Saving model and trainer state to vae_results/best_model.pth...\n",
            "2025-10-23 22:48:19,981 - VAETrainer - INFO - Successfully saved model and trainer state to vae_results/best_model.pth\n",
            "INFO:VAETrainer:Successfully saved model and trainer state to vae_results/best_model.pth\n",
            "2025-10-23 22:48:22,184 - VAETrainer - INFO - Epoch [41/100] Train TotalLoss: 18.0111, (ReconX: 0.9761, KLD: 0.1077*1.00, LossY: 1.6927*10.00), Time: 1.8s | Val TotalLoss: 17.2912, (ReconX: 0.9893, KLD: 0.1112, LossY: 1.6191)\n",
            "INFO:VAETrainer:Epoch [41/100] Train TotalLoss: 18.0111, (ReconX: 0.9761, KLD: 0.1077*1.00, LossY: 1.6927*10.00), Time: 1.8s | Val TotalLoss: 17.2912, (ReconX: 0.9893, KLD: 0.1112, LossY: 1.6191)\n",
            "2025-10-23 22:48:22,186 - VAETrainer - INFO - Saving model and trainer state to vae_results/best_model.pth...\n",
            "INFO:VAETrainer:Saving model and trainer state to vae_results/best_model.pth...\n",
            "2025-10-23 22:48:22,199 - VAETrainer - INFO - Successfully saved model and trainer state to vae_results/best_model.pth\n",
            "INFO:VAETrainer:Successfully saved model and trainer state to vae_results/best_model.pth\n",
            "2025-10-23 22:48:24,274 - VAETrainer - INFO - Epoch [42/100] Train TotalLoss: 17.9409, (ReconX: 0.9763, KLD: 0.1042*1.00, LossY: 1.6860*10.00), Time: 1.7s | Val TotalLoss: 17.1935, (ReconX: 0.9906, KLD: 0.1045, LossY: 1.6098)\n",
            "INFO:VAETrainer:Epoch [42/100] Train TotalLoss: 17.9409, (ReconX: 0.9763, KLD: 0.1042*1.00, LossY: 1.6860*10.00), Time: 1.7s | Val TotalLoss: 17.1935, (ReconX: 0.9906, KLD: 0.1045, LossY: 1.6098)\n",
            "2025-10-23 22:48:24,276 - VAETrainer - INFO - Saving model and trainer state to vae_results/best_model.pth...\n",
            "INFO:VAETrainer:Saving model and trainer state to vae_results/best_model.pth...\n",
            "2025-10-23 22:48:24,294 - VAETrainer - INFO - Successfully saved model and trainer state to vae_results/best_model.pth\n",
            "INFO:VAETrainer:Successfully saved model and trainer state to vae_results/best_model.pth\n",
            "2025-10-23 22:48:26,519 - VAETrainer - INFO - Epoch [43/100] Train TotalLoss: 17.8323, (ReconX: 0.9773, KLD: 0.1021*1.00, LossY: 1.6753*10.00), Time: 1.9s | Val TotalLoss: 17.1010, (ReconX: 0.9900, KLD: 0.1041, LossY: 1.6007)\n",
            "INFO:VAETrainer:Epoch [43/100] Train TotalLoss: 17.8323, (ReconX: 0.9773, KLD: 0.1021*1.00, LossY: 1.6753*10.00), Time: 1.9s | Val TotalLoss: 17.1010, (ReconX: 0.9900, KLD: 0.1041, LossY: 1.6007)\n",
            "2025-10-23 22:48:26,520 - VAETrainer - INFO - Saving model and trainer state to vae_results/best_model.pth...\n",
            "INFO:VAETrainer:Saving model and trainer state to vae_results/best_model.pth...\n",
            "2025-10-23 22:48:26,534 - VAETrainer - INFO - Successfully saved model and trainer state to vae_results/best_model.pth\n",
            "INFO:VAETrainer:Successfully saved model and trainer state to vae_results/best_model.pth\n",
            "2025-10-23 22:48:28,740 - VAETrainer - INFO - Epoch [44/100] Train TotalLoss: 17.7727, (ReconX: 0.9771, KLD: 0.1010*1.00, LossY: 1.6695*10.00), Time: 1.8s | Val TotalLoss: 17.1080, (ReconX: 0.9918, KLD: 0.1001, LossY: 1.6016)\n",
            "INFO:VAETrainer:Epoch [44/100] Train TotalLoss: 17.7727, (ReconX: 0.9771, KLD: 0.1010*1.00, LossY: 1.6695*10.00), Time: 1.8s | Val TotalLoss: 17.1080, (ReconX: 0.9918, KLD: 0.1001, LossY: 1.6016)\n",
            "2025-10-23 22:48:30,888 - VAETrainer - INFO - Epoch [45/100] Train TotalLoss: 17.5382, (ReconX: 0.9780, KLD: 0.0988*1.00, LossY: 1.6462*10.00), Time: 1.8s | Val TotalLoss: 16.8700, (ReconX: 0.9924, KLD: 0.0990, LossY: 1.5779)\n",
            "INFO:VAETrainer:Epoch [45/100] Train TotalLoss: 17.5382, (ReconX: 0.9780, KLD: 0.0988*1.00, LossY: 1.6462*10.00), Time: 1.8s | Val TotalLoss: 16.8700, (ReconX: 0.9924, KLD: 0.0990, LossY: 1.5779)\n",
            "2025-10-23 22:48:30,890 - VAETrainer - INFO - Saving model and trainer state to vae_results/best_model.pth...\n",
            "INFO:VAETrainer:Saving model and trainer state to vae_results/best_model.pth...\n",
            "2025-10-23 22:48:30,903 - VAETrainer - INFO - Successfully saved model and trainer state to vae_results/best_model.pth\n",
            "INFO:VAETrainer:Successfully saved model and trainer state to vae_results/best_model.pth\n",
            "2025-10-23 22:48:33,101 - VAETrainer - INFO - Epoch [46/100] Train TotalLoss: 17.5203, (ReconX: 0.9777, KLD: 0.0983*1.00, LossY: 1.6444*10.00), Time: 1.8s | Val TotalLoss: 17.1446, (ReconX: 0.9919, KLD: 0.0983, LossY: 1.6054)\n",
            "INFO:VAETrainer:Epoch [46/100] Train TotalLoss: 17.5203, (ReconX: 0.9777, KLD: 0.0983*1.00, LossY: 1.6444*10.00), Time: 1.8s | Val TotalLoss: 17.1446, (ReconX: 0.9919, KLD: 0.0983, LossY: 1.6054)\n",
            "2025-10-23 22:48:35,189 - VAETrainer - INFO - Epoch [47/100] Train TotalLoss: 17.3388, (ReconX: 0.9785, KLD: 0.0948*1.00, LossY: 1.6266*10.00), Time: 1.7s | Val TotalLoss: 16.7993, (ReconX: 0.9937, KLD: 0.0934, LossY: 1.5712)\n",
            "INFO:VAETrainer:Epoch [47/100] Train TotalLoss: 17.3388, (ReconX: 0.9785, KLD: 0.0948*1.00, LossY: 1.6266*10.00), Time: 1.7s | Val TotalLoss: 16.7993, (ReconX: 0.9937, KLD: 0.0934, LossY: 1.5712)\n",
            "2025-10-23 22:48:35,191 - VAETrainer - INFO - Saving model and trainer state to vae_results/best_model.pth...\n",
            "INFO:VAETrainer:Saving model and trainer state to vae_results/best_model.pth...\n",
            "2025-10-23 22:48:35,206 - VAETrainer - INFO - Successfully saved model and trainer state to vae_results/best_model.pth\n",
            "INFO:VAETrainer:Successfully saved model and trainer state to vae_results/best_model.pth\n",
            "2025-10-23 22:48:37,344 - VAETrainer - INFO - Epoch [48/100] Train TotalLoss: 17.4771, (ReconX: 0.9790, KLD: 0.0930*1.00, LossY: 1.6405*10.00), Time: 1.8s | Val TotalLoss: 16.5973, (ReconX: 0.9945, KLD: 0.0918, LossY: 1.5511)\n",
            "INFO:VAETrainer:Epoch [48/100] Train TotalLoss: 17.4771, (ReconX: 0.9790, KLD: 0.0930*1.00, LossY: 1.6405*10.00), Time: 1.8s | Val TotalLoss: 16.5973, (ReconX: 0.9945, KLD: 0.0918, LossY: 1.5511)\n",
            "2025-10-23 22:48:37,346 - VAETrainer - INFO - Saving model and trainer state to vae_results/best_model.pth...\n",
            "INFO:VAETrainer:Saving model and trainer state to vae_results/best_model.pth...\n",
            "2025-10-23 22:48:37,358 - VAETrainer - INFO - Successfully saved model and trainer state to vae_results/best_model.pth\n",
            "INFO:VAETrainer:Successfully saved model and trainer state to vae_results/best_model.pth\n",
            "2025-10-23 22:48:39,614 - VAETrainer - INFO - Epoch [49/100] Train TotalLoss: 17.2042, (ReconX: 0.9790, KLD: 0.0912*1.00, LossY: 1.6134*10.00), Time: 1.9s | Val TotalLoss: 16.5557, (ReconX: 0.9935, KLD: 0.0939, LossY: 1.5468)\n",
            "INFO:VAETrainer:Epoch [49/100] Train TotalLoss: 17.2042, (ReconX: 0.9790, KLD: 0.0912*1.00, LossY: 1.6134*10.00), Time: 1.9s | Val TotalLoss: 16.5557, (ReconX: 0.9935, KLD: 0.0939, LossY: 1.5468)\n",
            "2025-10-23 22:48:39,615 - VAETrainer - INFO - Saving model and trainer state to vae_results/best_model.pth...\n",
            "INFO:VAETrainer:Saving model and trainer state to vae_results/best_model.pth...\n",
            "2025-10-23 22:48:39,629 - VAETrainer - INFO - Successfully saved model and trainer state to vae_results/best_model.pth\n",
            "INFO:VAETrainer:Successfully saved model and trainer state to vae_results/best_model.pth\n",
            "2025-10-23 22:48:41,764 - VAETrainer - INFO - Epoch [50/100] Train TotalLoss: 17.3554, (ReconX: 0.9799, KLD: 0.0892*1.00, LossY: 1.6286*10.00), Time: 1.8s | Val TotalLoss: 16.3912, (ReconX: 0.9918, KLD: 0.0916, LossY: 1.5308)\n",
            "INFO:VAETrainer:Epoch [50/100] Train TotalLoss: 17.3554, (ReconX: 0.9799, KLD: 0.0892*1.00, LossY: 1.6286*10.00), Time: 1.8s | Val TotalLoss: 16.3912, (ReconX: 0.9918, KLD: 0.0916, LossY: 1.5308)\n",
            "2025-10-23 22:48:41,765 - VAETrainer - INFO - Saving model and trainer state to vae_results/best_model.pth...\n",
            "INFO:VAETrainer:Saving model and trainer state to vae_results/best_model.pth...\n",
            "2025-10-23 22:48:41,780 - VAETrainer - INFO - Successfully saved model and trainer state to vae_results/best_model.pth\n",
            "INFO:VAETrainer:Successfully saved model and trainer state to vae_results/best_model.pth\n",
            "2025-10-23 22:48:43,915 - VAETrainer - INFO - Epoch [51/100] Train TotalLoss: 17.1839, (ReconX: 0.9799, KLD: 0.0878*1.00, LossY: 1.6116*10.00), Time: 1.7s | Val TotalLoss: 16.5006, (ReconX: 0.9941, KLD: 0.0876, LossY: 1.5419)\n",
            "INFO:VAETrainer:Epoch [51/100] Train TotalLoss: 17.1839, (ReconX: 0.9799, KLD: 0.0878*1.00, LossY: 1.6116*10.00), Time: 1.7s | Val TotalLoss: 16.5006, (ReconX: 0.9941, KLD: 0.0876, LossY: 1.5419)\n",
            "2025-10-23 22:48:46,077 - VAETrainer - INFO - Epoch [52/100] Train TotalLoss: 17.1613, (ReconX: 0.9796, KLD: 0.0859*1.00, LossY: 1.6096*10.00), Time: 1.8s | Val TotalLoss: 16.4141, (ReconX: 0.9951, KLD: 0.0845, LossY: 1.5335)\n",
            "INFO:VAETrainer:Epoch [52/100] Train TotalLoss: 17.1613, (ReconX: 0.9796, KLD: 0.0859*1.00, LossY: 1.6096*10.00), Time: 1.8s | Val TotalLoss: 16.4141, (ReconX: 0.9951, KLD: 0.0845, LossY: 1.5335)\n",
            "2025-10-23 22:48:48,168 - VAETrainer - INFO - Epoch [53/100] Train TotalLoss: 17.0340, (ReconX: 0.9802, KLD: 0.0851*1.00, LossY: 1.5969*10.00), Time: 1.7s | Val TotalLoss: 16.1136, (ReconX: 0.9922, KLD: 0.0881, LossY: 1.5033)\n",
            "INFO:VAETrainer:Epoch [53/100] Train TotalLoss: 17.0340, (ReconX: 0.9802, KLD: 0.0851*1.00, LossY: 1.5969*10.00), Time: 1.7s | Val TotalLoss: 16.1136, (ReconX: 0.9922, KLD: 0.0881, LossY: 1.5033)\n",
            "2025-10-23 22:48:48,170 - VAETrainer - INFO - Saving model and trainer state to vae_results/best_model.pth...\n",
            "INFO:VAETrainer:Saving model and trainer state to vae_results/best_model.pth...\n",
            "2025-10-23 22:48:48,183 - VAETrainer - INFO - Successfully saved model and trainer state to vae_results/best_model.pth\n",
            "INFO:VAETrainer:Successfully saved model and trainer state to vae_results/best_model.pth\n",
            "2025-10-23 22:48:50,326 - VAETrainer - INFO - Epoch [54/100] Train TotalLoss: 16.8562, (ReconX: 0.9802, KLD: 0.0845*1.00, LossY: 1.5791*10.00), Time: 1.7s | Val TotalLoss: 16.0510, (ReconX: 0.9963, KLD: 0.0824, LossY: 1.4972)\n",
            "INFO:VAETrainer:Epoch [54/100] Train TotalLoss: 16.8562, (ReconX: 0.9802, KLD: 0.0845*1.00, LossY: 1.5791*10.00), Time: 1.7s | Val TotalLoss: 16.0510, (ReconX: 0.9963, KLD: 0.0824, LossY: 1.4972)\n",
            "2025-10-23 22:48:50,328 - VAETrainer - INFO - Saving model and trainer state to vae_results/best_model.pth...\n",
            "INFO:VAETrainer:Saving model and trainer state to vae_results/best_model.pth...\n",
            "2025-10-23 22:48:50,345 - VAETrainer - INFO - Successfully saved model and trainer state to vae_results/best_model.pth\n",
            "INFO:VAETrainer:Successfully saved model and trainer state to vae_results/best_model.pth\n",
            "2025-10-23 22:48:52,434 - VAETrainer - INFO - Epoch [55/100] Train TotalLoss: 16.7982, (ReconX: 0.9814, KLD: 0.0812*1.00, LossY: 1.5736*10.00), Time: 1.7s | Val TotalLoss: 16.0635, (ReconX: 0.9948, KLD: 0.0820, LossY: 1.4987)\n",
            "INFO:VAETrainer:Epoch [55/100] Train TotalLoss: 16.7982, (ReconX: 0.9814, KLD: 0.0812*1.00, LossY: 1.5736*10.00), Time: 1.7s | Val TotalLoss: 16.0635, (ReconX: 0.9948, KLD: 0.0820, LossY: 1.4987)\n",
            "2025-10-23 22:48:54,582 - VAETrainer - INFO - Epoch [56/100] Train TotalLoss: 16.7407, (ReconX: 0.9812, KLD: 0.0820*1.00, LossY: 1.5677*10.00), Time: 1.8s | Val TotalLoss: 16.2780, (ReconX: 0.9962, KLD: 0.0796, LossY: 1.5202)\n",
            "INFO:VAETrainer:Epoch [56/100] Train TotalLoss: 16.7407, (ReconX: 0.9812, KLD: 0.0820*1.00, LossY: 1.5677*10.00), Time: 1.8s | Val TotalLoss: 16.2780, (ReconX: 0.9962, KLD: 0.0796, LossY: 1.5202)\n",
            "2025-10-23 22:48:56,792 - VAETrainer - INFO - Epoch [57/100] Train TotalLoss: 16.6342, (ReconX: 0.9813, KLD: 0.0801*1.00, LossY: 1.5573*10.00), Time: 1.8s | Val TotalLoss: 15.8567, (ReconX: 0.9964, KLD: 0.0805, LossY: 1.4780)\n",
            "INFO:VAETrainer:Epoch [57/100] Train TotalLoss: 16.6342, (ReconX: 0.9813, KLD: 0.0801*1.00, LossY: 1.5573*10.00), Time: 1.8s | Val TotalLoss: 15.8567, (ReconX: 0.9964, KLD: 0.0805, LossY: 1.4780)\n",
            "2025-10-23 22:48:56,794 - VAETrainer - INFO - Saving model and trainer state to vae_results/best_model.pth...\n",
            "INFO:VAETrainer:Saving model and trainer state to vae_results/best_model.pth...\n",
            "2025-10-23 22:48:56,812 - VAETrainer - INFO - Successfully saved model and trainer state to vae_results/best_model.pth\n",
            "INFO:VAETrainer:Successfully saved model and trainer state to vae_results/best_model.pth\n",
            "2025-10-23 22:48:58,924 - VAETrainer - INFO - Epoch [58/100] Train TotalLoss: 16.4818, (ReconX: 0.9812, KLD: 0.0786*1.00, LossY: 1.5422*10.00), Time: 1.7s | Val TotalLoss: 15.8544, (ReconX: 0.9952, KLD: 0.0767, LossY: 1.4783)\n",
            "INFO:VAETrainer:Epoch [58/100] Train TotalLoss: 16.4818, (ReconX: 0.9812, KLD: 0.0786*1.00, LossY: 1.5422*10.00), Time: 1.7s | Val TotalLoss: 15.8544, (ReconX: 0.9952, KLD: 0.0767, LossY: 1.4783)\n",
            "2025-10-23 22:48:58,925 - VAETrainer - INFO - Saving model and trainer state to vae_results/best_model.pth...\n",
            "INFO:VAETrainer:Saving model and trainer state to vae_results/best_model.pth...\n",
            "2025-10-23 22:48:58,944 - VAETrainer - INFO - Successfully saved model and trainer state to vae_results/best_model.pth\n",
            "INFO:VAETrainer:Successfully saved model and trainer state to vae_results/best_model.pth\n",
            "2025-10-23 22:49:01,090 - VAETrainer - INFO - Epoch [59/100] Train TotalLoss: 16.5253, (ReconX: 0.9819, KLD: 0.0775*1.00, LossY: 1.5466*10.00), Time: 1.8s | Val TotalLoss: 15.8917, (ReconX: 0.9958, KLD: 0.0774, LossY: 1.4818)\n",
            "INFO:VAETrainer:Epoch [59/100] Train TotalLoss: 16.5253, (ReconX: 0.9819, KLD: 0.0775*1.00, LossY: 1.5466*10.00), Time: 1.8s | Val TotalLoss: 15.8917, (ReconX: 0.9958, KLD: 0.0774, LossY: 1.4818)\n",
            "2025-10-23 22:49:03,367 - VAETrainer - INFO - Epoch [60/100] Train TotalLoss: 16.3010, (ReconX: 0.9817, KLD: 0.0774*1.00, LossY: 1.5242*10.00), Time: 1.9s | Val TotalLoss: 15.5302, (ReconX: 0.9979, KLD: 0.0764, LossY: 1.4456)\n",
            "INFO:VAETrainer:Epoch [60/100] Train TotalLoss: 16.3010, (ReconX: 0.9817, KLD: 0.0774*1.00, LossY: 1.5242*10.00), Time: 1.9s | Val TotalLoss: 15.5302, (ReconX: 0.9979, KLD: 0.0764, LossY: 1.4456)\n",
            "2025-10-23 22:49:03,369 - VAETrainer - INFO - Saving model and trainer state to vae_results/best_model.pth...\n",
            "INFO:VAETrainer:Saving model and trainer state to vae_results/best_model.pth...\n",
            "2025-10-23 22:49:03,383 - VAETrainer - INFO - Successfully saved model and trainer state to vae_results/best_model.pth\n",
            "INFO:VAETrainer:Successfully saved model and trainer state to vae_results/best_model.pth\n",
            "2025-10-23 22:49:05,544 - VAETrainer - INFO - Epoch [61/100] Train TotalLoss: 16.3442, (ReconX: 0.9828, KLD: 0.0752*1.00, LossY: 1.5286*10.00), Time: 1.8s | Val TotalLoss: 15.6635, (ReconX: 0.9971, KLD: 0.0739, LossY: 1.4593)\n",
            "INFO:VAETrainer:Epoch [61/100] Train TotalLoss: 16.3442, (ReconX: 0.9828, KLD: 0.0752*1.00, LossY: 1.5286*10.00), Time: 1.8s | Val TotalLoss: 15.6635, (ReconX: 0.9971, KLD: 0.0739, LossY: 1.4593)\n",
            "2025-10-23 22:49:07,659 - VAETrainer - INFO - Epoch [62/100] Train TotalLoss: 16.1008, (ReconX: 0.9821, KLD: 0.0745*1.00, LossY: 1.5044*10.00), Time: 1.7s | Val TotalLoss: 15.1846, (ReconX: 0.9979, KLD: 0.0778, LossY: 1.4109)\n",
            "INFO:VAETrainer:Epoch [62/100] Train TotalLoss: 16.1008, (ReconX: 0.9821, KLD: 0.0745*1.00, LossY: 1.5044*10.00), Time: 1.7s | Val TotalLoss: 15.1846, (ReconX: 0.9979, KLD: 0.0778, LossY: 1.4109)\n",
            "2025-10-23 22:49:07,660 - VAETrainer - INFO - Saving model and trainer state to vae_results/best_model.pth...\n",
            "INFO:VAETrainer:Saving model and trainer state to vae_results/best_model.pth...\n",
            "2025-10-23 22:49:07,673 - VAETrainer - INFO - Successfully saved model and trainer state to vae_results/best_model.pth\n",
            "INFO:VAETrainer:Successfully saved model and trainer state to vae_results/best_model.pth\n",
            "2025-10-23 22:49:09,867 - VAETrainer - INFO - Epoch [63/100] Train TotalLoss: 16.2564, (ReconX: 0.9827, KLD: 0.0736*1.00, LossY: 1.5200*10.00), Time: 1.8s | Val TotalLoss: 15.3177, (ReconX: 0.9959, KLD: 0.0729, LossY: 1.4249)\n",
            "INFO:VAETrainer:Epoch [63/100] Train TotalLoss: 16.2564, (ReconX: 0.9827, KLD: 0.0736*1.00, LossY: 1.5200*10.00), Time: 1.8s | Val TotalLoss: 15.3177, (ReconX: 0.9959, KLD: 0.0729, LossY: 1.4249)\n",
            "2025-10-23 22:49:11,954 - VAETrainer - INFO - Epoch [64/100] Train TotalLoss: 16.0151, (ReconX: 0.9835, KLD: 0.0722*1.00, LossY: 1.4959*10.00), Time: 1.7s | Val TotalLoss: 15.2934, (ReconX: 0.9960, KLD: 0.0714, LossY: 1.4226)\n",
            "INFO:VAETrainer:Epoch [64/100] Train TotalLoss: 16.0151, (ReconX: 0.9835, KLD: 0.0722*1.00, LossY: 1.4959*10.00), Time: 1.7s | Val TotalLoss: 15.2934, (ReconX: 0.9960, KLD: 0.0714, LossY: 1.4226)\n",
            "2025-10-23 22:49:14,076 - VAETrainer - INFO - Epoch [65/100] Train TotalLoss: 15.8801, (ReconX: 0.9832, KLD: 0.0717*1.00, LossY: 1.4825*10.00), Time: 1.8s | Val TotalLoss: 15.3523, (ReconX: 0.9971, KLD: 0.0719, LossY: 1.4283)\n",
            "INFO:VAETrainer:Epoch [65/100] Train TotalLoss: 15.8801, (ReconX: 0.9832, KLD: 0.0717*1.00, LossY: 1.4825*10.00), Time: 1.8s | Val TotalLoss: 15.3523, (ReconX: 0.9971, KLD: 0.0719, LossY: 1.4283)\n",
            "2025-10-23 22:49:16,160 - VAETrainer - INFO - Epoch [66/100] Train TotalLoss: 15.8349, (ReconX: 0.9830, KLD: 0.0718*1.00, LossY: 1.4780*10.00), Time: 1.7s | Val TotalLoss: 14.8488, (ReconX: 0.9959, KLD: 0.0721, LossY: 1.3781)\n",
            "INFO:VAETrainer:Epoch [66/100] Train TotalLoss: 15.8349, (ReconX: 0.9830, KLD: 0.0718*1.00, LossY: 1.4780*10.00), Time: 1.7s | Val TotalLoss: 14.8488, (ReconX: 0.9959, KLD: 0.0721, LossY: 1.3781)\n",
            "2025-10-23 22:49:16,162 - VAETrainer - INFO - Saving model and trainer state to vae_results/best_model.pth...\n",
            "INFO:VAETrainer:Saving model and trainer state to vae_results/best_model.pth...\n",
            "2025-10-23 22:49:16,176 - VAETrainer - INFO - Successfully saved model and trainer state to vae_results/best_model.pth\n",
            "INFO:VAETrainer:Successfully saved model and trainer state to vae_results/best_model.pth\n",
            "2025-10-23 22:49:18,275 - VAETrainer - INFO - Epoch [67/100] Train TotalLoss: 16.0765, (ReconX: 0.9831, KLD: 0.0696*1.00, LossY: 1.5024*10.00), Time: 1.7s | Val TotalLoss: 16.3241, (ReconX: 0.9994, KLD: 0.0678, LossY: 1.5257)\n",
            "INFO:VAETrainer:Epoch [67/100] Train TotalLoss: 16.0765, (ReconX: 0.9831, KLD: 0.0696*1.00, LossY: 1.5024*10.00), Time: 1.7s | Val TotalLoss: 16.3241, (ReconX: 0.9994, KLD: 0.0678, LossY: 1.5257)\n",
            "2025-10-23 22:49:20,411 - VAETrainer - INFO - Epoch [68/100] Train TotalLoss: 15.5773, (ReconX: 0.9834, KLD: 0.0692*1.00, LossY: 1.4525*10.00), Time: 1.7s | Val TotalLoss: 15.0277, (ReconX: 0.9979, KLD: 0.0700, LossY: 1.3960)\n",
            "INFO:VAETrainer:Epoch [68/100] Train TotalLoss: 15.5773, (ReconX: 0.9834, KLD: 0.0692*1.00, LossY: 1.4525*10.00), Time: 1.7s | Val TotalLoss: 15.0277, (ReconX: 0.9979, KLD: 0.0700, LossY: 1.3960)\n",
            "2025-10-23 22:49:22,573 - VAETrainer - INFO - Epoch [69/100] Train TotalLoss: 15.8638, (ReconX: 0.9840, KLD: 0.0688*1.00, LossY: 1.4811*10.00), Time: 1.8s | Val TotalLoss: 15.4995, (ReconX: 0.9965, KLD: 0.0682, LossY: 1.4435)\n",
            "INFO:VAETrainer:Epoch [69/100] Train TotalLoss: 15.8638, (ReconX: 0.9840, KLD: 0.0688*1.00, LossY: 1.4811*10.00), Time: 1.8s | Val TotalLoss: 15.4995, (ReconX: 0.9965, KLD: 0.0682, LossY: 1.4435)\n",
            "2025-10-23 22:49:24,773 - VAETrainer - INFO - Epoch [70/100] Train TotalLoss: 15.6739, (ReconX: 0.9844, KLD: 0.0667*1.00, LossY: 1.4623*10.00), Time: 1.8s | Val TotalLoss: 14.7054, (ReconX: 0.9985, KLD: 0.0676, LossY: 1.3639)\n",
            "INFO:VAETrainer:Epoch [70/100] Train TotalLoss: 15.6739, (ReconX: 0.9844, KLD: 0.0667*1.00, LossY: 1.4623*10.00), Time: 1.8s | Val TotalLoss: 14.7054, (ReconX: 0.9985, KLD: 0.0676, LossY: 1.3639)\n",
            "2025-10-23 22:49:24,774 - VAETrainer - INFO - Saving model and trainer state to vae_results/best_model.pth...\n",
            "INFO:VAETrainer:Saving model and trainer state to vae_results/best_model.pth...\n",
            "2025-10-23 22:49:24,788 - VAETrainer - INFO - Successfully saved model and trainer state to vae_results/best_model.pth\n",
            "INFO:VAETrainer:Successfully saved model and trainer state to vae_results/best_model.pth\n",
            "2025-10-23 22:49:26,919 - VAETrainer - INFO - Epoch [71/100] Train TotalLoss: 15.5306, (ReconX: 0.9841, KLD: 0.0683*1.00, LossY: 1.4478*10.00), Time: 1.8s | Val TotalLoss: 14.6868, (ReconX: 0.9974, KLD: 0.0670, LossY: 1.3622)\n",
            "INFO:VAETrainer:Epoch [71/100] Train TotalLoss: 15.5306, (ReconX: 0.9841, KLD: 0.0683*1.00, LossY: 1.4478*10.00), Time: 1.8s | Val TotalLoss: 14.6868, (ReconX: 0.9974, KLD: 0.0670, LossY: 1.3622)\n",
            "2025-10-23 22:49:26,921 - VAETrainer - INFO - Saving model and trainer state to vae_results/best_model.pth...\n",
            "INFO:VAETrainer:Saving model and trainer state to vae_results/best_model.pth...\n",
            "2025-10-23 22:49:26,934 - VAETrainer - INFO - Successfully saved model and trainer state to vae_results/best_model.pth\n",
            "INFO:VAETrainer:Successfully saved model and trainer state to vae_results/best_model.pth\n",
            "2025-10-23 22:49:29,075 - VAETrainer - INFO - Epoch [72/100] Train TotalLoss: 15.4458, (ReconX: 0.9840, KLD: 0.0669*1.00, LossY: 1.4395*10.00), Time: 1.8s | Val TotalLoss: 14.4395, (ReconX: 0.9996, KLD: 0.0665, LossY: 1.3373)\n",
            "INFO:VAETrainer:Epoch [72/100] Train TotalLoss: 15.4458, (ReconX: 0.9840, KLD: 0.0669*1.00, LossY: 1.4395*10.00), Time: 1.8s | Val TotalLoss: 14.4395, (ReconX: 0.9996, KLD: 0.0665, LossY: 1.3373)\n",
            "2025-10-23 22:49:29,077 - VAETrainer - INFO - Saving model and trainer state to vae_results/best_model.pth...\n",
            "INFO:VAETrainer:Saving model and trainer state to vae_results/best_model.pth...\n",
            "2025-10-23 22:49:29,090 - VAETrainer - INFO - Successfully saved model and trainer state to vae_results/best_model.pth\n",
            "INFO:VAETrainer:Successfully saved model and trainer state to vae_results/best_model.pth\n",
            "2025-10-23 22:49:31,149 - VAETrainer - INFO - Epoch [73/100] Train TotalLoss: 15.2780, (ReconX: 0.9834, KLD: 0.0672*1.00, LossY: 1.4227*10.00), Time: 1.7s | Val TotalLoss: 14.3568, (ReconX: 0.9976, KLD: 0.0680, LossY: 1.3291)\n",
            "INFO:VAETrainer:Epoch [73/100] Train TotalLoss: 15.2780, (ReconX: 0.9834, KLD: 0.0672*1.00, LossY: 1.4227*10.00), Time: 1.7s | Val TotalLoss: 14.3568, (ReconX: 0.9976, KLD: 0.0680, LossY: 1.3291)\n",
            "2025-10-23 22:49:31,151 - VAETrainer - INFO - Saving model and trainer state to vae_results/best_model.pth...\n",
            "INFO:VAETrainer:Saving model and trainer state to vae_results/best_model.pth...\n",
            "2025-10-23 22:49:31,165 - VAETrainer - INFO - Successfully saved model and trainer state to vae_results/best_model.pth\n",
            "INFO:VAETrainer:Successfully saved model and trainer state to vae_results/best_model.pth\n",
            "2025-10-23 22:49:33,367 - VAETrainer - INFO - Epoch [74/100] Train TotalLoss: 15.3321, (ReconX: 0.9837, KLD: 0.0670*1.00, LossY: 1.4281*10.00), Time: 1.8s | Val TotalLoss: 14.2335, (ReconX: 0.9975, KLD: 0.0694, LossY: 1.3167)\n",
            "INFO:VAETrainer:Epoch [74/100] Train TotalLoss: 15.3321, (ReconX: 0.9837, KLD: 0.0670*1.00, LossY: 1.4281*10.00), Time: 1.8s | Val TotalLoss: 14.2335, (ReconX: 0.9975, KLD: 0.0694, LossY: 1.3167)\n",
            "2025-10-23 22:49:33,369 - VAETrainer - INFO - Saving model and trainer state to vae_results/best_model.pth...\n",
            "INFO:VAETrainer:Saving model and trainer state to vae_results/best_model.pth...\n",
            "2025-10-23 22:49:33,383 - VAETrainer - INFO - Successfully saved model and trainer state to vae_results/best_model.pth\n",
            "INFO:VAETrainer:Successfully saved model and trainer state to vae_results/best_model.pth\n",
            "2025-10-23 22:49:35,435 - VAETrainer - INFO - Epoch [75/100] Train TotalLoss: 15.3332, (ReconX: 0.9842, KLD: 0.0655*1.00, LossY: 1.4283*10.00), Time: 1.7s | Val TotalLoss: 14.6637, (ReconX: 0.9994, KLD: 0.0656, LossY: 1.3599)\n",
            "INFO:VAETrainer:Epoch [75/100] Train TotalLoss: 15.3332, (ReconX: 0.9842, KLD: 0.0655*1.00, LossY: 1.4283*10.00), Time: 1.7s | Val TotalLoss: 14.6637, (ReconX: 0.9994, KLD: 0.0656, LossY: 1.3599)\n",
            "2025-10-23 22:49:37,487 - VAETrainer - INFO - Epoch [76/100] Train TotalLoss: 15.5047, (ReconX: 0.9845, KLD: 0.0654*1.00, LossY: 1.4455*10.00), Time: 1.7s | Val TotalLoss: 14.3543, (ReconX: 0.9998, KLD: 0.0662, LossY: 1.3288)\n",
            "INFO:VAETrainer:Epoch [76/100] Train TotalLoss: 15.5047, (ReconX: 0.9845, KLD: 0.0654*1.00, LossY: 1.4455*10.00), Time: 1.7s | Val TotalLoss: 14.3543, (ReconX: 0.9998, KLD: 0.0662, LossY: 1.3288)\n",
            "2025-10-23 22:49:39,634 - VAETrainer - INFO - Epoch [77/100] Train TotalLoss: 15.3342, (ReconX: 0.9840, KLD: 0.0649*1.00, LossY: 1.4285*10.00), Time: 1.8s | Val TotalLoss: 16.3091, (ReconX: 0.9986, KLD: 0.0661, LossY: 1.5244)\n",
            "INFO:VAETrainer:Epoch [77/100] Train TotalLoss: 15.3342, (ReconX: 0.9840, KLD: 0.0649*1.00, LossY: 1.4285*10.00), Time: 1.8s | Val TotalLoss: 16.3091, (ReconX: 0.9986, KLD: 0.0661, LossY: 1.5244)\n",
            "2025-10-23 22:49:41,774 - VAETrainer - INFO - Epoch [78/100] Train TotalLoss: 15.3202, (ReconX: 0.9847, KLD: 0.0637*1.00, LossY: 1.4272*10.00), Time: 1.8s | Val TotalLoss: 14.2069, (ReconX: 0.9997, KLD: 0.0651, LossY: 1.3142)\n",
            "INFO:VAETrainer:Epoch [78/100] Train TotalLoss: 15.3202, (ReconX: 0.9847, KLD: 0.0637*1.00, LossY: 1.4272*10.00), Time: 1.8s | Val TotalLoss: 14.2069, (ReconX: 0.9997, KLD: 0.0651, LossY: 1.3142)\n",
            "2025-10-23 22:49:41,777 - VAETrainer - INFO - Saving model and trainer state to vae_results/best_model.pth...\n",
            "INFO:VAETrainer:Saving model and trainer state to vae_results/best_model.pth...\n",
            "2025-10-23 22:49:41,790 - VAETrainer - INFO - Successfully saved model and trainer state to vae_results/best_model.pth\n",
            "INFO:VAETrainer:Successfully saved model and trainer state to vae_results/best_model.pth\n",
            "2025-10-23 22:49:44,002 - VAETrainer - INFO - Epoch [79/100] Train TotalLoss: 15.2574, (ReconX: 0.9846, KLD: 0.0641*1.00, LossY: 1.4209*10.00), Time: 1.8s | Val TotalLoss: 14.4149, (ReconX: 1.0007, KLD: 0.0608, LossY: 1.3353)\n",
            "INFO:VAETrainer:Epoch [79/100] Train TotalLoss: 15.2574, (ReconX: 0.9846, KLD: 0.0641*1.00, LossY: 1.4209*10.00), Time: 1.8s | Val TotalLoss: 14.4149, (ReconX: 1.0007, KLD: 0.0608, LossY: 1.3353)\n",
            "2025-10-23 22:49:46,173 - VAETrainer - INFO - Epoch [80/100] Train TotalLoss: 15.3401, (ReconX: 0.9850, KLD: 0.0624*1.00, LossY: 1.4293*10.00), Time: 1.8s | Val TotalLoss: 14.1801, (ReconX: 0.9991, KLD: 0.0602, LossY: 1.3121)\n",
            "INFO:VAETrainer:Epoch [80/100] Train TotalLoss: 15.3401, (ReconX: 0.9850, KLD: 0.0624*1.00, LossY: 1.4293*10.00), Time: 1.8s | Val TotalLoss: 14.1801, (ReconX: 0.9991, KLD: 0.0602, LossY: 1.3121)\n",
            "2025-10-23 22:49:46,175 - VAETrainer - INFO - Saving model and trainer state to vae_results/best_model.pth...\n",
            "INFO:VAETrainer:Saving model and trainer state to vae_results/best_model.pth...\n",
            "2025-10-23 22:49:46,188 - VAETrainer - INFO - Successfully saved model and trainer state to vae_results/best_model.pth\n",
            "INFO:VAETrainer:Successfully saved model and trainer state to vae_results/best_model.pth\n",
            "2025-10-23 22:49:48,335 - VAETrainer - INFO - Epoch [81/100] Train TotalLoss: 15.1039, (ReconX: 0.9849, KLD: 0.0620*1.00, LossY: 1.4057*10.00), Time: 1.8s | Val TotalLoss: 13.9395, (ReconX: 0.9984, KLD: 0.0631, LossY: 1.2878)\n",
            "INFO:VAETrainer:Epoch [81/100] Train TotalLoss: 15.1039, (ReconX: 0.9849, KLD: 0.0620*1.00, LossY: 1.4057*10.00), Time: 1.8s | Val TotalLoss: 13.9395, (ReconX: 0.9984, KLD: 0.0631, LossY: 1.2878)\n",
            "2025-10-23 22:49:48,337 - VAETrainer - INFO - Saving model and trainer state to vae_results/best_model.pth...\n",
            "INFO:VAETrainer:Saving model and trainer state to vae_results/best_model.pth...\n",
            "2025-10-23 22:49:48,350 - VAETrainer - INFO - Successfully saved model and trainer state to vae_results/best_model.pth\n",
            "INFO:VAETrainer:Successfully saved model and trainer state to vae_results/best_model.pth\n",
            "2025-10-23 22:49:50,482 - VAETrainer - INFO - Epoch [82/100] Train TotalLoss: 15.4317, (ReconX: 0.9849, KLD: 0.0627*1.00, LossY: 1.4384*10.00), Time: 1.7s | Val TotalLoss: 14.3670, (ReconX: 1.0002, KLD: 0.0637, LossY: 1.3303)\n",
            "INFO:VAETrainer:Epoch [82/100] Train TotalLoss: 15.4317, (ReconX: 0.9849, KLD: 0.0627*1.00, LossY: 1.4384*10.00), Time: 1.7s | Val TotalLoss: 14.3670, (ReconX: 1.0002, KLD: 0.0637, LossY: 1.3303)\n",
            "2025-10-23 22:49:52,638 - VAETrainer - INFO - Epoch [83/100] Train TotalLoss: 15.0863, (ReconX: 0.9848, KLD: 0.0618*1.00, LossY: 1.4040*10.00), Time: 1.8s | Val TotalLoss: 15.0069, (ReconX: 0.9986, KLD: 0.0611, LossY: 1.3947)\n",
            "INFO:VAETrainer:Epoch [83/100] Train TotalLoss: 15.0863, (ReconX: 0.9848, KLD: 0.0618*1.00, LossY: 1.4040*10.00), Time: 1.8s | Val TotalLoss: 15.0069, (ReconX: 0.9986, KLD: 0.0611, LossY: 1.3947)\n",
            "2025-10-23 22:49:54,740 - VAETrainer - INFO - Epoch [84/100] Train TotalLoss: 15.0239, (ReconX: 0.9854, KLD: 0.0612*1.00, LossY: 1.3977*10.00), Time: 1.7s | Val TotalLoss: 15.6151, (ReconX: 0.9982, KLD: 0.0636, LossY: 1.4553)\n",
            "INFO:VAETrainer:Epoch [84/100] Train TotalLoss: 15.0239, (ReconX: 0.9854, KLD: 0.0612*1.00, LossY: 1.3977*10.00), Time: 1.7s | Val TotalLoss: 15.6151, (ReconX: 0.9982, KLD: 0.0636, LossY: 1.4553)\n",
            "2025-10-23 22:49:56,949 - VAETrainer - INFO - Epoch [85/100] Train TotalLoss: 15.2209, (ReconX: 0.9855, KLD: 0.0603*1.00, LossY: 1.4175*10.00), Time: 1.8s | Val TotalLoss: 14.6158, (ReconX: 1.0005, KLD: 0.0580, LossY: 1.3557)\n",
            "INFO:VAETrainer:Epoch [85/100] Train TotalLoss: 15.2209, (ReconX: 0.9855, KLD: 0.0603*1.00, LossY: 1.4175*10.00), Time: 1.8s | Val TotalLoss: 14.6158, (ReconX: 1.0005, KLD: 0.0580, LossY: 1.3557)\n",
            "2025-10-23 22:49:59,115 - VAETrainer - INFO - Epoch [86/100] Train TotalLoss: 14.9427, (ReconX: 0.9853, KLD: 0.0607*1.00, LossY: 1.3897*10.00), Time: 1.8s | Val TotalLoss: 14.8961, (ReconX: 0.9993, KLD: 0.0611, LossY: 1.3836)\n",
            "INFO:VAETrainer:Epoch [86/100] Train TotalLoss: 14.9427, (ReconX: 0.9853, KLD: 0.0607*1.00, LossY: 1.3897*10.00), Time: 1.8s | Val TotalLoss: 14.8961, (ReconX: 0.9993, KLD: 0.0611, LossY: 1.3836)\n",
            "2025-10-23 22:50:01,343 - VAETrainer - INFO - Epoch [87/100] Train TotalLoss: 15.0483, (ReconX: 0.9857, KLD: 0.0611*1.00, LossY: 1.4002*10.00), Time: 1.8s | Val TotalLoss: 13.8005, (ReconX: 0.9998, KLD: 0.0612, LossY: 1.2740)\n",
            "INFO:VAETrainer:Epoch [87/100] Train TotalLoss: 15.0483, (ReconX: 0.9857, KLD: 0.0611*1.00, LossY: 1.4002*10.00), Time: 1.8s | Val TotalLoss: 13.8005, (ReconX: 0.9998, KLD: 0.0612, LossY: 1.2740)\n",
            "2025-10-23 22:50:01,345 - VAETrainer - INFO - Saving model and trainer state to vae_results/best_model.pth...\n",
            "INFO:VAETrainer:Saving model and trainer state to vae_results/best_model.pth...\n",
            "2025-10-23 22:50:01,359 - VAETrainer - INFO - Successfully saved model and trainer state to vae_results/best_model.pth\n",
            "INFO:VAETrainer:Successfully saved model and trainer state to vae_results/best_model.pth\n",
            "2025-10-23 22:50:03,403 - VAETrainer - INFO - Epoch [88/100] Train TotalLoss: 15.2121, (ReconX: 0.9858, KLD: 0.0606*1.00, LossY: 1.4166*10.00), Time: 1.7s | Val TotalLoss: 13.9368, (ReconX: 0.9994, KLD: 0.0590, LossY: 1.2878)\n",
            "INFO:VAETrainer:Epoch [88/100] Train TotalLoss: 15.2121, (ReconX: 0.9858, KLD: 0.0606*1.00, LossY: 1.4166*10.00), Time: 1.7s | Val TotalLoss: 13.9368, (ReconX: 0.9994, KLD: 0.0590, LossY: 1.2878)\n",
            "2025-10-23 22:50:05,576 - VAETrainer - INFO - Epoch [89/100] Train TotalLoss: 15.0950, (ReconX: 0.9856, KLD: 0.0595*1.00, LossY: 1.4050*10.00), Time: 1.8s | Val TotalLoss: 13.5335, (ReconX: 0.9987, KLD: 0.0631, LossY: 1.2472)\n",
            "INFO:VAETrainer:Epoch [89/100] Train TotalLoss: 15.0950, (ReconX: 0.9856, KLD: 0.0595*1.00, LossY: 1.4050*10.00), Time: 1.8s | Val TotalLoss: 13.5335, (ReconX: 0.9987, KLD: 0.0631, LossY: 1.2472)\n",
            "2025-10-23 22:50:05,577 - VAETrainer - INFO - Saving model and trainer state to vae_results/best_model.pth...\n",
            "INFO:VAETrainer:Saving model and trainer state to vae_results/best_model.pth...\n",
            "2025-10-23 22:50:05,591 - VAETrainer - INFO - Successfully saved model and trainer state to vae_results/best_model.pth\n",
            "INFO:VAETrainer:Successfully saved model and trainer state to vae_results/best_model.pth\n",
            "2025-10-23 22:50:07,695 - VAETrainer - INFO - Epoch [90/100] Train TotalLoss: 14.9142, (ReconX: 0.9855, KLD: 0.0607*1.00, LossY: 1.3868*10.00), Time: 1.7s | Val TotalLoss: 13.8118, (ReconX: 0.9991, KLD: 0.0609, LossY: 1.2752)\n",
            "INFO:VAETrainer:Epoch [90/100] Train TotalLoss: 14.9142, (ReconX: 0.9855, KLD: 0.0607*1.00, LossY: 1.3868*10.00), Time: 1.7s | Val TotalLoss: 13.8118, (ReconX: 0.9991, KLD: 0.0609, LossY: 1.2752)\n",
            "2025-10-23 22:50:09,873 - VAETrainer - INFO - Epoch [91/100] Train TotalLoss: 14.9239, (ReconX: 0.9854, KLD: 0.0598*1.00, LossY: 1.3879*10.00), Time: 1.8s | Val TotalLoss: 14.1181, (ReconX: 1.0007, KLD: 0.0596, LossY: 1.3058)\n",
            "INFO:VAETrainer:Epoch [91/100] Train TotalLoss: 14.9239, (ReconX: 0.9854, KLD: 0.0598*1.00, LossY: 1.3879*10.00), Time: 1.8s | Val TotalLoss: 14.1181, (ReconX: 1.0007, KLD: 0.0596, LossY: 1.3058)\n",
            "2025-10-23 22:50:12,000 - VAETrainer - INFO - Epoch [92/100] Train TotalLoss: 14.8585, (ReconX: 0.9860, KLD: 0.0594*1.00, LossY: 1.3813*10.00), Time: 1.8s | Val TotalLoss: 13.8206, (ReconX: 0.9997, KLD: 0.0606, LossY: 1.2760)\n",
            "INFO:VAETrainer:Epoch [92/100] Train TotalLoss: 14.8585, (ReconX: 0.9860, KLD: 0.0594*1.00, LossY: 1.3813*10.00), Time: 1.8s | Val TotalLoss: 13.8206, (ReconX: 0.9997, KLD: 0.0606, LossY: 1.2760)\n",
            "2025-10-23 22:50:14,167 - VAETrainer - INFO - Epoch [93/100] Train TotalLoss: 15.4483, (ReconX: 0.9855, KLD: 0.0597*1.00, LossY: 1.4403*10.00), Time: 1.8s | Val TotalLoss: 14.6966, (ReconX: 1.0018, KLD: 0.0592, LossY: 1.3636)\n",
            "INFO:VAETrainer:Epoch [93/100] Train TotalLoss: 15.4483, (ReconX: 0.9855, KLD: 0.0597*1.00, LossY: 1.4403*10.00), Time: 1.8s | Val TotalLoss: 14.6966, (ReconX: 1.0018, KLD: 0.0592, LossY: 1.3636)\n",
            "2025-10-23 22:50:16,416 - VAETrainer - INFO - Epoch [94/100] Train TotalLoss: 15.1783, (ReconX: 0.9855, KLD: 0.0601*1.00, LossY: 1.4133*10.00), Time: 1.9s | Val TotalLoss: 16.0192, (ReconX: 1.0006, KLD: 0.0586, LossY: 1.4960)\n",
            "INFO:VAETrainer:Epoch [94/100] Train TotalLoss: 15.1783, (ReconX: 0.9855, KLD: 0.0601*1.00, LossY: 1.4133*10.00), Time: 1.9s | Val TotalLoss: 16.0192, (ReconX: 1.0006, KLD: 0.0586, LossY: 1.4960)\n",
            "2025-10-23 22:50:18,539 - VAETrainer - INFO - Epoch [95/100] Train TotalLoss: 14.9863, (ReconX: 0.9862, KLD: 0.0585*1.00, LossY: 1.3942*10.00), Time: 1.7s | Val TotalLoss: 13.5838, (ReconX: 0.9996, KLD: 0.0593, LossY: 1.2525)\n",
            "INFO:VAETrainer:Epoch [95/100] Train TotalLoss: 14.9863, (ReconX: 0.9862, KLD: 0.0585*1.00, LossY: 1.3942*10.00), Time: 1.7s | Val TotalLoss: 13.5838, (ReconX: 0.9996, KLD: 0.0593, LossY: 1.2525)\n",
            "2025-10-23 22:50:20,583 - VAETrainer - INFO - Epoch [96/100] Train TotalLoss: 14.7532, (ReconX: 0.9856, KLD: 0.0591*1.00, LossY: 1.3709*10.00), Time: 1.7s | Val TotalLoss: 13.4993, (ReconX: 1.0003, KLD: 0.0598, LossY: 1.2439)\n",
            "INFO:VAETrainer:Epoch [96/100] Train TotalLoss: 14.7532, (ReconX: 0.9856, KLD: 0.0591*1.00, LossY: 1.3709*10.00), Time: 1.7s | Val TotalLoss: 13.4993, (ReconX: 1.0003, KLD: 0.0598, LossY: 1.2439)\n",
            "2025-10-23 22:50:20,585 - VAETrainer - INFO - Saving model and trainer state to vae_results/best_model.pth...\n",
            "INFO:VAETrainer:Saving model and trainer state to vae_results/best_model.pth...\n",
            "2025-10-23 22:50:20,602 - VAETrainer - INFO - Successfully saved model and trainer state to vae_results/best_model.pth\n",
            "INFO:VAETrainer:Successfully saved model and trainer state to vae_results/best_model.pth\n",
            "2025-10-23 22:50:22,848 - VAETrainer - INFO - Epoch [97/100] Train TotalLoss: 14.6434, (ReconX: 0.9866, KLD: 0.0588*1.00, LossY: 1.3598*10.00), Time: 1.9s | Val TotalLoss: 13.5807, (ReconX: 1.0003, KLD: 0.0593, LossY: 1.2521)\n",
            "INFO:VAETrainer:Epoch [97/100] Train TotalLoss: 14.6434, (ReconX: 0.9866, KLD: 0.0588*1.00, LossY: 1.3598*10.00), Time: 1.9s | Val TotalLoss: 13.5807, (ReconX: 1.0003, KLD: 0.0593, LossY: 1.2521)\n",
            "2025-10-23 22:50:24,933 - VAETrainer - INFO - Epoch [98/100] Train TotalLoss: 14.7382, (ReconX: 0.9861, KLD: 0.0583*1.00, LossY: 1.3694*10.00), Time: 1.7s | Val TotalLoss: 13.8202, (ReconX: 1.0008, KLD: 0.0560, LossY: 1.2763)\n",
            "INFO:VAETrainer:Epoch [98/100] Train TotalLoss: 14.7382, (ReconX: 0.9861, KLD: 0.0583*1.00, LossY: 1.3694*10.00), Time: 1.7s | Val TotalLoss: 13.8202, (ReconX: 1.0008, KLD: 0.0560, LossY: 1.2763)\n",
            "2025-10-23 22:50:26,979 - VAETrainer - INFO - Epoch [99/100] Train TotalLoss: 14.4920, (ReconX: 0.9858, KLD: 0.0589*1.00, LossY: 1.3447*10.00), Time: 1.7s | Val TotalLoss: 13.4143, (ReconX: 1.0002, KLD: 0.0614, LossY: 1.2353)\n",
            "INFO:VAETrainer:Epoch [99/100] Train TotalLoss: 14.4920, (ReconX: 0.9858, KLD: 0.0589*1.00, LossY: 1.3447*10.00), Time: 1.7s | Val TotalLoss: 13.4143, (ReconX: 1.0002, KLD: 0.0614, LossY: 1.2353)\n",
            "2025-10-23 22:50:26,981 - VAETrainer - INFO - Saving model and trainer state to vae_results/best_model.pth...\n",
            "INFO:VAETrainer:Saving model and trainer state to vae_results/best_model.pth...\n",
            "2025-10-23 22:50:26,998 - VAETrainer - INFO - Successfully saved model and trainer state to vae_results/best_model.pth\n",
            "INFO:VAETrainer:Successfully saved model and trainer state to vae_results/best_model.pth\n",
            "2025-10-23 22:50:29,155 - VAETrainer - INFO - Epoch [100/100] Train TotalLoss: 14.7914, (ReconX: 0.9858, KLD: 0.0589*1.00, LossY: 1.3747*10.00), Time: 1.8s | Val TotalLoss: 13.7790, (ReconX: 1.0005, KLD: 0.0583, LossY: 1.2720)\n",
            "INFO:VAETrainer:Epoch [100/100] Train TotalLoss: 14.7914, (ReconX: 0.9858, KLD: 0.0589*1.00, LossY: 1.3747*10.00), Time: 1.8s | Val TotalLoss: 13.7790, (ReconX: 1.0005, KLD: 0.0583, LossY: 1.2720)\n",
            "2025-10-23 22:50:29,157 - VAETrainer - INFO - Training loop finished.\n",
            "INFO:VAETrainer:Training loop finished.\n",
            "2025-10-23 22:50:29,158 - VAETrainer - INFO - Loading best model state from 'vae_results/best_model.pth' (Val Loss: 13.4143).\n",
            "INFO:VAETrainer:Loading best model state from 'vae_results/best_model.pth' (Val Loss: 13.4143).\n",
            "2025-10-23 22:50:29,160 - VAETrainer_Loader - INFO - Loading VAE model and trainer state from vae_results/best_model.pth to device cuda...\n",
            "INFO:VAETrainer_Loader:Loading VAE model and trainer state from vae_results/best_model.pth to device cuda...\n",
            "2025-10-23 22:50:29,211 - SemiSupMIWAE(X:42,Y:1,Lat:3) - INFO - SemiSupMIWAE Initialized: InputXDim=42, TargetYDim=1, LatentDim=3, AlphaPriceLoss=10.0, Device=cuda\n",
            "INFO:SemiSupMIWAE(X:42,Y:1,Lat:3):SemiSupMIWAE Initialized: InputXDim=42, TargetYDim=1, LatentDim=3, AlphaPriceLoss=10.0, Device=cuda\n",
            "2025-10-23 22:50:29,214 - VAETrainer_Loader - INFO - SemiSupMIWAE model created and state loaded successfully.\n",
            "INFO:VAETrainer_Loader:SemiSupMIWAE model created and state loaded successfully.\n",
            "2025-10-23 22:50:29,217 - VAETrainer - INFO - VAETrainer initialized for SemiSupMIWAE. Device: cuda. Optimizer: adam. LR: 0.0003. ReconLoss(X): 'mse'. Alpha(Y Loss): 10.0. LossType(Y): 'gaussian_nll'\n",
            "INFO:VAETrainer:VAETrainer initialized for SemiSupMIWAE. Device: cuda. Optimizer: adam. LR: 0.0003. ReconLoss(X): 'mse'. Alpha(Y Loss): 10.0. LossType(Y): 'gaussian_nll'\n",
            "2025-10-23 22:50:29,218 - VAETrainer_Loader - INFO - VAETrainer instance created and state loaded for model from vae_results/best_model.pth.\n",
            "INFO:VAETrainer_Loader:VAETrainer instance created and state loaded for model from vae_results/best_model.pth.\n",
            "2025-10-23 22:50:29,227 - VAETrainer - INFO - Predicting target (Y) mean and log-variance for 12271 samples...\n",
            "INFO:VAETrainer:Predicting target (Y) mean and log-variance for 12271 samples...\n",
            "2025-10-23 22:50:29,295 - MainRealEstateAnalysis_MIWAE - INFO - [Fold 7] Stored test predictions for 12271 rows.\n",
            "INFO:MainRealEstateAnalysis_MIWAE:[Fold 7] Stored test predictions for 12271 rows.\n",
            "2025-10-23 22:50:29,307 - MainRealEstateAnalysis_MIWAE - INFO - [Fold 8/10] sizes → Train=98170 (80.0%), Val=12271 (10.0%), Test=12271 (10.0%)\n",
            "INFO:MainRealEstateAnalysis_MIWAE:[Fold 8/10] sizes → Train=98170 (80.0%), Val=12271 (10.0%), Test=12271 (10.0%)\n",
            "2025-10-23 22:50:29,308 - __main__.create_vae_from_artifacts - INFO - Creating VAE model. Target column for supervision: log_sale_price.\n",
            "2025-10-23 22:50:29,308 - __main__.create_vae_from_artifacts - INFO - Creating VAE model. Target column for supervision: log_sale_price.\n",
            "INFO:__main__.create_vae_from_artifacts:Creating VAE model. Target column for supervision: log_sale_price.\n",
            "2025-10-23 22:50:29,309 - __main__.create_vae_from_artifacts - INFO - Number of input features for VAE (X): 42 (Features: ['lotarea', 'assess_total', 'exempt_total', 'assessland', 'exemptland']...)\n",
            "2025-10-23 22:50:29,309 - __main__.create_vae_from_artifacts - INFO - Number of input features for VAE (X): 42 (Features: ['lotarea', 'assess_total', 'exempt_total', 'assessland', 'exemptland']...)\n",
            "INFO:__main__.create_vae_from_artifacts:Number of input features for VAE (X): 42 (Features: ['lotarea', 'assess_total', 'exempt_total', 'assessland', 'exemptland']...)\n",
            "2025-10-23 22:50:29,311 - __main__.create_vae_from_artifacts - INFO - Semi-supervised mode: Target 'log_sale_price' (y_dim=1). Alpha for price loss: 10.0.\n",
            "2025-10-23 22:50:29,311 - __main__.create_vae_from_artifacts - INFO - Semi-supervised mode: Target 'log_sale_price' (y_dim=1). Alpha for price loss: 10.0.\n",
            "INFO:__main__.create_vae_from_artifacts:Semi-supervised mode: Target 'log_sale_price' (y_dim=1). Alpha for price loss: 10.0.\n",
            "2025-10-23 22:50:29,312 - __main__.create_vae_from_artifacts - INFO - Using overridden latent_dim: 3\n",
            "2025-10-23 22:50:29,312 - __main__.create_vae_from_artifacts - INFO - Using overridden latent_dim: 3\n",
            "INFO:__main__.create_vae_from_artifacts:Using overridden latent_dim: 3\n",
            "2025-10-23 22:50:29,313 - __main__.create_vae_from_artifacts - INFO - Final effective Latent Dimension: 3\n",
            "2025-10-23 22:50:29,313 - __main__.create_vae_from_artifacts - INFO - Final effective Latent Dimension: 3\n",
            "INFO:__main__.create_vae_from_artifacts:Final effective Latent Dimension: 3\n",
            "2025-10-23 22:50:29,314 - __main__.create_vae_from_artifacts - INFO - Encoder layers: [21], Decoder layers: [21]\n",
            "2025-10-23 22:50:29,314 - __main__.create_vae_from_artifacts - INFO - Encoder layers: [21], Decoder layers: [21]\n",
            "INFO:__main__.create_vae_from_artifacts:Encoder layers: [21], Decoder layers: [21]\n",
            "2025-10-23 22:50:29,316 - __main__.create_vae_from_artifacts - INFO - Using determined prior: Type=student_t_mixture, K=2 (from overrides or artifact inference).\n",
            "2025-10-23 22:50:29,316 - __main__.create_vae_from_artifacts - INFO - Using determined prior: Type=student_t_mixture, K=2 (from overrides or artifact inference).\n",
            "INFO:__main__.create_vae_from_artifacts:Using determined prior: Type=student_t_mixture, K=2 (from overrides or artifact inference).\n",
            "2025-10-23 22:50:29,317 - __main__.create_vae_from_artifacts - INFO - Initializing parameters for student_t_mixture with K=2 components...\n",
            "2025-10-23 22:50:29,317 - __main__.create_vae_from_artifacts - INFO - Initializing parameters for student_t_mixture with K=2 components...\n",
            "INFO:__main__.create_vae_from_artifacts:Initializing parameters for student_t_mixture with K=2 components...\n",
            "2025-10-23 22:50:29,318 - __main__.create_vae_from_artifacts - WARNING - DP-GMM artifacts are unsuitable or insufficient for initializing mixture prior parameters. Default parameters (e.g., zeros for means/logits, identity for covariances) will be used by the VAE.\n",
            "2025-10-23 22:50:29,318 - __main__.create_vae_from_artifacts - WARNING - DP-GMM artifacts are unsuitable or insufficient for initializing mixture prior parameters. Default parameters (e.g., zeros for means/logits, identity for covariances) will be used by the VAE.\n",
            "WARNING:__main__.create_vae_from_artifacts:DP-GMM artifacts are unsuitable or insufficient for initializing mixture prior parameters. Default parameters (e.g., zeros for means/logits, identity for covariances) will be used by the VAE.\n",
            "2025-10-23 22:50:29,320 - __main__.create_vae_from_artifacts - INFO - Setting Student-t df to 4.0 for prior.\n",
            "2025-10-23 22:50:29,320 - __main__.create_vae_from_artifacts - INFO - Setting Student-t df to 4.0 for prior.\n",
            "INFO:__main__.create_vae_from_artifacts:Setting Student-t df to 4.0 for prior.\n",
            "2025-10-23 22:50:29,321 - __main__.create_vae_from_artifacts - INFO - Instantiating SemiSupMIWAE model...\n",
            "2025-10-23 22:50:29,321 - __main__.create_vae_from_artifacts - INFO - Instantiating SemiSupMIWAE model...\n",
            "INFO:__main__.create_vae_from_artifacts:Instantiating SemiSupMIWAE model...\n",
            "2025-10-23 22:50:29,322 - __main__.create_vae_from_artifacts - INFO - Price head layers for SemiSupMIWAE: [8, 4]\n",
            "2025-10-23 22:50:29,322 - __main__.create_vae_from_artifacts - INFO - Price head layers for SemiSupMIWAE: [8, 4]\n",
            "INFO:__main__.create_vae_from_artifacts:Price head layers for SemiSupMIWAE: [8, 4]\n",
            "2025-10-23 22:50:29,330 - SemiSupMIWAE(X:42,Y:1,Lat:3) - INFO - SemiSupMIWAE Initialized: InputXDim=42, TargetYDim=1, LatentDim=3, AlphaPriceLoss=10.0, Device=cuda\n",
            "INFO:SemiSupMIWAE(X:42,Y:1,Lat:3):SemiSupMIWAE Initialized: InputXDim=42, TargetYDim=1, LatentDim=3, AlphaPriceLoss=10.0, Device=cuda\n",
            "2025-10-23 22:50:29,331 - __main__.create_vae_from_artifacts - INFO - SemiSupMIWAE model created successfully on device: cuda.\n",
            "2025-10-23 22:50:29,331 - __main__.create_vae_from_artifacts - INFO - SemiSupMIWAE model created successfully on device: cuda.\n",
            "INFO:__main__.create_vae_from_artifacts:SemiSupMIWAE model created successfully on device: cuda.\n",
            "2025-10-23 22:50:29,333 - VAETrainer - INFO - VAETrainer initialized for SemiSupMIWAE. Device: cuda. Optimizer: adam. LR: 0.0003. ReconLoss(X): 'mse'. Alpha(Y Loss): 10.0. LossType(Y): 'gaussian_nll'\n",
            "INFO:VAETrainer:VAETrainer initialized for SemiSupMIWAE. Device: cuda. Optimizer: adam. LR: 0.0003. ReconLoss(X): 'mse'. Alpha(Y Loss): 10.0. LossType(Y): 'gaussian_nll'\n",
            "2025-10-23 22:50:29,334 - VAETrainer - INFO - Starting training: Epochs=100, BatchSize=512, KLD_weight=1.0, KLD_AnnealEpochs=30, Alpha(Y_Loss)=10.0.\n",
            "INFO:VAETrainer:Starting training: Epochs=100, BatchSize=512, KLD_weight=1.0, KLD_AnnealEpochs=30, Alpha(Y_Loss)=10.0.\n",
            "2025-10-23 22:50:29,335 - VAETrainer - INFO - Train loader: 98170 samples, 192 batches. Drop last: False\n",
            "INFO:VAETrainer:Train loader: 98170 samples, 192 batches. Drop last: False\n",
            "2025-10-23 22:50:29,336 - VAETrainer - INFO - Validation loader: 12271 samples, 24 batches.\n",
            "INFO:VAETrainer:Validation loader: 12271 samples, 24 batches.\n",
            "2025-10-23 22:50:31,616 - VAETrainer - INFO - Epoch [1/100] Train TotalLoss: 909.9898, (ReconX: 1.0689, KLD: 0.6924*0.03, LossY: 90.8898*10.00), Time: 1.9s | Val TotalLoss: 684.8076, (ReconX: 1.0018, KLD: 0.6893, LossY: 68.3117)\n",
            "INFO:VAETrainer:Epoch [1/100] Train TotalLoss: 909.9898, (ReconX: 1.0689, KLD: 0.6924*0.03, LossY: 90.8898*10.00), Time: 1.9s | Val TotalLoss: 684.8076, (ReconX: 1.0018, KLD: 0.6893, LossY: 68.3117)\n",
            "2025-10-23 22:50:31,618 - VAETrainer - INFO - Saving model and trainer state to vae_results/best_model.pth...\n",
            "INFO:VAETrainer:Saving model and trainer state to vae_results/best_model.pth...\n",
            "2025-10-23 22:50:31,635 - VAETrainer - INFO - Successfully saved model and trainer state to vae_results/best_model.pth\n",
            "INFO:VAETrainer:Successfully saved model and trainer state to vae_results/best_model.pth\n",
            "2025-10-23 22:50:33,826 - VAETrainer - INFO - Epoch [2/100] Train TotalLoss: 623.4760, (ReconX: 0.9701, KLD: 0.6830*0.07, LossY: 62.2460*10.00), Time: 1.8s | Val TotalLoss: 544.4875, (ReconX: 0.9465, KLD: 0.7068, LossY: 54.2834)\n",
            "INFO:VAETrainer:Epoch [2/100] Train TotalLoss: 623.4760, (ReconX: 0.9701, KLD: 0.6830*0.07, LossY: 62.2460*10.00), Time: 1.8s | Val TotalLoss: 544.4875, (ReconX: 0.9465, KLD: 0.7068, LossY: 54.2834)\n",
            "2025-10-23 22:50:33,827 - VAETrainer - INFO - Saving model and trainer state to vae_results/best_model.pth...\n",
            "INFO:VAETrainer:Saving model and trainer state to vae_results/best_model.pth...\n",
            "2025-10-23 22:50:33,844 - VAETrainer - INFO - Successfully saved model and trainer state to vae_results/best_model.pth\n",
            "INFO:VAETrainer:Successfully saved model and trainer state to vae_results/best_model.pth\n",
            "2025-10-23 22:50:36,040 - VAETrainer - INFO - Epoch [3/100] Train TotalLoss: 505.9564, (ReconX: 0.9347, KLD: 0.6723*0.10, LossY: 50.4954*10.00), Time: 1.8s | Val TotalLoss: 450.5941, (ReconX: 0.9284, KLD: 0.6913, LossY: 44.8974)\n",
            "INFO:VAETrainer:Epoch [3/100] Train TotalLoss: 505.9564, (ReconX: 0.9347, KLD: 0.6723*0.10, LossY: 50.4954*10.00), Time: 1.8s | Val TotalLoss: 450.5941, (ReconX: 0.9284, KLD: 0.6913, LossY: 44.8974)\n",
            "2025-10-23 22:50:36,042 - VAETrainer - INFO - Saving model and trainer state to vae_results/best_model.pth...\n",
            "INFO:VAETrainer:Saving model and trainer state to vae_results/best_model.pth...\n",
            "2025-10-23 22:50:36,054 - VAETrainer - INFO - Successfully saved model and trainer state to vae_results/best_model.pth\n",
            "INFO:VAETrainer:Successfully saved model and trainer state to vae_results/best_model.pth\n",
            "2025-10-23 22:50:38,138 - VAETrainer - INFO - Epoch [4/100] Train TotalLoss: 418.0138, (ReconX: 0.9210, KLD: 0.6293*0.13, LossY: 41.7009*10.00), Time: 1.7s | Val TotalLoss: 370.1773, (ReconX: 0.9181, KLD: 0.6293, LossY: 36.8630)\n",
            "INFO:VAETrainer:Epoch [4/100] Train TotalLoss: 418.0138, (ReconX: 0.9210, KLD: 0.6293*0.13, LossY: 41.7009*10.00), Time: 1.7s | Val TotalLoss: 370.1773, (ReconX: 0.9181, KLD: 0.6293, LossY: 36.8630)\n",
            "2025-10-23 22:50:38,139 - VAETrainer - INFO - Saving model and trainer state to vae_results/best_model.pth...\n",
            "INFO:VAETrainer:Saving model and trainer state to vae_results/best_model.pth...\n",
            "2025-10-23 22:50:38,152 - VAETrainer - INFO - Successfully saved model and trainer state to vae_results/best_model.pth\n",
            "INFO:VAETrainer:Successfully saved model and trainer state to vae_results/best_model.pth\n",
            "2025-10-23 22:50:40,331 - VAETrainer - INFO - Epoch [5/100] Train TotalLoss: 347.7043, (ReconX: 0.9156, KLD: 0.6057*0.17, LossY: 34.6688*10.00), Time: 1.8s | Val TotalLoss: 312.4079, (ReconX: 0.9117, KLD: 0.6280, LossY: 31.0868)\n",
            "INFO:VAETrainer:Epoch [5/100] Train TotalLoss: 347.7043, (ReconX: 0.9156, KLD: 0.6057*0.17, LossY: 34.6688*10.00), Time: 1.8s | Val TotalLoss: 312.4079, (ReconX: 0.9117, KLD: 0.6280, LossY: 31.0868)\n",
            "2025-10-23 22:50:40,332 - VAETrainer - INFO - Saving model and trainer state to vae_results/best_model.pth...\n",
            "INFO:VAETrainer:Saving model and trainer state to vae_results/best_model.pth...\n",
            "2025-10-23 22:50:40,345 - VAETrainer - INFO - Successfully saved model and trainer state to vae_results/best_model.pth\n",
            "INFO:VAETrainer:Successfully saved model and trainer state to vae_results/best_model.pth\n",
            "2025-10-23 22:50:42,495 - VAETrainer - INFO - Epoch [6/100] Train TotalLoss: 291.9064, (ReconX: 0.9111, KLD: 0.6021*0.20, LossY: 29.0875*10.00), Time: 1.8s | Val TotalLoss: 258.5770, (ReconX: 0.9112, KLD: 0.6099, LossY: 25.7056)\n",
            "INFO:VAETrainer:Epoch [6/100] Train TotalLoss: 291.9064, (ReconX: 0.9111, KLD: 0.6021*0.20, LossY: 29.0875*10.00), Time: 1.8s | Val TotalLoss: 258.5770, (ReconX: 0.9112, KLD: 0.6099, LossY: 25.7056)\n",
            "2025-10-23 22:50:42,497 - VAETrainer - INFO - Saving model and trainer state to vae_results/best_model.pth...\n",
            "INFO:VAETrainer:Saving model and trainer state to vae_results/best_model.pth...\n",
            "2025-10-23 22:50:42,513 - VAETrainer - INFO - Successfully saved model and trainer state to vae_results/best_model.pth\n",
            "INFO:VAETrainer:Successfully saved model and trainer state to vae_results/best_model.pth\n",
            "2025-10-23 22:50:44,805 - VAETrainer - INFO - Epoch [7/100] Train TotalLoss: 247.3711, (ReconX: 0.9094, KLD: 0.6040*0.23, LossY: 24.6321*10.00), Time: 1.9s | Val TotalLoss: 223.6574, (ReconX: 0.9092, KLD: 0.6098, LossY: 22.2138)\n",
            "INFO:VAETrainer:Epoch [7/100] Train TotalLoss: 247.3711, (ReconX: 0.9094, KLD: 0.6040*0.23, LossY: 24.6321*10.00), Time: 1.9s | Val TotalLoss: 223.6574, (ReconX: 0.9092, KLD: 0.6098, LossY: 22.2138)\n",
            "2025-10-23 22:50:44,807 - VAETrainer - INFO - Saving model and trainer state to vae_results/best_model.pth...\n",
            "INFO:VAETrainer:Saving model and trainer state to vae_results/best_model.pth...\n",
            "2025-10-23 22:50:44,820 - VAETrainer - INFO - Successfully saved model and trainer state to vae_results/best_model.pth\n",
            "INFO:VAETrainer:Successfully saved model and trainer state to vae_results/best_model.pth\n",
            "2025-10-23 22:50:47,044 - VAETrainer - INFO - Epoch [8/100] Train TotalLoss: 211.2937, (ReconX: 0.9088, KLD: 0.6074*0.27, LossY: 21.0223*10.00), Time: 1.8s | Val TotalLoss: 189.9750, (ReconX: 0.9098, KLD: 0.6167, LossY: 18.8448)\n",
            "INFO:VAETrainer:Epoch [8/100] Train TotalLoss: 211.2937, (ReconX: 0.9088, KLD: 0.6074*0.27, LossY: 21.0223*10.00), Time: 1.8s | Val TotalLoss: 189.9750, (ReconX: 0.9098, KLD: 0.6167, LossY: 18.8448)\n",
            "2025-10-23 22:50:47,046 - VAETrainer - INFO - Saving model and trainer state to vae_results/best_model.pth...\n",
            "INFO:VAETrainer:Saving model and trainer state to vae_results/best_model.pth...\n",
            "2025-10-23 22:50:47,060 - VAETrainer - INFO - Successfully saved model and trainer state to vae_results/best_model.pth\n",
            "INFO:VAETrainer:Successfully saved model and trainer state to vae_results/best_model.pth\n",
            "2025-10-23 22:50:49,192 - VAETrainer - INFO - Epoch [9/100] Train TotalLoss: 181.3871, (ReconX: 0.9079, KLD: 0.6086*0.30, LossY: 18.0297*10.00), Time: 1.8s | Val TotalLoss: 167.4068, (ReconX: 0.9094, KLD: 0.6196, LossY: 16.5878)\n",
            "INFO:VAETrainer:Epoch [9/100] Train TotalLoss: 181.3871, (ReconX: 0.9079, KLD: 0.6086*0.30, LossY: 18.0297*10.00), Time: 1.8s | Val TotalLoss: 167.4068, (ReconX: 0.9094, KLD: 0.6196, LossY: 16.5878)\n",
            "2025-10-23 22:50:49,194 - VAETrainer - INFO - Saving model and trainer state to vae_results/best_model.pth...\n",
            "INFO:VAETrainer:Saving model and trainer state to vae_results/best_model.pth...\n",
            "2025-10-23 22:50:49,207 - VAETrainer - INFO - Successfully saved model and trainer state to vae_results/best_model.pth\n",
            "INFO:VAETrainer:Successfully saved model and trainer state to vae_results/best_model.pth\n",
            "2025-10-23 22:50:51,364 - VAETrainer - INFO - Epoch [10/100] Train TotalLoss: 156.9728, (ReconX: 0.9089, KLD: 0.6076*0.33, LossY: 15.5861*10.00), Time: 1.8s | Val TotalLoss: 149.0154, (ReconX: 0.9090, KLD: 0.6240, LossY: 14.7482)\n",
            "INFO:VAETrainer:Epoch [10/100] Train TotalLoss: 156.9728, (ReconX: 0.9089, KLD: 0.6076*0.33, LossY: 15.5861*10.00), Time: 1.8s | Val TotalLoss: 149.0154, (ReconX: 0.9090, KLD: 0.6240, LossY: 14.7482)\n",
            "2025-10-23 22:50:51,366 - VAETrainer - INFO - Saving model and trainer state to vae_results/best_model.pth...\n",
            "INFO:VAETrainer:Saving model and trainer state to vae_results/best_model.pth...\n",
            "2025-10-23 22:50:51,380 - VAETrainer - INFO - Successfully saved model and trainer state to vae_results/best_model.pth\n",
            "INFO:VAETrainer:Successfully saved model and trainer state to vae_results/best_model.pth\n",
            "2025-10-23 22:50:53,529 - VAETrainer - INFO - Epoch [11/100] Train TotalLoss: 136.4880, (ReconX: 0.9086, KLD: 0.6052*0.37, LossY: 13.5357*10.00), Time: 1.8s | Val TotalLoss: 126.7672, (ReconX: 0.9100, KLD: 0.6124, LossY: 12.5245)\n",
            "INFO:VAETrainer:Epoch [11/100] Train TotalLoss: 136.4880, (ReconX: 0.9086, KLD: 0.6052*0.37, LossY: 13.5357*10.00), Time: 1.8s | Val TotalLoss: 126.7672, (ReconX: 0.9100, KLD: 0.6124, LossY: 12.5245)\n",
            "2025-10-23 22:50:53,531 - VAETrainer - INFO - Saving model and trainer state to vae_results/best_model.pth...\n",
            "INFO:VAETrainer:Saving model and trainer state to vae_results/best_model.pth...\n",
            "2025-10-23 22:50:53,544 - VAETrainer - INFO - Successfully saved model and trainer state to vae_results/best_model.pth\n",
            "INFO:VAETrainer:Successfully saved model and trainer state to vae_results/best_model.pth\n",
            "2025-10-23 22:50:55,758 - VAETrainer - INFO - Epoch [12/100] Train TotalLoss: 119.8025, (ReconX: 0.9085, KLD: 0.5993*0.40, LossY: 11.8654*10.00), Time: 1.8s | Val TotalLoss: 111.8963, (ReconX: 0.9085, KLD: 0.6067, LossY: 11.0381)\n",
            "INFO:VAETrainer:Epoch [12/100] Train TotalLoss: 119.8025, (ReconX: 0.9085, KLD: 0.5993*0.40, LossY: 11.8654*10.00), Time: 1.8s | Val TotalLoss: 111.8963, (ReconX: 0.9085, KLD: 0.6067, LossY: 11.0381)\n",
            "2025-10-23 22:50:55,759 - VAETrainer - INFO - Saving model and trainer state to vae_results/best_model.pth...\n",
            "INFO:VAETrainer:Saving model and trainer state to vae_results/best_model.pth...\n",
            "2025-10-23 22:50:55,772 - VAETrainer - INFO - Successfully saved model and trainer state to vae_results/best_model.pth\n",
            "INFO:VAETrainer:Successfully saved model and trainer state to vae_results/best_model.pth\n",
            "2025-10-23 22:50:58,021 - VAETrainer - INFO - Epoch [13/100] Train TotalLoss: 105.8669, (ReconX: 0.9097, KLD: 0.5928*0.43, LossY: 10.4700*10.00), Time: 1.9s | Val TotalLoss: 98.2391, (ReconX: 0.9119, KLD: 0.5932, LossY: 9.6734)\n",
            "INFO:VAETrainer:Epoch [13/100] Train TotalLoss: 105.8669, (ReconX: 0.9097, KLD: 0.5928*0.43, LossY: 10.4700*10.00), Time: 1.9s | Val TotalLoss: 98.2391, (ReconX: 0.9119, KLD: 0.5932, LossY: 9.6734)\n",
            "2025-10-23 22:50:58,023 - VAETrainer - INFO - Saving model and trainer state to vae_results/best_model.pth...\n",
            "INFO:VAETrainer:Saving model and trainer state to vae_results/best_model.pth...\n",
            "2025-10-23 22:50:58,036 - VAETrainer - INFO - Successfully saved model and trainer state to vae_results/best_model.pth\n",
            "INFO:VAETrainer:Successfully saved model and trainer state to vae_results/best_model.pth\n",
            "2025-10-23 22:51:00,235 - VAETrainer - INFO - Epoch [14/100] Train TotalLoss: 94.0196, (ReconX: 0.9102, KLD: 0.5805*0.47, LossY: 9.2839*10.00), Time: 1.8s | Val TotalLoss: 86.7506, (ReconX: 0.9108, KLD: 0.5832, LossY: 8.5257)\n",
            "INFO:VAETrainer:Epoch [14/100] Train TotalLoss: 94.0196, (ReconX: 0.9102, KLD: 0.5805*0.47, LossY: 9.2839*10.00), Time: 1.8s | Val TotalLoss: 86.7506, (ReconX: 0.9108, KLD: 0.5832, LossY: 8.5257)\n",
            "2025-10-23 22:51:00,236 - VAETrainer - INFO - Saving model and trainer state to vae_results/best_model.pth...\n",
            "INFO:VAETrainer:Saving model and trainer state to vae_results/best_model.pth...\n",
            "2025-10-23 22:51:00,249 - VAETrainer - INFO - Successfully saved model and trainer state to vae_results/best_model.pth\n",
            "INFO:VAETrainer:Successfully saved model and trainer state to vae_results/best_model.pth\n",
            "2025-10-23 22:51:02,356 - VAETrainer - INFO - Epoch [15/100] Train TotalLoss: 83.5832, (ReconX: 0.9116, KLD: 0.5676*0.50, LossY: 8.2388*10.00), Time: 1.7s | Val TotalLoss: 78.3262, (ReconX: 0.9132, KLD: 0.5702, LossY: 7.6843)\n",
            "INFO:VAETrainer:Epoch [15/100] Train TotalLoss: 83.5832, (ReconX: 0.9116, KLD: 0.5676*0.50, LossY: 8.2388*10.00), Time: 1.7s | Val TotalLoss: 78.3262, (ReconX: 0.9132, KLD: 0.5702, LossY: 7.6843)\n",
            "2025-10-23 22:51:02,359 - VAETrainer - INFO - Saving model and trainer state to vae_results/best_model.pth...\n",
            "INFO:VAETrainer:Saving model and trainer state to vae_results/best_model.pth...\n",
            "2025-10-23 22:51:02,372 - VAETrainer - INFO - Successfully saved model and trainer state to vae_results/best_model.pth\n",
            "INFO:VAETrainer:Successfully saved model and trainer state to vae_results/best_model.pth\n",
            "2025-10-23 22:51:04,452 - VAETrainer - INFO - Epoch [16/100] Train TotalLoss: 74.8103, (ReconX: 0.9120, KLD: 0.5568*0.53, LossY: 7.3601*10.00), Time: 1.7s | Val TotalLoss: 72.5292, (ReconX: 0.9137, KLD: 0.5576, LossY: 7.1058)\n",
            "INFO:VAETrainer:Epoch [16/100] Train TotalLoss: 74.8103, (ReconX: 0.9120, KLD: 0.5568*0.53, LossY: 7.3601*10.00), Time: 1.7s | Val TotalLoss: 72.5292, (ReconX: 0.9137, KLD: 0.5576, LossY: 7.1058)\n",
            "2025-10-23 22:51:04,454 - VAETrainer - INFO - Saving model and trainer state to vae_results/best_model.pth...\n",
            "INFO:VAETrainer:Saving model and trainer state to vae_results/best_model.pth...\n",
            "2025-10-23 22:51:04,467 - VAETrainer - INFO - Successfully saved model and trainer state to vae_results/best_model.pth\n",
            "INFO:VAETrainer:Successfully saved model and trainer state to vae_results/best_model.pth\n",
            "2025-10-23 22:51:06,620 - VAETrainer - INFO - Epoch [17/100] Train TotalLoss: 67.6781, (ReconX: 0.9132, KLD: 0.5459*0.57, LossY: 6.6456*10.00), Time: 1.8s | Val TotalLoss: 63.5714, (ReconX: 0.9141, KLD: 0.5439, LossY: 6.2113)\n",
            "INFO:VAETrainer:Epoch [17/100] Train TotalLoss: 67.6781, (ReconX: 0.9132, KLD: 0.5459*0.57, LossY: 6.6456*10.00), Time: 1.8s | Val TotalLoss: 63.5714, (ReconX: 0.9141, KLD: 0.5439, LossY: 6.2113)\n",
            "2025-10-23 22:51:06,622 - VAETrainer - INFO - Saving model and trainer state to vae_results/best_model.pth...\n",
            "INFO:VAETrainer:Saving model and trainer state to vae_results/best_model.pth...\n",
            "2025-10-23 22:51:06,634 - VAETrainer - INFO - Successfully saved model and trainer state to vae_results/best_model.pth\n",
            "INFO:VAETrainer:Successfully saved model and trainer state to vae_results/best_model.pth\n",
            "2025-10-23 22:51:08,704 - VAETrainer - INFO - Epoch [18/100] Train TotalLoss: 61.7672, (ReconX: 0.9138, KLD: 0.5332*0.60, LossY: 6.0534*10.00), Time: 1.7s | Val TotalLoss: 58.5394, (ReconX: 0.9138, KLD: 0.5327, LossY: 5.7093)\n",
            "INFO:VAETrainer:Epoch [18/100] Train TotalLoss: 61.7672, (ReconX: 0.9138, KLD: 0.5332*0.60, LossY: 6.0534*10.00), Time: 1.7s | Val TotalLoss: 58.5394, (ReconX: 0.9138, KLD: 0.5327, LossY: 5.7093)\n",
            "2025-10-23 22:51:08,706 - VAETrainer - INFO - Saving model and trainer state to vae_results/best_model.pth...\n",
            "INFO:VAETrainer:Saving model and trainer state to vae_results/best_model.pth...\n",
            "2025-10-23 22:51:08,719 - VAETrainer - INFO - Successfully saved model and trainer state to vae_results/best_model.pth\n",
            "INFO:VAETrainer:Successfully saved model and trainer state to vae_results/best_model.pth\n",
            "2025-10-23 22:51:10,836 - VAETrainer - INFO - Epoch [19/100] Train TotalLoss: 57.0995, (ReconX: 0.9148, KLD: 0.5187*0.63, LossY: 5.5856*10.00), Time: 1.7s | Val TotalLoss: 55.2482, (ReconX: 0.9146, KLD: 0.5160, LossY: 5.3818)\n",
            "INFO:VAETrainer:Epoch [19/100] Train TotalLoss: 57.0995, (ReconX: 0.9148, KLD: 0.5187*0.63, LossY: 5.5856*10.00), Time: 1.7s | Val TotalLoss: 55.2482, (ReconX: 0.9146, KLD: 0.5160, LossY: 5.3818)\n",
            "2025-10-23 22:51:10,837 - VAETrainer - INFO - Saving model and trainer state to vae_results/best_model.pth...\n",
            "INFO:VAETrainer:Saving model and trainer state to vae_results/best_model.pth...\n",
            "2025-10-23 22:51:10,850 - VAETrainer - INFO - Successfully saved model and trainer state to vae_results/best_model.pth\n",
            "INFO:VAETrainer:Successfully saved model and trainer state to vae_results/best_model.pth\n",
            "2025-10-23 22:51:12,975 - VAETrainer - INFO - Epoch [20/100] Train TotalLoss: 53.1970, (ReconX: 0.9164, KLD: 0.5051*0.67, LossY: 5.1944*10.00), Time: 1.7s | Val TotalLoss: 51.3447, (ReconX: 0.9173, KLD: 0.5000, LossY: 4.9927)\n",
            "INFO:VAETrainer:Epoch [20/100] Train TotalLoss: 53.1970, (ReconX: 0.9164, KLD: 0.5051*0.67, LossY: 5.1944*10.00), Time: 1.7s | Val TotalLoss: 51.3447, (ReconX: 0.9173, KLD: 0.5000, LossY: 4.9927)\n",
            "2025-10-23 22:51:12,977 - VAETrainer - INFO - Saving model and trainer state to vae_results/best_model.pth...\n",
            "INFO:VAETrainer:Saving model and trainer state to vae_results/best_model.pth...\n",
            "2025-10-23 22:51:12,990 - VAETrainer - INFO - Successfully saved model and trainer state to vae_results/best_model.pth\n",
            "INFO:VAETrainer:Successfully saved model and trainer state to vae_results/best_model.pth\n",
            "2025-10-23 22:51:15,044 - VAETrainer - INFO - Epoch [21/100] Train TotalLoss: 49.9727, (ReconX: 0.9171, KLD: 0.4895*0.70, LossY: 4.8713*10.00), Time: 1.7s | Val TotalLoss: 48.1461, (ReconX: 0.9200, KLD: 0.4834, LossY: 4.6743)\n",
            "INFO:VAETrainer:Epoch [21/100] Train TotalLoss: 49.9727, (ReconX: 0.9171, KLD: 0.4895*0.70, LossY: 4.8713*10.00), Time: 1.7s | Val TotalLoss: 48.1461, (ReconX: 0.9200, KLD: 0.4834, LossY: 4.6743)\n",
            "2025-10-23 22:51:15,046 - VAETrainer - INFO - Saving model and trainer state to vae_results/best_model.pth...\n",
            "INFO:VAETrainer:Saving model and trainer state to vae_results/best_model.pth...\n",
            "2025-10-23 22:51:15,059 - VAETrainer - INFO - Successfully saved model and trainer state to vae_results/best_model.pth\n",
            "INFO:VAETrainer:Successfully saved model and trainer state to vae_results/best_model.pth\n",
            "2025-10-23 22:51:17,236 - VAETrainer - INFO - Epoch [22/100] Train TotalLoss: 47.2965, (ReconX: 0.9191, KLD: 0.4742*0.73, LossY: 4.6030*10.00), Time: 1.8s | Val TotalLoss: 46.8935, (ReconX: 0.9194, KLD: 0.4703, LossY: 4.5504)\n",
            "INFO:VAETrainer:Epoch [22/100] Train TotalLoss: 47.2965, (ReconX: 0.9191, KLD: 0.4742*0.73, LossY: 4.6030*10.00), Time: 1.8s | Val TotalLoss: 46.8935, (ReconX: 0.9194, KLD: 0.4703, LossY: 4.5504)\n",
            "2025-10-23 22:51:17,237 - VAETrainer - INFO - Saving model and trainer state to vae_results/best_model.pth...\n",
            "INFO:VAETrainer:Saving model and trainer state to vae_results/best_model.pth...\n",
            "2025-10-23 22:51:17,254 - VAETrainer - INFO - Successfully saved model and trainer state to vae_results/best_model.pth\n",
            "INFO:VAETrainer:Successfully saved model and trainer state to vae_results/best_model.pth\n",
            "2025-10-23 22:51:19,493 - VAETrainer - INFO - Epoch [23/100] Train TotalLoss: 45.2387, (ReconX: 0.9213, KLD: 0.4586*0.77, LossY: 4.3966*10.00), Time: 1.9s | Val TotalLoss: 44.6851, (ReconX: 0.9214, KLD: 0.4559, LossY: 4.3308)\n",
            "INFO:VAETrainer:Epoch [23/100] Train TotalLoss: 45.2387, (ReconX: 0.9213, KLD: 0.4586*0.77, LossY: 4.3966*10.00), Time: 1.9s | Val TotalLoss: 44.6851, (ReconX: 0.9214, KLD: 0.4559, LossY: 4.3308)\n",
            "2025-10-23 22:51:19,495 - VAETrainer - INFO - Saving model and trainer state to vae_results/best_model.pth...\n",
            "INFO:VAETrainer:Saving model and trainer state to vae_results/best_model.pth...\n",
            "2025-10-23 22:51:19,508 - VAETrainer - INFO - Successfully saved model and trainer state to vae_results/best_model.pth\n",
            "INFO:VAETrainer:Successfully saved model and trainer state to vae_results/best_model.pth\n",
            "2025-10-23 22:51:21,573 - VAETrainer - INFO - Epoch [24/100] Train TotalLoss: 43.4076, (ReconX: 0.9220, KLD: 0.4451*0.80, LossY: 4.2130*10.00), Time: 1.7s | Val TotalLoss: 42.2234, (ReconX: 0.9231, KLD: 0.4403, LossY: 4.0860)\n",
            "INFO:VAETrainer:Epoch [24/100] Train TotalLoss: 43.4076, (ReconX: 0.9220, KLD: 0.4451*0.80, LossY: 4.2130*10.00), Time: 1.7s | Val TotalLoss: 42.2234, (ReconX: 0.9231, KLD: 0.4403, LossY: 4.0860)\n",
            "2025-10-23 22:51:21,574 - VAETrainer - INFO - Saving model and trainer state to vae_results/best_model.pth...\n",
            "INFO:VAETrainer:Saving model and trainer state to vae_results/best_model.pth...\n",
            "2025-10-23 22:51:21,588 - VAETrainer - INFO - Successfully saved model and trainer state to vae_results/best_model.pth\n",
            "INFO:VAETrainer:Successfully saved model and trainer state to vae_results/best_model.pth\n",
            "2025-10-23 22:51:23,773 - VAETrainer - INFO - Epoch [25/100] Train TotalLoss: 41.8350, (ReconX: 0.9228, KLD: 0.4293*0.83, LossY: 4.0554*10.00), Time: 1.8s | Val TotalLoss: 41.4545, (ReconX: 0.9241, KLD: 0.4303, LossY: 4.0100)\n",
            "INFO:VAETrainer:Epoch [25/100] Train TotalLoss: 41.8350, (ReconX: 0.9228, KLD: 0.4293*0.83, LossY: 4.0554*10.00), Time: 1.8s | Val TotalLoss: 41.4545, (ReconX: 0.9241, KLD: 0.4303, LossY: 4.0100)\n",
            "2025-10-23 22:51:23,774 - VAETrainer - INFO - Saving model and trainer state to vae_results/best_model.pth...\n",
            "INFO:VAETrainer:Saving model and trainer state to vae_results/best_model.pth...\n",
            "2025-10-23 22:51:23,787 - VAETrainer - INFO - Successfully saved model and trainer state to vae_results/best_model.pth\n",
            "INFO:VAETrainer:Successfully saved model and trainer state to vae_results/best_model.pth\n",
            "2025-10-23 22:51:25,964 - VAETrainer - INFO - Epoch [26/100] Train TotalLoss: 40.4277, (ReconX: 0.9252, KLD: 0.4153*0.87, LossY: 3.9143*10.00), Time: 1.8s | Val TotalLoss: 39.8350, (ReconX: 0.9262, KLD: 0.4112, LossY: 3.8498)\n",
            "INFO:VAETrainer:Epoch [26/100] Train TotalLoss: 40.4277, (ReconX: 0.9252, KLD: 0.4153*0.87, LossY: 3.9143*10.00), Time: 1.8s | Val TotalLoss: 39.8350, (ReconX: 0.9262, KLD: 0.4112, LossY: 3.8498)\n",
            "2025-10-23 22:51:25,966 - VAETrainer - INFO - Saving model and trainer state to vae_results/best_model.pth...\n",
            "INFO:VAETrainer:Saving model and trainer state to vae_results/best_model.pth...\n",
            "2025-10-23 22:51:25,979 - VAETrainer - INFO - Successfully saved model and trainer state to vae_results/best_model.pth\n",
            "INFO:VAETrainer:Successfully saved model and trainer state to vae_results/best_model.pth\n",
            "2025-10-23 22:51:28,078 - VAETrainer - INFO - Epoch [27/100] Train TotalLoss: 39.3538, (ReconX: 0.9260, KLD: 0.4016*0.90, LossY: 3.8066*10.00), Time: 1.7s | Val TotalLoss: 38.2358, (ReconX: 0.9273, KLD: 0.3930, LossY: 3.6915)\n",
            "INFO:VAETrainer:Epoch [27/100] Train TotalLoss: 39.3538, (ReconX: 0.9260, KLD: 0.4016*0.90, LossY: 3.8066*10.00), Time: 1.7s | Val TotalLoss: 38.2358, (ReconX: 0.9273, KLD: 0.3930, LossY: 3.6915)\n",
            "2025-10-23 22:51:28,080 - VAETrainer - INFO - Saving model and trainer state to vae_results/best_model.pth...\n",
            "INFO:VAETrainer:Saving model and trainer state to vae_results/best_model.pth...\n",
            "2025-10-23 22:51:28,093 - VAETrainer - INFO - Successfully saved model and trainer state to vae_results/best_model.pth\n",
            "INFO:VAETrainer:Successfully saved model and trainer state to vae_results/best_model.pth\n",
            "2025-10-23 22:51:30,157 - VAETrainer - INFO - Epoch [28/100] Train TotalLoss: 38.3338, (ReconX: 0.9283, KLD: 0.3858*0.93, LossY: 3.7045*10.00), Time: 1.7s | Val TotalLoss: 36.8648, (ReconX: 0.9294, KLD: 0.3808, LossY: 3.5555)\n",
            "INFO:VAETrainer:Epoch [28/100] Train TotalLoss: 38.3338, (ReconX: 0.9283, KLD: 0.3858*0.93, LossY: 3.7045*10.00), Time: 1.7s | Val TotalLoss: 36.8648, (ReconX: 0.9294, KLD: 0.3808, LossY: 3.5555)\n",
            "2025-10-23 22:51:30,159 - VAETrainer - INFO - Saving model and trainer state to vae_results/best_model.pth...\n",
            "INFO:VAETrainer:Saving model and trainer state to vae_results/best_model.pth...\n",
            "2025-10-23 22:51:30,173 - VAETrainer - INFO - Successfully saved model and trainer state to vae_results/best_model.pth\n",
            "INFO:VAETrainer:Successfully saved model and trainer state to vae_results/best_model.pth\n",
            "2025-10-23 22:51:32,318 - VAETrainer - INFO - Epoch [29/100] Train TotalLoss: 37.4473, (ReconX: 0.9298, KLD: 0.3704*0.97, LossY: 3.6159*10.00), Time: 1.8s | Val TotalLoss: 37.3221, (ReconX: 0.9335, KLD: 0.3638, LossY: 3.6025)\n",
            "INFO:VAETrainer:Epoch [29/100] Train TotalLoss: 37.4473, (ReconX: 0.9298, KLD: 0.3704*0.97, LossY: 3.6159*10.00), Time: 1.8s | Val TotalLoss: 37.3221, (ReconX: 0.9335, KLD: 0.3638, LossY: 3.6025)\n",
            "2025-10-23 22:51:34,451 - VAETrainer - INFO - Epoch [30/100] Train TotalLoss: 36.8317, (ReconX: 0.9314, KLD: 0.3563*1.00, LossY: 3.5544*10.00), Time: 1.8s | Val TotalLoss: 35.9842, (ReconX: 0.9341, KLD: 0.3500, LossY: 3.4700)\n",
            "INFO:VAETrainer:Epoch [30/100] Train TotalLoss: 36.8317, (ReconX: 0.9314, KLD: 0.3563*1.00, LossY: 3.5544*10.00), Time: 1.8s | Val TotalLoss: 35.9842, (ReconX: 0.9341, KLD: 0.3500, LossY: 3.4700)\n",
            "2025-10-23 22:51:34,453 - VAETrainer - INFO - Saving model and trainer state to vae_results/best_model.pth...\n",
            "INFO:VAETrainer:Saving model and trainer state to vae_results/best_model.pth...\n",
            "2025-10-23 22:51:34,466 - VAETrainer - INFO - Successfully saved model and trainer state to vae_results/best_model.pth\n",
            "INFO:VAETrainer:Successfully saved model and trainer state to vae_results/best_model.pth\n",
            "2025-10-23 22:51:36,709 - VAETrainer - INFO - Epoch [31/100] Train TotalLoss: 36.0830, (ReconX: 0.9324, KLD: 0.3412*1.00, LossY: 3.4809*10.00), Time: 1.9s | Val TotalLoss: 35.0092, (ReconX: 0.9360, KLD: 0.3362, LossY: 3.3737)\n",
            "INFO:VAETrainer:Epoch [31/100] Train TotalLoss: 36.0830, (ReconX: 0.9324, KLD: 0.3412*1.00, LossY: 3.4809*10.00), Time: 1.9s | Val TotalLoss: 35.0092, (ReconX: 0.9360, KLD: 0.3362, LossY: 3.3737)\n",
            "2025-10-23 22:51:36,711 - VAETrainer - INFO - Saving model and trainer state to vae_results/best_model.pth...\n",
            "INFO:VAETrainer:Saving model and trainer state to vae_results/best_model.pth...\n",
            "2025-10-23 22:51:36,728 - VAETrainer - INFO - Successfully saved model and trainer state to vae_results/best_model.pth\n",
            "INFO:VAETrainer:Successfully saved model and trainer state to vae_results/best_model.pth\n",
            "2025-10-23 22:51:38,932 - VAETrainer - INFO - Epoch [32/100] Train TotalLoss: 35.6220, (ReconX: 0.9349, KLD: 0.3276*1.00, LossY: 3.4359*10.00), Time: 1.8s | Val TotalLoss: 34.6936, (ReconX: 0.9379, KLD: 0.3232, LossY: 3.3433)\n",
            "INFO:VAETrainer:Epoch [32/100] Train TotalLoss: 35.6220, (ReconX: 0.9349, KLD: 0.3276*1.00, LossY: 3.4359*10.00), Time: 1.8s | Val TotalLoss: 34.6936, (ReconX: 0.9379, KLD: 0.3232, LossY: 3.3433)\n",
            "2025-10-23 22:51:38,934 - VAETrainer - INFO - Saving model and trainer state to vae_results/best_model.pth...\n",
            "INFO:VAETrainer:Saving model and trainer state to vae_results/best_model.pth...\n",
            "2025-10-23 22:51:38,947 - VAETrainer - INFO - Successfully saved model and trainer state to vae_results/best_model.pth\n",
            "INFO:VAETrainer:Successfully saved model and trainer state to vae_results/best_model.pth\n",
            "2025-10-23 22:51:41,037 - VAETrainer - INFO - Epoch [33/100] Train TotalLoss: 34.9909, (ReconX: 0.9368, KLD: 0.3136*1.00, LossY: 3.3740*10.00), Time: 1.7s | Val TotalLoss: 34.0686, (ReconX: 0.9380, KLD: 0.3091, LossY: 3.2821)\n",
            "INFO:VAETrainer:Epoch [33/100] Train TotalLoss: 34.9909, (ReconX: 0.9368, KLD: 0.3136*1.00, LossY: 3.3740*10.00), Time: 1.7s | Val TotalLoss: 34.0686, (ReconX: 0.9380, KLD: 0.3091, LossY: 3.2821)\n",
            "2025-10-23 22:51:41,039 - VAETrainer - INFO - Saving model and trainer state to vae_results/best_model.pth...\n",
            "INFO:VAETrainer:Saving model and trainer state to vae_results/best_model.pth...\n",
            "2025-10-23 22:51:41,052 - VAETrainer - INFO - Successfully saved model and trainer state to vae_results/best_model.pth\n",
            "INFO:VAETrainer:Successfully saved model and trainer state to vae_results/best_model.pth\n",
            "2025-10-23 22:51:43,212 - VAETrainer - INFO - Epoch [34/100] Train TotalLoss: 34.6379, (ReconX: 0.9386, KLD: 0.3003*1.00, LossY: 3.3399*10.00), Time: 1.8s | Val TotalLoss: 33.7307, (ReconX: 0.9402, KLD: 0.2982, LossY: 3.2492)\n",
            "INFO:VAETrainer:Epoch [34/100] Train TotalLoss: 34.6379, (ReconX: 0.9386, KLD: 0.3003*1.00, LossY: 3.3399*10.00), Time: 1.8s | Val TotalLoss: 33.7307, (ReconX: 0.9402, KLD: 0.2982, LossY: 3.2492)\n",
            "2025-10-23 22:51:43,215 - VAETrainer - INFO - Saving model and trainer state to vae_results/best_model.pth...\n",
            "INFO:VAETrainer:Saving model and trainer state to vae_results/best_model.pth...\n",
            "2025-10-23 22:51:43,229 - VAETrainer - INFO - Successfully saved model and trainer state to vae_results/best_model.pth\n",
            "INFO:VAETrainer:Successfully saved model and trainer state to vae_results/best_model.pth\n",
            "2025-10-23 22:51:45,406 - VAETrainer - INFO - Epoch [35/100] Train TotalLoss: 34.1368, (ReconX: 0.9408, KLD: 0.2868*1.00, LossY: 3.2909*10.00), Time: 1.8s | Val TotalLoss: 33.5673, (ReconX: 0.9417, KLD: 0.2810, LossY: 3.2345)\n",
            "INFO:VAETrainer:Epoch [35/100] Train TotalLoss: 34.1368, (ReconX: 0.9408, KLD: 0.2868*1.00, LossY: 3.2909*10.00), Time: 1.8s | Val TotalLoss: 33.5673, (ReconX: 0.9417, KLD: 0.2810, LossY: 3.2345)\n",
            "2025-10-23 22:51:45,408 - VAETrainer - INFO - Saving model and trainer state to vae_results/best_model.pth...\n",
            "INFO:VAETrainer:Saving model and trainer state to vae_results/best_model.pth...\n",
            "2025-10-23 22:51:45,420 - VAETrainer - INFO - Successfully saved model and trainer state to vae_results/best_model.pth\n",
            "INFO:VAETrainer:Successfully saved model and trainer state to vae_results/best_model.pth\n",
            "2025-10-23 22:51:47,587 - VAETrainer - INFO - Epoch [36/100] Train TotalLoss: 33.8027, (ReconX: 0.9423, KLD: 0.2738*1.00, LossY: 3.2587*10.00), Time: 1.8s | Val TotalLoss: 33.0920, (ReconX: 0.9438, KLD: 0.2688, LossY: 3.1879)\n",
            "INFO:VAETrainer:Epoch [36/100] Train TotalLoss: 33.8027, (ReconX: 0.9423, KLD: 0.2738*1.00, LossY: 3.2587*10.00), Time: 1.8s | Val TotalLoss: 33.0920, (ReconX: 0.9438, KLD: 0.2688, LossY: 3.1879)\n",
            "2025-10-23 22:51:47,589 - VAETrainer - INFO - Saving model and trainer state to vae_results/best_model.pth...\n",
            "INFO:VAETrainer:Saving model and trainer state to vae_results/best_model.pth...\n",
            "2025-10-23 22:51:47,603 - VAETrainer - INFO - Successfully saved model and trainer state to vae_results/best_model.pth\n",
            "INFO:VAETrainer:Successfully saved model and trainer state to vae_results/best_model.pth\n",
            "2025-10-23 22:51:49,790 - VAETrainer - INFO - Epoch [37/100] Train TotalLoss: 33.4404, (ReconX: 0.9454, KLD: 0.2614*1.00, LossY: 3.2234*10.00), Time: 1.8s | Val TotalLoss: 32.9272, (ReconX: 0.9467, KLD: 0.2566, LossY: 3.1724)\n",
            "INFO:VAETrainer:Epoch [37/100] Train TotalLoss: 33.4404, (ReconX: 0.9454, KLD: 0.2614*1.00, LossY: 3.2234*10.00), Time: 1.8s | Val TotalLoss: 32.9272, (ReconX: 0.9467, KLD: 0.2566, LossY: 3.1724)\n",
            "2025-10-23 22:51:49,792 - VAETrainer - INFO - Saving model and trainer state to vae_results/best_model.pth...\n",
            "INFO:VAETrainer:Saving model and trainer state to vae_results/best_model.pth...\n",
            "2025-10-23 22:51:49,805 - VAETrainer - INFO - Successfully saved model and trainer state to vae_results/best_model.pth\n",
            "INFO:VAETrainer:Successfully saved model and trainer state to vae_results/best_model.pth\n",
            "2025-10-23 22:51:51,972 - VAETrainer - INFO - Epoch [38/100] Train TotalLoss: 33.1623, (ReconX: 0.9456, KLD: 0.2503*1.00, LossY: 3.1966*10.00), Time: 1.8s | Val TotalLoss: 32.6623, (ReconX: 0.9496, KLD: 0.2494, LossY: 3.1463)\n",
            "INFO:VAETrainer:Epoch [38/100] Train TotalLoss: 33.1623, (ReconX: 0.9456, KLD: 0.2503*1.00, LossY: 3.1966*10.00), Time: 1.8s | Val TotalLoss: 32.6623, (ReconX: 0.9496, KLD: 0.2494, LossY: 3.1463)\n",
            "2025-10-23 22:51:51,974 - VAETrainer - INFO - Saving model and trainer state to vae_results/best_model.pth...\n",
            "INFO:VAETrainer:Saving model and trainer state to vae_results/best_model.pth...\n",
            "2025-10-23 22:51:51,991 - VAETrainer - INFO - Successfully saved model and trainer state to vae_results/best_model.pth\n",
            "INFO:VAETrainer:Successfully saved model and trainer state to vae_results/best_model.pth\n",
            "2025-10-23 22:51:54,147 - VAETrainer - INFO - Epoch [39/100] Train TotalLoss: 32.7437, (ReconX: 0.9479, KLD: 0.2396*1.00, LossY: 3.1556*10.00), Time: 1.8s | Val TotalLoss: 32.3227, (ReconX: 0.9513, KLD: 0.2310, LossY: 3.1140)\n",
            "INFO:VAETrainer:Epoch [39/100] Train TotalLoss: 32.7437, (ReconX: 0.9479, KLD: 0.2396*1.00, LossY: 3.1556*10.00), Time: 1.8s | Val TotalLoss: 32.3227, (ReconX: 0.9513, KLD: 0.2310, LossY: 3.1140)\n",
            "2025-10-23 22:51:54,149 - VAETrainer - INFO - Saving model and trainer state to vae_results/best_model.pth...\n",
            "INFO:VAETrainer:Saving model and trainer state to vae_results/best_model.pth...\n",
            "2025-10-23 22:51:54,162 - VAETrainer - INFO - Successfully saved model and trainer state to vae_results/best_model.pth\n",
            "INFO:VAETrainer:Successfully saved model and trainer state to vae_results/best_model.pth\n",
            "2025-10-23 22:51:56,305 - VAETrainer - INFO - Epoch [40/100] Train TotalLoss: 32.3978, (ReconX: 0.9494, KLD: 0.2264*1.00, LossY: 3.1222*10.00), Time: 1.8s | Val TotalLoss: 31.9904, (ReconX: 0.9507, KLD: 0.2212, LossY: 3.0818)\n",
            "INFO:VAETrainer:Epoch [40/100] Train TotalLoss: 32.3978, (ReconX: 0.9494, KLD: 0.2264*1.00, LossY: 3.1222*10.00), Time: 1.8s | Val TotalLoss: 31.9904, (ReconX: 0.9507, KLD: 0.2212, LossY: 3.0818)\n",
            "2025-10-23 22:51:56,307 - VAETrainer - INFO - Saving model and trainer state to vae_results/best_model.pth...\n",
            "INFO:VAETrainer:Saving model and trainer state to vae_results/best_model.pth...\n",
            "2025-10-23 22:51:56,320 - VAETrainer - INFO - Successfully saved model and trainer state to vae_results/best_model.pth\n",
            "INFO:VAETrainer:Successfully saved model and trainer state to vae_results/best_model.pth\n",
            "2025-10-23 22:51:58,575 - VAETrainer - INFO - Epoch [41/100] Train TotalLoss: 32.0744, (ReconX: 0.9523, KLD: 0.2156*1.00, LossY: 3.0907*10.00), Time: 1.9s | Val TotalLoss: 31.6141, (ReconX: 0.9541, KLD: 0.2106, LossY: 3.0449)\n",
            "INFO:VAETrainer:Epoch [41/100] Train TotalLoss: 32.0744, (ReconX: 0.9523, KLD: 0.2156*1.00, LossY: 3.0907*10.00), Time: 1.9s | Val TotalLoss: 31.6141, (ReconX: 0.9541, KLD: 0.2106, LossY: 3.0449)\n",
            "2025-10-23 22:51:58,577 - VAETrainer - INFO - Saving model and trainer state to vae_results/best_model.pth...\n",
            "INFO:VAETrainer:Saving model and trainer state to vae_results/best_model.pth...\n",
            "2025-10-23 22:51:58,592 - VAETrainer - INFO - Successfully saved model and trainer state to vae_results/best_model.pth\n",
            "INFO:VAETrainer:Successfully saved model and trainer state to vae_results/best_model.pth\n",
            "2025-10-23 22:52:00,825 - VAETrainer - INFO - Epoch [42/100] Train TotalLoss: 31.4956, (ReconX: 0.9535, KLD: 0.2044*1.00, LossY: 3.0338*10.00), Time: 1.9s | Val TotalLoss: 30.8593, (ReconX: 0.9582, KLD: 0.1971, LossY: 2.9704)\n",
            "INFO:VAETrainer:Epoch [42/100] Train TotalLoss: 31.4956, (ReconX: 0.9535, KLD: 0.2044*1.00, LossY: 3.0338*10.00), Time: 1.9s | Val TotalLoss: 30.8593, (ReconX: 0.9582, KLD: 0.1971, LossY: 2.9704)\n",
            "2025-10-23 22:52:00,827 - VAETrainer - INFO - Saving model and trainer state to vae_results/best_model.pth...\n",
            "INFO:VAETrainer:Saving model and trainer state to vae_results/best_model.pth...\n",
            "2025-10-23 22:52:00,841 - VAETrainer - INFO - Successfully saved model and trainer state to vae_results/best_model.pth\n",
            "INFO:VAETrainer:Successfully saved model and trainer state to vae_results/best_model.pth\n",
            "2025-10-23 22:52:02,953 - VAETrainer - INFO - Epoch [43/100] Train TotalLoss: 30.4323, (ReconX: 0.9577, KLD: 0.1876*1.00, LossY: 2.9287*10.00), Time: 1.7s | Val TotalLoss: 29.9160, (ReconX: 0.9592, KLD: 0.1815, LossY: 2.8775)\n",
            "INFO:VAETrainer:Epoch [43/100] Train TotalLoss: 30.4323, (ReconX: 0.9577, KLD: 0.1876*1.00, LossY: 2.9287*10.00), Time: 1.7s | Val TotalLoss: 29.9160, (ReconX: 0.9592, KLD: 0.1815, LossY: 2.8775)\n",
            "2025-10-23 22:52:02,955 - VAETrainer - INFO - Saving model and trainer state to vae_results/best_model.pth...\n",
            "INFO:VAETrainer:Saving model and trainer state to vae_results/best_model.pth...\n",
            "2025-10-23 22:52:02,968 - VAETrainer - INFO - Successfully saved model and trainer state to vae_results/best_model.pth\n",
            "INFO:VAETrainer:Successfully saved model and trainer state to vae_results/best_model.pth\n",
            "2025-10-23 22:52:05,072 - VAETrainer - INFO - Epoch [44/100] Train TotalLoss: 29.5231, (ReconX: 0.9596, KLD: 0.1760*1.00, LossY: 2.8388*10.00), Time: 1.7s | Val TotalLoss: 28.5837, (ReconX: 0.9637, KLD: 0.1697, LossY: 2.7450)\n",
            "INFO:VAETrainer:Epoch [44/100] Train TotalLoss: 29.5231, (ReconX: 0.9596, KLD: 0.1760*1.00, LossY: 2.8388*10.00), Time: 1.7s | Val TotalLoss: 28.5837, (ReconX: 0.9637, KLD: 0.1697, LossY: 2.7450)\n",
            "2025-10-23 22:52:05,074 - VAETrainer - INFO - Saving model and trainer state to vae_results/best_model.pth...\n",
            "INFO:VAETrainer:Saving model and trainer state to vae_results/best_model.pth...\n",
            "2025-10-23 22:52:05,088 - VAETrainer - INFO - Successfully saved model and trainer state to vae_results/best_model.pth\n",
            "INFO:VAETrainer:Successfully saved model and trainer state to vae_results/best_model.pth\n",
            "2025-10-23 22:52:07,196 - VAETrainer - INFO - Epoch [45/100] Train TotalLoss: 28.5829, (ReconX: 0.9627, KLD: 0.1638*1.00, LossY: 2.7456*10.00), Time: 1.7s | Val TotalLoss: 28.1007, (ReconX: 0.9653, KLD: 0.1576, LossY: 2.6978)\n",
            "INFO:VAETrainer:Epoch [45/100] Train TotalLoss: 28.5829, (ReconX: 0.9627, KLD: 0.1638*1.00, LossY: 2.7456*10.00), Time: 1.7s | Val TotalLoss: 28.1007, (ReconX: 0.9653, KLD: 0.1576, LossY: 2.6978)\n",
            "2025-10-23 22:52:07,197 - VAETrainer - INFO - Saving model and trainer state to vae_results/best_model.pth...\n",
            "INFO:VAETrainer:Saving model and trainer state to vae_results/best_model.pth...\n",
            "2025-10-23 22:52:07,210 - VAETrainer - INFO - Successfully saved model and trainer state to vae_results/best_model.pth\n",
            "INFO:VAETrainer:Successfully saved model and trainer state to vae_results/best_model.pth\n",
            "2025-10-23 22:52:09,385 - VAETrainer - INFO - Epoch [46/100] Train TotalLoss: 27.9001, (ReconX: 0.9649, KLD: 0.1513*1.00, LossY: 2.6784*10.00), Time: 1.8s | Val TotalLoss: 27.1076, (ReconX: 0.9690, KLD: 0.1433, LossY: 2.5995)\n",
            "INFO:VAETrainer:Epoch [46/100] Train TotalLoss: 27.9001, (ReconX: 0.9649, KLD: 0.1513*1.00, LossY: 2.6784*10.00), Time: 1.8s | Val TotalLoss: 27.1076, (ReconX: 0.9690, KLD: 0.1433, LossY: 2.5995)\n",
            "2025-10-23 22:52:09,387 - VAETrainer - INFO - Saving model and trainer state to vae_results/best_model.pth...\n",
            "INFO:VAETrainer:Saving model and trainer state to vae_results/best_model.pth...\n",
            "2025-10-23 22:52:09,400 - VAETrainer - INFO - Successfully saved model and trainer state to vae_results/best_model.pth\n",
            "INFO:VAETrainer:Successfully saved model and trainer state to vae_results/best_model.pth\n",
            "2025-10-23 22:52:11,557 - VAETrainer - INFO - Epoch [47/100] Train TotalLoss: 26.5453, (ReconX: 0.9673, KLD: 0.1394*1.00, LossY: 2.5439*10.00), Time: 1.8s | Val TotalLoss: 25.6642, (ReconX: 0.9691, KLD: 0.1363, LossY: 2.4559)\n",
            "INFO:VAETrainer:Epoch [47/100] Train TotalLoss: 26.5453, (ReconX: 0.9673, KLD: 0.1394*1.00, LossY: 2.5439*10.00), Time: 1.8s | Val TotalLoss: 25.6642, (ReconX: 0.9691, KLD: 0.1363, LossY: 2.4559)\n",
            "2025-10-23 22:52:11,558 - VAETrainer - INFO - Saving model and trainer state to vae_results/best_model.pth...\n",
            "INFO:VAETrainer:Saving model and trainer state to vae_results/best_model.pth...\n",
            "2025-10-23 22:52:11,576 - VAETrainer - INFO - Successfully saved model and trainer state to vae_results/best_model.pth\n",
            "INFO:VAETrainer:Successfully saved model and trainer state to vae_results/best_model.pth\n",
            "2025-10-23 22:52:13,806 - VAETrainer - INFO - Epoch [48/100] Train TotalLoss: 25.3284, (ReconX: 0.9683, KLD: 0.1345*1.00, LossY: 2.4226*10.00), Time: 1.8s | Val TotalLoss: 24.1479, (ReconX: 0.9698, KLD: 0.1329, LossY: 2.3045)\n",
            "INFO:VAETrainer:Epoch [48/100] Train TotalLoss: 25.3284, (ReconX: 0.9683, KLD: 0.1345*1.00, LossY: 2.4226*10.00), Time: 1.8s | Val TotalLoss: 24.1479, (ReconX: 0.9698, KLD: 0.1329, LossY: 2.3045)\n",
            "2025-10-23 22:52:13,808 - VAETrainer - INFO - Saving model and trainer state to vae_results/best_model.pth...\n",
            "INFO:VAETrainer:Saving model and trainer state to vae_results/best_model.pth...\n",
            "2025-10-23 22:52:13,821 - VAETrainer - INFO - Successfully saved model and trainer state to vae_results/best_model.pth\n",
            "INFO:VAETrainer:Successfully saved model and trainer state to vae_results/best_model.pth\n",
            "2025-10-23 22:52:15,999 - VAETrainer - INFO - Epoch [49/100] Train TotalLoss: 24.4542, (ReconX: 0.9693, KLD: 0.1295*1.00, LossY: 2.3355*10.00), Time: 1.8s | Val TotalLoss: 23.9645, (ReconX: 0.9712, KLD: 0.1236, LossY: 2.2870)\n",
            "INFO:VAETrainer:Epoch [49/100] Train TotalLoss: 24.4542, (ReconX: 0.9693, KLD: 0.1295*1.00, LossY: 2.3355*10.00), Time: 1.8s | Val TotalLoss: 23.9645, (ReconX: 0.9712, KLD: 0.1236, LossY: 2.2870)\n",
            "2025-10-23 22:52:16,000 - VAETrainer - INFO - Saving model and trainer state to vae_results/best_model.pth...\n",
            "INFO:VAETrainer:Saving model and trainer state to vae_results/best_model.pth...\n",
            "2025-10-23 22:52:16,014 - VAETrainer - INFO - Successfully saved model and trainer state to vae_results/best_model.pth\n",
            "INFO:VAETrainer:Successfully saved model and trainer state to vae_results/best_model.pth\n",
            "2025-10-23 22:52:18,290 - VAETrainer - INFO - Epoch [50/100] Train TotalLoss: 23.7643, (ReconX: 0.9707, KLD: 0.1238*1.00, LossY: 2.2670*10.00), Time: 1.9s | Val TotalLoss: 23.0017, (ReconX: 0.9707, KLD: 0.1213, LossY: 2.1910)\n",
            "INFO:VAETrainer:Epoch [50/100] Train TotalLoss: 23.7643, (ReconX: 0.9707, KLD: 0.1238*1.00, LossY: 2.2670*10.00), Time: 1.9s | Val TotalLoss: 23.0017, (ReconX: 0.9707, KLD: 0.1213, LossY: 2.1910)\n",
            "2025-10-23 22:52:18,292 - VAETrainer - INFO - Saving model and trainer state to vae_results/best_model.pth...\n",
            "INFO:VAETrainer:Saving model and trainer state to vae_results/best_model.pth...\n",
            "2025-10-23 22:52:18,306 - VAETrainer - INFO - Successfully saved model and trainer state to vae_results/best_model.pth\n",
            "INFO:VAETrainer:Successfully saved model and trainer state to vae_results/best_model.pth\n",
            "2025-10-23 22:52:20,422 - VAETrainer - INFO - Epoch [51/100] Train TotalLoss: 23.1395, (ReconX: 0.9716, KLD: 0.1187*1.00, LossY: 2.2049*10.00), Time: 1.7s | Val TotalLoss: 22.4114, (ReconX: 0.9732, KLD: 0.1178, LossY: 2.1320)\n",
            "INFO:VAETrainer:Epoch [51/100] Train TotalLoss: 23.1395, (ReconX: 0.9716, KLD: 0.1187*1.00, LossY: 2.2049*10.00), Time: 1.7s | Val TotalLoss: 22.4114, (ReconX: 0.9732, KLD: 0.1178, LossY: 2.1320)\n",
            "2025-10-23 22:52:20,424 - VAETrainer - INFO - Saving model and trainer state to vae_results/best_model.pth...\n",
            "INFO:VAETrainer:Saving model and trainer state to vae_results/best_model.pth...\n",
            "2025-10-23 22:52:20,437 - VAETrainer - INFO - Successfully saved model and trainer state to vae_results/best_model.pth\n",
            "INFO:VAETrainer:Successfully saved model and trainer state to vae_results/best_model.pth\n",
            "2025-10-23 22:52:22,533 - VAETrainer - INFO - Epoch [52/100] Train TotalLoss: 22.6562, (ReconX: 0.9727, KLD: 0.1135*1.00, LossY: 2.1570*10.00), Time: 1.7s | Val TotalLoss: 21.8427, (ReconX: 0.9758, KLD: 0.1129, LossY: 2.0754)\n",
            "INFO:VAETrainer:Epoch [52/100] Train TotalLoss: 22.6562, (ReconX: 0.9727, KLD: 0.1135*1.00, LossY: 2.1570*10.00), Time: 1.7s | Val TotalLoss: 21.8427, (ReconX: 0.9758, KLD: 0.1129, LossY: 2.0754)\n",
            "2025-10-23 22:52:22,534 - VAETrainer - INFO - Saving model and trainer state to vae_results/best_model.pth...\n",
            "INFO:VAETrainer:Saving model and trainer state to vae_results/best_model.pth...\n",
            "2025-10-23 22:52:22,547 - VAETrainer - INFO - Successfully saved model and trainer state to vae_results/best_model.pth\n",
            "INFO:VAETrainer:Successfully saved model and trainer state to vae_results/best_model.pth\n",
            "2025-10-23 22:52:24,670 - VAETrainer - INFO - Epoch [53/100] Train TotalLoss: 21.9594, (ReconX: 0.9731, KLD: 0.1104*1.00, LossY: 2.0876*10.00), Time: 1.7s | Val TotalLoss: 21.7373, (ReconX: 0.9774, KLD: 0.1093, LossY: 2.0651)\n",
            "INFO:VAETrainer:Epoch [53/100] Train TotalLoss: 21.9594, (ReconX: 0.9731, KLD: 0.1104*1.00, LossY: 2.0876*10.00), Time: 1.7s | Val TotalLoss: 21.7373, (ReconX: 0.9774, KLD: 0.1093, LossY: 2.0651)\n",
            "2025-10-23 22:52:24,671 - VAETrainer - INFO - Saving model and trainer state to vae_results/best_model.pth...\n",
            "INFO:VAETrainer:Saving model and trainer state to vae_results/best_model.pth...\n",
            "2025-10-23 22:52:24,686 - VAETrainer - INFO - Successfully saved model and trainer state to vae_results/best_model.pth\n",
            "INFO:VAETrainer:Successfully saved model and trainer state to vae_results/best_model.pth\n",
            "2025-10-23 22:52:26,894 - VAETrainer - INFO - Epoch [54/100] Train TotalLoss: 21.5495, (ReconX: 0.9746, KLD: 0.1066*1.00, LossY: 2.0468*10.00), Time: 1.8s | Val TotalLoss: 20.7324, (ReconX: 0.9758, KLD: 0.1024, LossY: 1.9654)\n",
            "INFO:VAETrainer:Epoch [54/100] Train TotalLoss: 21.5495, (ReconX: 0.9746, KLD: 0.1066*1.00, LossY: 2.0468*10.00), Time: 1.8s | Val TotalLoss: 20.7324, (ReconX: 0.9758, KLD: 0.1024, LossY: 1.9654)\n",
            "2025-10-23 22:52:26,896 - VAETrainer - INFO - Saving model and trainer state to vae_results/best_model.pth...\n",
            "INFO:VAETrainer:Saving model and trainer state to vae_results/best_model.pth...\n",
            "2025-10-23 22:52:26,909 - VAETrainer - INFO - Successfully saved model and trainer state to vae_results/best_model.pth\n",
            "INFO:VAETrainer:Successfully saved model and trainer state to vae_results/best_model.pth\n",
            "2025-10-23 22:52:28,988 - VAETrainer - INFO - Epoch [55/100] Train TotalLoss: 21.0796, (ReconX: 0.9749, KLD: 0.1037*1.00, LossY: 2.0001*10.00), Time: 1.7s | Val TotalLoss: 20.1516, (ReconX: 0.9772, KLD: 0.1038, LossY: 1.9071)\n",
            "INFO:VAETrainer:Epoch [55/100] Train TotalLoss: 21.0796, (ReconX: 0.9749, KLD: 0.1037*1.00, LossY: 2.0001*10.00), Time: 1.7s | Val TotalLoss: 20.1516, (ReconX: 0.9772, KLD: 0.1038, LossY: 1.9071)\n",
            "2025-10-23 22:52:28,990 - VAETrainer - INFO - Saving model and trainer state to vae_results/best_model.pth...\n",
            "INFO:VAETrainer:Saving model and trainer state to vae_results/best_model.pth...\n",
            "2025-10-23 22:52:29,003 - VAETrainer - INFO - Successfully saved model and trainer state to vae_results/best_model.pth\n",
            "INFO:VAETrainer:Successfully saved model and trainer state to vae_results/best_model.pth\n",
            "2025-10-23 22:52:31,142 - VAETrainer - INFO - Epoch [56/100] Train TotalLoss: 20.3670, (ReconX: 0.9762, KLD: 0.0991*1.00, LossY: 1.9292*10.00), Time: 1.8s | Val TotalLoss: 19.4622, (ReconX: 0.9770, KLD: 0.0973, LossY: 1.8388)\n",
            "INFO:VAETrainer:Epoch [56/100] Train TotalLoss: 20.3670, (ReconX: 0.9762, KLD: 0.0991*1.00, LossY: 1.9292*10.00), Time: 1.8s | Val TotalLoss: 19.4622, (ReconX: 0.9770, KLD: 0.0973, LossY: 1.8388)\n",
            "2025-10-23 22:52:31,143 - VAETrainer - INFO - Saving model and trainer state to vae_results/best_model.pth...\n",
            "INFO:VAETrainer:Saving model and trainer state to vae_results/best_model.pth...\n",
            "2025-10-23 22:52:31,156 - VAETrainer - INFO - Successfully saved model and trainer state to vae_results/best_model.pth\n",
            "INFO:VAETrainer:Successfully saved model and trainer state to vae_results/best_model.pth\n",
            "2025-10-23 22:52:33,256 - VAETrainer - INFO - Epoch [57/100] Train TotalLoss: 20.1346, (ReconX: 0.9758, KLD: 0.0975*1.00, LossY: 1.9061*10.00), Time: 1.7s | Val TotalLoss: 19.1835, (ReconX: 0.9769, KLD: 0.0970, LossY: 1.8110)\n",
            "INFO:VAETrainer:Epoch [57/100] Train TotalLoss: 20.1346, (ReconX: 0.9758, KLD: 0.0975*1.00, LossY: 1.9061*10.00), Time: 1.7s | Val TotalLoss: 19.1835, (ReconX: 0.9769, KLD: 0.0970, LossY: 1.8110)\n",
            "2025-10-23 22:52:33,258 - VAETrainer - INFO - Saving model and trainer state to vae_results/best_model.pth...\n",
            "INFO:VAETrainer:Saving model and trainer state to vae_results/best_model.pth...\n",
            "2025-10-23 22:52:33,271 - VAETrainer - INFO - Successfully saved model and trainer state to vae_results/best_model.pth\n",
            "INFO:VAETrainer:Successfully saved model and trainer state to vae_results/best_model.pth\n",
            "2025-10-23 22:52:35,472 - VAETrainer - INFO - Epoch [58/100] Train TotalLoss: 19.5095, (ReconX: 0.9764, KLD: 0.0968*1.00, LossY: 1.8436*10.00), Time: 1.8s | Val TotalLoss: 18.2259, (ReconX: 0.9772, KLD: 0.0988, LossY: 1.7150)\n",
            "INFO:VAETrainer:Epoch [58/100] Train TotalLoss: 19.5095, (ReconX: 0.9764, KLD: 0.0968*1.00, LossY: 1.8436*10.00), Time: 1.8s | Val TotalLoss: 18.2259, (ReconX: 0.9772, KLD: 0.0988, LossY: 1.7150)\n",
            "2025-10-23 22:52:35,474 - VAETrainer - INFO - Saving model and trainer state to vae_results/best_model.pth...\n",
            "INFO:VAETrainer:Saving model and trainer state to vae_results/best_model.pth...\n",
            "2025-10-23 22:52:35,488 - VAETrainer - INFO - Successfully saved model and trainer state to vae_results/best_model.pth\n",
            "INFO:VAETrainer:Successfully saved model and trainer state to vae_results/best_model.pth\n",
            "2025-10-23 22:52:37,695 - VAETrainer - INFO - Epoch [59/100] Train TotalLoss: 19.0976, (ReconX: 0.9766, KLD: 0.0964*1.00, LossY: 1.8025*10.00), Time: 1.8s | Val TotalLoss: 17.9103, (ReconX: 0.9759, KLD: 0.0996, LossY: 1.6835)\n",
            "INFO:VAETrainer:Epoch [59/100] Train TotalLoss: 19.0976, (ReconX: 0.9766, KLD: 0.0964*1.00, LossY: 1.8025*10.00), Time: 1.8s | Val TotalLoss: 17.9103, (ReconX: 0.9759, KLD: 0.0996, LossY: 1.6835)\n",
            "2025-10-23 22:52:37,697 - VAETrainer - INFO - Saving model and trainer state to vae_results/best_model.pth...\n",
            "INFO:VAETrainer:Saving model and trainer state to vae_results/best_model.pth...\n",
            "2025-10-23 22:52:37,711 - VAETrainer - INFO - Successfully saved model and trainer state to vae_results/best_model.pth\n",
            "INFO:VAETrainer:Successfully saved model and trainer state to vae_results/best_model.pth\n",
            "2025-10-23 22:52:39,930 - VAETrainer - INFO - Epoch [60/100] Train TotalLoss: 18.4610, (ReconX: 0.9772, KLD: 0.0966*1.00, LossY: 1.7387*10.00), Time: 1.8s | Val TotalLoss: 19.0131, (ReconX: 0.9774, KLD: 0.0971, LossY: 1.7939)\n",
            "INFO:VAETrainer:Epoch [60/100] Train TotalLoss: 18.4610, (ReconX: 0.9772, KLD: 0.0966*1.00, LossY: 1.7387*10.00), Time: 1.8s | Val TotalLoss: 19.0131, (ReconX: 0.9774, KLD: 0.0971, LossY: 1.7939)\n",
            "2025-10-23 22:52:42,053 - VAETrainer - INFO - Epoch [61/100] Train TotalLoss: 18.0423, (ReconX: 0.9769, KLD: 0.0973*1.00, LossY: 1.6968*10.00), Time: 1.7s | Val TotalLoss: 18.2248, (ReconX: 0.9774, KLD: 0.0978, LossY: 1.7150)\n",
            "INFO:VAETrainer:Epoch [61/100] Train TotalLoss: 18.0423, (ReconX: 0.9769, KLD: 0.0973*1.00, LossY: 1.6968*10.00), Time: 1.7s | Val TotalLoss: 18.2248, (ReconX: 0.9774, KLD: 0.0978, LossY: 1.7150)\n",
            "2025-10-23 22:52:44,187 - VAETrainer - INFO - Epoch [62/100] Train TotalLoss: 17.7083, (ReconX: 0.9763, KLD: 0.0980*1.00, LossY: 1.6634*10.00), Time: 1.8s | Val TotalLoss: 17.0090, (ReconX: 0.9785, KLD: 0.0967, LossY: 1.5934)\n",
            "INFO:VAETrainer:Epoch [62/100] Train TotalLoss: 17.7083, (ReconX: 0.9763, KLD: 0.0980*1.00, LossY: 1.6634*10.00), Time: 1.8s | Val TotalLoss: 17.0090, (ReconX: 0.9785, KLD: 0.0967, LossY: 1.5934)\n",
            "2025-10-23 22:52:44,189 - VAETrainer - INFO - Saving model and trainer state to vae_results/best_model.pth...\n",
            "INFO:VAETrainer:Saving model and trainer state to vae_results/best_model.pth...\n",
            "2025-10-23 22:52:44,205 - VAETrainer - INFO - Successfully saved model and trainer state to vae_results/best_model.pth\n",
            "INFO:VAETrainer:Successfully saved model and trainer state to vae_results/best_model.pth\n",
            "2025-10-23 22:52:46,380 - VAETrainer - INFO - Epoch [63/100] Train TotalLoss: 17.5187, (ReconX: 0.9767, KLD: 0.0972*1.00, LossY: 1.6445*10.00), Time: 1.8s | Val TotalLoss: 16.9799, (ReconX: 0.9776, KLD: 0.0979, LossY: 1.5904)\n",
            "INFO:VAETrainer:Epoch [63/100] Train TotalLoss: 17.5187, (ReconX: 0.9767, KLD: 0.0972*1.00, LossY: 1.6445*10.00), Time: 1.8s | Val TotalLoss: 16.9799, (ReconX: 0.9776, KLD: 0.0979, LossY: 1.5904)\n",
            "2025-10-23 22:52:46,382 - VAETrainer - INFO - Saving model and trainer state to vae_results/best_model.pth...\n",
            "INFO:VAETrainer:Saving model and trainer state to vae_results/best_model.pth...\n",
            "2025-10-23 22:52:46,395 - VAETrainer - INFO - Successfully saved model and trainer state to vae_results/best_model.pth\n",
            "INFO:VAETrainer:Successfully saved model and trainer state to vae_results/best_model.pth\n",
            "2025-10-23 22:52:48,527 - VAETrainer - INFO - Epoch [64/100] Train TotalLoss: 17.1515, (ReconX: 0.9761, KLD: 0.0984*1.00, LossY: 1.6077*10.00), Time: 1.8s | Val TotalLoss: 16.3614, (ReconX: 0.9775, KLD: 0.0973, LossY: 1.5287)\n",
            "INFO:VAETrainer:Epoch [64/100] Train TotalLoss: 17.1515, (ReconX: 0.9761, KLD: 0.0984*1.00, LossY: 1.6077*10.00), Time: 1.8s | Val TotalLoss: 16.3614, (ReconX: 0.9775, KLD: 0.0973, LossY: 1.5287)\n",
            "2025-10-23 22:52:48,529 - VAETrainer - INFO - Saving model and trainer state to vae_results/best_model.pth...\n",
            "INFO:VAETrainer:Saving model and trainer state to vae_results/best_model.pth...\n",
            "2025-10-23 22:52:48,546 - VAETrainer - INFO - Successfully saved model and trainer state to vae_results/best_model.pth\n",
            "INFO:VAETrainer:Successfully saved model and trainer state to vae_results/best_model.pth\n",
            "2025-10-23 22:52:50,744 - VAETrainer - INFO - Epoch [65/100] Train TotalLoss: 16.9340, (ReconX: 0.9767, KLD: 0.0972*1.00, LossY: 1.5860*10.00), Time: 1.8s | Val TotalLoss: 16.4403, (ReconX: 0.9767, KLD: 0.0965, LossY: 1.5367)\n",
            "INFO:VAETrainer:Epoch [65/100] Train TotalLoss: 16.9340, (ReconX: 0.9767, KLD: 0.0972*1.00, LossY: 1.5860*10.00), Time: 1.8s | Val TotalLoss: 16.4403, (ReconX: 0.9767, KLD: 0.0965, LossY: 1.5367)\n",
            "2025-10-23 22:52:52,786 - VAETrainer - INFO - Epoch [66/100] Train TotalLoss: 16.5520, (ReconX: 0.9766, KLD: 0.0958*1.00, LossY: 1.5480*10.00), Time: 1.7s | Val TotalLoss: 16.4400, (ReconX: 0.9804, KLD: 0.0937, LossY: 1.5366)\n",
            "INFO:VAETrainer:Epoch [66/100] Train TotalLoss: 16.5520, (ReconX: 0.9766, KLD: 0.0958*1.00, LossY: 1.5480*10.00), Time: 1.7s | Val TotalLoss: 16.4400, (ReconX: 0.9804, KLD: 0.0937, LossY: 1.5366)\n",
            "2025-10-23 22:52:54,991 - VAETrainer - INFO - Epoch [67/100] Train TotalLoss: 16.4544, (ReconX: 0.9774, KLD: 0.0958*1.00, LossY: 1.5381*10.00), Time: 1.8s | Val TotalLoss: 15.9364, (ReconX: 0.9786, KLD: 0.0950, LossY: 1.4863)\n",
            "INFO:VAETrainer:Epoch [67/100] Train TotalLoss: 16.4544, (ReconX: 0.9774, KLD: 0.0958*1.00, LossY: 1.5381*10.00), Time: 1.8s | Val TotalLoss: 15.9364, (ReconX: 0.9786, KLD: 0.0950, LossY: 1.4863)\n",
            "2025-10-23 22:52:54,993 - VAETrainer - INFO - Saving model and trainer state to vae_results/best_model.pth...\n",
            "INFO:VAETrainer:Saving model and trainer state to vae_results/best_model.pth...\n",
            "2025-10-23 22:52:55,007 - VAETrainer - INFO - Successfully saved model and trainer state to vae_results/best_model.pth\n",
            "INFO:VAETrainer:Successfully saved model and trainer state to vae_results/best_model.pth\n",
            "2025-10-23 22:52:57,185 - VAETrainer - INFO - Epoch [68/100] Train TotalLoss: 16.3967, (ReconX: 0.9777, KLD: 0.0951*1.00, LossY: 1.5324*10.00), Time: 1.8s | Val TotalLoss: 15.9904, (ReconX: 0.9785, KLD: 0.0946, LossY: 1.4917)\n",
            "INFO:VAETrainer:Epoch [68/100] Train TotalLoss: 16.3967, (ReconX: 0.9777, KLD: 0.0951*1.00, LossY: 1.5324*10.00), Time: 1.8s | Val TotalLoss: 15.9904, (ReconX: 0.9785, KLD: 0.0946, LossY: 1.4917)\n",
            "2025-10-23 22:52:59,389 - VAETrainer - INFO - Epoch [69/100] Train TotalLoss: 16.2600, (ReconX: 0.9772, KLD: 0.0943*1.00, LossY: 1.5188*10.00), Time: 1.8s | Val TotalLoss: 15.5220, (ReconX: 0.9792, KLD: 0.0945, LossY: 1.4448)\n",
            "INFO:VAETrainer:Epoch [69/100] Train TotalLoss: 16.2600, (ReconX: 0.9772, KLD: 0.0943*1.00, LossY: 1.5188*10.00), Time: 1.8s | Val TotalLoss: 15.5220, (ReconX: 0.9792, KLD: 0.0945, LossY: 1.4448)\n",
            "2025-10-23 22:52:59,391 - VAETrainer - INFO - Saving model and trainer state to vae_results/best_model.pth...\n",
            "INFO:VAETrainer:Saving model and trainer state to vae_results/best_model.pth...\n",
            "2025-10-23 22:52:59,404 - VAETrainer - INFO - Successfully saved model and trainer state to vae_results/best_model.pth\n",
            "INFO:VAETrainer:Successfully saved model and trainer state to vae_results/best_model.pth\n",
            "2025-10-23 22:53:01,532 - VAETrainer - INFO - Epoch [70/100] Train TotalLoss: 16.2845, (ReconX: 0.9777, KLD: 0.0945*1.00, LossY: 1.5212*10.00), Time: 1.7s | Val TotalLoss: 15.6489, (ReconX: 0.9794, KLD: 0.0932, LossY: 1.4576)\n",
            "INFO:VAETrainer:Epoch [70/100] Train TotalLoss: 16.2845, (ReconX: 0.9777, KLD: 0.0945*1.00, LossY: 1.5212*10.00), Time: 1.7s | Val TotalLoss: 15.6489, (ReconX: 0.9794, KLD: 0.0932, LossY: 1.4576)\n",
            "2025-10-23 22:53:03,701 - VAETrainer - INFO - Epoch [71/100] Train TotalLoss: 16.1316, (ReconX: 0.9776, KLD: 0.0957*1.00, LossY: 1.5058*10.00), Time: 1.8s | Val TotalLoss: 15.4029, (ReconX: 0.9794, KLD: 0.0968, LossY: 1.4327)\n",
            "INFO:VAETrainer:Epoch [71/100] Train TotalLoss: 16.1316, (ReconX: 0.9776, KLD: 0.0957*1.00, LossY: 1.5058*10.00), Time: 1.8s | Val TotalLoss: 15.4029, (ReconX: 0.9794, KLD: 0.0968, LossY: 1.4327)\n",
            "2025-10-23 22:53:03,703 - VAETrainer - INFO - Saving model and trainer state to vae_results/best_model.pth...\n",
            "INFO:VAETrainer:Saving model and trainer state to vae_results/best_model.pth...\n",
            "2025-10-23 22:53:03,717 - VAETrainer - INFO - Successfully saved model and trainer state to vae_results/best_model.pth\n",
            "INFO:VAETrainer:Successfully saved model and trainer state to vae_results/best_model.pth\n",
            "2025-10-23 22:53:05,889 - VAETrainer - INFO - Epoch [72/100] Train TotalLoss: 15.9173, (ReconX: 0.9778, KLD: 0.0942*1.00, LossY: 1.4845*10.00), Time: 1.8s | Val TotalLoss: 15.3111, (ReconX: 0.9802, KLD: 0.0937, LossY: 1.4237)\n",
            "INFO:VAETrainer:Epoch [72/100] Train TotalLoss: 15.9173, (ReconX: 0.9778, KLD: 0.0942*1.00, LossY: 1.4845*10.00), Time: 1.8s | Val TotalLoss: 15.3111, (ReconX: 0.9802, KLD: 0.0937, LossY: 1.4237)\n",
            "2025-10-23 22:53:05,891 - VAETrainer - INFO - Saving model and trainer state to vae_results/best_model.pth...\n",
            "INFO:VAETrainer:Saving model and trainer state to vae_results/best_model.pth...\n",
            "2025-10-23 22:53:05,909 - VAETrainer - INFO - Successfully saved model and trainer state to vae_results/best_model.pth\n",
            "INFO:VAETrainer:Successfully saved model and trainer state to vae_results/best_model.pth\n",
            "2025-10-23 22:53:08,002 - VAETrainer - INFO - Epoch [73/100] Train TotalLoss: 15.7684, (ReconX: 0.9785, KLD: 0.0938*1.00, LossY: 1.4696*10.00), Time: 1.7s | Val TotalLoss: 15.1407, (ReconX: 0.9782, KLD: 0.0924, LossY: 1.4070)\n",
            "INFO:VAETrainer:Epoch [73/100] Train TotalLoss: 15.7684, (ReconX: 0.9785, KLD: 0.0938*1.00, LossY: 1.4696*10.00), Time: 1.7s | Val TotalLoss: 15.1407, (ReconX: 0.9782, KLD: 0.0924, LossY: 1.4070)\n",
            "2025-10-23 22:53:08,003 - VAETrainer - INFO - Saving model and trainer state to vae_results/best_model.pth...\n",
            "INFO:VAETrainer:Saving model and trainer state to vae_results/best_model.pth...\n",
            "2025-10-23 22:53:08,017 - VAETrainer - INFO - Successfully saved model and trainer state to vae_results/best_model.pth\n",
            "INFO:VAETrainer:Successfully saved model and trainer state to vae_results/best_model.pth\n",
            "2025-10-23 22:53:10,124 - VAETrainer - INFO - Epoch [74/100] Train TotalLoss: 15.9112, (ReconX: 0.9782, KLD: 0.0936*1.00, LossY: 1.4839*10.00), Time: 1.7s | Val TotalLoss: 14.9133, (ReconX: 0.9783, KLD: 0.0963, LossY: 1.3839)\n",
            "INFO:VAETrainer:Epoch [74/100] Train TotalLoss: 15.9112, (ReconX: 0.9782, KLD: 0.0936*1.00, LossY: 1.4839*10.00), Time: 1.7s | Val TotalLoss: 14.9133, (ReconX: 0.9783, KLD: 0.0963, LossY: 1.3839)\n",
            "2025-10-23 22:53:10,126 - VAETrainer - INFO - Saving model and trainer state to vae_results/best_model.pth...\n",
            "INFO:VAETrainer:Saving model and trainer state to vae_results/best_model.pth...\n",
            "2025-10-23 22:53:10,140 - VAETrainer - INFO - Successfully saved model and trainer state to vae_results/best_model.pth\n",
            "INFO:VAETrainer:Successfully saved model and trainer state to vae_results/best_model.pth\n",
            "2025-10-23 22:53:12,201 - VAETrainer - INFO - Epoch [75/100] Train TotalLoss: 15.9651, (ReconX: 0.9779, KLD: 0.0955*1.00, LossY: 1.4892*10.00), Time: 1.7s | Val TotalLoss: 15.8882, (ReconX: 0.9786, KLD: 0.0941, LossY: 1.4816)\n",
            "INFO:VAETrainer:Epoch [75/100] Train TotalLoss: 15.9651, (ReconX: 0.9779, KLD: 0.0955*1.00, LossY: 1.4892*10.00), Time: 1.7s | Val TotalLoss: 15.8882, (ReconX: 0.9786, KLD: 0.0941, LossY: 1.4816)\n",
            "2025-10-23 22:53:14,368 - VAETrainer - INFO - Epoch [76/100] Train TotalLoss: 15.7069, (ReconX: 0.9787, KLD: 0.0930*1.00, LossY: 1.4635*10.00), Time: 1.8s | Val TotalLoss: 15.0908, (ReconX: 0.9793, KLD: 0.0914, LossY: 1.4020)\n",
            "INFO:VAETrainer:Epoch [76/100] Train TotalLoss: 15.7069, (ReconX: 0.9787, KLD: 0.0930*1.00, LossY: 1.4635*10.00), Time: 1.8s | Val TotalLoss: 15.0908, (ReconX: 0.9793, KLD: 0.0914, LossY: 1.4020)\n",
            "2025-10-23 22:53:16,533 - VAETrainer - INFO - Epoch [77/100] Train TotalLoss: 15.7349, (ReconX: 0.9783, KLD: 0.0929*1.00, LossY: 1.4664*10.00), Time: 1.8s | Val TotalLoss: 14.7738, (ReconX: 0.9795, KLD: 0.0927, LossY: 1.3702)\n",
            "INFO:VAETrainer:Epoch [77/100] Train TotalLoss: 15.7349, (ReconX: 0.9783, KLD: 0.0929*1.00, LossY: 1.4664*10.00), Time: 1.8s | Val TotalLoss: 14.7738, (ReconX: 0.9795, KLD: 0.0927, LossY: 1.3702)\n",
            "2025-10-23 22:53:16,535 - VAETrainer - INFO - Saving model and trainer state to vae_results/best_model.pth...\n",
            "INFO:VAETrainer:Saving model and trainer state to vae_results/best_model.pth...\n",
            "2025-10-23 22:53:16,548 - VAETrainer - INFO - Successfully saved model and trainer state to vae_results/best_model.pth\n",
            "INFO:VAETrainer:Successfully saved model and trainer state to vae_results/best_model.pth\n",
            "2025-10-23 22:53:18,795 - VAETrainer - INFO - Epoch [78/100] Train TotalLoss: 15.5706, (ReconX: 0.9780, KLD: 0.0928*1.00, LossY: 1.4500*10.00), Time: 1.9s | Val TotalLoss: 15.0792, (ReconX: 0.9793, KLD: 0.0915, LossY: 1.4008)\n",
            "INFO:VAETrainer:Epoch [78/100] Train TotalLoss: 15.5706, (ReconX: 0.9780, KLD: 0.0928*1.00, LossY: 1.4500*10.00), Time: 1.9s | Val TotalLoss: 15.0792, (ReconX: 0.9793, KLD: 0.0915, LossY: 1.4008)\n",
            "2025-10-23 22:53:20,942 - VAETrainer - INFO - Epoch [79/100] Train TotalLoss: 15.4495, (ReconX: 0.9790, KLD: 0.0918*1.00, LossY: 1.4379*10.00), Time: 1.8s | Val TotalLoss: 15.4672, (ReconX: 0.9797, KLD: 0.0911, LossY: 1.4396)\n",
            "INFO:VAETrainer:Epoch [79/100] Train TotalLoss: 15.4495, (ReconX: 0.9790, KLD: 0.0918*1.00, LossY: 1.4379*10.00), Time: 1.8s | Val TotalLoss: 15.4672, (ReconX: 0.9797, KLD: 0.0911, LossY: 1.4396)\n",
            "2025-10-23 22:53:23,110 - VAETrainer - INFO - Epoch [80/100] Train TotalLoss: 15.4007, (ReconX: 0.9793, KLD: 0.0906*1.00, LossY: 1.4331*10.00), Time: 1.8s | Val TotalLoss: 14.5972, (ReconX: 0.9798, KLD: 0.0899, LossY: 1.3527)\n",
            "INFO:VAETrainer:Epoch [80/100] Train TotalLoss: 15.4007, (ReconX: 0.9793, KLD: 0.0906*1.00, LossY: 1.4331*10.00), Time: 1.8s | Val TotalLoss: 14.5972, (ReconX: 0.9798, KLD: 0.0899, LossY: 1.3527)\n",
            "2025-10-23 22:53:23,112 - VAETrainer - INFO - Saving model and trainer state to vae_results/best_model.pth...\n",
            "INFO:VAETrainer:Saving model and trainer state to vae_results/best_model.pth...\n",
            "2025-10-23 22:53:23,125 - VAETrainer - INFO - Successfully saved model and trainer state to vae_results/best_model.pth\n",
            "INFO:VAETrainer:Successfully saved model and trainer state to vae_results/best_model.pth\n",
            "2025-10-23 22:53:25,306 - VAETrainer - INFO - Epoch [81/100] Train TotalLoss: 15.4308, (ReconX: 0.9789, KLD: 0.0911*1.00, LossY: 1.4361*10.00), Time: 1.8s | Val TotalLoss: 14.6761, (ReconX: 0.9812, KLD: 0.0894, LossY: 1.3606)\n",
            "INFO:VAETrainer:Epoch [81/100] Train TotalLoss: 15.4308, (ReconX: 0.9789, KLD: 0.0911*1.00, LossY: 1.4361*10.00), Time: 1.8s | Val TotalLoss: 14.6761, (ReconX: 0.9812, KLD: 0.0894, LossY: 1.3606)\n",
            "2025-10-23 22:53:27,483 - VAETrainer - INFO - Epoch [82/100] Train TotalLoss: 15.4406, (ReconX: 0.9782, KLD: 0.0917*1.00, LossY: 1.4371*10.00), Time: 1.8s | Val TotalLoss: 14.7004, (ReconX: 0.9801, KLD: 0.0902, LossY: 1.3630)\n",
            "INFO:VAETrainer:Epoch [82/100] Train TotalLoss: 15.4406, (ReconX: 0.9782, KLD: 0.0917*1.00, LossY: 1.4371*10.00), Time: 1.8s | Val TotalLoss: 14.7004, (ReconX: 0.9801, KLD: 0.0902, LossY: 1.3630)\n",
            "2025-10-23 22:53:29,582 - VAETrainer - INFO - Epoch [83/100] Train TotalLoss: 15.2714, (ReconX: 0.9789, KLD: 0.0903*1.00, LossY: 1.4202*10.00), Time: 1.7s | Val TotalLoss: 14.5611, (ReconX: 0.9826, KLD: 0.0892, LossY: 1.3489)\n",
            "INFO:VAETrainer:Epoch [83/100] Train TotalLoss: 15.2714, (ReconX: 0.9789, KLD: 0.0903*1.00, LossY: 1.4202*10.00), Time: 1.7s | Val TotalLoss: 14.5611, (ReconX: 0.9826, KLD: 0.0892, LossY: 1.3489)\n",
            "2025-10-23 22:53:29,584 - VAETrainer - INFO - Saving model and trainer state to vae_results/best_model.pth...\n",
            "INFO:VAETrainer:Saving model and trainer state to vae_results/best_model.pth...\n",
            "2025-10-23 22:53:29,597 - VAETrainer - INFO - Successfully saved model and trainer state to vae_results/best_model.pth\n",
            "INFO:VAETrainer:Successfully saved model and trainer state to vae_results/best_model.pth\n",
            "2025-10-23 22:53:31,797 - VAETrainer - INFO - Epoch [84/100] Train TotalLoss: 15.4286, (ReconX: 0.9795, KLD: 0.0891*1.00, LossY: 1.4360*10.00), Time: 1.8s | Val TotalLoss: 14.4943, (ReconX: 0.9810, KLD: 0.0895, LossY: 1.3424)\n",
            "INFO:VAETrainer:Epoch [84/100] Train TotalLoss: 15.4286, (ReconX: 0.9795, KLD: 0.0891*1.00, LossY: 1.4360*10.00), Time: 1.8s | Val TotalLoss: 14.4943, (ReconX: 0.9810, KLD: 0.0895, LossY: 1.3424)\n",
            "2025-10-23 22:53:31,799 - VAETrainer - INFO - Saving model and trainer state to vae_results/best_model.pth...\n",
            "INFO:VAETrainer:Saving model and trainer state to vae_results/best_model.pth...\n",
            "2025-10-23 22:53:31,812 - VAETrainer - INFO - Successfully saved model and trainer state to vae_results/best_model.pth\n",
            "INFO:VAETrainer:Successfully saved model and trainer state to vae_results/best_model.pth\n",
            "2025-10-23 22:53:34,097 - VAETrainer - INFO - Epoch [85/100] Train TotalLoss: 15.4702, (ReconX: 0.9793, KLD: 0.0899*1.00, LossY: 1.4401*10.00), Time: 1.9s | Val TotalLoss: 14.5217, (ReconX: 0.9785, KLD: 0.0894, LossY: 1.3454)\n",
            "INFO:VAETrainer:Epoch [85/100] Train TotalLoss: 15.4702, (ReconX: 0.9793, KLD: 0.0899*1.00, LossY: 1.4401*10.00), Time: 1.9s | Val TotalLoss: 14.5217, (ReconX: 0.9785, KLD: 0.0894, LossY: 1.3454)\n",
            "2025-10-23 22:53:36,192 - VAETrainer - INFO - Epoch [86/100] Train TotalLoss: 15.4777, (ReconX: 0.9789, KLD: 0.0893*1.00, LossY: 1.4409*10.00), Time: 1.7s | Val TotalLoss: 14.5140, (ReconX: 0.9792, KLD: 0.0906, LossY: 1.3444)\n",
            "INFO:VAETrainer:Epoch [86/100] Train TotalLoss: 15.4777, (ReconX: 0.9789, KLD: 0.0893*1.00, LossY: 1.4409*10.00), Time: 1.7s | Val TotalLoss: 14.5140, (ReconX: 0.9792, KLD: 0.0906, LossY: 1.3444)\n",
            "2025-10-23 22:53:38,424 - VAETrainer - INFO - Epoch [87/100] Train TotalLoss: 15.1889, (ReconX: 0.9796, KLD: 0.0888*1.00, LossY: 1.4121*10.00), Time: 1.8s | Val TotalLoss: 14.4333, (ReconX: 0.9800, KLD: 0.0870, LossY: 1.3366)\n",
            "INFO:VAETrainer:Epoch [87/100] Train TotalLoss: 15.1889, (ReconX: 0.9796, KLD: 0.0888*1.00, LossY: 1.4121*10.00), Time: 1.8s | Val TotalLoss: 14.4333, (ReconX: 0.9800, KLD: 0.0870, LossY: 1.3366)\n",
            "2025-10-23 22:53:38,426 - VAETrainer - INFO - Saving model and trainer state to vae_results/best_model.pth...\n",
            "INFO:VAETrainer:Saving model and trainer state to vae_results/best_model.pth...\n",
            "2025-10-23 22:53:38,440 - VAETrainer - INFO - Successfully saved model and trainer state to vae_results/best_model.pth\n",
            "INFO:VAETrainer:Successfully saved model and trainer state to vae_results/best_model.pth\n",
            "2025-10-23 22:53:40,629 - VAETrainer - INFO - Epoch [88/100] Train TotalLoss: 15.3751, (ReconX: 0.9799, KLD: 0.0883*1.00, LossY: 1.4307*10.00), Time: 1.8s | Val TotalLoss: 14.5647, (ReconX: 0.9814, KLD: 0.0868, LossY: 1.3497)\n",
            "INFO:VAETrainer:Epoch [88/100] Train TotalLoss: 15.3751, (ReconX: 0.9799, KLD: 0.0883*1.00, LossY: 1.4307*10.00), Time: 1.8s | Val TotalLoss: 14.5647, (ReconX: 0.9814, KLD: 0.0868, LossY: 1.3497)\n",
            "2025-10-23 22:53:42,780 - VAETrainer - INFO - Epoch [89/100] Train TotalLoss: 15.1551, (ReconX: 0.9799, KLD: 0.0874*1.00, LossY: 1.4088*10.00), Time: 1.8s | Val TotalLoss: 14.4521, (ReconX: 0.9811, KLD: 0.0897, LossY: 1.3381)\n",
            "INFO:VAETrainer:Epoch [89/100] Train TotalLoss: 15.1551, (ReconX: 0.9799, KLD: 0.0874*1.00, LossY: 1.4088*10.00), Time: 1.8s | Val TotalLoss: 14.4521, (ReconX: 0.9811, KLD: 0.0897, LossY: 1.3381)\n",
            "2025-10-23 22:53:44,857 - VAETrainer - INFO - Epoch [90/100] Train TotalLoss: 14.9069, (ReconX: 0.9803, KLD: 0.0868*1.00, LossY: 1.3840*10.00), Time: 1.7s | Val TotalLoss: 14.4797, (ReconX: 0.9817, KLD: 0.0865, LossY: 1.3411)\n",
            "INFO:VAETrainer:Epoch [90/100] Train TotalLoss: 14.9069, (ReconX: 0.9803, KLD: 0.0868*1.00, LossY: 1.3840*10.00), Time: 1.7s | Val TotalLoss: 14.4797, (ReconX: 0.9817, KLD: 0.0865, LossY: 1.3411)\n",
            "2025-10-23 22:53:47,074 - VAETrainer - INFO - Epoch [91/100] Train TotalLoss: 15.0330, (ReconX: 0.9790, KLD: 0.0888*1.00, LossY: 1.3965*10.00), Time: 1.8s | Val TotalLoss: 14.3549, (ReconX: 0.9831, KLD: 0.0878, LossY: 1.3284)\n",
            "INFO:VAETrainer:Epoch [91/100] Train TotalLoss: 15.0330, (ReconX: 0.9790, KLD: 0.0888*1.00, LossY: 1.3965*10.00), Time: 1.8s | Val TotalLoss: 14.3549, (ReconX: 0.9831, KLD: 0.0878, LossY: 1.3284)\n",
            "2025-10-23 22:53:47,075 - VAETrainer - INFO - Saving model and trainer state to vae_results/best_model.pth...\n",
            "INFO:VAETrainer:Saving model and trainer state to vae_results/best_model.pth...\n",
            "2025-10-23 22:53:47,090 - VAETrainer - INFO - Successfully saved model and trainer state to vae_results/best_model.pth\n",
            "INFO:VAETrainer:Successfully saved model and trainer state to vae_results/best_model.pth\n",
            "2025-10-23 22:53:49,246 - VAETrainer - INFO - Epoch [92/100] Train TotalLoss: 15.0847, (ReconX: 0.9801, KLD: 0.0869*1.00, LossY: 1.4018*10.00), Time: 1.8s | Val TotalLoss: 14.8255, (ReconX: 0.9813, KLD: 0.0882, LossY: 1.3756)\n",
            "INFO:VAETrainer:Epoch [92/100] Train TotalLoss: 15.0847, (ReconX: 0.9801, KLD: 0.0869*1.00, LossY: 1.4018*10.00), Time: 1.8s | Val TotalLoss: 14.8255, (ReconX: 0.9813, KLD: 0.0882, LossY: 1.3756)\n",
            "2025-10-23 22:53:51,482 - VAETrainer - INFO - Epoch [93/100] Train TotalLoss: 14.9012, (ReconX: 0.9799, KLD: 0.0870*1.00, LossY: 1.3834*10.00), Time: 1.9s | Val TotalLoss: 14.1657, (ReconX: 0.9811, KLD: 0.0864, LossY: 1.3098)\n",
            "INFO:VAETrainer:Epoch [93/100] Train TotalLoss: 14.9012, (ReconX: 0.9799, KLD: 0.0870*1.00, LossY: 1.3834*10.00), Time: 1.9s | Val TotalLoss: 14.1657, (ReconX: 0.9811, KLD: 0.0864, LossY: 1.3098)\n",
            "2025-10-23 22:53:51,485 - VAETrainer - INFO - Saving model and trainer state to vae_results/best_model.pth...\n",
            "INFO:VAETrainer:Saving model and trainer state to vae_results/best_model.pth...\n",
            "2025-10-23 22:53:51,498 - VAETrainer - INFO - Successfully saved model and trainer state to vae_results/best_model.pth\n",
            "INFO:VAETrainer:Successfully saved model and trainer state to vae_results/best_model.pth\n",
            "2025-10-23 22:53:53,702 - VAETrainer - INFO - Epoch [94/100] Train TotalLoss: 14.9158, (ReconX: 0.9800, KLD: 0.0869*1.00, LossY: 1.3849*10.00), Time: 1.8s | Val TotalLoss: 14.8697, (ReconX: 0.9808, KLD: 0.0841, LossY: 1.3805)\n",
            "INFO:VAETrainer:Epoch [94/100] Train TotalLoss: 14.9158, (ReconX: 0.9800, KLD: 0.0869*1.00, LossY: 1.3849*10.00), Time: 1.8s | Val TotalLoss: 14.8697, (ReconX: 0.9808, KLD: 0.0841, LossY: 1.3805)\n",
            "2025-10-23 22:53:55,868 - VAETrainer - INFO - Epoch [95/100] Train TotalLoss: 14.8659, (ReconX: 0.9806, KLD: 0.0853*1.00, LossY: 1.3800*10.00), Time: 1.8s | Val TotalLoss: 14.1126, (ReconX: 0.9811, KLD: 0.0849, LossY: 1.3047)\n",
            "INFO:VAETrainer:Epoch [95/100] Train TotalLoss: 14.8659, (ReconX: 0.9806, KLD: 0.0853*1.00, LossY: 1.3800*10.00), Time: 1.8s | Val TotalLoss: 14.1126, (ReconX: 0.9811, KLD: 0.0849, LossY: 1.3047)\n",
            "2025-10-23 22:53:55,870 - VAETrainer - INFO - Saving model and trainer state to vae_results/best_model.pth...\n",
            "INFO:VAETrainer:Saving model and trainer state to vae_results/best_model.pth...\n",
            "2025-10-23 22:53:55,885 - VAETrainer - INFO - Successfully saved model and trainer state to vae_results/best_model.pth\n",
            "INFO:VAETrainer:Successfully saved model and trainer state to vae_results/best_model.pth\n",
            "2025-10-23 22:53:58,010 - VAETrainer - INFO - Epoch [96/100] Train TotalLoss: 14.9325, (ReconX: 0.9810, KLD: 0.0855*1.00, LossY: 1.3866*10.00), Time: 1.7s | Val TotalLoss: 14.0699, (ReconX: 0.9820, KLD: 0.0832, LossY: 1.3005)\n",
            "INFO:VAETrainer:Epoch [96/100] Train TotalLoss: 14.9325, (ReconX: 0.9810, KLD: 0.0855*1.00, LossY: 1.3866*10.00), Time: 1.7s | Val TotalLoss: 14.0699, (ReconX: 0.9820, KLD: 0.0832, LossY: 1.3005)\n",
            "2025-10-23 22:53:58,011 - VAETrainer - INFO - Saving model and trainer state to vae_results/best_model.pth...\n",
            "INFO:VAETrainer:Saving model and trainer state to vae_results/best_model.pth...\n",
            "2025-10-23 22:53:58,025 - VAETrainer - INFO - Successfully saved model and trainer state to vae_results/best_model.pth\n",
            "INFO:VAETrainer:Successfully saved model and trainer state to vae_results/best_model.pth\n",
            "2025-10-23 22:54:00,184 - VAETrainer - INFO - Epoch [97/100] Train TotalLoss: 14.9637, (ReconX: 0.9803, KLD: 0.0864*1.00, LossY: 1.3897*10.00), Time: 1.8s | Val TotalLoss: 14.1656, (ReconX: 0.9809, KLD: 0.0868, LossY: 1.3098)\n",
            "INFO:VAETrainer:Epoch [97/100] Train TotalLoss: 14.9637, (ReconX: 0.9803, KLD: 0.0864*1.00, LossY: 1.3897*10.00), Time: 1.8s | Val TotalLoss: 14.1656, (ReconX: 0.9809, KLD: 0.0868, LossY: 1.3098)\n",
            "2025-10-23 22:54:02,358 - VAETrainer - INFO - Epoch [98/100] Train TotalLoss: 14.8996, (ReconX: 0.9802, KLD: 0.0863*1.00, LossY: 1.3833*10.00), Time: 1.8s | Val TotalLoss: 14.0554, (ReconX: 0.9825, KLD: 0.0850, LossY: 1.2988)\n",
            "INFO:VAETrainer:Epoch [98/100] Train TotalLoss: 14.8996, (ReconX: 0.9802, KLD: 0.0863*1.00, LossY: 1.3833*10.00), Time: 1.8s | Val TotalLoss: 14.0554, (ReconX: 0.9825, KLD: 0.0850, LossY: 1.2988)\n",
            "2025-10-23 22:54:02,359 - VAETrainer - INFO - Saving model and trainer state to vae_results/best_model.pth...\n",
            "INFO:VAETrainer:Saving model and trainer state to vae_results/best_model.pth...\n",
            "2025-10-23 22:54:02,375 - VAETrainer - INFO - Successfully saved model and trainer state to vae_results/best_model.pth\n",
            "INFO:VAETrainer:Successfully saved model and trainer state to vae_results/best_model.pth\n",
            "2025-10-23 22:54:04,505 - VAETrainer - INFO - Epoch [99/100] Train TotalLoss: 14.8877, (ReconX: 0.9809, KLD: 0.0859*1.00, LossY: 1.3821*10.00), Time: 1.7s | Val TotalLoss: 14.0707, (ReconX: 0.9796, KLD: 0.0865, LossY: 1.3005)\n",
            "INFO:VAETrainer:Epoch [99/100] Train TotalLoss: 14.8877, (ReconX: 0.9809, KLD: 0.0859*1.00, LossY: 1.3821*10.00), Time: 1.7s | Val TotalLoss: 14.0707, (ReconX: 0.9796, KLD: 0.0865, LossY: 1.3005)\n",
            "2025-10-23 22:54:06,672 - VAETrainer - INFO - Epoch [100/100] Train TotalLoss: 14.7526, (ReconX: 0.9811, KLD: 0.0845*1.00, LossY: 1.3687*10.00), Time: 1.8s | Val TotalLoss: 14.2259, (ReconX: 0.9820, KLD: 0.0846, LossY: 1.3159)\n",
            "INFO:VAETrainer:Epoch [100/100] Train TotalLoss: 14.7526, (ReconX: 0.9811, KLD: 0.0845*1.00, LossY: 1.3687*10.00), Time: 1.8s | Val TotalLoss: 14.2259, (ReconX: 0.9820, KLD: 0.0846, LossY: 1.3159)\n",
            "2025-10-23 22:54:06,674 - VAETrainer - INFO - Training loop finished.\n",
            "INFO:VAETrainer:Training loop finished.\n",
            "2025-10-23 22:54:06,675 - VAETrainer - INFO - Loading best model state from 'vae_results/best_model.pth' (Val Loss: 14.0554).\n",
            "INFO:VAETrainer:Loading best model state from 'vae_results/best_model.pth' (Val Loss: 14.0554).\n",
            "2025-10-23 22:54:06,677 - VAETrainer_Loader - INFO - Loading VAE model and trainer state from vae_results/best_model.pth to device cuda...\n",
            "INFO:VAETrainer_Loader:Loading VAE model and trainer state from vae_results/best_model.pth to device cuda...\n",
            "2025-10-23 22:54:06,716 - SemiSupMIWAE(X:42,Y:1,Lat:3) - INFO - SemiSupMIWAE Initialized: InputXDim=42, TargetYDim=1, LatentDim=3, AlphaPriceLoss=10.0, Device=cuda\n",
            "INFO:SemiSupMIWAE(X:42,Y:1,Lat:3):SemiSupMIWAE Initialized: InputXDim=42, TargetYDim=1, LatentDim=3, AlphaPriceLoss=10.0, Device=cuda\n",
            "2025-10-23 22:54:06,719 - VAETrainer_Loader - INFO - SemiSupMIWAE model created and state loaded successfully.\n",
            "INFO:VAETrainer_Loader:SemiSupMIWAE model created and state loaded successfully.\n",
            "2025-10-23 22:54:06,721 - VAETrainer - INFO - VAETrainer initialized for SemiSupMIWAE. Device: cuda. Optimizer: adam. LR: 0.0003. ReconLoss(X): 'mse'. Alpha(Y Loss): 10.0. LossType(Y): 'gaussian_nll'\n",
            "INFO:VAETrainer:VAETrainer initialized for SemiSupMIWAE. Device: cuda. Optimizer: adam. LR: 0.0003. ReconLoss(X): 'mse'. Alpha(Y Loss): 10.0. LossType(Y): 'gaussian_nll'\n",
            "2025-10-23 22:54:06,722 - VAETrainer_Loader - INFO - VAETrainer instance created and state loaded for model from vae_results/best_model.pth.\n",
            "INFO:VAETrainer_Loader:VAETrainer instance created and state loaded for model from vae_results/best_model.pth.\n",
            "2025-10-23 22:54:06,731 - VAETrainer - INFO - Predicting target (Y) mean and log-variance for 12271 samples...\n",
            "INFO:VAETrainer:Predicting target (Y) mean and log-variance for 12271 samples...\n",
            "2025-10-23 22:54:06,796 - MainRealEstateAnalysis_MIWAE - INFO - [Fold 8] Stored test predictions for 12271 rows.\n",
            "INFO:MainRealEstateAnalysis_MIWAE:[Fold 8] Stored test predictions for 12271 rows.\n",
            "2025-10-23 22:54:06,809 - MainRealEstateAnalysis_MIWAE - INFO - [Fold 9/10] sizes → Train=98170 (80.0%), Val=12271 (10.0%), Test=12271 (10.0%)\n",
            "INFO:MainRealEstateAnalysis_MIWAE:[Fold 9/10] sizes → Train=98170 (80.0%), Val=12271 (10.0%), Test=12271 (10.0%)\n",
            "2025-10-23 22:54:06,810 - __main__.create_vae_from_artifacts - INFO - Creating VAE model. Target column for supervision: log_sale_price.\n",
            "2025-10-23 22:54:06,810 - __main__.create_vae_from_artifacts - INFO - Creating VAE model. Target column for supervision: log_sale_price.\n",
            "INFO:__main__.create_vae_from_artifacts:Creating VAE model. Target column for supervision: log_sale_price.\n",
            "2025-10-23 22:54:06,812 - __main__.create_vae_from_artifacts - INFO - Number of input features for VAE (X): 42 (Features: ['lotarea', 'assess_total', 'exempt_total', 'assessland', 'exemptland']...)\n",
            "2025-10-23 22:54:06,812 - __main__.create_vae_from_artifacts - INFO - Number of input features for VAE (X): 42 (Features: ['lotarea', 'assess_total', 'exempt_total', 'assessland', 'exemptland']...)\n",
            "INFO:__main__.create_vae_from_artifacts:Number of input features for VAE (X): 42 (Features: ['lotarea', 'assess_total', 'exempt_total', 'assessland', 'exemptland']...)\n",
            "2025-10-23 22:54:06,813 - __main__.create_vae_from_artifacts - INFO - Semi-supervised mode: Target 'log_sale_price' (y_dim=1). Alpha for price loss: 10.0.\n",
            "2025-10-23 22:54:06,813 - __main__.create_vae_from_artifacts - INFO - Semi-supervised mode: Target 'log_sale_price' (y_dim=1). Alpha for price loss: 10.0.\n",
            "INFO:__main__.create_vae_from_artifacts:Semi-supervised mode: Target 'log_sale_price' (y_dim=1). Alpha for price loss: 10.0.\n",
            "2025-10-23 22:54:06,815 - __main__.create_vae_from_artifacts - INFO - Using overridden latent_dim: 3\n",
            "2025-10-23 22:54:06,815 - __main__.create_vae_from_artifacts - INFO - Using overridden latent_dim: 3\n",
            "INFO:__main__.create_vae_from_artifacts:Using overridden latent_dim: 3\n",
            "2025-10-23 22:54:06,817 - __main__.create_vae_from_artifacts - INFO - Final effective Latent Dimension: 3\n",
            "2025-10-23 22:54:06,817 - __main__.create_vae_from_artifacts - INFO - Final effective Latent Dimension: 3\n",
            "INFO:__main__.create_vae_from_artifacts:Final effective Latent Dimension: 3\n",
            "2025-10-23 22:54:06,818 - __main__.create_vae_from_artifacts - INFO - Encoder layers: [21], Decoder layers: [21]\n",
            "2025-10-23 22:54:06,818 - __main__.create_vae_from_artifacts - INFO - Encoder layers: [21], Decoder layers: [21]\n",
            "INFO:__main__.create_vae_from_artifacts:Encoder layers: [21], Decoder layers: [21]\n",
            "2025-10-23 22:54:06,820 - __main__.create_vae_from_artifacts - INFO - Using determined prior: Type=student_t_mixture, K=2 (from overrides or artifact inference).\n",
            "2025-10-23 22:54:06,820 - __main__.create_vae_from_artifacts - INFO - Using determined prior: Type=student_t_mixture, K=2 (from overrides or artifact inference).\n",
            "INFO:__main__.create_vae_from_artifacts:Using determined prior: Type=student_t_mixture, K=2 (from overrides or artifact inference).\n",
            "2025-10-23 22:54:06,821 - __main__.create_vae_from_artifacts - INFO - Initializing parameters for student_t_mixture with K=2 components...\n",
            "2025-10-23 22:54:06,821 - __main__.create_vae_from_artifacts - INFO - Initializing parameters for student_t_mixture with K=2 components...\n",
            "INFO:__main__.create_vae_from_artifacts:Initializing parameters for student_t_mixture with K=2 components...\n",
            "2025-10-23 22:54:06,823 - __main__.create_vae_from_artifacts - WARNING - DP-GMM artifacts are unsuitable or insufficient for initializing mixture prior parameters. Default parameters (e.g., zeros for means/logits, identity for covariances) will be used by the VAE.\n",
            "2025-10-23 22:54:06,823 - __main__.create_vae_from_artifacts - WARNING - DP-GMM artifacts are unsuitable or insufficient for initializing mixture prior parameters. Default parameters (e.g., zeros for means/logits, identity for covariances) will be used by the VAE.\n",
            "WARNING:__main__.create_vae_from_artifacts:DP-GMM artifacts are unsuitable or insufficient for initializing mixture prior parameters. Default parameters (e.g., zeros for means/logits, identity for covariances) will be used by the VAE.\n",
            "2025-10-23 22:54:06,824 - __main__.create_vae_from_artifacts - INFO - Setting Student-t df to 4.0 for prior.\n",
            "2025-10-23 22:54:06,824 - __main__.create_vae_from_artifacts - INFO - Setting Student-t df to 4.0 for prior.\n",
            "INFO:__main__.create_vae_from_artifacts:Setting Student-t df to 4.0 for prior.\n",
            "2025-10-23 22:54:06,826 - __main__.create_vae_from_artifacts - INFO - Instantiating SemiSupMIWAE model...\n",
            "2025-10-23 22:54:06,826 - __main__.create_vae_from_artifacts - INFO - Instantiating SemiSupMIWAE model...\n",
            "INFO:__main__.create_vae_from_artifacts:Instantiating SemiSupMIWAE model...\n",
            "2025-10-23 22:54:06,829 - __main__.create_vae_from_artifacts - INFO - Price head layers for SemiSupMIWAE: [8, 4]\n",
            "2025-10-23 22:54:06,829 - __main__.create_vae_from_artifacts - INFO - Price head layers for SemiSupMIWAE: [8, 4]\n",
            "INFO:__main__.create_vae_from_artifacts:Price head layers for SemiSupMIWAE: [8, 4]\n",
            "2025-10-23 22:54:06,837 - SemiSupMIWAE(X:42,Y:1,Lat:3) - INFO - SemiSupMIWAE Initialized: InputXDim=42, TargetYDim=1, LatentDim=3, AlphaPriceLoss=10.0, Device=cuda\n",
            "INFO:SemiSupMIWAE(X:42,Y:1,Lat:3):SemiSupMIWAE Initialized: InputXDim=42, TargetYDim=1, LatentDim=3, AlphaPriceLoss=10.0, Device=cuda\n",
            "2025-10-23 22:54:06,838 - __main__.create_vae_from_artifacts - INFO - SemiSupMIWAE model created successfully on device: cuda.\n",
            "2025-10-23 22:54:06,838 - __main__.create_vae_from_artifacts - INFO - SemiSupMIWAE model created successfully on device: cuda.\n",
            "INFO:__main__.create_vae_from_artifacts:SemiSupMIWAE model created successfully on device: cuda.\n",
            "2025-10-23 22:54:06,841 - VAETrainer - INFO - VAETrainer initialized for SemiSupMIWAE. Device: cuda. Optimizer: adam. LR: 0.0003. ReconLoss(X): 'mse'. Alpha(Y Loss): 10.0. LossType(Y): 'gaussian_nll'\n",
            "INFO:VAETrainer:VAETrainer initialized for SemiSupMIWAE. Device: cuda. Optimizer: adam. LR: 0.0003. ReconLoss(X): 'mse'. Alpha(Y Loss): 10.0. LossType(Y): 'gaussian_nll'\n",
            "2025-10-23 22:54:06,843 - VAETrainer - INFO - Starting training: Epochs=100, BatchSize=512, KLD_weight=1.0, KLD_AnnealEpochs=30, Alpha(Y_Loss)=10.0.\n",
            "INFO:VAETrainer:Starting training: Epochs=100, BatchSize=512, KLD_weight=1.0, KLD_AnnealEpochs=30, Alpha(Y_Loss)=10.0.\n",
            "2025-10-23 22:54:06,844 - VAETrainer - INFO - Train loader: 98170 samples, 192 batches. Drop last: False\n",
            "INFO:VAETrainer:Train loader: 98170 samples, 192 batches. Drop last: False\n",
            "2025-10-23 22:54:06,845 - VAETrainer - INFO - Validation loader: 12271 samples, 24 batches.\n",
            "INFO:VAETrainer:Validation loader: 12271 samples, 24 batches.\n",
            "2025-10-23 22:54:08,999 - VAETrainer - INFO - Epoch [1/100] Train TotalLoss: 582.7531, (ReconX: 1.0823, KLD: 0.4784*0.03, LossY: 58.1655*10.00), Time: 1.8s | Val TotalLoss: 449.3838, (ReconX: 0.9982, KLD: 0.5108, LossY: 44.7875)\n",
            "INFO:VAETrainer:Epoch [1/100] Train TotalLoss: 582.7531, (ReconX: 1.0823, KLD: 0.4784*0.03, LossY: 58.1655*10.00), Time: 1.8s | Val TotalLoss: 449.3838, (ReconX: 0.9982, KLD: 0.5108, LossY: 44.7875)\n",
            "2025-10-23 22:54:09,001 - VAETrainer - INFO - Saving model and trainer state to vae_results/best_model.pth...\n",
            "INFO:VAETrainer:Saving model and trainer state to vae_results/best_model.pth...\n",
            "2025-10-23 22:54:09,014 - VAETrainer - INFO - Successfully saved model and trainer state to vae_results/best_model.pth\n",
            "INFO:VAETrainer:Successfully saved model and trainer state to vae_results/best_model.pth\n",
            "2025-10-23 22:54:11,105 - VAETrainer - INFO - Epoch [2/100] Train TotalLoss: 383.8643, (ReconX: 0.9684, KLD: 0.5295*0.07, LossY: 38.2861*10.00), Time: 1.7s | Val TotalLoss: 331.5229, (ReconX: 0.9293, KLD: 0.5835, LossY: 33.0010)\n",
            "INFO:VAETrainer:Epoch [2/100] Train TotalLoss: 383.8643, (ReconX: 0.9684, KLD: 0.5295*0.07, LossY: 38.2861*10.00), Time: 1.7s | Val TotalLoss: 331.5229, (ReconX: 0.9293, KLD: 0.5835, LossY: 33.0010)\n",
            "2025-10-23 22:54:11,106 - VAETrainer - INFO - Saving model and trainer state to vae_results/best_model.pth...\n",
            "INFO:VAETrainer:Saving model and trainer state to vae_results/best_model.pth...\n",
            "2025-10-23 22:54:11,123 - VAETrainer - INFO - Successfully saved model and trainer state to vae_results/best_model.pth\n",
            "INFO:VAETrainer:Successfully saved model and trainer state to vae_results/best_model.pth\n",
            "2025-10-23 22:54:13,316 - VAETrainer - INFO - Epoch [3/100] Train TotalLoss: 292.9329, (ReconX: 0.9216, KLD: 0.5822*0.10, LossY: 29.1953*10.00), Time: 1.8s | Val TotalLoss: 250.5045, (ReconX: 0.9012, KLD: 0.5918, LossY: 24.9011)\n",
            "INFO:VAETrainer:Epoch [3/100] Train TotalLoss: 292.9329, (ReconX: 0.9216, KLD: 0.5822*0.10, LossY: 29.1953*10.00), Time: 1.8s | Val TotalLoss: 250.5045, (ReconX: 0.9012, KLD: 0.5918, LossY: 24.9011)\n",
            "2025-10-23 22:54:13,318 - VAETrainer - INFO - Saving model and trainer state to vae_results/best_model.pth...\n",
            "INFO:VAETrainer:Saving model and trainer state to vae_results/best_model.pth...\n",
            "2025-10-23 22:54:13,331 - VAETrainer - INFO - Successfully saved model and trainer state to vae_results/best_model.pth\n",
            "INFO:VAETrainer:Successfully saved model and trainer state to vae_results/best_model.pth\n",
            "2025-10-23 22:54:15,530 - VAETrainer - INFO - Epoch [4/100] Train TotalLoss: 227.5785, (ReconX: 0.9059, KLD: 0.5860*0.13, LossY: 22.6594*10.00), Time: 1.8s | Val TotalLoss: 201.7175, (ReconX: 0.8942, KLD: 0.6000, LossY: 20.0223)\n",
            "INFO:VAETrainer:Epoch [4/100] Train TotalLoss: 227.5785, (ReconX: 0.9059, KLD: 0.5860*0.13, LossY: 22.6594*10.00), Time: 1.8s | Val TotalLoss: 201.7175, (ReconX: 0.8942, KLD: 0.6000, LossY: 20.0223)\n",
            "2025-10-23 22:54:15,532 - VAETrainer - INFO - Saving model and trainer state to vae_results/best_model.pth...\n",
            "INFO:VAETrainer:Saving model and trainer state to vae_results/best_model.pth...\n",
            "2025-10-23 22:54:15,546 - VAETrainer - INFO - Successfully saved model and trainer state to vae_results/best_model.pth\n",
            "INFO:VAETrainer:Successfully saved model and trainer state to vae_results/best_model.pth\n",
            "2025-10-23 22:54:17,706 - VAETrainer - INFO - Epoch [5/100] Train TotalLoss: 181.6342, (ReconX: 0.9030, KLD: 0.5712*0.17, LossY: 18.0636*10.00), Time: 1.8s | Val TotalLoss: 161.5053, (ReconX: 0.8949, KLD: 0.5753, LossY: 16.0035)\n",
            "INFO:VAETrainer:Epoch [5/100] Train TotalLoss: 181.6342, (ReconX: 0.9030, KLD: 0.5712*0.17, LossY: 18.0636*10.00), Time: 1.8s | Val TotalLoss: 161.5053, (ReconX: 0.8949, KLD: 0.5753, LossY: 16.0035)\n",
            "2025-10-23 22:54:17,708 - VAETrainer - INFO - Saving model and trainer state to vae_results/best_model.pth...\n",
            "INFO:VAETrainer:Saving model and trainer state to vae_results/best_model.pth...\n",
            "2025-10-23 22:54:17,722 - VAETrainer - INFO - Successfully saved model and trainer state to vae_results/best_model.pth\n",
            "INFO:VAETrainer:Successfully saved model and trainer state to vae_results/best_model.pth\n",
            "2025-10-23 22:54:19,893 - VAETrainer - INFO - Epoch [6/100] Train TotalLoss: 148.0322, (ReconX: 0.9052, KLD: 0.5565*0.20, LossY: 14.7016*10.00), Time: 1.8s | Val TotalLoss: 132.8412, (ReconX: 0.8971, KLD: 0.5540, LossY: 13.1390)\n",
            "INFO:VAETrainer:Epoch [6/100] Train TotalLoss: 148.0322, (ReconX: 0.9052, KLD: 0.5565*0.20, LossY: 14.7016*10.00), Time: 1.8s | Val TotalLoss: 132.8412, (ReconX: 0.8971, KLD: 0.5540, LossY: 13.1390)\n",
            "2025-10-23 22:54:19,895 - VAETrainer - INFO - Saving model and trainer state to vae_results/best_model.pth...\n",
            "INFO:VAETrainer:Saving model and trainer state to vae_results/best_model.pth...\n",
            "2025-10-23 22:54:19,908 - VAETrainer - INFO - Successfully saved model and trainer state to vae_results/best_model.pth\n",
            "INFO:VAETrainer:Successfully saved model and trainer state to vae_results/best_model.pth\n",
            "2025-10-23 22:54:21,978 - VAETrainer - INFO - Epoch [7/100] Train TotalLoss: 122.5483, (ReconX: 0.9067, KLD: 0.5461*0.23, LossY: 12.1514*10.00), Time: 1.7s | Val TotalLoss: 111.1645, (ReconX: 0.8986, KLD: 0.5431, LossY: 10.9723)\n",
            "INFO:VAETrainer:Epoch [7/100] Train TotalLoss: 122.5483, (ReconX: 0.9067, KLD: 0.5461*0.23, LossY: 12.1514*10.00), Time: 1.7s | Val TotalLoss: 111.1645, (ReconX: 0.8986, KLD: 0.5431, LossY: 10.9723)\n",
            "2025-10-23 22:54:21,979 - VAETrainer - INFO - Saving model and trainer state to vae_results/best_model.pth...\n",
            "INFO:VAETrainer:Saving model and trainer state to vae_results/best_model.pth...\n",
            "2025-10-23 22:54:21,992 - VAETrainer - INFO - Successfully saved model and trainer state to vae_results/best_model.pth\n",
            "INFO:VAETrainer:Successfully saved model and trainer state to vae_results/best_model.pth\n",
            "2025-10-23 22:54:24,193 - VAETrainer - INFO - Epoch [8/100] Train TotalLoss: 103.1525, (ReconX: 0.9086, KLD: 0.5413*0.27, LossY: 10.2100*10.00), Time: 1.8s | Val TotalLoss: 95.2520, (ReconX: 0.8992, KLD: 0.5491, LossY: 9.3804)\n",
            "INFO:VAETrainer:Epoch [8/100] Train TotalLoss: 103.1525, (ReconX: 0.9086, KLD: 0.5413*0.27, LossY: 10.2100*10.00), Time: 1.8s | Val TotalLoss: 95.2520, (ReconX: 0.8992, KLD: 0.5491, LossY: 9.3804)\n",
            "2025-10-23 22:54:24,194 - VAETrainer - INFO - Saving model and trainer state to vae_results/best_model.pth...\n",
            "INFO:VAETrainer:Saving model and trainer state to vae_results/best_model.pth...\n",
            "2025-10-23 22:54:24,208 - VAETrainer - INFO - Successfully saved model and trainer state to vae_results/best_model.pth\n",
            "INFO:VAETrainer:Successfully saved model and trainer state to vae_results/best_model.pth\n",
            "2025-10-23 22:54:26,468 - VAETrainer - INFO - Epoch [9/100] Train TotalLoss: 87.9698, (ReconX: 0.9100, KLD: 0.5413*0.30, LossY: 8.6897*10.00), Time: 1.9s | Val TotalLoss: 82.8267, (ReconX: 0.9020, KLD: 0.5544, LossY: 8.1370)\n",
            "INFO:VAETrainer:Epoch [9/100] Train TotalLoss: 87.9698, (ReconX: 0.9100, KLD: 0.5413*0.30, LossY: 8.6897*10.00), Time: 1.9s | Val TotalLoss: 82.8267, (ReconX: 0.9020, KLD: 0.5544, LossY: 8.1370)\n",
            "2025-10-23 22:54:26,470 - VAETrainer - INFO - Saving model and trainer state to vae_results/best_model.pth...\n",
            "INFO:VAETrainer:Saving model and trainer state to vae_results/best_model.pth...\n",
            "2025-10-23 22:54:26,483 - VAETrainer - INFO - Successfully saved model and trainer state to vae_results/best_model.pth\n",
            "INFO:VAETrainer:Successfully saved model and trainer state to vae_results/best_model.pth\n",
            "2025-10-23 22:54:28,772 - VAETrainer - INFO - Epoch [10/100] Train TotalLoss: 75.8532, (ReconX: 0.9107, KLD: 0.5440*0.33, LossY: 7.4761*10.00), Time: 1.9s | Val TotalLoss: 71.4738, (ReconX: 0.9034, KLD: 0.5433, LossY: 7.0027)\n",
            "INFO:VAETrainer:Epoch [10/100] Train TotalLoss: 75.8532, (ReconX: 0.9107, KLD: 0.5440*0.33, LossY: 7.4761*10.00), Time: 1.9s | Val TotalLoss: 71.4738, (ReconX: 0.9034, KLD: 0.5433, LossY: 7.0027)\n",
            "2025-10-23 22:54:28,774 - VAETrainer - INFO - Saving model and trainer state to vae_results/best_model.pth...\n",
            "INFO:VAETrainer:Saving model and trainer state to vae_results/best_model.pth...\n",
            "2025-10-23 22:54:28,787 - VAETrainer - INFO - Successfully saved model and trainer state to vae_results/best_model.pth\n",
            "INFO:VAETrainer:Successfully saved model and trainer state to vae_results/best_model.pth\n",
            "2025-10-23 22:54:31,017 - VAETrainer - INFO - Epoch [11/100] Train TotalLoss: 66.4788, (ReconX: 0.9112, KLD: 0.5541*0.37, LossY: 6.5364*10.00), Time: 1.8s | Val TotalLoss: 63.6280, (ReconX: 0.9035, KLD: 0.5645, LossY: 6.2160)\n",
            "INFO:VAETrainer:Epoch [11/100] Train TotalLoss: 66.4788, (ReconX: 0.9112, KLD: 0.5541*0.37, LossY: 6.5364*10.00), Time: 1.8s | Val TotalLoss: 63.6280, (ReconX: 0.9035, KLD: 0.5645, LossY: 6.2160)\n",
            "2025-10-23 22:54:31,019 - VAETrainer - INFO - Saving model and trainer state to vae_results/best_model.pth...\n",
            "INFO:VAETrainer:Saving model and trainer state to vae_results/best_model.pth...\n",
            "2025-10-23 22:54:31,032 - VAETrainer - INFO - Successfully saved model and trainer state to vae_results/best_model.pth\n",
            "INFO:VAETrainer:Successfully saved model and trainer state to vae_results/best_model.pth\n",
            "2025-10-23 22:54:33,179 - VAETrainer - INFO - Epoch [12/100] Train TotalLoss: 59.3130, (ReconX: 0.9111, KLD: 0.5638*0.40, LossY: 5.8176*10.00), Time: 1.8s | Val TotalLoss: 57.1391, (ReconX: 0.9012, KLD: 0.5633, LossY: 5.5675)\n",
            "INFO:VAETrainer:Epoch [12/100] Train TotalLoss: 59.3130, (ReconX: 0.9111, KLD: 0.5638*0.40, LossY: 5.8176*10.00), Time: 1.8s | Val TotalLoss: 57.1391, (ReconX: 0.9012, KLD: 0.5633, LossY: 5.5675)\n",
            "2025-10-23 22:54:33,181 - VAETrainer - INFO - Saving model and trainer state to vae_results/best_model.pth...\n",
            "INFO:VAETrainer:Saving model and trainer state to vae_results/best_model.pth...\n",
            "2025-10-23 22:54:33,194 - VAETrainer - INFO - Successfully saved model and trainer state to vae_results/best_model.pth\n",
            "INFO:VAETrainer:Successfully saved model and trainer state to vae_results/best_model.pth\n",
            "2025-10-23 22:54:35,337 - VAETrainer - INFO - Epoch [13/100] Train TotalLoss: 53.8427, (ReconX: 0.9118, KLD: 0.5682*0.43, LossY: 5.2685*10.00), Time: 1.8s | Val TotalLoss: 52.2460, (ReconX: 0.9026, KLD: 0.5707, LossY: 5.0773)\n",
            "INFO:VAETrainer:Epoch [13/100] Train TotalLoss: 53.8427, (ReconX: 0.9118, KLD: 0.5682*0.43, LossY: 5.2685*10.00), Time: 1.8s | Val TotalLoss: 52.2460, (ReconX: 0.9026, KLD: 0.5707, LossY: 5.0773)\n",
            "2025-10-23 22:54:35,339 - VAETrainer - INFO - Saving model and trainer state to vae_results/best_model.pth...\n",
            "INFO:VAETrainer:Saving model and trainer state to vae_results/best_model.pth...\n",
            "2025-10-23 22:54:35,352 - VAETrainer - INFO - Successfully saved model and trainer state to vae_results/best_model.pth\n",
            "INFO:VAETrainer:Successfully saved model and trainer state to vae_results/best_model.pth\n",
            "2025-10-23 22:54:37,665 - VAETrainer - INFO - Epoch [14/100] Train TotalLoss: 49.6825, (ReconX: 0.9119, KLD: 0.5662*0.47, LossY: 4.8506*10.00), Time: 1.9s | Val TotalLoss: 48.6415, (ReconX: 0.9015, KLD: 0.5634, LossY: 4.7177)\n",
            "INFO:VAETrainer:Epoch [14/100] Train TotalLoss: 49.6825, (ReconX: 0.9119, KLD: 0.5662*0.47, LossY: 4.8506*10.00), Time: 1.9s | Val TotalLoss: 48.6415, (ReconX: 0.9015, KLD: 0.5634, LossY: 4.7177)\n",
            "2025-10-23 22:54:37,667 - VAETrainer - INFO - Saving model and trainer state to vae_results/best_model.pth...\n",
            "INFO:VAETrainer:Saving model and trainer state to vae_results/best_model.pth...\n",
            "2025-10-23 22:54:37,680 - VAETrainer - INFO - Successfully saved model and trainer state to vae_results/best_model.pth\n",
            "INFO:VAETrainer:Successfully saved model and trainer state to vae_results/best_model.pth\n",
            "2025-10-23 22:54:39,930 - VAETrainer - INFO - Epoch [15/100] Train TotalLoss: 46.3047, (ReconX: 0.9121, KLD: 0.5610*0.50, LossY: 4.5112*10.00), Time: 1.9s | Val TotalLoss: 45.3278, (ReconX: 0.9040, KLD: 0.5480, LossY: 4.3876)\n",
            "INFO:VAETrainer:Epoch [15/100] Train TotalLoss: 46.3047, (ReconX: 0.9121, KLD: 0.5610*0.50, LossY: 4.5112*10.00), Time: 1.9s | Val TotalLoss: 45.3278, (ReconX: 0.9040, KLD: 0.5480, LossY: 4.3876)\n",
            "2025-10-23 22:54:39,933 - VAETrainer - INFO - Saving model and trainer state to vae_results/best_model.pth...\n",
            "INFO:VAETrainer:Saving model and trainer state to vae_results/best_model.pth...\n",
            "2025-10-23 22:54:39,947 - VAETrainer - INFO - Successfully saved model and trainer state to vae_results/best_model.pth\n",
            "INFO:VAETrainer:Successfully saved model and trainer state to vae_results/best_model.pth\n",
            "2025-10-23 22:54:42,208 - VAETrainer - INFO - Epoch [16/100] Train TotalLoss: 43.6856, (ReconX: 0.9128, KLD: 0.5478*0.53, LossY: 4.2481*10.00), Time: 1.9s | Val TotalLoss: 43.1576, (ReconX: 0.9052, KLD: 0.5462, LossY: 4.1706)\n",
            "INFO:VAETrainer:Epoch [16/100] Train TotalLoss: 43.6856, (ReconX: 0.9128, KLD: 0.5478*0.53, LossY: 4.2481*10.00), Time: 1.9s | Val TotalLoss: 43.1576, (ReconX: 0.9052, KLD: 0.5462, LossY: 4.1706)\n",
            "2025-10-23 22:54:42,210 - VAETrainer - INFO - Saving model and trainer state to vae_results/best_model.pth...\n",
            "INFO:VAETrainer:Saving model and trainer state to vae_results/best_model.pth...\n",
            "2025-10-23 22:54:42,224 - VAETrainer - INFO - Successfully saved model and trainer state to vae_results/best_model.pth\n",
            "INFO:VAETrainer:Successfully saved model and trainer state to vae_results/best_model.pth\n",
            "2025-10-23 22:54:44,337 - VAETrainer - INFO - Epoch [17/100] Train TotalLoss: 41.6931, (ReconX: 0.9139, KLD: 0.5340*0.57, LossY: 4.0477*10.00), Time: 1.7s | Val TotalLoss: 41.4106, (ReconX: 0.9089, KLD: 0.5193, LossY: 3.9982)\n",
            "INFO:VAETrainer:Epoch [17/100] Train TotalLoss: 41.6931, (ReconX: 0.9139, KLD: 0.5340*0.57, LossY: 4.0477*10.00), Time: 1.7s | Val TotalLoss: 41.4106, (ReconX: 0.9089, KLD: 0.5193, LossY: 3.9982)\n",
            "2025-10-23 22:54:44,339 - VAETrainer - INFO - Saving model and trainer state to vae_results/best_model.pth...\n",
            "INFO:VAETrainer:Saving model and trainer state to vae_results/best_model.pth...\n",
            "2025-10-23 22:54:44,352 - VAETrainer - INFO - Successfully saved model and trainer state to vae_results/best_model.pth\n",
            "INFO:VAETrainer:Successfully saved model and trainer state to vae_results/best_model.pth\n",
            "2025-10-23 22:54:46,556 - VAETrainer - INFO - Epoch [18/100] Train TotalLoss: 40.0991, (ReconX: 0.9159, KLD: 0.5159*0.60, LossY: 3.8874*10.00), Time: 1.8s | Val TotalLoss: 40.3925, (ReconX: 0.9058, KLD: 0.5191, LossY: 3.8968)\n",
            "INFO:VAETrainer:Epoch [18/100] Train TotalLoss: 40.0991, (ReconX: 0.9159, KLD: 0.5159*0.60, LossY: 3.8874*10.00), Time: 1.8s | Val TotalLoss: 40.3925, (ReconX: 0.9058, KLD: 0.5191, LossY: 3.8968)\n",
            "2025-10-23 22:54:46,558 - VAETrainer - INFO - Saving model and trainer state to vae_results/best_model.pth...\n",
            "INFO:VAETrainer:Saving model and trainer state to vae_results/best_model.pth...\n",
            "2025-10-23 22:54:46,571 - VAETrainer - INFO - Successfully saved model and trainer state to vae_results/best_model.pth\n",
            "INFO:VAETrainer:Successfully saved model and trainer state to vae_results/best_model.pth\n",
            "2025-10-23 22:54:48,822 - VAETrainer - INFO - Epoch [19/100] Train TotalLoss: 38.6156, (ReconX: 0.9179, KLD: 0.4949*0.63, LossY: 3.7384*10.00), Time: 1.9s | Val TotalLoss: 38.4765, (ReconX: 0.9084, KLD: 0.5000, LossY: 3.7068)\n",
            "INFO:VAETrainer:Epoch [19/100] Train TotalLoss: 38.6156, (ReconX: 0.9179, KLD: 0.4949*0.63, LossY: 3.7384*10.00), Time: 1.9s | Val TotalLoss: 38.4765, (ReconX: 0.9084, KLD: 0.5000, LossY: 3.7068)\n",
            "2025-10-23 22:54:48,823 - VAETrainer - INFO - Saving model and trainer state to vae_results/best_model.pth...\n",
            "INFO:VAETrainer:Saving model and trainer state to vae_results/best_model.pth...\n",
            "2025-10-23 22:54:48,836 - VAETrainer - INFO - Successfully saved model and trainer state to vae_results/best_model.pth\n",
            "INFO:VAETrainer:Successfully saved model and trainer state to vae_results/best_model.pth\n",
            "2025-10-23 22:54:51,064 - VAETrainer - INFO - Epoch [20/100] Train TotalLoss: 37.5732, (ReconX: 0.9195, KLD: 0.4696*0.67, LossY: 3.6341*10.00), Time: 1.9s | Val TotalLoss: 37.0558, (ReconX: 0.9123, KLD: 0.4538, LossY: 3.5690)\n",
            "INFO:VAETrainer:Epoch [20/100] Train TotalLoss: 37.5732, (ReconX: 0.9195, KLD: 0.4696*0.67, LossY: 3.6341*10.00), Time: 1.9s | Val TotalLoss: 37.0558, (ReconX: 0.9123, KLD: 0.4538, LossY: 3.5690)\n",
            "2025-10-23 22:54:51,065 - VAETrainer - INFO - Saving model and trainer state to vae_results/best_model.pth...\n",
            "INFO:VAETrainer:Saving model and trainer state to vae_results/best_model.pth...\n",
            "2025-10-23 22:54:51,080 - VAETrainer - INFO - Successfully saved model and trainer state to vae_results/best_model.pth\n",
            "INFO:VAETrainer:Successfully saved model and trainer state to vae_results/best_model.pth\n",
            "2025-10-23 22:54:53,203 - VAETrainer - INFO - Epoch [21/100] Train TotalLoss: 36.6645, (ReconX: 0.9227, KLD: 0.4412*0.70, LossY: 3.5433*10.00), Time: 1.7s | Val TotalLoss: 36.2628, (ReconX: 0.9175, KLD: 0.4197, LossY: 3.4926)\n",
            "INFO:VAETrainer:Epoch [21/100] Train TotalLoss: 36.6645, (ReconX: 0.9227, KLD: 0.4412*0.70, LossY: 3.5433*10.00), Time: 1.7s | Val TotalLoss: 36.2628, (ReconX: 0.9175, KLD: 0.4197, LossY: 3.4926)\n",
            "2025-10-23 22:54:53,204 - VAETrainer - INFO - Saving model and trainer state to vae_results/best_model.pth...\n",
            "INFO:VAETrainer:Saving model and trainer state to vae_results/best_model.pth...\n",
            "2025-10-23 22:54:53,217 - VAETrainer - INFO - Successfully saved model and trainer state to vae_results/best_model.pth\n",
            "INFO:VAETrainer:Successfully saved model and trainer state to vae_results/best_model.pth\n",
            "2025-10-23 22:54:55,546 - VAETrainer - INFO - Epoch [22/100] Train TotalLoss: 35.8928, (ReconX: 0.9254, KLD: 0.4126*0.73, LossY: 3.4665*10.00), Time: 2.0s | Val TotalLoss: 35.4711, (ReconX: 0.9180, KLD: 0.4010, LossY: 3.4152)\n",
            "INFO:VAETrainer:Epoch [22/100] Train TotalLoss: 35.8928, (ReconX: 0.9254, KLD: 0.4126*0.73, LossY: 3.4665*10.00), Time: 2.0s | Val TotalLoss: 35.4711, (ReconX: 0.9180, KLD: 0.4010, LossY: 3.4152)\n",
            "2025-10-23 22:54:55,548 - VAETrainer - INFO - Saving model and trainer state to vae_results/best_model.pth...\n",
            "INFO:VAETrainer:Saving model and trainer state to vae_results/best_model.pth...\n",
            "2025-10-23 22:54:55,561 - VAETrainer - INFO - Successfully saved model and trainer state to vae_results/best_model.pth\n",
            "INFO:VAETrainer:Successfully saved model and trainer state to vae_results/best_model.pth\n",
            "2025-10-23 22:54:57,723 - VAETrainer - INFO - Epoch [23/100] Train TotalLoss: 35.2895, (ReconX: 0.9287, KLD: 0.3830*0.77, LossY: 3.4067*10.00), Time: 1.8s | Val TotalLoss: 35.0201, (ReconX: 0.9235, KLD: 0.3530, LossY: 3.3744)\n",
            "INFO:VAETrainer:Epoch [23/100] Train TotalLoss: 35.2895, (ReconX: 0.9287, KLD: 0.3830*0.77, LossY: 3.4067*10.00), Time: 1.8s | Val TotalLoss: 35.0201, (ReconX: 0.9235, KLD: 0.3530, LossY: 3.3744)\n",
            "2025-10-23 22:54:57,726 - VAETrainer - INFO - Saving model and trainer state to vae_results/best_model.pth...\n",
            "INFO:VAETrainer:Saving model and trainer state to vae_results/best_model.pth...\n",
            "2025-10-23 22:54:57,739 - VAETrainer - INFO - Successfully saved model and trainer state to vae_results/best_model.pth\n",
            "INFO:VAETrainer:Successfully saved model and trainer state to vae_results/best_model.pth\n",
            "2025-10-23 22:54:59,906 - VAETrainer - INFO - Epoch [24/100] Train TotalLoss: 34.7194, (ReconX: 0.9322, KLD: 0.3535*0.80, LossY: 3.3504*10.00), Time: 1.8s | Val TotalLoss: 34.5091, (ReconX: 0.9275, KLD: 0.3312, LossY: 3.3250)\n",
            "INFO:VAETrainer:Epoch [24/100] Train TotalLoss: 34.7194, (ReconX: 0.9322, KLD: 0.3535*0.80, LossY: 3.3504*10.00), Time: 1.8s | Val TotalLoss: 34.5091, (ReconX: 0.9275, KLD: 0.3312, LossY: 3.3250)\n",
            "2025-10-23 22:54:59,908 - VAETrainer - INFO - Saving model and trainer state to vae_results/best_model.pth...\n",
            "INFO:VAETrainer:Saving model and trainer state to vae_results/best_model.pth...\n",
            "2025-10-23 22:54:59,921 - VAETrainer - INFO - Successfully saved model and trainer state to vae_results/best_model.pth\n",
            "INFO:VAETrainer:Successfully saved model and trainer state to vae_results/best_model.pth\n",
            "2025-10-23 22:55:01,930 - VAETrainer - INFO - Epoch [25/100] Train TotalLoss: 34.2605, (ReconX: 0.9357, KLD: 0.3249*0.83, LossY: 3.3054*10.00), Time: 1.6s | Val TotalLoss: 33.9133, (ReconX: 0.9277, KLD: 0.3161, LossY: 3.2670)\n",
            "INFO:VAETrainer:Epoch [25/100] Train TotalLoss: 34.2605, (ReconX: 0.9357, KLD: 0.3249*0.83, LossY: 3.3054*10.00), Time: 1.6s | Val TotalLoss: 33.9133, (ReconX: 0.9277, KLD: 0.3161, LossY: 3.2670)\n",
            "2025-10-23 22:55:01,932 - VAETrainer - INFO - Saving model and trainer state to vae_results/best_model.pth...\n",
            "INFO:VAETrainer:Saving model and trainer state to vae_results/best_model.pth...\n",
            "2025-10-23 22:55:01,947 - VAETrainer - INFO - Successfully saved model and trainer state to vae_results/best_model.pth\n",
            "INFO:VAETrainer:Successfully saved model and trainer state to vae_results/best_model.pth\n",
            "2025-10-23 22:55:04,088 - VAETrainer - INFO - Epoch [26/100] Train TotalLoss: 33.8769, (ReconX: 0.9398, KLD: 0.2998*0.87, LossY: 3.2677*10.00), Time: 1.8s | Val TotalLoss: 33.6359, (ReconX: 0.9312, KLD: 0.2865, LossY: 3.2418)\n",
            "INFO:VAETrainer:Epoch [26/100] Train TotalLoss: 33.8769, (ReconX: 0.9398, KLD: 0.2998*0.87, LossY: 3.2677*10.00), Time: 1.8s | Val TotalLoss: 33.6359, (ReconX: 0.9312, KLD: 0.2865, LossY: 3.2418)\n",
            "2025-10-23 22:55:04,090 - VAETrainer - INFO - Saving model and trainer state to vae_results/best_model.pth...\n",
            "INFO:VAETrainer:Saving model and trainer state to vae_results/best_model.pth...\n",
            "2025-10-23 22:55:04,103 - VAETrainer - INFO - Successfully saved model and trainer state to vae_results/best_model.pth\n",
            "INFO:VAETrainer:Successfully saved model and trainer state to vae_results/best_model.pth\n",
            "2025-10-23 22:55:06,326 - VAETrainer - INFO - Epoch [27/100] Train TotalLoss: 33.4980, (ReconX: 0.9442, KLD: 0.2749*0.90, LossY: 3.2306*10.00), Time: 1.8s | Val TotalLoss: 33.2924, (ReconX: 0.9392, KLD: 0.2600, LossY: 3.2093)\n",
            "INFO:VAETrainer:Epoch [27/100] Train TotalLoss: 33.4980, (ReconX: 0.9442, KLD: 0.2749*0.90, LossY: 3.2306*10.00), Time: 1.8s | Val TotalLoss: 33.2924, (ReconX: 0.9392, KLD: 0.2600, LossY: 3.2093)\n",
            "2025-10-23 22:55:06,327 - VAETrainer - INFO - Saving model and trainer state to vae_results/best_model.pth...\n",
            "INFO:VAETrainer:Saving model and trainer state to vae_results/best_model.pth...\n",
            "2025-10-23 22:55:06,343 - VAETrainer - INFO - Successfully saved model and trainer state to vae_results/best_model.pth\n",
            "INFO:VAETrainer:Successfully saved model and trainer state to vae_results/best_model.pth\n",
            "2025-10-23 22:55:08,406 - VAETrainer - INFO - Epoch [28/100] Train TotalLoss: 33.0851, (ReconX: 0.9466, KLD: 0.2507*0.93, LossY: 3.1904*10.00), Time: 1.7s | Val TotalLoss: 32.8943, (ReconX: 0.9440, KLD: 0.2382, LossY: 3.1712)\n",
            "INFO:VAETrainer:Epoch [28/100] Train TotalLoss: 33.0851, (ReconX: 0.9466, KLD: 0.2507*0.93, LossY: 3.1904*10.00), Time: 1.7s | Val TotalLoss: 32.8943, (ReconX: 0.9440, KLD: 0.2382, LossY: 3.1712)\n",
            "2025-10-23 22:55:08,407 - VAETrainer - INFO - Saving model and trainer state to vae_results/best_model.pth...\n",
            "INFO:VAETrainer:Saving model and trainer state to vae_results/best_model.pth...\n",
            "2025-10-23 22:55:08,421 - VAETrainer - INFO - Successfully saved model and trainer state to vae_results/best_model.pth\n",
            "INFO:VAETrainer:Successfully saved model and trainer state to vae_results/best_model.pth\n",
            "2025-10-23 22:55:10,510 - VAETrainer - INFO - Epoch [29/100] Train TotalLoss: 32.7108, (ReconX: 0.9505, KLD: 0.2275*0.97, LossY: 3.1540*10.00), Time: 1.7s | Val TotalLoss: 32.5555, (ReconX: 0.9432, KLD: 0.2202, LossY: 3.1392)\n",
            "INFO:VAETrainer:Epoch [29/100] Train TotalLoss: 32.7108, (ReconX: 0.9505, KLD: 0.2275*0.97, LossY: 3.1540*10.00), Time: 1.7s | Val TotalLoss: 32.5555, (ReconX: 0.9432, KLD: 0.2202, LossY: 3.1392)\n",
            "2025-10-23 22:55:10,511 - VAETrainer - INFO - Saving model and trainer state to vae_results/best_model.pth...\n",
            "INFO:VAETrainer:Saving model and trainer state to vae_results/best_model.pth...\n",
            "2025-10-23 22:55:10,526 - VAETrainer - INFO - Successfully saved model and trainer state to vae_results/best_model.pth\n",
            "INFO:VAETrainer:Successfully saved model and trainer state to vae_results/best_model.pth\n",
            "2025-10-23 22:55:12,570 - VAETrainer - INFO - Epoch [30/100] Train TotalLoss: 32.1403, (ReconX: 0.9557, KLD: 0.2041*1.00, LossY: 3.0981*10.00), Time: 1.7s | Val TotalLoss: 31.5320, (ReconX: 0.9500, KLD: 0.1976, LossY: 3.0384)\n",
            "INFO:VAETrainer:Epoch [30/100] Train TotalLoss: 32.1403, (ReconX: 0.9557, KLD: 0.2041*1.00, LossY: 3.0981*10.00), Time: 1.7s | Val TotalLoss: 31.5320, (ReconX: 0.9500, KLD: 0.1976, LossY: 3.0384)\n",
            "2025-10-23 22:55:12,572 - VAETrainer - INFO - Saving model and trainer state to vae_results/best_model.pth...\n",
            "INFO:VAETrainer:Saving model and trainer state to vae_results/best_model.pth...\n",
            "2025-10-23 22:55:12,586 - VAETrainer - INFO - Successfully saved model and trainer state to vae_results/best_model.pth\n",
            "INFO:VAETrainer:Successfully saved model and trainer state to vae_results/best_model.pth\n",
            "2025-10-23 22:55:14,708 - VAETrainer - INFO - Epoch [31/100] Train TotalLoss: 31.2770, (ReconX: 0.9574, KLD: 0.1893*1.00, LossY: 3.0130*10.00), Time: 1.7s | Val TotalLoss: 30.8240, (ReconX: 0.9499, KLD: 0.1857, LossY: 2.9688)\n",
            "INFO:VAETrainer:Epoch [31/100] Train TotalLoss: 31.2770, (ReconX: 0.9574, KLD: 0.1893*1.00, LossY: 3.0130*10.00), Time: 1.7s | Val TotalLoss: 30.8240, (ReconX: 0.9499, KLD: 0.1857, LossY: 2.9688)\n",
            "2025-10-23 22:55:14,709 - VAETrainer - INFO - Saving model and trainer state to vae_results/best_model.pth...\n",
            "INFO:VAETrainer:Saving model and trainer state to vae_results/best_model.pth...\n",
            "2025-10-23 22:55:14,723 - VAETrainer - INFO - Successfully saved model and trainer state to vae_results/best_model.pth\n",
            "INFO:VAETrainer:Successfully saved model and trainer state to vae_results/best_model.pth\n",
            "2025-10-23 22:55:16,819 - VAETrainer - INFO - Epoch [32/100] Train TotalLoss: 30.3443, (ReconX: 0.9610, KLD: 0.1748*1.00, LossY: 2.9208*10.00), Time: 1.7s | Val TotalLoss: 29.6417, (ReconX: 0.9528, KLD: 0.1719, LossY: 2.8517)\n",
            "INFO:VAETrainer:Epoch [32/100] Train TotalLoss: 30.3443, (ReconX: 0.9610, KLD: 0.1748*1.00, LossY: 2.9208*10.00), Time: 1.7s | Val TotalLoss: 29.6417, (ReconX: 0.9528, KLD: 0.1719, LossY: 2.8517)\n",
            "2025-10-23 22:55:16,821 - VAETrainer - INFO - Saving model and trainer state to vae_results/best_model.pth...\n",
            "INFO:VAETrainer:Saving model and trainer state to vae_results/best_model.pth...\n",
            "2025-10-23 22:55:16,833 - VAETrainer - INFO - Successfully saved model and trainer state to vae_results/best_model.pth\n",
            "INFO:VAETrainer:Successfully saved model and trainer state to vae_results/best_model.pth\n",
            "2025-10-23 22:55:18,974 - VAETrainer - INFO - Epoch [33/100] Train TotalLoss: 29.3386, (ReconX: 0.9636, KLD: 0.1635*1.00, LossY: 2.8212*10.00), Time: 1.7s | Val TotalLoss: 28.5919, (ReconX: 0.9551, KLD: 0.1595, LossY: 2.7477)\n",
            "INFO:VAETrainer:Epoch [33/100] Train TotalLoss: 29.3386, (ReconX: 0.9636, KLD: 0.1635*1.00, LossY: 2.8212*10.00), Time: 1.7s | Val TotalLoss: 28.5919, (ReconX: 0.9551, KLD: 0.1595, LossY: 2.7477)\n",
            "2025-10-23 22:55:18,975 - VAETrainer - INFO - Saving model and trainer state to vae_results/best_model.pth...\n",
            "INFO:VAETrainer:Saving model and trainer state to vae_results/best_model.pth...\n",
            "2025-10-23 22:55:18,988 - VAETrainer - INFO - Successfully saved model and trainer state to vae_results/best_model.pth\n",
            "INFO:VAETrainer:Successfully saved model and trainer state to vae_results/best_model.pth\n",
            "2025-10-23 22:55:21,139 - VAETrainer - INFO - Epoch [34/100] Train TotalLoss: 28.3646, (ReconX: 0.9645, KLD: 0.1563*1.00, LossY: 2.7244*10.00), Time: 1.8s | Val TotalLoss: 27.1694, (ReconX: 0.9570, KLD: 0.1508, LossY: 2.6062)\n",
            "INFO:VAETrainer:Epoch [34/100] Train TotalLoss: 28.3646, (ReconX: 0.9645, KLD: 0.1563*1.00, LossY: 2.7244*10.00), Time: 1.8s | Val TotalLoss: 27.1694, (ReconX: 0.9570, KLD: 0.1508, LossY: 2.6062)\n",
            "2025-10-23 22:55:21,141 - VAETrainer - INFO - Saving model and trainer state to vae_results/best_model.pth...\n",
            "INFO:VAETrainer:Saving model and trainer state to vae_results/best_model.pth...\n",
            "2025-10-23 22:55:21,154 - VAETrainer - INFO - Successfully saved model and trainer state to vae_results/best_model.pth\n",
            "INFO:VAETrainer:Successfully saved model and trainer state to vae_results/best_model.pth\n",
            "2025-10-23 22:55:23,213 - VAETrainer - INFO - Epoch [35/100] Train TotalLoss: 27.5638, (ReconX: 0.9643, KLD: 0.1553*1.00, LossY: 2.6444*10.00), Time: 1.7s | Val TotalLoss: 27.0606, (ReconX: 0.9556, KLD: 0.1553, LossY: 2.5950)\n",
            "INFO:VAETrainer:Epoch [35/100] Train TotalLoss: 27.5638, (ReconX: 0.9643, KLD: 0.1553*1.00, LossY: 2.6444*10.00), Time: 1.7s | Val TotalLoss: 27.0606, (ReconX: 0.9556, KLD: 0.1553, LossY: 2.5950)\n",
            "2025-10-23 22:55:23,215 - VAETrainer - INFO - Saving model and trainer state to vae_results/best_model.pth...\n",
            "INFO:VAETrainer:Saving model and trainer state to vae_results/best_model.pth...\n",
            "2025-10-23 22:55:23,228 - VAETrainer - INFO - Successfully saved model and trainer state to vae_results/best_model.pth\n",
            "INFO:VAETrainer:Successfully saved model and trainer state to vae_results/best_model.pth\n",
            "2025-10-23 22:55:25,454 - VAETrainer - INFO - Epoch [36/100] Train TotalLoss: 26.9736, (ReconX: 0.9643, KLD: 0.1552*1.00, LossY: 2.5854*10.00), Time: 1.9s | Val TotalLoss: 26.2133, (ReconX: 0.9551, KLD: 0.1562, LossY: 2.5102)\n",
            "INFO:VAETrainer:Epoch [36/100] Train TotalLoss: 26.9736, (ReconX: 0.9643, KLD: 0.1552*1.00, LossY: 2.5854*10.00), Time: 1.9s | Val TotalLoss: 26.2133, (ReconX: 0.9551, KLD: 0.1562, LossY: 2.5102)\n",
            "2025-10-23 22:55:25,456 - VAETrainer - INFO - Saving model and trainer state to vae_results/best_model.pth...\n",
            "INFO:VAETrainer:Saving model and trainer state to vae_results/best_model.pth...\n",
            "2025-10-23 22:55:25,469 - VAETrainer - INFO - Successfully saved model and trainer state to vae_results/best_model.pth\n",
            "INFO:VAETrainer:Successfully saved model and trainer state to vae_results/best_model.pth\n",
            "2025-10-23 22:55:27,934 - VAETrainer - INFO - Epoch [37/100] Train TotalLoss: 26.3929, (ReconX: 0.9645, KLD: 0.1559*1.00, LossY: 2.5273*10.00), Time: 2.1s | Val TotalLoss: 25.7073, (ReconX: 0.9541, KLD: 0.1568, LossY: 2.4596)\n",
            "INFO:VAETrainer:Epoch [37/100] Train TotalLoss: 26.3929, (ReconX: 0.9645, KLD: 0.1559*1.00, LossY: 2.5273*10.00), Time: 2.1s | Val TotalLoss: 25.7073, (ReconX: 0.9541, KLD: 0.1568, LossY: 2.4596)\n",
            "2025-10-23 22:55:27,935 - VAETrainer - INFO - Saving model and trainer state to vae_results/best_model.pth...\n",
            "INFO:VAETrainer:Saving model and trainer state to vae_results/best_model.pth...\n",
            "2025-10-23 22:55:27,948 - VAETrainer - INFO - Successfully saved model and trainer state to vae_results/best_model.pth\n",
            "INFO:VAETrainer:Successfully saved model and trainer state to vae_results/best_model.pth\n",
            "2025-10-23 22:55:30,120 - VAETrainer - INFO - Epoch [38/100] Train TotalLoss: 25.8474, (ReconX: 0.9647, KLD: 0.1559*1.00, LossY: 2.4727*10.00), Time: 1.8s | Val TotalLoss: 25.0823, (ReconX: 0.9556, KLD: 0.1516, LossY: 2.3975)\n",
            "INFO:VAETrainer:Epoch [38/100] Train TotalLoss: 25.8474, (ReconX: 0.9647, KLD: 0.1559*1.00, LossY: 2.4727*10.00), Time: 1.8s | Val TotalLoss: 25.0823, (ReconX: 0.9556, KLD: 0.1516, LossY: 2.3975)\n",
            "2025-10-23 22:55:30,122 - VAETrainer - INFO - Saving model and trainer state to vae_results/best_model.pth...\n",
            "INFO:VAETrainer:Saving model and trainer state to vae_results/best_model.pth...\n",
            "2025-10-23 22:55:30,135 - VAETrainer - INFO - Successfully saved model and trainer state to vae_results/best_model.pth\n",
            "INFO:VAETrainer:Successfully saved model and trainer state to vae_results/best_model.pth\n",
            "2025-10-23 22:55:32,610 - VAETrainer - INFO - Epoch [39/100] Train TotalLoss: 25.1516, (ReconX: 0.9647, KLD: 0.1512*1.00, LossY: 2.4036*10.00), Time: 2.1s | Val TotalLoss: 24.4836, (ReconX: 0.9590, KLD: 0.1456, LossY: 2.3379)\n",
            "INFO:VAETrainer:Epoch [39/100] Train TotalLoss: 25.1516, (ReconX: 0.9647, KLD: 0.1512*1.00, LossY: 2.4036*10.00), Time: 2.1s | Val TotalLoss: 24.4836, (ReconX: 0.9590, KLD: 0.1456, LossY: 2.3379)\n",
            "2025-10-23 22:55:32,612 - VAETrainer - INFO - Saving model and trainer state to vae_results/best_model.pth...\n",
            "INFO:VAETrainer:Saving model and trainer state to vae_results/best_model.pth...\n",
            "2025-10-23 22:55:32,625 - VAETrainer - INFO - Successfully saved model and trainer state to vae_results/best_model.pth\n",
            "INFO:VAETrainer:Successfully saved model and trainer state to vae_results/best_model.pth\n",
            "2025-10-23 22:55:35,048 - VAETrainer - INFO - Epoch [40/100] Train TotalLoss: 23.8998, (ReconX: 0.9668, KLD: 0.1430*1.00, LossY: 2.2790*10.00), Time: 2.0s | Val TotalLoss: 23.1799, (ReconX: 0.9615, KLD: 0.1384, LossY: 2.2080)\n",
            "INFO:VAETrainer:Epoch [40/100] Train TotalLoss: 23.8998, (ReconX: 0.9668, KLD: 0.1430*1.00, LossY: 2.2790*10.00), Time: 2.0s | Val TotalLoss: 23.1799, (ReconX: 0.9615, KLD: 0.1384, LossY: 2.2080)\n",
            "2025-10-23 22:55:35,050 - VAETrainer - INFO - Saving model and trainer state to vae_results/best_model.pth...\n",
            "INFO:VAETrainer:Saving model and trainer state to vae_results/best_model.pth...\n",
            "2025-10-23 22:55:35,064 - VAETrainer - INFO - Successfully saved model and trainer state to vae_results/best_model.pth\n",
            "INFO:VAETrainer:Successfully saved model and trainer state to vae_results/best_model.pth\n",
            "2025-10-23 22:55:37,440 - VAETrainer - INFO - Epoch [41/100] Train TotalLoss: 22.3983, (ReconX: 0.9673, KLD: 0.1400*1.00, LossY: 2.1291*10.00), Time: 2.0s | Val TotalLoss: 21.2902, (ReconX: 0.9604, KLD: 0.1446, LossY: 2.0185)\n",
            "INFO:VAETrainer:Epoch [41/100] Train TotalLoss: 22.3983, (ReconX: 0.9673, KLD: 0.1400*1.00, LossY: 2.1291*10.00), Time: 2.0s | Val TotalLoss: 21.2902, (ReconX: 0.9604, KLD: 0.1446, LossY: 2.0185)\n",
            "2025-10-23 22:55:37,442 - VAETrainer - INFO - Saving model and trainer state to vae_results/best_model.pth...\n",
            "INFO:VAETrainer:Saving model and trainer state to vae_results/best_model.pth...\n",
            "2025-10-23 22:55:37,455 - VAETrainer - INFO - Successfully saved model and trainer state to vae_results/best_model.pth\n",
            "INFO:VAETrainer:Successfully saved model and trainer state to vae_results/best_model.pth\n",
            "2025-10-23 22:55:39,839 - VAETrainer - INFO - Epoch [42/100] Train TotalLoss: 20.6515, (ReconX: 0.9679, KLD: 0.1426*1.00, LossY: 1.9541*10.00), Time: 2.0s | Val TotalLoss: 19.9660, (ReconX: 0.9579, KLD: 0.1460, LossY: 1.8862)\n",
            "INFO:VAETrainer:Epoch [42/100] Train TotalLoss: 20.6515, (ReconX: 0.9679, KLD: 0.1426*1.00, LossY: 1.9541*10.00), Time: 2.0s | Val TotalLoss: 19.9660, (ReconX: 0.9579, KLD: 0.1460, LossY: 1.8862)\n",
            "2025-10-23 22:55:39,841 - VAETrainer - INFO - Saving model and trainer state to vae_results/best_model.pth...\n",
            "INFO:VAETrainer:Saving model and trainer state to vae_results/best_model.pth...\n",
            "2025-10-23 22:55:39,854 - VAETrainer - INFO - Successfully saved model and trainer state to vae_results/best_model.pth\n",
            "INFO:VAETrainer:Successfully saved model and trainer state to vae_results/best_model.pth\n",
            "2025-10-23 22:55:41,898 - VAETrainer - INFO - Epoch [43/100] Train TotalLoss: 19.4524, (ReconX: 0.9686, KLD: 0.1427*1.00, LossY: 1.8341*10.00), Time: 1.7s | Val TotalLoss: 18.7113, (ReconX: 0.9629, KLD: 0.1375, LossY: 1.7611)\n",
            "INFO:VAETrainer:Epoch [43/100] Train TotalLoss: 19.4524, (ReconX: 0.9686, KLD: 0.1427*1.00, LossY: 1.8341*10.00), Time: 1.7s | Val TotalLoss: 18.7113, (ReconX: 0.9629, KLD: 0.1375, LossY: 1.7611)\n",
            "2025-10-23 22:55:41,900 - VAETrainer - INFO - Saving model and trainer state to vae_results/best_model.pth...\n",
            "INFO:VAETrainer:Saving model and trainer state to vae_results/best_model.pth...\n",
            "2025-10-23 22:55:41,917 - VAETrainer - INFO - Successfully saved model and trainer state to vae_results/best_model.pth\n",
            "INFO:VAETrainer:Successfully saved model and trainer state to vae_results/best_model.pth\n",
            "2025-10-23 22:55:44,071 - VAETrainer - INFO - Epoch [44/100] Train TotalLoss: 18.5039, (ReconX: 0.9687, KLD: 0.1397*1.00, LossY: 1.7395*10.00), Time: 1.8s | Val TotalLoss: 17.5361, (ReconX: 0.9606, KLD: 0.1341, LossY: 1.6441)\n",
            "INFO:VAETrainer:Epoch [44/100] Train TotalLoss: 18.5039, (ReconX: 0.9687, KLD: 0.1397*1.00, LossY: 1.7395*10.00), Time: 1.8s | Val TotalLoss: 17.5361, (ReconX: 0.9606, KLD: 0.1341, LossY: 1.6441)\n",
            "2025-10-23 22:55:44,073 - VAETrainer - INFO - Saving model and trainer state to vae_results/best_model.pth...\n",
            "INFO:VAETrainer:Saving model and trainer state to vae_results/best_model.pth...\n",
            "2025-10-23 22:55:44,086 - VAETrainer - INFO - Successfully saved model and trainer state to vae_results/best_model.pth\n",
            "INFO:VAETrainer:Successfully saved model and trainer state to vae_results/best_model.pth\n",
            "2025-10-23 22:55:46,153 - VAETrainer - INFO - Epoch [45/100] Train TotalLoss: 18.0216, (ReconX: 0.9698, KLD: 0.1352*1.00, LossY: 1.6917*10.00), Time: 1.7s | Val TotalLoss: 17.2381, (ReconX: 0.9649, KLD: 0.1304, LossY: 1.6143)\n",
            "INFO:VAETrainer:Epoch [45/100] Train TotalLoss: 18.0216, (ReconX: 0.9698, KLD: 0.1352*1.00, LossY: 1.6917*10.00), Time: 1.7s | Val TotalLoss: 17.2381, (ReconX: 0.9649, KLD: 0.1304, LossY: 1.6143)\n",
            "2025-10-23 22:55:46,155 - VAETrainer - INFO - Saving model and trainer state to vae_results/best_model.pth...\n",
            "INFO:VAETrainer:Saving model and trainer state to vae_results/best_model.pth...\n",
            "2025-10-23 22:55:46,168 - VAETrainer - INFO - Successfully saved model and trainer state to vae_results/best_model.pth\n",
            "INFO:VAETrainer:Successfully saved model and trainer state to vae_results/best_model.pth\n",
            "2025-10-23 22:55:48,326 - VAETrainer - INFO - Epoch [46/100] Train TotalLoss: 17.5005, (ReconX: 0.9709, KLD: 0.1289*1.00, LossY: 1.6401*10.00), Time: 1.8s | Val TotalLoss: 16.4204, (ReconX: 0.9621, KLD: 0.1278, LossY: 1.5331)\n",
            "INFO:VAETrainer:Epoch [46/100] Train TotalLoss: 17.5005, (ReconX: 0.9709, KLD: 0.1289*1.00, LossY: 1.6401*10.00), Time: 1.8s | Val TotalLoss: 16.4204, (ReconX: 0.9621, KLD: 0.1278, LossY: 1.5331)\n",
            "2025-10-23 22:55:48,328 - VAETrainer - INFO - Saving model and trainer state to vae_results/best_model.pth...\n",
            "INFO:VAETrainer:Saving model and trainer state to vae_results/best_model.pth...\n",
            "2025-10-23 22:55:48,341 - VAETrainer - INFO - Successfully saved model and trainer state to vae_results/best_model.pth\n",
            "INFO:VAETrainer:Successfully saved model and trainer state to vae_results/best_model.pth\n",
            "2025-10-23 22:55:50,481 - VAETrainer - INFO - Epoch [47/100] Train TotalLoss: 17.0668, (ReconX: 0.9724, KLD: 0.1224*1.00, LossY: 1.5972*10.00), Time: 1.8s | Val TotalLoss: 15.9459, (ReconX: 0.9658, KLD: 0.1153, LossY: 1.4865)\n",
            "INFO:VAETrainer:Epoch [47/100] Train TotalLoss: 17.0668, (ReconX: 0.9724, KLD: 0.1224*1.00, LossY: 1.5972*10.00), Time: 1.8s | Val TotalLoss: 15.9459, (ReconX: 0.9658, KLD: 0.1153, LossY: 1.4865)\n",
            "2025-10-23 22:55:50,482 - VAETrainer - INFO - Saving model and trainer state to vae_results/best_model.pth...\n",
            "INFO:VAETrainer:Saving model and trainer state to vae_results/best_model.pth...\n",
            "2025-10-23 22:55:50,500 - VAETrainer - INFO - Successfully saved model and trainer state to vae_results/best_model.pth\n",
            "INFO:VAETrainer:Successfully saved model and trainer state to vae_results/best_model.pth\n",
            "2025-10-23 22:55:52,638 - VAETrainer - INFO - Epoch [48/100] Train TotalLoss: 16.8184, (ReconX: 0.9742, KLD: 0.1158*1.00, LossY: 1.5728*10.00), Time: 1.7s | Val TotalLoss: 15.8595, (ReconX: 0.9661, KLD: 0.1159, LossY: 1.4777)\n",
            "INFO:VAETrainer:Epoch [48/100] Train TotalLoss: 16.8184, (ReconX: 0.9742, KLD: 0.1158*1.00, LossY: 1.5728*10.00), Time: 1.7s | Val TotalLoss: 15.8595, (ReconX: 0.9661, KLD: 0.1159, LossY: 1.4777)\n",
            "2025-10-23 22:55:52,640 - VAETrainer - INFO - Saving model and trainer state to vae_results/best_model.pth...\n",
            "INFO:VAETrainer:Saving model and trainer state to vae_results/best_model.pth...\n",
            "2025-10-23 22:55:52,653 - VAETrainer - INFO - Successfully saved model and trainer state to vae_results/best_model.pth\n",
            "INFO:VAETrainer:Successfully saved model and trainer state to vae_results/best_model.pth\n",
            "2025-10-23 22:55:54,839 - VAETrainer - INFO - Epoch [49/100] Train TotalLoss: 16.3865, (ReconX: 0.9752, KLD: 0.1105*1.00, LossY: 1.5301*10.00), Time: 1.8s | Val TotalLoss: 15.3695, (ReconX: 0.9684, KLD: 0.1065, LossY: 1.4295)\n",
            "INFO:VAETrainer:Epoch [49/100] Train TotalLoss: 16.3865, (ReconX: 0.9752, KLD: 0.1105*1.00, LossY: 1.5301*10.00), Time: 1.8s | Val TotalLoss: 15.3695, (ReconX: 0.9684, KLD: 0.1065, LossY: 1.4295)\n",
            "2025-10-23 22:55:54,841 - VAETrainer - INFO - Saving model and trainer state to vae_results/best_model.pth...\n",
            "INFO:VAETrainer:Saving model and trainer state to vae_results/best_model.pth...\n",
            "2025-10-23 22:55:54,854 - VAETrainer - INFO - Successfully saved model and trainer state to vae_results/best_model.pth\n",
            "INFO:VAETrainer:Successfully saved model and trainer state to vae_results/best_model.pth\n",
            "2025-10-23 22:55:57,014 - VAETrainer - INFO - Epoch [50/100] Train TotalLoss: 16.3639, (ReconX: 0.9757, KLD: 0.1080*1.00, LossY: 1.5280*10.00), Time: 1.8s | Val TotalLoss: 15.2010, (ReconX: 0.9690, KLD: 0.1051, LossY: 1.4127)\n",
            "INFO:VAETrainer:Epoch [50/100] Train TotalLoss: 16.3639, (ReconX: 0.9757, KLD: 0.1080*1.00, LossY: 1.5280*10.00), Time: 1.8s | Val TotalLoss: 15.2010, (ReconX: 0.9690, KLD: 0.1051, LossY: 1.4127)\n",
            "2025-10-23 22:55:57,016 - VAETrainer - INFO - Saving model and trainer state to vae_results/best_model.pth...\n",
            "INFO:VAETrainer:Saving model and trainer state to vae_results/best_model.pth...\n",
            "2025-10-23 22:55:57,029 - VAETrainer - INFO - Successfully saved model and trainer state to vae_results/best_model.pth\n",
            "INFO:VAETrainer:Successfully saved model and trainer state to vae_results/best_model.pth\n",
            "2025-10-23 22:55:59,180 - VAETrainer - INFO - Epoch [51/100] Train TotalLoss: 16.1867, (ReconX: 0.9774, KLD: 0.1041*1.00, LossY: 1.5105*10.00), Time: 1.8s | Val TotalLoss: 16.2581, (ReconX: 0.9690, KLD: 0.0981, LossY: 1.5191)\n",
            "INFO:VAETrainer:Epoch [51/100] Train TotalLoss: 16.1867, (ReconX: 0.9774, KLD: 0.1041*1.00, LossY: 1.5105*10.00), Time: 1.8s | Val TotalLoss: 16.2581, (ReconX: 0.9690, KLD: 0.0981, LossY: 1.5191)\n",
            "2025-10-23 22:56:01,307 - VAETrainer - INFO - Epoch [52/100] Train TotalLoss: 16.1412, (ReconX: 0.9775, KLD: 0.1015*1.00, LossY: 1.5062*10.00), Time: 1.8s | Val TotalLoss: 14.9658, (ReconX: 0.9703, KLD: 0.0979, LossY: 1.3898)\n",
            "INFO:VAETrainer:Epoch [52/100] Train TotalLoss: 16.1412, (ReconX: 0.9775, KLD: 0.1015*1.00, LossY: 1.5062*10.00), Time: 1.8s | Val TotalLoss: 14.9658, (ReconX: 0.9703, KLD: 0.0979, LossY: 1.3898)\n",
            "2025-10-23 22:56:01,309 - VAETrainer - INFO - Saving model and trainer state to vae_results/best_model.pth...\n",
            "INFO:VAETrainer:Saving model and trainer state to vae_results/best_model.pth...\n",
            "2025-10-23 22:56:01,323 - VAETrainer - INFO - Successfully saved model and trainer state to vae_results/best_model.pth\n",
            "INFO:VAETrainer:Successfully saved model and trainer state to vae_results/best_model.pth\n",
            "2025-10-23 22:56:03,764 - VAETrainer - INFO - Epoch [53/100] Train TotalLoss: 15.8798, (ReconX: 0.9783, KLD: 0.0981*1.00, LossY: 1.4803*10.00), Time: 2.1s | Val TotalLoss: 15.0991, (ReconX: 0.9707, KLD: 0.0969, LossY: 1.4032)\n",
            "INFO:VAETrainer:Epoch [53/100] Train TotalLoss: 15.8798, (ReconX: 0.9783, KLD: 0.0981*1.00, LossY: 1.4803*10.00), Time: 2.1s | Val TotalLoss: 15.0991, (ReconX: 0.9707, KLD: 0.0969, LossY: 1.4032)\n",
            "2025-10-23 22:56:05,908 - VAETrainer - INFO - Epoch [54/100] Train TotalLoss: 15.9291, (ReconX: 0.9786, KLD: 0.0949*1.00, LossY: 1.4856*10.00), Time: 1.8s | Val TotalLoss: 15.7341, (ReconX: 0.9719, KLD: 0.0955, LossY: 1.4667)\n",
            "INFO:VAETrainer:Epoch [54/100] Train TotalLoss: 15.9291, (ReconX: 0.9786, KLD: 0.0949*1.00, LossY: 1.4856*10.00), Time: 1.8s | Val TotalLoss: 15.7341, (ReconX: 0.9719, KLD: 0.0955, LossY: 1.4667)\n",
            "2025-10-23 22:56:08,083 - VAETrainer - INFO - Epoch [55/100] Train TotalLoss: 15.4842, (ReconX: 0.9797, KLD: 0.0925*1.00, LossY: 1.4412*10.00), Time: 1.8s | Val TotalLoss: 14.6922, (ReconX: 0.9717, KLD: 0.0906, LossY: 1.3630)\n",
            "INFO:VAETrainer:Epoch [55/100] Train TotalLoss: 15.4842, (ReconX: 0.9797, KLD: 0.0925*1.00, LossY: 1.4412*10.00), Time: 1.8s | Val TotalLoss: 14.6922, (ReconX: 0.9717, KLD: 0.0906, LossY: 1.3630)\n",
            "2025-10-23 22:56:08,085 - VAETrainer - INFO - Saving model and trainer state to vae_results/best_model.pth...\n",
            "INFO:VAETrainer:Saving model and trainer state to vae_results/best_model.pth...\n",
            "2025-10-23 22:56:08,098 - VAETrainer - INFO - Successfully saved model and trainer state to vae_results/best_model.pth\n",
            "INFO:VAETrainer:Successfully saved model and trainer state to vae_results/best_model.pth\n",
            "2025-10-23 22:56:10,209 - VAETrainer - INFO - Epoch [56/100] Train TotalLoss: 15.5004, (ReconX: 0.9809, KLD: 0.0882*1.00, LossY: 1.4431*10.00), Time: 1.7s | Val TotalLoss: 14.5912, (ReconX: 0.9726, KLD: 0.0861, LossY: 1.3533)\n",
            "INFO:VAETrainer:Epoch [56/100] Train TotalLoss: 15.5004, (ReconX: 0.9809, KLD: 0.0882*1.00, LossY: 1.4431*10.00), Time: 1.7s | Val TotalLoss: 14.5912, (ReconX: 0.9726, KLD: 0.0861, LossY: 1.3533)\n",
            "2025-10-23 22:56:10,211 - VAETrainer - INFO - Saving model and trainer state to vae_results/best_model.pth...\n",
            "INFO:VAETrainer:Saving model and trainer state to vae_results/best_model.pth...\n",
            "2025-10-23 22:56:10,224 - VAETrainer - INFO - Successfully saved model and trainer state to vae_results/best_model.pth\n",
            "INFO:VAETrainer:Successfully saved model and trainer state to vae_results/best_model.pth\n",
            "2025-10-23 22:56:12,324 - VAETrainer - INFO - Epoch [57/100] Train TotalLoss: 15.6018, (ReconX: 0.9819, KLD: 0.0867*1.00, LossY: 1.4533*10.00), Time: 1.7s | Val TotalLoss: 14.9389, (ReconX: 0.9742, KLD: 0.0832, LossY: 1.3882)\n",
            "INFO:VAETrainer:Epoch [57/100] Train TotalLoss: 15.6018, (ReconX: 0.9819, KLD: 0.0867*1.00, LossY: 1.4533*10.00), Time: 1.7s | Val TotalLoss: 14.9389, (ReconX: 0.9742, KLD: 0.0832, LossY: 1.3882)\n",
            "2025-10-23 22:56:14,443 - VAETrainer - INFO - Epoch [58/100] Train TotalLoss: 15.1691, (ReconX: 0.9816, KLD: 0.0847*1.00, LossY: 1.4103*10.00), Time: 1.8s | Val TotalLoss: 14.4992, (ReconX: 0.9743, KLD: 0.0845, LossY: 1.3440)\n",
            "INFO:VAETrainer:Epoch [58/100] Train TotalLoss: 15.1691, (ReconX: 0.9816, KLD: 0.0847*1.00, LossY: 1.4103*10.00), Time: 1.8s | Val TotalLoss: 14.4992, (ReconX: 0.9743, KLD: 0.0845, LossY: 1.3440)\n",
            "2025-10-23 22:56:14,444 - VAETrainer - INFO - Saving model and trainer state to vae_results/best_model.pth...\n",
            "INFO:VAETrainer:Saving model and trainer state to vae_results/best_model.pth...\n",
            "2025-10-23 22:56:14,457 - VAETrainer - INFO - Successfully saved model and trainer state to vae_results/best_model.pth\n",
            "INFO:VAETrainer:Successfully saved model and trainer state to vae_results/best_model.pth\n",
            "2025-10-23 22:56:16,885 - VAETrainer - INFO - Epoch [59/100] Train TotalLoss: 15.1788, (ReconX: 0.9820, KLD: 0.0831*1.00, LossY: 1.4114*10.00), Time: 2.0s | Val TotalLoss: 14.6400, (ReconX: 0.9731, KLD: 0.0847, LossY: 1.3582)\n",
            "INFO:VAETrainer:Epoch [59/100] Train TotalLoss: 15.1788, (ReconX: 0.9820, KLD: 0.0831*1.00, LossY: 1.4114*10.00), Time: 2.0s | Val TotalLoss: 14.6400, (ReconX: 0.9731, KLD: 0.0847, LossY: 1.3582)\n",
            "2025-10-23 22:56:19,399 - VAETrainer - INFO - Epoch [60/100] Train TotalLoss: 15.7241, (ReconX: 0.9824, KLD: 0.0822*1.00, LossY: 1.4659*10.00), Time: 2.1s | Val TotalLoss: 14.7742, (ReconX: 0.9718, KLD: 0.0886, LossY: 1.3714)\n",
            "INFO:VAETrainer:Epoch [60/100] Train TotalLoss: 15.7241, (ReconX: 0.9824, KLD: 0.0822*1.00, LossY: 1.4659*10.00), Time: 2.1s | Val TotalLoss: 14.7742, (ReconX: 0.9718, KLD: 0.0886, LossY: 1.3714)\n",
            "2025-10-23 22:56:21,519 - VAETrainer - INFO - Epoch [61/100] Train TotalLoss: 15.1403, (ReconX: 0.9813, KLD: 0.0849*1.00, LossY: 1.4074*10.00), Time: 1.7s | Val TotalLoss: 14.0122, (ReconX: 0.9738, KLD: 0.0811, LossY: 1.2957)\n",
            "INFO:VAETrainer:Epoch [61/100] Train TotalLoss: 15.1403, (ReconX: 0.9813, KLD: 0.0849*1.00, LossY: 1.4074*10.00), Time: 1.7s | Val TotalLoss: 14.0122, (ReconX: 0.9738, KLD: 0.0811, LossY: 1.2957)\n",
            "2025-10-23 22:56:21,520 - VAETrainer - INFO - Saving model and trainer state to vae_results/best_model.pth...\n",
            "INFO:VAETrainer:Saving model and trainer state to vae_results/best_model.pth...\n",
            "2025-10-23 22:56:21,534 - VAETrainer - INFO - Successfully saved model and trainer state to vae_results/best_model.pth\n",
            "INFO:VAETrainer:Successfully saved model and trainer state to vae_results/best_model.pth\n",
            "2025-10-23 22:56:23,649 - VAETrainer - INFO - Epoch [62/100] Train TotalLoss: 14.9017, (ReconX: 0.9827, KLD: 0.0815*1.00, LossY: 1.3837*10.00), Time: 1.7s | Val TotalLoss: 14.2367, (ReconX: 0.9774, KLD: 0.0764, LossY: 1.3183)\n",
            "INFO:VAETrainer:Epoch [62/100] Train TotalLoss: 14.9017, (ReconX: 0.9827, KLD: 0.0815*1.00, LossY: 1.3837*10.00), Time: 1.7s | Val TotalLoss: 14.2367, (ReconX: 0.9774, KLD: 0.0764, LossY: 1.3183)\n",
            "2025-10-23 22:56:25,783 - VAETrainer - INFO - Epoch [63/100] Train TotalLoss: 15.3385, (ReconX: 0.9830, KLD: 0.0791*1.00, LossY: 1.4276*10.00), Time: 1.8s | Val TotalLoss: 14.5663, (ReconX: 0.9736, KLD: 0.0819, LossY: 1.3511)\n",
            "INFO:VAETrainer:Epoch [63/100] Train TotalLoss: 15.3385, (ReconX: 0.9830, KLD: 0.0791*1.00, LossY: 1.4276*10.00), Time: 1.8s | Val TotalLoss: 14.5663, (ReconX: 0.9736, KLD: 0.0819, LossY: 1.3511)\n",
            "2025-10-23 22:56:27,916 - VAETrainer - INFO - Epoch [64/100] Train TotalLoss: 15.1735, (ReconX: 0.9827, KLD: 0.0804*1.00, LossY: 1.4110*10.00), Time: 1.8s | Val TotalLoss: 14.1635, (ReconX: 0.9760, KLD: 0.0806, LossY: 1.3107)\n",
            "INFO:VAETrainer:Epoch [64/100] Train TotalLoss: 15.1735, (ReconX: 0.9827, KLD: 0.0804*1.00, LossY: 1.4110*10.00), Time: 1.8s | Val TotalLoss: 14.1635, (ReconX: 0.9760, KLD: 0.0806, LossY: 1.3107)\n",
            "2025-10-23 22:56:30,008 - VAETrainer - INFO - Epoch [65/100] Train TotalLoss: 14.6925, (ReconX: 0.9841, KLD: 0.0783*1.00, LossY: 1.3630*10.00), Time: 1.7s | Val TotalLoss: 13.9865, (ReconX: 0.9753, KLD: 0.0752, LossY: 1.2936)\n",
            "INFO:VAETrainer:Epoch [65/100] Train TotalLoss: 14.6925, (ReconX: 0.9841, KLD: 0.0783*1.00, LossY: 1.3630*10.00), Time: 1.7s | Val TotalLoss: 13.9865, (ReconX: 0.9753, KLD: 0.0752, LossY: 1.2936)\n",
            "2025-10-23 22:56:30,010 - VAETrainer - INFO - Saving model and trainer state to vae_results/best_model.pth...\n",
            "INFO:VAETrainer:Saving model and trainer state to vae_results/best_model.pth...\n",
            "2025-10-23 22:56:30,024 - VAETrainer - INFO - Successfully saved model and trainer state to vae_results/best_model.pth\n",
            "INFO:VAETrainer:Successfully saved model and trainer state to vae_results/best_model.pth\n",
            "2025-10-23 22:56:32,221 - VAETrainer - INFO - Epoch [66/100] Train TotalLoss: 14.6495, (ReconX: 0.9840, KLD: 0.0766*1.00, LossY: 1.3589*10.00), Time: 1.8s | Val TotalLoss: 14.0428, (ReconX: 0.9764, KLD: 0.0743, LossY: 1.2992)\n",
            "INFO:VAETrainer:Epoch [66/100] Train TotalLoss: 14.6495, (ReconX: 0.9840, KLD: 0.0766*1.00, LossY: 1.3589*10.00), Time: 1.8s | Val TotalLoss: 14.0428, (ReconX: 0.9764, KLD: 0.0743, LossY: 1.2992)\n",
            "2025-10-23 22:56:34,452 - VAETrainer - INFO - Epoch [67/100] Train TotalLoss: 14.6449, (ReconX: 0.9838, KLD: 0.0760*1.00, LossY: 1.3585*10.00), Time: 1.9s | Val TotalLoss: 13.5891, (ReconX: 0.9757, KLD: 0.0754, LossY: 1.2538)\n",
            "INFO:VAETrainer:Epoch [67/100] Train TotalLoss: 14.6449, (ReconX: 0.9838, KLD: 0.0760*1.00, LossY: 1.3585*10.00), Time: 1.9s | Val TotalLoss: 13.5891, (ReconX: 0.9757, KLD: 0.0754, LossY: 1.2538)\n",
            "2025-10-23 22:56:34,454 - VAETrainer - INFO - Saving model and trainer state to vae_results/best_model.pth...\n",
            "INFO:VAETrainer:Saving model and trainer state to vae_results/best_model.pth...\n",
            "2025-10-23 22:56:34,467 - VAETrainer - INFO - Successfully saved model and trainer state to vae_results/best_model.pth\n",
            "INFO:VAETrainer:Successfully saved model and trainer state to vae_results/best_model.pth\n",
            "2025-10-23 22:56:36,796 - VAETrainer - INFO - Epoch [68/100] Train TotalLoss: 14.6890, (ReconX: 0.9849, KLD: 0.0745*1.00, LossY: 1.3630*10.00), Time: 1.9s | Val TotalLoss: 13.7523, (ReconX: 0.9773, KLD: 0.0763, LossY: 1.2699)\n",
            "INFO:VAETrainer:Epoch [68/100] Train TotalLoss: 14.6890, (ReconX: 0.9849, KLD: 0.0745*1.00, LossY: 1.3630*10.00), Time: 1.9s | Val TotalLoss: 13.7523, (ReconX: 0.9773, KLD: 0.0763, LossY: 1.2699)\n",
            "2025-10-23 22:56:39,042 - VAETrainer - INFO - Epoch [69/100] Train TotalLoss: 14.4765, (ReconX: 0.9843, KLD: 0.0725*1.00, LossY: 1.3420*10.00), Time: 1.9s | Val TotalLoss: 13.7651, (ReconX: 0.9766, KLD: 0.0740, LossY: 1.2715)\n",
            "INFO:VAETrainer:Epoch [69/100] Train TotalLoss: 14.4765, (ReconX: 0.9843, KLD: 0.0725*1.00, LossY: 1.3420*10.00), Time: 1.9s | Val TotalLoss: 13.7651, (ReconX: 0.9766, KLD: 0.0740, LossY: 1.2715)\n",
            "2025-10-23 22:56:41,246 - VAETrainer - INFO - Epoch [70/100] Train TotalLoss: 14.6416, (ReconX: 0.9849, KLD: 0.0744*1.00, LossY: 1.3582*10.00), Time: 1.8s | Val TotalLoss: 13.8928, (ReconX: 0.9759, KLD: 0.0727, LossY: 1.2844)\n",
            "INFO:VAETrainer:Epoch [70/100] Train TotalLoss: 14.6416, (ReconX: 0.9849, KLD: 0.0744*1.00, LossY: 1.3582*10.00), Time: 1.8s | Val TotalLoss: 13.8928, (ReconX: 0.9759, KLD: 0.0727, LossY: 1.2844)\n",
            "2025-10-23 22:56:43,365 - VAETrainer - INFO - Epoch [71/100] Train TotalLoss: 14.3664, (ReconX: 0.9850, KLD: 0.0732*1.00, LossY: 1.3308*10.00), Time: 1.7s | Val TotalLoss: 13.4195, (ReconX: 0.9787, KLD: 0.0698, LossY: 1.2371)\n",
            "INFO:VAETrainer:Epoch [71/100] Train TotalLoss: 14.3664, (ReconX: 0.9850, KLD: 0.0732*1.00, LossY: 1.3308*10.00), Time: 1.7s | Val TotalLoss: 13.4195, (ReconX: 0.9787, KLD: 0.0698, LossY: 1.2371)\n",
            "2025-10-23 22:56:43,367 - VAETrainer - INFO - Saving model and trainer state to vae_results/best_model.pth...\n",
            "INFO:VAETrainer:Saving model and trainer state to vae_results/best_model.pth...\n",
            "2025-10-23 22:56:43,382 - VAETrainer - INFO - Successfully saved model and trainer state to vae_results/best_model.pth\n",
            "INFO:VAETrainer:Successfully saved model and trainer state to vae_results/best_model.pth\n",
            "2025-10-23 22:56:45,647 - VAETrainer - INFO - Epoch [72/100] Train TotalLoss: 14.2858, (ReconX: 0.9838, KLD: 0.0736*1.00, LossY: 1.3228*10.00), Time: 1.9s | Val TotalLoss: 13.3732, (ReconX: 0.9782, KLD: 0.0698, LossY: 1.2325)\n",
            "INFO:VAETrainer:Epoch [72/100] Train TotalLoss: 14.2858, (ReconX: 0.9838, KLD: 0.0736*1.00, LossY: 1.3228*10.00), Time: 1.9s | Val TotalLoss: 13.3732, (ReconX: 0.9782, KLD: 0.0698, LossY: 1.2325)\n",
            "2025-10-23 22:56:45,649 - VAETrainer - INFO - Saving model and trainer state to vae_results/best_model.pth...\n",
            "INFO:VAETrainer:Saving model and trainer state to vae_results/best_model.pth...\n",
            "2025-10-23 22:56:45,662 - VAETrainer - INFO - Successfully saved model and trainer state to vae_results/best_model.pth\n",
            "INFO:VAETrainer:Successfully saved model and trainer state to vae_results/best_model.pth\n",
            "2025-10-23 22:56:47,819 - VAETrainer - INFO - Epoch [73/100] Train TotalLoss: 14.5437, (ReconX: 0.9852, KLD: 0.0714*1.00, LossY: 1.3487*10.00), Time: 1.8s | Val TotalLoss: 13.4045, (ReconX: 0.9780, KLD: 0.0693, LossY: 1.2357)\n",
            "INFO:VAETrainer:Epoch [73/100] Train TotalLoss: 14.5437, (ReconX: 0.9852, KLD: 0.0714*1.00, LossY: 1.3487*10.00), Time: 1.8s | Val TotalLoss: 13.4045, (ReconX: 0.9780, KLD: 0.0693, LossY: 1.2357)\n",
            "2025-10-23 22:56:49,888 - VAETrainer - INFO - Epoch [74/100] Train TotalLoss: 14.5098, (ReconX: 0.9852, KLD: 0.0712*1.00, LossY: 1.3453*10.00), Time: 1.7s | Val TotalLoss: 13.3321, (ReconX: 0.9795, KLD: 0.0684, LossY: 1.2284)\n",
            "INFO:VAETrainer:Epoch [74/100] Train TotalLoss: 14.5098, (ReconX: 0.9852, KLD: 0.0712*1.00, LossY: 1.3453*10.00), Time: 1.7s | Val TotalLoss: 13.3321, (ReconX: 0.9795, KLD: 0.0684, LossY: 1.2284)\n",
            "2025-10-23 22:56:49,890 - VAETrainer - INFO - Saving model and trainer state to vae_results/best_model.pth...\n",
            "INFO:VAETrainer:Saving model and trainer state to vae_results/best_model.pth...\n",
            "2025-10-23 22:56:49,903 - VAETrainer - INFO - Successfully saved model and trainer state to vae_results/best_model.pth\n",
            "INFO:VAETrainer:Successfully saved model and trainer state to vae_results/best_model.pth\n",
            "2025-10-23 22:56:52,119 - VAETrainer - INFO - Epoch [75/100] Train TotalLoss: 14.2601, (ReconX: 0.9851, KLD: 0.0706*1.00, LossY: 1.3204*10.00), Time: 1.8s | Val TotalLoss: 13.7509, (ReconX: 0.9770, KLD: 0.0742, LossY: 1.2700)\n",
            "INFO:VAETrainer:Epoch [75/100] Train TotalLoss: 14.2601, (ReconX: 0.9851, KLD: 0.0706*1.00, LossY: 1.3204*10.00), Time: 1.8s | Val TotalLoss: 13.7509, (ReconX: 0.9770, KLD: 0.0742, LossY: 1.2700)\n",
            "2025-10-23 22:56:54,328 - VAETrainer - INFO - Epoch [76/100] Train TotalLoss: 14.2949, (ReconX: 0.9850, KLD: 0.0711*1.00, LossY: 1.3239*10.00), Time: 1.8s | Val TotalLoss: 13.3397, (ReconX: 0.9769, KLD: 0.0703, LossY: 1.2292)\n",
            "INFO:VAETrainer:Epoch [76/100] Train TotalLoss: 14.2949, (ReconX: 0.9850, KLD: 0.0711*1.00, LossY: 1.3239*10.00), Time: 1.8s | Val TotalLoss: 13.3397, (ReconX: 0.9769, KLD: 0.0703, LossY: 1.2292)\n",
            "2025-10-23 22:56:56,542 - VAETrainer - INFO - Epoch [77/100] Train TotalLoss: 14.3293, (ReconX: 0.9852, KLD: 0.0713*1.00, LossY: 1.3273*10.00), Time: 1.8s | Val TotalLoss: 13.5547, (ReconX: 0.9800, KLD: 0.0688, LossY: 1.2506)\n",
            "INFO:VAETrainer:Epoch [77/100] Train TotalLoss: 14.3293, (ReconX: 0.9852, KLD: 0.0713*1.00, LossY: 1.3273*10.00), Time: 1.8s | Val TotalLoss: 13.5547, (ReconX: 0.9800, KLD: 0.0688, LossY: 1.2506)\n",
            "2025-10-23 22:56:58,708 - VAETrainer - INFO - Epoch [78/100] Train TotalLoss: 14.2775, (ReconX: 0.9851, KLD: 0.0698*1.00, LossY: 1.3223*10.00), Time: 1.8s | Val TotalLoss: 13.9320, (ReconX: 0.9779, KLD: 0.0681, LossY: 1.2886)\n",
            "INFO:VAETrainer:Epoch [78/100] Train TotalLoss: 14.2775, (ReconX: 0.9851, KLD: 0.0698*1.00, LossY: 1.3223*10.00), Time: 1.8s | Val TotalLoss: 13.9320, (ReconX: 0.9779, KLD: 0.0681, LossY: 1.2886)\n",
            "2025-10-23 22:57:00,836 - VAETrainer - INFO - Epoch [79/100] Train TotalLoss: 14.3602, (ReconX: 0.9857, KLD: 0.0688*1.00, LossY: 1.3306*10.00), Time: 1.8s | Val TotalLoss: 13.7726, (ReconX: 0.9798, KLD: 0.0661, LossY: 1.2727)\n",
            "INFO:VAETrainer:Epoch [79/100] Train TotalLoss: 14.3602, (ReconX: 0.9857, KLD: 0.0688*1.00, LossY: 1.3306*10.00), Time: 1.8s | Val TotalLoss: 13.7726, (ReconX: 0.9798, KLD: 0.0661, LossY: 1.2727)\n",
            "2025-10-23 22:57:02,966 - VAETrainer - INFO - Epoch [80/100] Train TotalLoss: 14.0971, (ReconX: 0.9858, KLD: 0.0681*1.00, LossY: 1.3043*10.00), Time: 1.7s | Val TotalLoss: 13.6943, (ReconX: 0.9786, KLD: 0.0683, LossY: 1.2647)\n",
            "INFO:VAETrainer:Epoch [80/100] Train TotalLoss: 14.0971, (ReconX: 0.9858, KLD: 0.0681*1.00, LossY: 1.3043*10.00), Time: 1.7s | Val TotalLoss: 13.6943, (ReconX: 0.9786, KLD: 0.0683, LossY: 1.2647)\n",
            "2025-10-23 22:57:05,110 - VAETrainer - INFO - Epoch [81/100] Train TotalLoss: 13.8177, (ReconX: 0.9848, KLD: 0.0690*1.00, LossY: 1.2764*10.00), Time: 1.8s | Val TotalLoss: 13.1967, (ReconX: 0.9782, KLD: 0.0686, LossY: 1.2150)\n",
            "INFO:VAETrainer:Epoch [81/100] Train TotalLoss: 13.8177, (ReconX: 0.9848, KLD: 0.0690*1.00, LossY: 1.2764*10.00), Time: 1.8s | Val TotalLoss: 13.1967, (ReconX: 0.9782, KLD: 0.0686, LossY: 1.2150)\n",
            "2025-10-23 22:57:05,112 - VAETrainer - INFO - Saving model and trainer state to vae_results/best_model.pth...\n",
            "INFO:VAETrainer:Saving model and trainer state to vae_results/best_model.pth...\n",
            "2025-10-23 22:57:05,125 - VAETrainer - INFO - Successfully saved model and trainer state to vae_results/best_model.pth\n",
            "INFO:VAETrainer:Successfully saved model and trainer state to vae_results/best_model.pth\n",
            "2025-10-23 22:57:07,303 - VAETrainer - INFO - Epoch [82/100] Train TotalLoss: 13.9080, (ReconX: 0.9860, KLD: 0.0682*1.00, LossY: 1.2854*10.00), Time: 1.8s | Val TotalLoss: 13.1706, (ReconX: 0.9783, KLD: 0.0686, LossY: 1.2124)\n",
            "INFO:VAETrainer:Epoch [82/100] Train TotalLoss: 13.9080, (ReconX: 0.9860, KLD: 0.0682*1.00, LossY: 1.2854*10.00), Time: 1.8s | Val TotalLoss: 13.1706, (ReconX: 0.9783, KLD: 0.0686, LossY: 1.2124)\n",
            "2025-10-23 22:57:07,306 - VAETrainer - INFO - Saving model and trainer state to vae_results/best_model.pth...\n",
            "INFO:VAETrainer:Saving model and trainer state to vae_results/best_model.pth...\n",
            "2025-10-23 22:57:07,324 - VAETrainer - INFO - Successfully saved model and trainer state to vae_results/best_model.pth\n",
            "INFO:VAETrainer:Successfully saved model and trainer state to vae_results/best_model.pth\n",
            "2025-10-23 22:57:09,501 - VAETrainer - INFO - Epoch [83/100] Train TotalLoss: 14.3200, (ReconX: 0.9855, KLD: 0.0688*1.00, LossY: 1.3266*10.00), Time: 1.8s | Val TotalLoss: 13.2118, (ReconX: 0.9795, KLD: 0.0685, LossY: 1.2164)\n",
            "INFO:VAETrainer:Epoch [83/100] Train TotalLoss: 14.3200, (ReconX: 0.9855, KLD: 0.0688*1.00, LossY: 1.3266*10.00), Time: 1.8s | Val TotalLoss: 13.2118, (ReconX: 0.9795, KLD: 0.0685, LossY: 1.2164)\n",
            "2025-10-23 22:57:11,670 - VAETrainer - INFO - Epoch [84/100] Train TotalLoss: 13.9306, (ReconX: 0.9857, KLD: 0.0688*1.00, LossY: 1.2876*10.00), Time: 1.8s | Val TotalLoss: 13.0045, (ReconX: 0.9769, KLD: 0.0685, LossY: 1.1959)\n",
            "INFO:VAETrainer:Epoch [84/100] Train TotalLoss: 13.9306, (ReconX: 0.9857, KLD: 0.0688*1.00, LossY: 1.2876*10.00), Time: 1.8s | Val TotalLoss: 13.0045, (ReconX: 0.9769, KLD: 0.0685, LossY: 1.1959)\n",
            "2025-10-23 22:57:11,672 - VAETrainer - INFO - Saving model and trainer state to vae_results/best_model.pth...\n",
            "INFO:VAETrainer:Saving model and trainer state to vae_results/best_model.pth...\n",
            "2025-10-23 22:57:11,686 - VAETrainer - INFO - Successfully saved model and trainer state to vae_results/best_model.pth\n",
            "INFO:VAETrainer:Successfully saved model and trainer state to vae_results/best_model.pth\n",
            "2025-10-23 22:57:13,765 - VAETrainer - INFO - Epoch [85/100] Train TotalLoss: 14.0654, (ReconX: 0.9857, KLD: 0.0684*1.00, LossY: 1.3011*10.00), Time: 1.7s | Val TotalLoss: 13.2952, (ReconX: 0.9767, KLD: 0.0683, LossY: 1.2250)\n",
            "INFO:VAETrainer:Epoch [85/100] Train TotalLoss: 14.0654, (ReconX: 0.9857, KLD: 0.0684*1.00, LossY: 1.3011*10.00), Time: 1.7s | Val TotalLoss: 13.2952, (ReconX: 0.9767, KLD: 0.0683, LossY: 1.2250)\n",
            "2025-10-23 22:57:15,905 - VAETrainer - INFO - Epoch [86/100] Train TotalLoss: 13.8826, (ReconX: 0.9858, KLD: 0.0682*1.00, LossY: 1.2829*10.00), Time: 1.8s | Val TotalLoss: 13.0388, (ReconX: 0.9769, KLD: 0.0672, LossY: 1.1995)\n",
            "INFO:VAETrainer:Epoch [86/100] Train TotalLoss: 13.8826, (ReconX: 0.9858, KLD: 0.0682*1.00, LossY: 1.2829*10.00), Time: 1.8s | Val TotalLoss: 13.0388, (ReconX: 0.9769, KLD: 0.0672, LossY: 1.1995)\n",
            "2025-10-23 22:57:18,071 - VAETrainer - INFO - Epoch [87/100] Train TotalLoss: 13.8089, (ReconX: 0.9855, KLD: 0.0680*1.00, LossY: 1.2755*10.00), Time: 1.8s | Val TotalLoss: 13.1017, (ReconX: 0.9771, KLD: 0.0670, LossY: 1.2058)\n",
            "INFO:VAETrainer:Epoch [87/100] Train TotalLoss: 13.8089, (ReconX: 0.9855, KLD: 0.0680*1.00, LossY: 1.2755*10.00), Time: 1.8s | Val TotalLoss: 13.1017, (ReconX: 0.9771, KLD: 0.0670, LossY: 1.2058)\n",
            "2025-10-23 22:57:20,174 - VAETrainer - INFO - Epoch [88/100] Train TotalLoss: 13.9449, (ReconX: 0.9854, KLD: 0.0678*1.00, LossY: 1.2892*10.00), Time: 1.7s | Val TotalLoss: 13.2155, (ReconX: 0.9793, KLD: 0.0681, LossY: 1.2168)\n",
            "INFO:VAETrainer:Epoch [88/100] Train TotalLoss: 13.9449, (ReconX: 0.9854, KLD: 0.0678*1.00, LossY: 1.2892*10.00), Time: 1.7s | Val TotalLoss: 13.2155, (ReconX: 0.9793, KLD: 0.0681, LossY: 1.2168)\n",
            "2025-10-23 22:57:22,320 - VAETrainer - INFO - Epoch [89/100] Train TotalLoss: 14.0941, (ReconX: 0.9858, KLD: 0.0682*1.00, LossY: 1.3040*10.00), Time: 1.8s | Val TotalLoss: 13.2231, (ReconX: 0.9794, KLD: 0.0663, LossY: 1.2177)\n",
            "INFO:VAETrainer:Epoch [89/100] Train TotalLoss: 14.0941, (ReconX: 0.9858, KLD: 0.0682*1.00, LossY: 1.3040*10.00), Time: 1.8s | Val TotalLoss: 13.2231, (ReconX: 0.9794, KLD: 0.0663, LossY: 1.2177)\n",
            "2025-10-23 22:57:24,474 - VAETrainer - INFO - Epoch [90/100] Train TotalLoss: 13.7962, (ReconX: 0.9860, KLD: 0.0676*1.00, LossY: 1.2743*10.00), Time: 1.8s | Val TotalLoss: 13.2425, (ReconX: 0.9786, KLD: 0.0683, LossY: 1.2196)\n",
            "INFO:VAETrainer:Epoch [90/100] Train TotalLoss: 13.7962, (ReconX: 0.9860, KLD: 0.0676*1.00, LossY: 1.2743*10.00), Time: 1.8s | Val TotalLoss: 13.2425, (ReconX: 0.9786, KLD: 0.0683, LossY: 1.2196)\n",
            "2025-10-23 22:57:26,573 - VAETrainer - INFO - Epoch [91/100] Train TotalLoss: 13.7794, (ReconX: 0.9863, KLD: 0.0677*1.00, LossY: 1.2725*10.00), Time: 1.7s | Val TotalLoss: 13.0387, (ReconX: 0.9781, KLD: 0.0654, LossY: 1.1995)\n",
            "INFO:VAETrainer:Epoch [91/100] Train TotalLoss: 13.7794, (ReconX: 0.9863, KLD: 0.0677*1.00, LossY: 1.2725*10.00), Time: 1.7s | Val TotalLoss: 13.0387, (ReconX: 0.9781, KLD: 0.0654, LossY: 1.1995)\n",
            "2025-10-23 22:57:28,660 - VAETrainer - INFO - Epoch [92/100] Train TotalLoss: 13.8241, (ReconX: 0.9861, KLD: 0.0678*1.00, LossY: 1.2770*10.00), Time: 1.7s | Val TotalLoss: 13.1120, (ReconX: 0.9783, KLD: 0.0667, LossY: 1.2067)\n",
            "INFO:VAETrainer:Epoch [92/100] Train TotalLoss: 13.8241, (ReconX: 0.9861, KLD: 0.0678*1.00, LossY: 1.2770*10.00), Time: 1.7s | Val TotalLoss: 13.1120, (ReconX: 0.9783, KLD: 0.0667, LossY: 1.2067)\n",
            "2025-10-23 22:57:30,817 - VAETrainer - INFO - Epoch [93/100] Train TotalLoss: 13.8738, (ReconX: 0.9860, KLD: 0.0677*1.00, LossY: 1.2820*10.00), Time: 1.8s | Val TotalLoss: 13.2013, (ReconX: 0.9775, KLD: 0.0670, LossY: 1.2157)\n",
            "INFO:VAETrainer:Epoch [93/100] Train TotalLoss: 13.8738, (ReconX: 0.9860, KLD: 0.0677*1.00, LossY: 1.2820*10.00), Time: 1.8s | Val TotalLoss: 13.2013, (ReconX: 0.9775, KLD: 0.0670, LossY: 1.2157)\n",
            "2025-10-23 22:57:32,917 - VAETrainer - INFO - Epoch [94/100] Train TotalLoss: 13.5806, (ReconX: 0.9864, KLD: 0.0676*1.00, LossY: 1.2527*10.00), Time: 1.7s | Val TotalLoss: 13.0302, (ReconX: 0.9794, KLD: 0.0670, LossY: 1.1984)\n",
            "INFO:VAETrainer:Epoch [94/100] Train TotalLoss: 13.5806, (ReconX: 0.9864, KLD: 0.0676*1.00, LossY: 1.2527*10.00), Time: 1.7s | Val TotalLoss: 13.0302, (ReconX: 0.9794, KLD: 0.0670, LossY: 1.1984)\n",
            "2025-10-23 22:57:35,009 - VAETrainer - INFO - Epoch [95/100] Train TotalLoss: 13.7146, (ReconX: 0.9852, KLD: 0.0676*1.00, LossY: 1.2662*10.00), Time: 1.7s | Val TotalLoss: 13.1032, (ReconX: 0.9795, KLD: 0.0673, LossY: 1.2056)\n",
            "INFO:VAETrainer:Epoch [95/100] Train TotalLoss: 13.7146, (ReconX: 0.9852, KLD: 0.0676*1.00, LossY: 1.2662*10.00), Time: 1.7s | Val TotalLoss: 13.1032, (ReconX: 0.9795, KLD: 0.0673, LossY: 1.2056)\n",
            "2025-10-23 22:57:37,083 - VAETrainer - INFO - Epoch [96/100] Train TotalLoss: 13.7471, (ReconX: 0.9860, KLD: 0.0675*1.00, LossY: 1.2694*10.00), Time: 1.7s | Val TotalLoss: 13.1500, (ReconX: 0.9779, KLD: 0.0679, LossY: 1.2104)\n",
            "INFO:VAETrainer:Epoch [96/100] Train TotalLoss: 13.7471, (ReconX: 0.9860, KLD: 0.0675*1.00, LossY: 1.2694*10.00), Time: 1.7s | Val TotalLoss: 13.1500, (ReconX: 0.9779, KLD: 0.0679, LossY: 1.2104)\n",
            "2025-10-23 22:57:39,291 - VAETrainer - INFO - Epoch [97/100] Train TotalLoss: 13.6831, (ReconX: 0.9860, KLD: 0.0676*1.00, LossY: 1.2630*10.00), Time: 1.8s | Val TotalLoss: 13.0851, (ReconX: 0.9781, KLD: 0.0664, LossY: 1.2041)\n",
            "INFO:VAETrainer:Epoch [97/100] Train TotalLoss: 13.6831, (ReconX: 0.9860, KLD: 0.0676*1.00, LossY: 1.2630*10.00), Time: 1.8s | Val TotalLoss: 13.0851, (ReconX: 0.9781, KLD: 0.0664, LossY: 1.2041)\n",
            "2025-10-23 22:57:41,369 - VAETrainer - INFO - Epoch [98/100] Train TotalLoss: 13.8535, (ReconX: 0.9858, KLD: 0.0676*1.00, LossY: 1.2800*10.00), Time: 1.7s | Val TotalLoss: 12.9914, (ReconX: 0.9783, KLD: 0.0662, LossY: 1.1947)\n",
            "INFO:VAETrainer:Epoch [98/100] Train TotalLoss: 13.8535, (ReconX: 0.9858, KLD: 0.0676*1.00, LossY: 1.2800*10.00), Time: 1.7s | Val TotalLoss: 12.9914, (ReconX: 0.9783, KLD: 0.0662, LossY: 1.1947)\n",
            "2025-10-23 22:57:41,371 - VAETrainer - INFO - Saving model and trainer state to vae_results/best_model.pth...\n",
            "INFO:VAETrainer:Saving model and trainer state to vae_results/best_model.pth...\n",
            "2025-10-23 22:57:41,386 - VAETrainer - INFO - Successfully saved model and trainer state to vae_results/best_model.pth\n",
            "INFO:VAETrainer:Successfully saved model and trainer state to vae_results/best_model.pth\n",
            "2025-10-23 22:57:43,452 - VAETrainer - INFO - Epoch [99/100] Train TotalLoss: 13.7989, (ReconX: 0.9854, KLD: 0.0675*1.00, LossY: 1.2746*10.00), Time: 1.7s | Val TotalLoss: 12.9351, (ReconX: 0.9787, KLD: 0.0668, LossY: 1.1890)\n",
            "INFO:VAETrainer:Epoch [99/100] Train TotalLoss: 13.7989, (ReconX: 0.9854, KLD: 0.0675*1.00, LossY: 1.2746*10.00), Time: 1.7s | Val TotalLoss: 12.9351, (ReconX: 0.9787, KLD: 0.0668, LossY: 1.1890)\n",
            "2025-10-23 22:57:43,453 - VAETrainer - INFO - Saving model and trainer state to vae_results/best_model.pth...\n",
            "INFO:VAETrainer:Saving model and trainer state to vae_results/best_model.pth...\n",
            "2025-10-23 22:57:43,471 - VAETrainer - INFO - Successfully saved model and trainer state to vae_results/best_model.pth\n",
            "INFO:VAETrainer:Successfully saved model and trainer state to vae_results/best_model.pth\n",
            "2025-10-23 22:57:45,595 - VAETrainer - INFO - Epoch [100/100] Train TotalLoss: 13.6472, (ReconX: 0.9861, KLD: 0.0675*1.00, LossY: 1.2594*10.00), Time: 1.7s | Val TotalLoss: 13.0397, (ReconX: 0.9785, KLD: 0.0657, LossY: 1.1996)\n",
            "INFO:VAETrainer:Epoch [100/100] Train TotalLoss: 13.6472, (ReconX: 0.9861, KLD: 0.0675*1.00, LossY: 1.2594*10.00), Time: 1.7s | Val TotalLoss: 13.0397, (ReconX: 0.9785, KLD: 0.0657, LossY: 1.1996)\n",
            "2025-10-23 22:57:45,597 - VAETrainer - INFO - Training loop finished.\n",
            "INFO:VAETrainer:Training loop finished.\n",
            "2025-10-23 22:57:45,598 - VAETrainer - INFO - Loading best model state from 'vae_results/best_model.pth' (Val Loss: 12.9351).\n",
            "INFO:VAETrainer:Loading best model state from 'vae_results/best_model.pth' (Val Loss: 12.9351).\n",
            "2025-10-23 22:57:45,599 - VAETrainer_Loader - INFO - Loading VAE model and trainer state from vae_results/best_model.pth to device cuda...\n",
            "INFO:VAETrainer_Loader:Loading VAE model and trainer state from vae_results/best_model.pth to device cuda...\n",
            "2025-10-23 22:57:45,637 - SemiSupMIWAE(X:42,Y:1,Lat:3) - INFO - SemiSupMIWAE Initialized: InputXDim=42, TargetYDim=1, LatentDim=3, AlphaPriceLoss=10.0, Device=cuda\n",
            "INFO:SemiSupMIWAE(X:42,Y:1,Lat:3):SemiSupMIWAE Initialized: InputXDim=42, TargetYDim=1, LatentDim=3, AlphaPriceLoss=10.0, Device=cuda\n",
            "2025-10-23 22:57:45,640 - VAETrainer_Loader - INFO - SemiSupMIWAE model created and state loaded successfully.\n",
            "INFO:VAETrainer_Loader:SemiSupMIWAE model created and state loaded successfully.\n",
            "2025-10-23 22:57:45,643 - VAETrainer - INFO - VAETrainer initialized for SemiSupMIWAE. Device: cuda. Optimizer: adam. LR: 0.0003. ReconLoss(X): 'mse'. Alpha(Y Loss): 10.0. LossType(Y): 'gaussian_nll'\n",
            "INFO:VAETrainer:VAETrainer initialized for SemiSupMIWAE. Device: cuda. Optimizer: adam. LR: 0.0003. ReconLoss(X): 'mse'. Alpha(Y Loss): 10.0. LossType(Y): 'gaussian_nll'\n",
            "2025-10-23 22:57:45,644 - VAETrainer_Loader - INFO - VAETrainer instance created and state loaded for model from vae_results/best_model.pth.\n",
            "INFO:VAETrainer_Loader:VAETrainer instance created and state loaded for model from vae_results/best_model.pth.\n",
            "2025-10-23 22:57:45,656 - VAETrainer - INFO - Predicting target (Y) mean and log-variance for 12271 samples...\n",
            "INFO:VAETrainer:Predicting target (Y) mean and log-variance for 12271 samples...\n",
            "2025-10-23 22:57:45,726 - MainRealEstateAnalysis_MIWAE - INFO - [Fold 9] Stored test predictions for 12271 rows.\n",
            "INFO:MainRealEstateAnalysis_MIWAE:[Fold 9] Stored test predictions for 12271 rows.\n",
            "2025-10-23 22:57:45,739 - MainRealEstateAnalysis_MIWAE - INFO - [Fold 10/10] sizes → Train=98169 (80.0%), Val=12272 (10.0%), Test=12271 (10.0%)\n",
            "INFO:MainRealEstateAnalysis_MIWAE:[Fold 10/10] sizes → Train=98169 (80.0%), Val=12272 (10.0%), Test=12271 (10.0%)\n",
            "2025-10-23 22:57:45,740 - __main__.create_vae_from_artifacts - INFO - Creating VAE model. Target column for supervision: log_sale_price.\n",
            "2025-10-23 22:57:45,740 - __main__.create_vae_from_artifacts - INFO - Creating VAE model. Target column for supervision: log_sale_price.\n",
            "INFO:__main__.create_vae_from_artifacts:Creating VAE model. Target column for supervision: log_sale_price.\n",
            "2025-10-23 22:57:45,741 - __main__.create_vae_from_artifacts - INFO - Number of input features for VAE (X): 42 (Features: ['lotarea', 'assess_total', 'exempt_total', 'assessland', 'exemptland']...)\n",
            "2025-10-23 22:57:45,741 - __main__.create_vae_from_artifacts - INFO - Number of input features for VAE (X): 42 (Features: ['lotarea', 'assess_total', 'exempt_total', 'assessland', 'exemptland']...)\n",
            "INFO:__main__.create_vae_from_artifacts:Number of input features for VAE (X): 42 (Features: ['lotarea', 'assess_total', 'exempt_total', 'assessland', 'exemptland']...)\n",
            "2025-10-23 22:57:45,742 - __main__.create_vae_from_artifacts - INFO - Semi-supervised mode: Target 'log_sale_price' (y_dim=1). Alpha for price loss: 10.0.\n",
            "2025-10-23 22:57:45,742 - __main__.create_vae_from_artifacts - INFO - Semi-supervised mode: Target 'log_sale_price' (y_dim=1). Alpha for price loss: 10.0.\n",
            "INFO:__main__.create_vae_from_artifacts:Semi-supervised mode: Target 'log_sale_price' (y_dim=1). Alpha for price loss: 10.0.\n",
            "2025-10-23 22:57:45,744 - __main__.create_vae_from_artifacts - INFO - Using overridden latent_dim: 3\n",
            "2025-10-23 22:57:45,744 - __main__.create_vae_from_artifacts - INFO - Using overridden latent_dim: 3\n",
            "INFO:__main__.create_vae_from_artifacts:Using overridden latent_dim: 3\n",
            "2025-10-23 22:57:45,745 - __main__.create_vae_from_artifacts - INFO - Final effective Latent Dimension: 3\n",
            "2025-10-23 22:57:45,745 - __main__.create_vae_from_artifacts - INFO - Final effective Latent Dimension: 3\n",
            "INFO:__main__.create_vae_from_artifacts:Final effective Latent Dimension: 3\n",
            "2025-10-23 22:57:45,746 - __main__.create_vae_from_artifacts - INFO - Encoder layers: [21], Decoder layers: [21]\n",
            "2025-10-23 22:57:45,746 - __main__.create_vae_from_artifacts - INFO - Encoder layers: [21], Decoder layers: [21]\n",
            "INFO:__main__.create_vae_from_artifacts:Encoder layers: [21], Decoder layers: [21]\n",
            "2025-10-23 22:57:45,747 - __main__.create_vae_from_artifacts - INFO - Using determined prior: Type=student_t_mixture, K=2 (from overrides or artifact inference).\n",
            "2025-10-23 22:57:45,747 - __main__.create_vae_from_artifacts - INFO - Using determined prior: Type=student_t_mixture, K=2 (from overrides or artifact inference).\n",
            "INFO:__main__.create_vae_from_artifacts:Using determined prior: Type=student_t_mixture, K=2 (from overrides or artifact inference).\n",
            "2025-10-23 22:57:45,749 - __main__.create_vae_from_artifacts - INFO - Initializing parameters for student_t_mixture with K=2 components...\n",
            "2025-10-23 22:57:45,749 - __main__.create_vae_from_artifacts - INFO - Initializing parameters for student_t_mixture with K=2 components...\n",
            "INFO:__main__.create_vae_from_artifacts:Initializing parameters for student_t_mixture with K=2 components...\n",
            "2025-10-23 22:57:45,750 - __main__.create_vae_from_artifacts - WARNING - DP-GMM artifacts are unsuitable or insufficient for initializing mixture prior parameters. Default parameters (e.g., zeros for means/logits, identity for covariances) will be used by the VAE.\n",
            "2025-10-23 22:57:45,750 - __main__.create_vae_from_artifacts - WARNING - DP-GMM artifacts are unsuitable or insufficient for initializing mixture prior parameters. Default parameters (e.g., zeros for means/logits, identity for covariances) will be used by the VAE.\n",
            "WARNING:__main__.create_vae_from_artifacts:DP-GMM artifacts are unsuitable or insufficient for initializing mixture prior parameters. Default parameters (e.g., zeros for means/logits, identity for covariances) will be used by the VAE.\n",
            "2025-10-23 22:57:45,751 - __main__.create_vae_from_artifacts - INFO - Setting Student-t df to 4.0 for prior.\n",
            "2025-10-23 22:57:45,751 - __main__.create_vae_from_artifacts - INFO - Setting Student-t df to 4.0 for prior.\n",
            "INFO:__main__.create_vae_from_artifacts:Setting Student-t df to 4.0 for prior.\n",
            "2025-10-23 22:57:45,752 - __main__.create_vae_from_artifacts - INFO - Instantiating SemiSupMIWAE model...\n",
            "2025-10-23 22:57:45,752 - __main__.create_vae_from_artifacts - INFO - Instantiating SemiSupMIWAE model...\n",
            "INFO:__main__.create_vae_from_artifacts:Instantiating SemiSupMIWAE model...\n",
            "2025-10-23 22:57:45,754 - __main__.create_vae_from_artifacts - INFO - Price head layers for SemiSupMIWAE: [8, 4]\n",
            "2025-10-23 22:57:45,754 - __main__.create_vae_from_artifacts - INFO - Price head layers for SemiSupMIWAE: [8, 4]\n",
            "INFO:__main__.create_vae_from_artifacts:Price head layers for SemiSupMIWAE: [8, 4]\n",
            "2025-10-23 22:57:45,765 - SemiSupMIWAE(X:42,Y:1,Lat:3) - INFO - SemiSupMIWAE Initialized: InputXDim=42, TargetYDim=1, LatentDim=3, AlphaPriceLoss=10.0, Device=cuda\n",
            "INFO:SemiSupMIWAE(X:42,Y:1,Lat:3):SemiSupMIWAE Initialized: InputXDim=42, TargetYDim=1, LatentDim=3, AlphaPriceLoss=10.0, Device=cuda\n",
            "2025-10-23 22:57:45,766 - __main__.create_vae_from_artifacts - INFO - SemiSupMIWAE model created successfully on device: cuda.\n",
            "2025-10-23 22:57:45,766 - __main__.create_vae_from_artifacts - INFO - SemiSupMIWAE model created successfully on device: cuda.\n",
            "INFO:__main__.create_vae_from_artifacts:SemiSupMIWAE model created successfully on device: cuda.\n",
            "2025-10-23 22:57:45,769 - VAETrainer - INFO - VAETrainer initialized for SemiSupMIWAE. Device: cuda. Optimizer: adam. LR: 0.0003. ReconLoss(X): 'mse'. Alpha(Y Loss): 10.0. LossType(Y): 'gaussian_nll'\n",
            "INFO:VAETrainer:VAETrainer initialized for SemiSupMIWAE. Device: cuda. Optimizer: adam. LR: 0.0003. ReconLoss(X): 'mse'. Alpha(Y Loss): 10.0. LossType(Y): 'gaussian_nll'\n",
            "2025-10-23 22:57:45,770 - VAETrainer - INFO - Starting training: Epochs=100, BatchSize=512, KLD_weight=1.0, KLD_AnnealEpochs=30, Alpha(Y_Loss)=10.0.\n",
            "INFO:VAETrainer:Starting training: Epochs=100, BatchSize=512, KLD_weight=1.0, KLD_AnnealEpochs=30, Alpha(Y_Loss)=10.0.\n",
            "2025-10-23 22:57:45,771 - VAETrainer - INFO - Train loader: 98169 samples, 192 batches. Drop last: False\n",
            "INFO:VAETrainer:Train loader: 98169 samples, 192 batches. Drop last: False\n",
            "2025-10-23 22:57:45,773 - VAETrainer - INFO - Validation loader: 12272 samples, 24 batches.\n",
            "INFO:VAETrainer:Validation loader: 12272 samples, 24 batches.\n",
            "2025-10-23 22:57:48,361 - VAETrainer - INFO - Epoch [1/100] Train TotalLoss: 371.3995, (ReconX: 1.0830, KLD: 0.4685*0.03, LossY: 37.0301*10.00), Time: 2.2s | Val TotalLoss: 284.6196, (ReconX: 1.0035, KLD: 0.4684, LossY: 28.3148)\n",
            "INFO:VAETrainer:Epoch [1/100] Train TotalLoss: 371.3995, (ReconX: 1.0830, KLD: 0.4685*0.03, LossY: 37.0301*10.00), Time: 2.2s | Val TotalLoss: 284.6196, (ReconX: 1.0035, KLD: 0.4684, LossY: 28.3148)\n",
            "2025-10-23 22:57:48,363 - VAETrainer - INFO - Saving model and trainer state to vae_results/best_model.pth...\n",
            "INFO:VAETrainer:Saving model and trainer state to vae_results/best_model.pth...\n",
            "2025-10-23 22:57:48,378 - VAETrainer - INFO - Successfully saved model and trainer state to vae_results/best_model.pth\n",
            "INFO:VAETrainer:Successfully saved model and trainer state to vae_results/best_model.pth\n",
            "2025-10-23 22:57:50,816 - VAETrainer - INFO - Epoch [2/100] Train TotalLoss: 247.9749, (ReconX: 0.9899, KLD: 0.4499*0.07, LossY: 24.6955*10.00), Time: 2.0s | Val TotalLoss: 213.9511, (ReconX: 0.9614, KLD: 0.4565, LossY: 21.2533)\n",
            "INFO:VAETrainer:Epoch [2/100] Train TotalLoss: 247.9749, (ReconX: 0.9899, KLD: 0.4499*0.07, LossY: 24.6955*10.00), Time: 2.0s | Val TotalLoss: 213.9511, (ReconX: 0.9614, KLD: 0.4565, LossY: 21.2533)\n",
            "2025-10-23 22:57:50,818 - VAETrainer - INFO - Saving model and trainer state to vae_results/best_model.pth...\n",
            "INFO:VAETrainer:Saving model and trainer state to vae_results/best_model.pth...\n",
            "2025-10-23 22:57:50,832 - VAETrainer - INFO - Successfully saved model and trainer state to vae_results/best_model.pth\n",
            "INFO:VAETrainer:Successfully saved model and trainer state to vae_results/best_model.pth\n",
            "2025-10-23 22:57:53,344 - VAETrainer - INFO - Epoch [3/100] Train TotalLoss: 190.9577, (ReconX: 0.9655, KLD: 0.4107*0.10, LossY: 18.9951*10.00), Time: 2.1s | Val TotalLoss: 169.0779, (ReconX: 0.9464, KLD: 0.3956, LossY: 16.7736)\n",
            "INFO:VAETrainer:Epoch [3/100] Train TotalLoss: 190.9577, (ReconX: 0.9655, KLD: 0.4107*0.10, LossY: 18.9951*10.00), Time: 2.1s | Val TotalLoss: 169.0779, (ReconX: 0.9464, KLD: 0.3956, LossY: 16.7736)\n",
            "2025-10-23 22:57:53,346 - VAETrainer - INFO - Saving model and trainer state to vae_results/best_model.pth...\n",
            "INFO:VAETrainer:Saving model and trainer state to vae_results/best_model.pth...\n",
            "2025-10-23 22:57:53,359 - VAETrainer - INFO - Successfully saved model and trainer state to vae_results/best_model.pth\n",
            "INFO:VAETrainer:Successfully saved model and trainer state to vae_results/best_model.pth\n",
            "Exception in thread Thread-1811 (_pin_memory_loop):\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/lib/python3.12/threading.py\", line 1075, in _bootstrap_inner\n",
            "    self.run()\n",
            "  File \"/usr/lib/python3.12/threading.py\", line 1012, in run\n",
            "    self._target(*self._args, **self._kwargs)\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/torch/utils/data/_utils/pin_memory.py\", line 61, in _pin_memory_loop\n",
            "    do_one_step()\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/torch/utils/data/_utils/pin_memory.py\", line 37, in do_one_step\n",
            "    r = in_queue.get(timeout=MP_STATUS_CHECK_INTERVAL)\n",
            "        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.12/multiprocessing/queues.py\", line 122, in get\n",
            "    return _ForkingPickler.loads(res)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/torch/multiprocessing/reductions.py\", line 541, in rebuild_storage_fd\n",
            "    fd = df.detach()\n",
            "         ^^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.12/multiprocessing/resource_sharer.py\", line 57, in detach\n",
            "    with _resource_sharer.get_connection(self._id) as conn:\n",
            "         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.12/multiprocessing/resource_sharer.py\", line 86, in get_connection\n",
            "    c = Client(address, authkey=process.current_process().authkey)\n",
            "        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.12/multiprocessing/connection.py\", line 519, in Client\n",
            "    c = SocketClient(address)\n",
            "        ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.12/multiprocessing/connection.py\", line 647, in SocketClient\n",
            "    s.connect(address)\n",
            "FileNotFoundError: [Errno 2] No such file or directory\n"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-3610940215.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    402\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    403\u001b[0m         \u001b[0;31m# train with early stopping on this fold's val set\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 404\u001b[0;31m         history = vae_trainer_instance.train(\n\u001b[0m\u001b[1;32m    405\u001b[0m             \u001b[0mtrain_dataset\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrain_vae_dataset\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    406\u001b[0m             \u001b[0mval_dataset\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mval_vae_dataset\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-1010516794.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, train_dataset, val_dataset, epochs, batch_size, kld_anneal_epochs, early_stopping_patience, print_every_n_epochs, scheduler_patience, scheduler_factor)\u001b[0m\n\u001b[1;32m    259\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    260\u001b[0m                     \u001b[0mmodel_outputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_b\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 261\u001b[0;31m                     loss, recon_lx, kld_l, loss_ly = self._calculate_loss(\n\u001b[0m\u001b[1;32m    262\u001b[0m                         \u001b[0mx_b\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx_m_b\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_b\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_m_b\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel_outputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcurrent_beta_kld\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    263\u001b[0m                     )\n",
            "\u001b[0;32m/tmp/ipython-input-1010516794.py\u001b[0m in \u001b[0;36m_calculate_loss\u001b[0;34m(self, x_batch, x_mask_batch, y_batch, y_mask_batch, model_outputs, current_beta_kld)\u001b[0m\n\u001b[1;32m    164\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    165\u001b[0m         \u001b[0;31m# --- 2. KL Divergence ---\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 166\u001b[0;31m         \u001b[0mkld_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m0.5\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mlogvar_latent\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mmu_latent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mlogvar_latent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    167\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misnan\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkld_loss\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misinf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkld_loss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    168\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0merror\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"KLD loss is {kld_loss.item()}. Inputs: mu_mean={mu_latent.mean():.2f}, logvar_mean={logvar_latent.mean():.2f}. Setting KLD to 0.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "import math\n",
        "import os\n",
        "import pickle\n",
        "import time\n",
        "import logging\n",
        "from typing import List, Optional, Dict, Tuple, Union, Any\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "# PyTorch Imports for VAE Dataset/Loader\n",
        "from torch.utils.data import DataLoader, TensorDataset, Subset, random_split\n",
        "\n",
        "# ML Model Imports are removed as classification/regression is handled by VAE now\n",
        "from sklearn.preprocessing import StandardScaler # Keep for potential use outside VAE prep\n",
        "\n",
        "\n",
        "# --- Main Analysis Script Logic ---\n",
        "if __name__ == \"__main__\":\n",
        "    # --- Basic Setup ---\n",
        "    main_logger = get_logger(\"MainRealEstateAnalysis_MIWAE\", verbose=True)\n",
        "    main_logger.info(\"================================================================\")\n",
        "    main_logger.info(\"==== Robust Real Estate Analysis - Semi-Supervised MIWAE ====\")\n",
        "    main_logger.info(\"================================================================\")\n",
        "    RANDOM_SEED = 42\n",
        "    np.random.seed(RANDOM_SEED)\n",
        "    torch.manual_seed(RANDOM_SEED)\n",
        "    DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "    main_logger.info(f\"Using device: {DEVICE}\")\n",
        "    if torch.cuda.is_available():\n",
        "        torch.cuda.manual_seed_all(RANDOM_SEED)\n",
        "\n",
        "    # --- Configuration Parameters ---\n",
        "    main_logger.info(\"--- Configuring Analysis Parameters ---\")\n",
        "    # Data Paths\n",
        "    PARQUET_PATH = '/content/drive/MyDrive/e6691_2025Spring_nyre_local/data/processed_building_ml.parquet'\n",
        "    CSV_PATH = 'building_ml_merged.csv' # Fallback\n",
        "\n",
        "    # Core Columns\n",
        "    PRICE_COL = 'sale_price' # Original scale price column name used by FS and Orchestrator\n",
        "    LOG_PRICE_COL = 'log_sale_price' # Log-transformed price column (target for VAE Y head)\n",
        "    DATE_COL_FOR_SPLIT = 'sale_date' # Used for time series validation if enabled\n",
        "\n",
        "    # General Settings\n",
        "    N_JOBS = -1 # For parallel processing where applicable\n",
        "    VERBOSE_LOGGING = True\n",
        "\n",
        "    # Feature Selector Config\n",
        "    FS_AUTO_K_METHOD = \"coverage\"\n",
        "    FS_TARGET_K = None # Set integer K if not using auto_k_method\n",
        "    FS_CORRELATION_THRESHOLD = 0.9\n",
        "    FS_COVERAGE_THRESHOLD = 0.9\n",
        "    FS_METHODS_TO_RUN = ['variance', 'laplacian', 'pseudo_label']\n",
        "    main_logger.info(f\"Feature Selector Config: AutoK='{FS_AUTO_K_METHOD}', TargetK={FS_TARGET_K}, CorrThresh={FS_CORRELATION_THRESHOLD}, CovThresh={FS_COVERAGE_THRESHOLD}\")\n",
        "\n",
        "    # Analysis Orchestrator & Data Processing Config\n",
        "    MAX_TIME_PIPELINE_SECONDS = 3600 * 2 # Max time for orchestrator\n",
        "    MAX_CORE_ANALYSIS_SAMPLES = 20000 # Limit samples for computationally intensive analysis within orchestrator\n",
        "    RUN_BOOTSTRAP_ORCHESTRATOR = False # Run bootstrapping in orchestrator?\n",
        "    N_BOOTSTRAP_ITER = 30\n",
        "    APPLY_SPATIAL = False # Apply spatial analysis in orchestrator?\n",
        "    AUTO_TRANSFORM_SKEWED = True # Transform skewed features in orchestrator's preprocessor?\n",
        "    TRANSFORM_TYPE = 'quantile' # Default transform type\n",
        "    TRANSFORM_SKEW_THRESHOLD = 1.0 # Skewness threshold for transformation\n",
        "\n",
        "    # Semi-Supervised MIWAE Training Config\n",
        "    VAE_MODEL_TYPE = 'SemiSupMIWAE' # Model class to use\n",
        "    VAE_LEARNING_RATE = 3e-4\n",
        "    VAE_EPOCHS = 100\n",
        "    VAE_KL_ANNEAL_EPOCHS = 30 # Epochs to linearly increase KLD weight (beta) to full value\n",
        "    VAE_EARLY_STOPPING_PATIENCE = 15 # Patience for early stopping based on validation loss\n",
        "    VAE_BATCH_SIZE = 512\n",
        "    VAE_RECON_LOSS_X = 'mse' # Reconstruction loss type for X features ('mse' or 'huber')\n",
        "    VAE_PRICE_LOSS_Y = 'gaussian_nll' # Loss type for Y target ('gaussian_nll' or 'mse')\n",
        "    VAE_ALPHA_PRICE_LOSS = 10.0 # Weight (alpha) for the supervised price loss term (Y loss)\n",
        "    main_logger.info(f\"VAE Config: LR={VAE_LEARNING_RATE}, Epochs={VAE_EPOCHS}, Batch={VAE_BATCH_SIZE}, \"\n",
        "                     f\"ReconLossX='{VAE_RECON_LOSS_X}', PriceLossY='{VAE_PRICE_LOSS_Y}', AlphaY={VAE_ALPHA_PRICE_LOSS}\")\n",
        "\n",
        "    # Prediction Config\n",
        "    UNCERTAINTY_THRESHOLD = 1.0 # Threshold for filtering predictions based on log_std_dev\n",
        "\n",
        "    # Validation Config\n",
        "    USE_TIME_SERIES_VALIDATION = False # Use time-series split for VAE train/val? (Requires VAETrainer modifications not implemented here)\n",
        "    N_TIME_SERIES_SPLITS = 5\n",
        "    VAL_SPLIT_RATIO = 0.15 # Used for simple random train/val split if time series is off\n",
        "\n",
        "    # Cross-validation config (non-stratified by year)\n",
        "    N_FOLDS = 10                    # 10-way fold\n",
        "    CV_SHUFFLE_SEED = RANDOM_SEED   # deterministic fold assignment\n",
        "\n",
        "    # Output Files\n",
        "    RESULTS_HISTORY_PKL = 'miwae_analysis_results.pkl' # For saving summary dict\n",
        "    SAVED_VAE_MODEL_PATH = \"miwae_final_model.pth\" # Path to save final VAE model/trainer state\n",
        "    SAVED_PREDICTIONS_PATH = \"final_data_with_miwae_predictions.parquet\" # Path for df with predictions\n",
        "    FAILED_ORCHESTRATOR_PKL = \"FAILED_orchestrator_results.pkl\" # File if orchestrator fails\n",
        "\n",
        "    # --- Step 1: Load Data ---\n",
        "    main_logger.info(f\"--- [Step 1/9] Loading Data ---\")\n",
        "    main_logger.info(f\"Attempting to load Parquet: {PARQUET_PATH}\")\n",
        "    df = load_nyc_data(file_path=PARQUET_PATH, logger_instance=main_logger)\n",
        "    if df is None:\n",
        "        main_logger.warning(f\"Parquet failed, trying CSV: {CSV_PATH}\")\n",
        "        df = load_nyc_data(file_path=CSV_PATH, logger_instance=main_logger)\n",
        "    if df is None:\n",
        "        main_logger.critical(\"Data loading failed from both Parquet and CSV paths. Halting.\")\n",
        "        raise FileNotFoundError(f\"Could not load data from {PARQUET_PATH} or {CSV_PATH}\")\n",
        "    main_logger.info(f\"Data loaded successfully. Initial shape: {df.shape}. Memory usage: {df.memory_usage(index=True, deep=True).sum() / (1024**2):.2f} MB\")\n",
        "    main_logger.debug(f\"Initial columns: {df.columns.tolist()}\")\n",
        "\n",
        "    # --- Step 2: Define and Verify Price Column ---\n",
        "    main_logger.info(f\"--- [Step 2/9] Verifying Price Column ('{PRICE_COL}') ---\")\n",
        "    if PRICE_COL not in df.columns:\n",
        "        fallback_price_col = 'building_sales_mean' # Example fallback\n",
        "        if fallback_price_col in df.columns:\n",
        "            df[PRICE_COL] = df[fallback_price_col]\n",
        "            main_logger.warning(f\"Original price column '{PRICE_COL}' not found. Using fallback column '{fallback_price_col}' as '{PRICE_COL}'.\")\n",
        "        else:\n",
        "            main_logger.critical(f\"Required price column '{PRICE_COL}' (and fallback '{fallback_price_col}') not found in DataFrame. Cannot proceed.\")\n",
        "            raise KeyError(f\"Price column '{PRICE_COL}' not found.\")\n",
        "    else:\n",
        "        main_logger.info(f\"Using column '{PRICE_COL}' as the original scale price.\")\n",
        "    # Ensure log price column doesn't exist yet if it shouldn't (it will be created by orchestrator/preprocessor)\n",
        "    if LOG_PRICE_COL in df.columns:\n",
        "        main_logger.warning(f\"Log-price column '{LOG_PRICE_COL}' already exists in input data. It will likely be overwritten by preprocessing steps.\")\n",
        "\n",
        "    # --- Step 3: Feature Selection ---\n",
        "    main_logger.info(f\"--- [Step 3/9] Performing Feature Selection (Targeting based on '{PRICE_COL}') ---\")\n",
        "    top_features_for_analysis: List[str] = []\n",
        "    try:\n",
        "        # Define ID/non-feature columns to ignore during selection\n",
        "        # Note: PRICE_COL and LOG_PRICE_COL are explicitly excluded here\n",
        "        id_cols_to_ignore_fs = list(set([\n",
        "            'bbl', 'borough', 'block', 'lot', 'zipcode', 'address',\n",
        "            'sale_date', PRICE_COL, LOG_PRICE_COL, # Exclude targets and date explicitly\n",
        "            'xcoord','ycoord', 'latitude','longitude',\n",
        "            # Various administrative/geographic IDs (add more specific ones if needed)\n",
        "            'cd', 'council', 'schooldist', 'healtharea', 'policeprct', 'firecomp',\n",
        "            'sanitboro', 'sanitdistrict', 'sanitsub', 'ct2010', 'tract2010',\n",
        "            'nta', 'bct2020', 'bctcb2020', 'cb2000', 'firm07_flag', 'pfirm15_flag',\n",
        "            'mappluto_f', 'plutomapid', 'version', 'notes', 'sanitdistr',\n",
        "            'index', 'id', 'rowid', 'gid', 'uniqueid', 'parcelid', 'parid', 'parcelnumb',\n",
        "            'condono', 'appbbl', 'tbl',\n",
        "            # Columns potentially created later (shouldn't affect FS if run first)\n",
        "            'has_positive_price', 'price_band', 'is_price_outlier', 'sample_weight'\n",
        "        ]))\n",
        "        id_cols_to_ignore_fs = [col for col in id_cols_to_ignore_fs if col in df.columns] # Keep only existing columns\n",
        "        main_logger.debug(f\"Columns ignored by Feature Selector: {id_cols_to_ignore_fs}\")\n",
        "\n",
        "        fs_config = FeatureSelectorConfig( # Use actual constructor if available\n",
        "                variance_threshold=1e-4, correlation_threshold=FS_CORRELATION_THRESHOLD,\n",
        "                methods_to_run=FS_METHODS_TO_RUN, coverage_threshold=FS_COVERAGE_THRESHOLD,\n",
        "                bootstrap_B=30, bootstrap_k_per_iter=15 # Add other params as needed by your FS class\n",
        "        )\n",
        "        selector = FeatureSelector(\n",
        "            df.copy(), # Use a copy for safety\n",
        "            id_cols=id_cols_to_ignore_fs,\n",
        "            target_col=PRICE_COL, # FS targets the original price column\n",
        "            random_state=RANDOM_SEED, verbose=VERBOSE_LOGGING, n_jobs=N_JOBS, config=fs_config\n",
        "        )\n",
        "\n",
        "        # Execute selection based on configuration\n",
        "        if FS_AUTO_K_METHOD:\n",
        "            main_logger.info(f\"Running FS with auto_k_method='{FS_AUTO_K_METHOD}'...\")\n",
        "            top_features_for_analysis = selector.select(auto_k_selection_method=FS_AUTO_K_METHOD)\n",
        "        elif FS_TARGET_K is not None and FS_TARGET_K > 0:\n",
        "            main_logger.info(f\"Running FS targeting k={FS_TARGET_K} features...\")\n",
        "            top_features_for_analysis = selector.select(k=FS_TARGET_K)\n",
        "        else:\n",
        "            raise ValueError(\"Feature selection requires either FS_AUTO_K_METHOD or a positive FS_TARGET_K.\")\n",
        "\n",
        "        if not top_features_for_analysis:\n",
        "            main_logger.critical(\"Feature selection returned an empty list of features. Halting.\")\n",
        "            raise ValueError(\"Feature selection failed to identify any features.\")\n",
        "\n",
        "        main_logger.info(f\"Feature selection complete. Selected {len(top_features_for_analysis)} features for subsequent analysis.\")\n",
        "        main_logger.debug(f\"Selected features: {top_features_for_analysis}\")\n",
        "\n",
        "    except Exception as e:\n",
        "        main_logger.error(f\"Feature selection process failed critically: {e}\", exc_info=True)\n",
        "        raise # Re-raise the exception to halt execution\n",
        "\n",
        "    # --- Step 4: Analysis Orchestration (Preprocessing, Clustering, Dim Reduction Insights) ---\n",
        "    main_logger.info(f\"--- [Step 4/9] Running Analysis Orchestrator ---\")\n",
        "    main_logger.info(f\"Orchestrator PriceCol: '{PRICE_COL}', LogPriceCol: '{LOG_PRICE_COL}', MaxSamples: {MAX_CORE_ANALYSIS_SAMPLES}\")\n",
        "    df_processed : Optional[pd.DataFrame] = None # Initialize\n",
        "    main_artifacts : Dict[str, Any] = {}     # Initialize\n",
        "    try:\n",
        "        orchestrator_instance = AnalysisOrchestrator(\n",
        "            random_state=RANDOM_SEED, verbose=VERBOSE_LOGGING, n_jobs=N_JOBS,\n",
        "            max_total_time_seconds=MAX_TIME_PIPELINE_SECONDS,\n",
        "            price_col=PRICE_COL, log_price_col_name=LOG_PRICE_COL, # Pass both price column names\n",
        "            apply_spatial_analysis=APPLY_SPATIAL,\n",
        "            min_price_percentile=0.005, max_price_percentile=0.995, # Example percentiles for internal price processing\n",
        "            max_samples_limit=MAX_CORE_ANALYSIS_SAMPLES,\n",
        "            max_bootstraps=N_BOOTSTRAP_ITER,\n",
        "            default_transform_type=TRANSFORM_TYPE, # Pass transform settings\n",
        "            transform_skew_threshold=TRANSFORM_SKEW_THRESHOLD,\n",
        "            # Add other orchestrator specific parameters here based on its definition\n",
        "        )\n",
        "        analysis_run_output = orchestrator_instance.full_analysis_pipeline(\n",
        "            df_input=df.copy(), # Pass original df\n",
        "            numeric_features_for_analysis=top_features_for_analysis, # Pass selected features\n",
        "            run_bootstrap=RUN_BOOTSTRAP_ORCHESTRATOR,\n",
        "            max_samples_for_core_analysis=MAX_CORE_ANALYSIS_SAMPLES,\n",
        "        )\n",
        "\n",
        "        # --- Check Orchestrator Output ---\n",
        "        run_status = analysis_run_output.get(\"status\", \"unknown\")\n",
        "        if run_status != \"completed\":\n",
        "            error_msg = analysis_run_output.get(\"error\", \"Unknown orchestrator error\")\n",
        "            main_logger.error(f\"Analysis Orchestrator failed! Status: '{run_status}', Error: '{error_msg}'.\")\n",
        "            main_logger.error(f\"Keys available in orchestrator output: {list(analysis_run_output.keys())}\")\n",
        "            # Save partial results for debugging\n",
        "            try:\n",
        "                results_to_save = {\"orchestrator_output\": analysis_run_output, \"selected_features\": top_features_for_analysis}\n",
        "                with open(FAILED_ORCHESTRATOR_PKL, 'wb') as f: pickle.dump(results_to_save, f)\n",
        "                main_logger.info(f\"Saved partial orchestrator results to '{FAILED_ORCHESTRATOR_PKL}'.\")\n",
        "            except Exception as e_save: main_logger.error(f\"Failed to save partial orchestrator results: {e_save}\")\n",
        "            raise RuntimeError(f\"Analysis Orchestrator failed: {error_msg}\") # Halt execution\n",
        "\n",
        "        # --- Extract Processed Data and Artifacts ---\n",
        "        # **Important**: Adjust key based on actual orchestrator output structure\n",
        "        # Assuming 'df_processed_and_rebalanced' holds the final DF state intended for VAE\n",
        "        df_processed = analysis_run_output.get('df_processed_and_rebalanced')\n",
        "        # Fallback to key from original script if above is not found\n",
        "        if df_processed is None:\n",
        "             df_processed = analysis_run_output.get('detailed_results_history', {}).get('df_fully_processed_before_sampling')\n",
        "\n",
        "        main_artifacts = analysis_run_output.get(\"main_artifacts\", {})\n",
        "\n",
        "        if not isinstance(df_processed, pd.DataFrame) or df_processed.empty:\n",
        "            main_logger.critical(\"Orchestrator completed but failed to return a valid processed DataFrame. Check orchestrator's results structure. Halting.\")\n",
        "            raise ValueError(\"Processed DataFrame ('df_processed_and_rebalanced' or similar key) not found or empty in orchestrator output.\")\n",
        "\n",
        "        main_logger.info(f\"Analysis Orchestrator completed successfully. Processed DataFrame shape: {df_processed.shape}\")\n",
        "        main_logger.debug(f\"Processed DataFrame columns: {df_processed.columns.tolist()}\")\n",
        "        main_logger.debug(f\"Main artifacts keys: {list(main_artifacts.keys())}\")\n",
        "\n",
        "    except Exception as e_orch:\n",
        "        main_logger.error(f\"Analysis Orchestrator process failed critically: {e_orch}\", exc_info=True)\n",
        "        raise # Re-raise to halt execution\n",
        "\n",
        "    # --- Step 5: Prepare Data for Semi-Supervised MIWAE ---\n",
        "    main_logger.info(f\"--- [Step 5/9] Preparing Data for {VAE_MODEL_TYPE} using '{LOG_PRICE_COL}' as target ---\")\n",
        "\n",
        "    # Define columns for VAE input: Should include selected X features + the LOG_PRICE_COL\n",
        "    vae_input_feature_candidates = list(dict.fromkeys(top_features_for_analysis + [LOG_PRICE_COL]))\n",
        "    # Verify these columns exist in the df_processed from orchestrator\n",
        "    vae_input_feature_candidates = [f for f in vae_input_feature_candidates if f in df_processed.columns]\n",
        "    if LOG_PRICE_COL not in vae_input_feature_candidates:\n",
        "         main_logger.critical(f\"CRITICAL: Target column '{LOG_PRICE_COL}' is missing from df_processed BEFORE VAE preparation. Check Orchestrator's output DataFrame. Halting.\")\n",
        "         raise ValueError(f\"Target column '{LOG_PRICE_COL}' missing from processed DataFrame.\")\n",
        "\n",
        "    main_logger.info(f\"Columns passed to prepare_vae_input: {len(vae_input_feature_candidates)}. Target (Y) column: '{LOG_PRICE_COL}'.\")\n",
        "    main_logger.debug(f\"Columns for prepare_vae_input: {vae_input_feature_candidates}\")\n",
        "\n",
        "    try:\n",
        "        # Prepare X and Y for MIWAE\n",
        "        vae_X_filled_np, vae_X_mask_np, \\\n",
        "        vae_y_filled_np, vae_y_mask_np, \\\n",
        "        vae_feature_names_x_out, _, _ = prepare_vae_input(\n",
        "            df=df_processed.copy(),               # Processed DataFrame from the orchestrator\n",
        "            feature_columns=vae_input_feature_candidates,  # All candidate columns (X features + LOG_PRICE_COL)\n",
        "            target_col=LOG_PRICE_COL,               # Specify the log-price as the supervised target\n",
        "            scaler_type='standard'                  # Apply StandardScaler to X\n",
        "        )\n",
        "\n",
        "        # Validate prepare_vae_input outputs\n",
        "        if vae_X_filled_np is None or vae_X_mask_np is None:\n",
        "            raise ValueError(\"VAE input preparation failed for features (X). Check prepare_vae_input logs.\")\n",
        "        if vae_y_filled_np is None or vae_y_mask_np is None:\n",
        "            raise ValueError(f\"VAE input preparation failed for target ({LOG_PRICE_COL}). Check prepare_vae_input logs.\")\n",
        "        if not vae_feature_names_x_out:\n",
        "            raise ValueError(\"No X feature names returned by prepare_vae_input.\")\n",
        "\n",
        "        vae_feature_names_x = vae_feature_names_x_out  # Final list of X columns\n",
        "        main_logger.info(\n",
        "            f\"VAE data prepared. X features: {len(vae_feature_names_x)}  \"\n",
        "            f\"(shapes X_filled={vae_X_filled_np.shape}, X_mask={vae_X_mask_np.shape}; \"\n",
        "            f\"Y_filled={vae_y_filled_np.shape}, Y_mask={vae_y_mask_np.shape})\"\n",
        "        )\n",
        "        main_logger.debug(f\"Actual X features for VAE: {vae_feature_names_x}\")\n",
        "\n",
        "        # Build TensorDataset for training\n",
        "        full_vae_dataset = TensorDataset(\n",
        "            torch.from_numpy(vae_X_filled_np).float(),\n",
        "            torch.from_numpy(vae_X_mask_np).float(),\n",
        "            torch.from_numpy(vae_y_filled_np).float(),\n",
        "            torch.from_numpy(vae_y_mask_np).float(),\n",
        "        )\n",
        "        main_logger.info(f\"Created MIWAE TensorDataset with {len(full_vae_dataset)} samples.\")\n",
        "\n",
        "        # --- 10-fold, non-stratified 80/10/10 split (indices only) ---\n",
        "        N = len(full_vae_dataset)\n",
        "        all_idx = np.arange(N)\n",
        "\n",
        "        rng = np.random.default_rng(CV_SHUFFLE_SEED)\n",
        "        rng.shuffle(all_idx)\n",
        "\n",
        "        # split shuffled indices into 10 folds with near-equal sizes\n",
        "        folds: List[np.ndarray] = np.array_split(all_idx, N_FOLDS)\n",
        "        main_logger.info(f\"Constructed {N_FOLDS} non-stratified folds. Fold sizes: {[len(f) for f in folds]}\")\n",
        "\n",
        "    except Exception as e_vae_prep:\n",
        "        main_logger.error(f\"VAE data preparation failed critically: {e_vae_prep}\", exc_info=True)\n",
        "        raise\n",
        "\n",
        "    # --- Step 6: Initialize/Load VAE Model and Trainer ---\n",
        "    main_logger.info(f\"--- [Step 6/9] Initializing {VAE_MODEL_TYPE} Model and Trainer ---\")\n",
        "    vae_model: Optional[Union[VariationalAutoencoderBase, SemiSupMIWAE]] = None\n",
        "    vae_trainer_instance: Optional[VAETrainer] = None\n",
        "    try:\n",
        "        #TODO: reconcile 2-dim reality against 3-dim visualization needs\n",
        "        # vae_model = create_vae_from_artifacts(\n",
        "        #     artifacts=main_artifacts,\n",
        "        #     feature_order=vae_feature_names_x + [LOG_PRICE_COL], # Pass ONLY X features here\n",
        "        #     target_col_name=LOG_PRICE_COL, # Pass Y target name for SemiSup logic\n",
        "        #     alpha_price_loss=VAE_ALPHA_PRICE_LOSS, # Pass alpha for model init\n",
        "        #     # Add overrides if needed: latent_dim_override=..., price_head_layer_sizes=... etc.\n",
        "        #     device=torch.device(DEVICE)\n",
        "        # )\n",
        "        # force at least 3 latent dims so downstream PCA(3) can actually run\n",
        "        suggested = main_artifacts.get(\"latent_dim_suggestion\", 3)\n",
        "        vae_model = create_vae_from_artifacts(\n",
        "            artifacts=main_artifacts,\n",
        "            feature_order=vae_feature_names_x + [LOG_PRICE_COL],\n",
        "            target_col_name=LOG_PRICE_COL,\n",
        "            alpha_price_loss=VAE_ALPHA_PRICE_LOSS,\n",
        "            # ← override latent_dim here:\n",
        "            latent_dim_override = max(3, suggested),\n",
        "            device=torch.device(DEVICE)\n",
        "        )\n",
        "        if vae_model is None:\n",
        "            raise RuntimeError(f\"Failed to create {VAE_MODEL_TYPE} model using create_vae_from_artifacts.\")\n",
        "\n",
        "        # Instantiate Trainer with the created model\n",
        "        vae_trainer_instance = VAETrainer(\n",
        "            model=vae_model,\n",
        "            learning_rate=VAE_LEARNING_RATE,\n",
        "            device=torch.device(DEVICE),\n",
        "            verbose=VERBOSE_LOGGING,\n",
        "            reconstruction_loss_type_x=VAE_RECON_LOSS_X, # Pass X loss type\n",
        "            price_loss_type_y=VAE_PRICE_LOSS_Y,      # Pass Y loss type\n",
        "            alpha_price_loss=VAE_ALPHA_PRICE_LOSS,   # Pass trainer's alpha (can override model's)\n",
        "            kld_weight=1.0 # Base KLD weight before annealing\n",
        "        )\n",
        "        main_logger.info(f\"{vae_model.__class__.__name__} model and VAETrainer initialized successfully.\")\n",
        "\n",
        "    except Exception as e_vae_init:\n",
        "        main_logger.error(f\"VAE model or trainer initialization failed: {e_vae_init}\", exc_info=True)\n",
        "        raise # Halt execution\n",
        "\n",
        "    # --- [Step 7/9] 10-fold CV training with 80/10/10 (non-stratified) ---\n",
        "    main_logger.info(f\"--- [Step 7/9] Starting 10-fold CV: each run uses 80% train / 10% val / 10% test ---\")\n",
        "\n",
        "    # storage for stitched test predictions/uncertainties across folds\n",
        "    stitched_mu_log = np.full((N,), np.nan, dtype=np.float64)\n",
        "    stitched_logvar = np.full((N,), np.nan, dtype=np.float64)\n",
        "\n",
        "    fold_histories: List[Dict[str, Any]] = []\n",
        "    train_history = {} # Use this to store the list of fold histories\n",
        "    vae_model_trained = None  # updated per fold\n",
        "\n",
        "    for fold_id in range(N_FOLDS):\n",
        "        test_idx = folds[fold_id]\n",
        "        val_idx  = folds[(fold_id + 1) % N_FOLDS]\n",
        "        train_idx = np.setdiff1d(all_idx, np.concatenate([test_idx, val_idx]), assume_unique=False)\n",
        "\n",
        "        train_vae_dataset = Subset(full_vae_dataset, train_idx.tolist())\n",
        "        val_vae_dataset   = Subset(full_vae_dataset, val_idx.tolist())\n",
        "        test_vae_dataset  = Subset(full_vae_dataset, test_idx.tolist())\n",
        "\n",
        "        main_logger.info(\n",
        "            f\"[Fold {fold_id+1}/{N_FOLDS}] sizes → \"\n",
        "            f\"Train={len(train_idx)} ({len(train_idx)/N:.1%}), \"\n",
        "            f\"Val={len(val_idx)} ({len(val_idx)/N:.1%}), \"\n",
        "            f\"Test={len(test_idx)} ({len(test_idx)/N:.1%})\"\n",
        "        )\n",
        "\n",
        "        # fresh trainer each fold (reuse the already-constructed model class & artifacts)\n",
        "        vae_model = create_vae_from_artifacts(\n",
        "            artifacts=main_artifacts,\n",
        "            feature_order=vae_feature_names_x + [LOG_PRICE_COL],\n",
        "            target_col_name=LOG_PRICE_COL,\n",
        "            alpha_price_loss=VAE_ALPHA_PRICE_LOSS,\n",
        "            latent_dim_override=max(3, main_artifacts.get(\"latent_dim_suggestion\", 3)),\n",
        "            device=torch.device(DEVICE)\n",
        "        )\n",
        "        if vae_model is None:\n",
        "            raise RuntimeError(f\"Fold {fold_id+1}: failed to create {VAE_MODEL_TYPE} model.\")\n",
        "\n",
        "        vae_trainer_instance = VAETrainer(\n",
        "            model=vae_model,\n",
        "            learning_rate=VAE_LEARNING_RATE,\n",
        "            device=torch.device(DEVICE),\n",
        "            verbose=VERBOSE_LOGGING,\n",
        "            reconstruction_loss_type_x=VAE_RECON_LOSS_X,\n",
        "            price_loss_type_y=VAE_PRICE_LOSS_Y,\n",
        "            alpha_price_loss=VAE_ALPHA_PRICE_LOSS,\n",
        "            kld_weight=1.0\n",
        "        )\n",
        "\n",
        "        # train with early stopping on this fold's val set\n",
        "        history = vae_trainer_instance.train(\n",
        "            train_dataset=train_vae_dataset,\n",
        "            val_dataset=val_vae_dataset,\n",
        "            epochs=VAE_EPOCHS,\n",
        "            batch_size=VAE_BATCH_SIZE,\n",
        "            kld_anneal_epochs=VAE_KL_ANNEAL_EPOCHS,\n",
        "            early_stopping_patience=VAE_EARLY_STOPPING_PATIENCE\n",
        "        )\n",
        "        fold_histories.append(history)\n",
        "        vae_model_trained = vae_trainer_instance.model # Keep the last trained model\n",
        "\n",
        "        # --- predict on this fold's test set and stitch into global arrays ---\n",
        "        X_full = full_vae_dataset.tensors[0]  # [N, d]\n",
        "        X_test = X_full[test_idx].to(DEVICE)\n",
        "\n",
        "        if hasattr(vae_trainer_instance, 'predict_price_and_uncertainty'):\n",
        "            log_mu, log_var = vae_trainer_instance.predict_price_and_uncertainty(\n",
        "                X_test, batch_size=VAE_BATCH_SIZE * 2\n",
        "            )\n",
        "            if (log_mu is None) or (log_var is None):\n",
        "                main_logger.error(f\"[Fold {fold_id+1}] Prediction returned None.\")\n",
        "            else:\n",
        "                stitched_mu_log[test_idx] = log_mu.reshape(-1)\n",
        "                stitched_logvar[test_idx] = log_var.reshape(-1)\n",
        "                main_logger.info(f\"[Fold {fold_id+1}] Stored test predictions for {len(test_idx)} rows.\")\n",
        "        else:\n",
        "            main_logger.error(\"VAETrainer missing 'predict_price_and_uncertainty'. Cannot predict.\")\n",
        "\n",
        "    train_history = {\"fold_histories\": fold_histories} # Store all histories\n",
        "    main_logger.info(f\"VAE 10-fold CV training finished.\")\n",
        "\n",
        "\n",
        "    # --- [Step 8/9] Stitched test-fold predictions across CV runs ---\n",
        "    main_logger.info(f\"--- [Step 8/9] Consolidating stitched predictions from 10-fold CV ---\")\n",
        "    df_for_predictions = df_processed.copy()\n",
        "\n",
        "    # Convert log(mu), logvar → original scale + log-scale std\n",
        "    # Note: model emits predictions in raw log(price) space\n",
        "    pred_ok_mask = ~np.isnan(stitched_mu_log)\n",
        "    num_pred = int(pred_ok_mask.sum())\n",
        "\n",
        "    if num_pred == 0:\n",
        "        main_logger.warning(\"No stitched predictions found. Skipping consolidation.\")\n",
        "    else:\n",
        "        # Get predictions for rows where prediction was successful\n",
        "        valid_log_mu = stitched_mu_log[pred_ok_mask]\n",
        "        valid_log_var = stitched_logvar[pred_ok_mask]\n",
        "\n",
        "        log_std = np.sqrt(np.exp(valid_log_var))\n",
        "        price_pred = np.maximum(0.0, np.exp(valid_log_mu)) # Ensure non-negative\n",
        "\n",
        "        pred_price_col_name = f\"cv_predicted_{PRICE_COL}\"\n",
        "        pred_std_col_name   = f\"cv_pred_uncertainty_{LOG_PRICE_COL}_std\"\n",
        "        pred_flag_col_name  = \"cv_prediction_available\"\n",
        "        pred_ok_col_name = \"prediction_uncertainty_ok\" # Keep this name for consistency\n",
        "\n",
        "        # init as NaN/False\n",
        "        df_for_predictions[pred_price_col_name] = np.nan\n",
        "        df_for_predictions[pred_std_col_name]   = np.nan\n",
        "        df_for_predictions[pred_flag_col_name]  = False\n",
        "        df_for_predictions[pred_ok_col_name]    = False # Init as False\n",
        "\n",
        "        # Fill in the valid predictions\n",
        "        df_for_predictions.loc[pred_ok_mask, pred_price_col_name] = price_pred\n",
        "        df_for_predictions.loc[pred_ok_mask, pred_std_col_name]   = log_std\n",
        "        df_for_predictions.loc[pred_ok_mask, pred_flag_col_name]  = True\n",
        "\n",
        "        # Calculate uncertainty flag only for rows with predictions\n",
        "        df_for_predictions.loc[pred_ok_mask, pred_ok_col_name] = \\\n",
        "            (df_for_predictions.loc[pred_ok_mask, pred_std_col_name] < UNCERTAINTY_THRESHOLD)\n",
        "\n",
        "        ok_under_thresh = (df_for_predictions[pred_ok_col_name]).sum()\n",
        "        total_count = len(df_for_predictions)\n",
        "        main_logger.info(\n",
        "            f\"Stitched predictions filled for {num_pred}/{total_count} rows \"\n",
        "            f\"({num_pred/total_count:.1%}).\"\n",
        "        )\n",
        "        main_logger.info(f\"Predictions within uncertainty threshold ({UNCERTAINTY_THRESHOLD}): {ok_under_thresh}/{num_pred} ({ok_under_thresh/num_pred*100:.1f}% of predicted)\")\n",
        "        main_logger.debug(f\"Prediction summary:\\n{df_for_predictions[[pred_price_col_name, pred_std_col_name]].describe()}\")\n",
        "\n",
        "\n",
        "    # --- Step 9: Save Final Results ---\n",
        "    main_logger.info(f\"--- [Step 9/9] Saving Final Results ---\")\n",
        "    try:\n",
        "        # Save summary dictionary (excluding large objects)\n",
        "        results_summary = {\n",
        "            'orchestrator_summary': analysis_run_output.get('summary_report', {}),\n",
        "            'orchestrator_artifacts_summary': {k: v for k, v in main_artifacts.items() if not isinstance(v, (pd.DataFrame, np.ndarray, StandardScaler, torch.nn.Module))},\n",
        "            'selected_features_for_analysis': top_features_for_analysis, # Features selected by FS\n",
        "            'vae_actual_x_features_used': vae_feature_names_x, # Actual X features input to VAE\n",
        "            'vae_target_y_column': LOG_PRICE_COL if vae_model_trained and isinstance(vae_model_trained, SemiSupMIWAE) else None,\n",
        "            'vae_training_history': train_history, # This now contains fold_histories\n",
        "            'config_summary': { # Log key run parameters\n",
        "                'PRICE_COL': PRICE_COL, 'LOG_PRICE_COL': LOG_PRICE_COL,\n",
        "                'FS_AUTO_K_METHOD': FS_AUTO_K_METHOD, 'FS_TARGET_K': FS_TARGET_K, 'FS_COVERAGE_THRESHOLD': FS_COVERAGE_THRESHOLD,\n",
        "                'MAX_CORE_ANALYSIS_SAMPLES': MAX_CORE_ANALYSIS_SAMPLES, 'RUN_BOOTSTRAP_ORCHESTRATOR': RUN_BOOTSTRAP_ORCHESTRATOR,\n",
        "                'VAE_MODEL_TYPE': VAE_MODEL_TYPE, 'VAE_ALPHA_PRICE_LOSS': VAE_ALPHA_PRICE_LOSS, 'VAE_EPOCHS': VAE_EPOCHS,\n",
        "                'VAE_RECON_LOSS_X': VAE_RECON_LOSS_X, 'VAE_PRICE_LOSS_Y': VAE_PRICE_LOSS_Y,\n",
        "                'UNCERTAINTY_THRESHOLD': UNCERTAINTY_THRESHOLD,\n",
        "                'N_FOLDS_CV': N_FOLDS # Add CV info\n",
        "            }\n",
        "        }\n",
        "        with open(RESULTS_HISTORY_PKL, 'wb') as f:\n",
        "            pickle.dump(results_summary, f)\n",
        "        main_logger.info(f\"Summary results dictionary saved to '{RESULTS_HISTORY_PKL}'\")\n",
        "\n",
        "        # Save final DataFrame with predictions if they exist\n",
        "        pred_col_name_check = f\"cv_predicted_{PRICE_COL}\" # Check for the CV prediction column\n",
        "        if pred_col_name_check in df_for_predictions.columns:\n",
        "            try:\n",
        "                df_for_predictions.to_parquet(SAVED_PREDICTIONS_PATH)\n",
        "                main_logger.info(f\"DataFrame with predictions saved to '{SAVED_PREDICTIONS_PATH}'\")\n",
        "            except Exception as e_save_df:\n",
        "                main_logger.error(f\"Failed to save DataFrame with predictions to Parquet: {e_save_df}\")\n",
        "                try: # Fallback to CSV\n",
        "                    df_for_predictions.to_csv(SAVED_PREDICTIONS_PATH.replace('.parquet','.csv'), index=False)\n",
        "                    main_logger.info(f\"DataFrame with predictions saved to CSV fallback: '{SAVED_PREDICTIONS_PATH.replace('.parquet','.csv')}'\")\n",
        "                except Exception as e_save_csv:\n",
        "                    main_logger.error(f\"Failed to save DataFrame with predictions to CSV fallback: {e_save_csv}\")\n",
        "        else:\n",
        "            main_logger.warning(\"No CV prediction columns found in the final DataFrame. Skipping save of prediction DataFrame.\")\n",
        "\n",
        "    except Exception as e_final_save:\n",
        "        main_logger.error(f\"Error occurred during final saving steps: {e_final_save}\", exc_info=True)\n",
        "\n",
        "    main_logger.info(f\"==== Main Analysis Script ({VAE_MODEL_TYPE} with {N_FOLDS}-Fold CV) Finished ====\")\n",
        "    main_logger.info(\"=================================================================================\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os, glob, pickle\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# ----------------------------------------------------------\n",
        "# 0. Mount Drive in Colab if available\n",
        "# ----------------------------------------------------------\n",
        "try:\n",
        "    from google.colab import drive  # type: ignore\n",
        "    if not os.path.ismount(\"/content/drive\"):\n",
        "        drive.mount(\"/content/drive\", force_remount=False)\n",
        "        print(\"[Convergence] Mounted Google Drive at /content/drive\")\n",
        "except Exception:\n",
        "    print(\"[Convergence] google.colab not available; assuming local filesystem.\")\n",
        "\n",
        "# ----------------------------------------------------------\n",
        "# 1. Locate latest run dir and load results_summary\n",
        "# ----------------------------------------------------------\n",
        "RESULTS_ROOT = \"/content/drive/MyDrive/e6691_2025Spring_nyre_local/results\"\n",
        "RESULTS_PKL_NAME = \"miwae_analysis_results.pkl\"\n",
        "\n",
        "def _latest_run_dir(results_root: str) -> str:\n",
        "    subdirs = [d for d in glob.glob(os.path.join(results_root, \"*\")) if os.path.isdir(d)]\n",
        "    if not subdirs:\n",
        "        raise FileNotFoundError(f\"No subdirectories under {results_root}\")\n",
        "    return max(subdirs, key=os.path.getmtime)\n",
        "\n",
        "run_dir = _latest_run_dir(RESULTS_ROOT)\n",
        "print(f\"[Convergence] Using run dir: {run_dir}\")\n",
        "\n",
        "results_pkl_path = os.path.join(run_dir, RESULTS_PKL_NAME)\n",
        "if not os.path.exists(results_pkl_path):\n",
        "    raise FileNotFoundError(f\"Could not find {results_pkl_path}\")\n",
        "\n",
        "with open(results_pkl_path, \"rb\") as f:\n",
        "    results_summary = pickle.load(f)\n",
        "\n",
        "print(f\"[Convergence] Loaded results_summary with top-level keys: {list(results_summary.keys())}\")\n",
        "\n",
        "# ----------------------------------------------------------\n",
        "# 1b. Extract fold_histories in a robust way\n",
        "# ----------------------------------------------------------\n",
        "def _extract_fold_histories(summary_dict):\n",
        "    \"\"\"\n",
        "    Try to recover a list of history dicts from several possible layouts:\n",
        "    - summary['vae_training_history']['fold_histories'] (new multi-fold)\n",
        "    - summary['vae_training_history']['history'] (single history nested)\n",
        "    - summary['vae_training_history'] itself (flat history dict)\n",
        "    - summary['history'] at top level (e.g. direct checkpoint save_content)\n",
        "    \"\"\"\n",
        "    fold_histories = []\n",
        "\n",
        "    vae_hist = summary_dict.get(\"vae_training_history\", None)\n",
        "\n",
        "    # Case A: New format: {'vae_training_history': {'fold_histories': [history_dict, ...]}}\n",
        "    if isinstance(vae_hist, dict) and isinstance(vae_hist.get(\"fold_histories\"), list):\n",
        "        fold_histories = vae_hist[\"fold_histories\"]\n",
        "        print(f\"[Convergence] Found fold_histories under results_summary['vae_training_history']['fold_histories'] \"\n",
        "              f\"(n_folds={len(fold_histories)}).\")\n",
        "        return fold_histories\n",
        "\n",
        "    # Case B: Nested single history: {'vae_training_history': {'history': {...}}}\n",
        "    if isinstance(vae_hist, dict) and isinstance(vae_hist.get(\"history\"), dict):\n",
        "        fold_histories = [vae_hist[\"history\"]]\n",
        "        print(\"[Convergence] Found single history under results_summary['vae_training_history']['history']; \"\n",
        "              \"treating it as one fold.\")\n",
        "        return fold_histories\n",
        "\n",
        "    # Case C: 'vae_training_history' itself looks like a history dict\n",
        "    if isinstance(vae_hist, dict):\n",
        "        has_train_val = any(\n",
        "            k.startswith(\"train_\") or k.startswith(\"val_\")\n",
        "            for k in vae_hist.keys()\n",
        "        )\n",
        "        if has_train_val:\n",
        "            fold_histories = [vae_hist]\n",
        "            print(\"[Convergence] results_summary['vae_training_history'] looks like a flat history dict; \"\n",
        "                  \"treating it as one fold.\")\n",
        "            return fold_histories\n",
        "\n",
        "    # Case D: Top-level 'history' (e.g. from VAETrainer.save_model)\n",
        "    top_hist = summary_dict.get(\"history\", None)\n",
        "    if isinstance(top_hist, dict):\n",
        "        fold_histories = [top_hist]\n",
        "        print(\"[Convergence] Found top-level 'history' dict; treating as one fold.\")\n",
        "        return fold_histories\n",
        "\n",
        "    # Nothing worked\n",
        "    raise ValueError(\n",
        "        \"Could not locate any usable training histories. \"\n",
        "        f\"Top-level keys: {list(summary_dict.keys())}\"\n",
        "    )\n",
        "\n",
        "fold_histories = _extract_fold_histories(results_summary)\n",
        "\n",
        "if not fold_histories:\n",
        "    raise ValueError(\"No training histories found after attempting all layouts.\")\n",
        "\n",
        "# ----------------------------------------------------------\n",
        "# 2. Plot train/val loss vs epoch\n",
        "# ----------------------------------------------------------\n",
        "plt.figure(figsize=(8, 5))\n",
        "\n",
        "for i, hist in enumerate(fold_histories):\n",
        "    # Try multiple possible key names, fall back gracefully\n",
        "    train_key = next(\n",
        "        (k for k in [\"train_total_loss\", \"train_loss\", \"train_elbo\"] if k in hist),\n",
        "        None,\n",
        "    )\n",
        "    val_key = next(\n",
        "        (k for k in [\"val_total_loss\", \"val_loss\", \"val_elbo\"] if k in hist),\n",
        "        None,\n",
        "    )\n",
        "\n",
        "    if train_key is None or val_key is None:\n",
        "        print(f\"[Convergence] Fold {i+1}: missing train/val keys; has {list(hist.keys())}\")\n",
        "        continue\n",
        "\n",
        "    train_losses = np.array(hist[train_key], dtype=float)\n",
        "    val_losses = np.array(hist[val_key], dtype=float)\n",
        "    epochs = np.arange(1, len(train_losses) + 1)\n",
        "\n",
        "    plt.plot(\n",
        "        epochs,\n",
        "        train_losses,\n",
        "        alpha=0.3,\n",
        "        label=\"train (fold 1)\" if i == 0 else None,\n",
        "    )\n",
        "    plt.plot(\n",
        "        epochs,\n",
        "        val_losses,\n",
        "        alpha=0.3,\n",
        "        linestyle=\"--\",\n",
        "        label=\"val (fold 1)\" if i == 0 else None,\n",
        "    )\n",
        "\n",
        "plt.xlabel(\"Epoch\")\n",
        "plt.ylabel(\"Total loss (≈ -ELBO)\")\n",
        "plt.title(\"SemiSupMIWAE convergence: train vs val loss across folds\")\n",
        "plt.legend()\n",
        "plt.grid(True, alpha=0.3)\n",
        "plt.show()\n",
        "\n",
        "# ----------------------------------------------------------\n",
        "# 3. Optional summary of epochs per fold\n",
        "# ----------------------------------------------------------\n",
        "epochs_per_fold = []\n",
        "for hist in fold_histories:\n",
        "    train_key = next(\n",
        "        (k for k in [\"train_total_loss\", \"train_loss\", \"train_elbo\"] if k in hist),\n",
        "        None,\n",
        "    )\n",
        "    if train_key is not None and isinstance(hist[train_key], (list, tuple)):\n",
        "        epochs_per_fold.append(len(hist[train_key]))\n",
        "\n",
        "if epochs_per_fold:\n",
        "    print(f\"[Convergence] Epochs per fold: {epochs_per_fold}\")\n",
        "    print(f\"[Convergence] Mean epochs to convergence (approx): {np.mean(epochs_per_fold):.1f}\")\n",
        "else:\n",
        "    print(\"[Convergence] Could not compute epochs_per_fold (no usable train_* series).\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 595
        },
        "id": "LsGMqCbCq5bM",
        "outputId": "02b9f292-3a79-4c1d-ac62-51b8abde84d6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Convergence] Using run dir: /content/drive/MyDrive/e6691_2025Spring_nyre_local/results/2025-10-12_01-58-16\n",
            "[Convergence] Loaded results_summary with top-level keys: ['orchestrator_summary', 'orchestrator_artifacts_summary', 'selected_features_for_analysis', 'vae_actual_x_features_used', 'vae_target_y_column', 'vae_training_history', 'config_summary']\n",
            "[Convergence] results_summary['vae_training_history'] looks like a flat history dict; treating it as one fold.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 800x500 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Convergence] Epochs per fold: [100]\n",
            "[Convergence] Mean epochs to convergence (approx): 100.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# === POST-TRAINING EVAL + VIZ (robust to missing cv_* columns) ===\n",
        "\n",
        "import os, glob, math, logging, pickle\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.preprocessing import StandardScaler  # only used in summary artifacts\n",
        "\n",
        "# Optional: for QQ-plot; we guard against missing scipy.\n",
        "try:\n",
        "    from scipy import stats\n",
        "    HAVE_SCIPY = True\n",
        "except Exception:\n",
        "    HAVE_SCIPY = False\n",
        "    print(\"[Eval] WARNING: scipy not available; QQ-plot will be skipped.\")\n",
        "\n",
        "# ----------------------------------------------------------\n",
        "# 0. Mount Drive (Colab) and basic setup\n",
        "# ----------------------------------------------------------\n",
        "try:\n",
        "    from google.colab import drive  # type: ignore\n",
        "    drive.mount(\"/content/drive\", force_remount=False)\n",
        "except Exception:\n",
        "    pass\n",
        "\n",
        "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"[Eval] Using device: {DEVICE}\")\n",
        "\n",
        "logger = logging.getLogger(\"MIWAE_Eval\")\n",
        "logger.setLevel(logging.INFO)\n",
        "if not logger.handlers:\n",
        "    logger.addHandler(logging.StreamHandler())\n",
        "\n",
        "# ----------------------------------------------------------\n",
        "# 1. Paths (aligned with your AlphaScan cell)\n",
        "# ----------------------------------------------------------\n",
        "RESULTS_ROOT = \"/content/drive/MyDrive/e6691_2025Spring_nyre_local/results\"\n",
        "FALLBACK_RUN_DIR = \"/content/drive/MyDrive/e6691_2025Spring_nyre_local/results/2025-10-12_01-58-16\"\n",
        "\n",
        "RESULTS_PKL_NAME = \"miwae_analysis_results.pkl\"\n",
        "PRED_PARQUET_NAME = \"final_data_with_miwae_predictions.parquet\"\n",
        "PRED_CSV_NAME = \"final_data_with_miwae_predictions.csv\"\n",
        "\n",
        "def _latest_run_dir(results_root: str, fallback: str = None) -> str:\n",
        "    subdirs = [d for d in glob.glob(os.path.join(results_root, \"*\")) if os.path.isdir(d)]\n",
        "    if not subdirs:\n",
        "        if fallback is None:\n",
        "            raise FileNotFoundError(f\"No subdirectories under {results_root}\")\n",
        "        logger.info(f\"[Eval] No subdirs under {results_root}, using fallback {fallback}\")\n",
        "        return fallback\n",
        "    latest = max(subdirs, key=os.path.getmtime)\n",
        "    logger.info(f\"[Eval] Using latest run dir: {latest}\")\n",
        "    return latest\n",
        "\n",
        "run_dir = _latest_run_dir(RESULTS_ROOT, fallback=FALLBACK_RUN_DIR)\n",
        "print(f\"[Eval] RUN_DIR = {run_dir}\")\n",
        "\n",
        "RESULTS_HISTORY_PKL = os.path.join(run_dir, RESULTS_PKL_NAME)\n",
        "PRED_PARQUET_PATH   = os.path.join(run_dir, PRED_PARQUET_NAME)\n",
        "PRED_CSV_PATH       = os.path.join(run_dir, PRED_CSV_NAME)\n",
        "\n",
        "print(f\"[Eval] RESULTS_HISTORY_PKL: {RESULTS_HISTORY_PKL}\")\n",
        "print(f\"[Eval] PRED_PARQUET_PATH:   {PRED_PARQUET_PATH}\")\n",
        "print(f\"[Eval] PRED_CSV_PATH:       {PRED_CSV_PATH}\")\n",
        "\n",
        "best_model_candidates = glob.glob(os.path.join(run_dir, \"**\", \"best_model.pth\"), recursive=True)\n",
        "if best_model_candidates:\n",
        "    BEST_MODEL_PATH = best_model_candidates[0]\n",
        "    print(f\"[Eval] BEST_MODEL_PATH:   {BEST_MODEL_PATH}\")\n",
        "else:\n",
        "    BEST_MODEL_PATH = None\n",
        "    print(\"[Eval] WARNING: No best_model.pth found; metrics will skip model-based fallback.\")\n",
        "\n",
        "# ----------------------------------------------------------\n",
        "# 2. Load results_summary and predictions DataFrame\n",
        "# ----------------------------------------------------------\n",
        "if not os.path.exists(RESULTS_HISTORY_PKL):\n",
        "    raise FileNotFoundError(f\"[Eval] Could not find {RESULTS_HISTORY_PKL}\")\n",
        "\n",
        "with open(RESULTS_HISTORY_PKL, \"rb\") as f:\n",
        "    results_summary = pickle.load(f)\n",
        "print(\"[Eval] Loaded results_summary from pickle.\")\n",
        "\n",
        "if os.path.exists(PRED_PARQUET_PATH):\n",
        "    df_pred = pd.read_parquet(PRED_PARQUET_PATH)\n",
        "    print(f\"[Eval] Loaded predictions from Parquet. Shape: {df_pred.shape}\")\n",
        "elif os.path.exists(PRED_CSV_PATH):\n",
        "    df_pred = pd.read_csv(PRED_CSV_PATH)\n",
        "    print(f\"[Eval] Loaded predictions from CSV. Shape: {df_pred.shape}\")\n",
        "else:\n",
        "    raise FileNotFoundError(\n",
        "        f\"[Eval] Neither {PRED_PARQUET_PATH} nor {PRED_CSV_PATH} exists.\"\n",
        "    )\n",
        "\n",
        "config = results_summary.get(\"config_summary\", {})\n",
        "price_col = config.get(\"PRICE_COL\", \"sale_price\")\n",
        "log_y_col = results_summary.get(\"vae_target_y_column\", None)\n",
        "\n",
        "if log_y_col is None:\n",
        "    raise ValueError(\"[Eval] 'vae_target_y_column' is None in results_summary – cannot evaluate.\")\n",
        "if log_y_col not in df_pred.columns:\n",
        "    raise KeyError(f\"[Eval] Target column '{log_y_col}' not found in df_pred.\")\n",
        "\n",
        "# ----------------------------------------------------------\n",
        "# 3. Build VAE inputs and load model/trainer\n",
        "#     (used for both fallback metrics and latent viz)\n",
        "# ----------------------------------------------------------\n",
        "vae_x_cols = results_summary.get(\"vae_actual_x_features_used\", [])\n",
        "if not vae_x_cols:\n",
        "    raise ValueError(\"[Eval] 'vae_actual_x_features_used' is empty – cannot rebuild VAE inputs.\")\n",
        "\n",
        "feature_columns_for_vae = list(dict.fromkeys(vae_x_cols + [log_y_col]))\n",
        "missing_feats = [c for c in feature_columns_for_vae if c not in df_pred.columns]\n",
        "if missing_feats:\n",
        "    print(f\"[Eval] WARNING: some VAE feature columns missing in df_pred: {missing_feats}\")\n",
        "\n",
        "(X_filled_np, X_mask_np,\n",
        " y_filled_np, y_mask_np,\n",
        " feature_names_x_out, _, _) = prepare_vae_input(\n",
        "    df=df_pred.copy(),\n",
        "    feature_columns=feature_columns_for_vae,\n",
        "    target_col=log_y_col,\n",
        "    scaler_type=\"standard\"\n",
        ")\n",
        "\n",
        "print(f\"[Eval] VAE input shapes: X={X_filled_np.shape}, Y={y_filled_np.shape}\")\n",
        "\n",
        "artifacts_summary = results_summary.get(\"orchestrator_artifacts_summary\", {})\n",
        "latent_dim_suggestion = artifacts_summary.get(\"latent_dim_suggestion\", 3)\n",
        "latent_dim_override = max(3, int(latent_dim_suggestion))\n",
        "\n",
        "vae_model = create_vae_from_artifacts(\n",
        "    artifacts=artifacts_summary,\n",
        "    feature_order=feature_names_x_out + [log_y_col],\n",
        "    target_col_name=log_y_col,\n",
        "    alpha_price_loss=config.get(\"VAE_ALPHA_PRICE_LOSS\", 10.0),\n",
        "    latent_dim_override=latent_dim_override,\n",
        "    device=DEVICE\n",
        ")\n",
        "\n",
        "if BEST_MODEL_PATH is not None and os.path.exists(BEST_MODEL_PATH):\n",
        "    checkpoint = torch.load(BEST_MODEL_PATH, map_location=DEVICE)\n",
        "    if isinstance(checkpoint, dict) and \"model_state_dict\" in checkpoint:\n",
        "        state_dict = checkpoint[\"model_state_dict\"]\n",
        "    else:\n",
        "        state_dict = checkpoint\n",
        "    vae_model.load_state_dict(state_dict)\n",
        "    print(\"[Eval] Loaded model_state_dict into VAE model from best_model.pth\")\n",
        "else:\n",
        "    print(\"[Eval] WARNING: Using fresh-initialized MIWAE weights (no best_model.pth found).\")\n",
        "\n",
        "vae_trainer = VAETrainer(\n",
        "    model=vae_model,\n",
        "    learning_rate=config.get(\"VAE_LEARNING_RATE\", 3e-4),\n",
        "    device=DEVICE,\n",
        "    verbose=False,\n",
        "    reconstruction_loss_type_x=config.get(\"VAE_RECON_LOSS_X\", \"mse\"),\n",
        "    price_loss_type_y=config.get(\"VAE_PRICE_LOSS_Y\", \"gaussian_nll\"),\n",
        "    alpha_price_loss=config.get(\"VAE_ALPHA_PRICE_LOSS\", 10.0),\n",
        "    kld_weight=1.0,\n",
        ")\n",
        "\n",
        "# ----------------------------------------------------------\n",
        "# 4. Posterior predictive metrics\n",
        "#     (use cv_* columns if present; otherwise fallback to best_model)\n",
        "#     + store residuals for diagnostics\n",
        "# ----------------------------------------------------------\n",
        "pred_price_col_cv = f\"cv_predicted_{price_col}\"\n",
        "pred_std_col_cv   = f\"cv_pred_uncertainty_{log_y_col}_std\"\n",
        "pred_flag_col_cv  = \"cv_prediction_available\"\n",
        "\n",
        "have_cv = all(c in df_pred.columns for c in [pred_price_col_cv, pred_std_col_cv, pred_flag_col_cv])\n",
        "\n",
        "# For later residual diagnostics (log space)\n",
        "y_true_log_eval = None\n",
        "mu_log_eval = None\n",
        "\n",
        "def _report_metrics(y_true_log, y_true, mu_log, var_log, label_prefix=\"\"):\n",
        "    var_log = np.clip(var_log, 1e-9, np.inf)\n",
        "\n",
        "    # Gaussian log-likelihood in log-price space\n",
        "    log_pp = -0.5 * (np.log(2 * math.pi * var_log) + ((y_true_log - mu_log) ** 2) / var_log)\n",
        "    avg_log_pp = float(np.mean(log_pp))\n",
        "    avg_nll_log = -avg_log_pp\n",
        "\n",
        "    # basic errors in log space\n",
        "    resid_log = y_true_log - mu_log\n",
        "    mse_log = float(np.mean(resid_log ** 2))\n",
        "    rmse_log = math.sqrt(mse_log)\n",
        "    mae_log = float(np.mean(np.abs(resid_log)))\n",
        "\n",
        "    # robust log-space metrics for the writeup\n",
        "    abs_resid_log = np.abs(resid_log)\n",
        "    median_abs_log = float(np.median(abs_resid_log))\n",
        "    p95_abs_log    = float(np.percentile(abs_resid_log, 95))\n",
        "\n",
        "    # errors in level space\n",
        "    mse_price = float(np.mean((y_true - np.exp(mu_log)) ** 2))\n",
        "    rmse_price = math.sqrt(mse_price)\n",
        "    mae_price = float(np.mean(np.abs(y_true - np.exp(mu_log))))\n",
        "\n",
        "    positive_mask = y_true > 0\n",
        "    if positive_mask.sum() > 0:\n",
        "        mape_price = float(\n",
        "            np.mean(\n",
        "                np.abs(\n",
        "                    (y_true[positive_mask] - np.exp(mu_log[positive_mask]))\n",
        "                    / y_true[positive_mask]\n",
        "                )\n",
        "            )\n",
        "        )\n",
        "    else:\n",
        "        mape_price = float(\"nan\")\n",
        "\n",
        "    tag = f\" ({label_prefix})\" if label_prefix else \"\"\n",
        "    print(f\"\\n=== Posterior Predictive Metrics{tag} ===\")\n",
        "    print(f\"Average log posterior predictive (log p(y_log | x)): {avg_log_pp:.4f}\")\n",
        "    print(f\"Average NLL (log-price space):                     {avg_nll_log:.4f}\")\n",
        "    print(f\"RMSE (log-price):                                  {rmse_log:.4f}\")\n",
        "    print(f\"MAE  (log-price):                                  {mae_log:.4f}\")\n",
        "    print(f\"RMSE (price):                                      {rmse_price:,.4f}\")\n",
        "    print(f\"MAE  (price):                                      {mae_price:,.4f}\")\n",
        "    print(f\"MAPE (price, y_true > 0):                          {mape_price * 100:,.2f}%\")\n",
        "    print(f\"Median |y - ŷ| (log):                              {median_abs_log:.4f}\")\n",
        "    print(f\"95th pct |y - ŷ| (log):                            {p95_abs_log:.4f}\")\n",
        "\n",
        "if have_cv:\n",
        "    print(\"\\n[Eval] Found cv_* prediction columns; using CV-stitched predictions for held-out metrics.\")\n",
        "    mask_pred = df_pred[pred_flag_col_cv].astype(bool).values\n",
        "    df_eval = df_pred.loc[mask_pred].copy()\n",
        "    print(f\"[Eval] Rows with CV predictions: {df_eval.shape[0]} / {df_pred.shape[0]}\")\n",
        "\n",
        "    y_true_log = df_eval[log_y_col].astype(float).values\n",
        "    y_true     = df_eval[price_col].astype(float).values\n",
        "    y_pred     = df_eval[pred_price_col_cv].astype(float).values\n",
        "    sigma_log  = df_eval[pred_std_col_cv].astype(float).values\n",
        "\n",
        "    mu_log = np.log(np.clip(y_pred, 1e-12, np.inf))\n",
        "    var_log = np.square(sigma_log)\n",
        "\n",
        "    _report_metrics(y_true_log, y_true, mu_log, var_log, label_prefix=\"CV stitched\")\n",
        "\n",
        "    # store for residual diagnostics\n",
        "    y_true_log_eval = y_true_log\n",
        "    mu_log_eval = mu_log\n",
        "\n",
        "    # optional metrics on \"uncertainty OK\" subset\n",
        "    unc_ok_col = \"prediction_uncertainty_ok\"\n",
        "    if unc_ok_col in df_eval.columns:\n",
        "        mask_unc_ok = df_eval[unc_ok_col].astype(bool).values\n",
        "        if mask_unc_ok.sum() > 0:\n",
        "            print(f\"\\n[Eval] Metrics on subset with {unc_ok_col} == True\")\n",
        "            y_true_log_ok = y_true_log[mask_unc_ok]\n",
        "            y_true_ok     = y_true[mask_unc_ok]\n",
        "            mu_log_ok     = mu_log[mask_unc_ok]\n",
        "            var_log_ok    = var_log[mask_unc_ok]\n",
        "            _report_metrics(y_true_log_ok, y_true_ok, mu_log_ok, var_log_ok,\n",
        "                            label_prefix=\"CV stitched, unc_ok\")\n",
        "\n",
        "else:\n",
        "    print(\"\\n[Eval] cv_* columns not found; falling back to direct predictions from best_model on a random hold-out split.\")\n",
        "    if BEST_MODEL_PATH is None:\n",
        "        raise RuntimeError(\"[Eval] No cv_* columns and no best_model.pth; cannot compute predictive metrics.\")\n",
        "\n",
        "    # mask for rows where the target is observed\n",
        "    y_mask = y_mask_np.squeeze()\n",
        "    obs_idx = np.where(y_mask > 0.0)[0]\n",
        "    if obs_idx.size == 0:\n",
        "        raise RuntimeError(\"[Eval] No observed targets according to VAE mask; cannot evaluate.\")\n",
        "\n",
        "    y_true_log_all = df_pred[log_y_col].astype(float).to_numpy()[obs_idx]\n",
        "    y_true_all     = df_pred[price_col].astype(float).to_numpy()[obs_idx]\n",
        "    X_obs = X_filled_np[obs_idx]\n",
        "\n",
        "    N_obs = X_obs.shape[0]\n",
        "    rng = np.random.default_rng(42)\n",
        "    perm = rng.permutation(N_obs)\n",
        "    split = int(0.8 * N_obs)\n",
        "    test_rel_idx = perm[split:]\n",
        "    X_test = X_obs[test_rel_idx]\n",
        "    y_true_log = y_true_log_all[test_rel_idx]\n",
        "    y_true     = y_true_all[test_rel_idx]\n",
        "\n",
        "    X_test_tensor = torch.from_numpy(X_test).float().to(DEVICE)\n",
        "    log_mu, log_var = vae_trainer.predict_price_and_uncertainty(X_test_tensor, batch_size=1024)\n",
        "    log_mu = np.asarray(log_mu).reshape(-1)\n",
        "    log_var = np.asarray(log_var).reshape(-1)\n",
        "    var_log = np.exp(log_var)\n",
        "\n",
        "    print(f\"[Eval] Evaluating on random hold-out: {len(test_rel_idx)} / {N_obs} observed rows.\")\n",
        "    _report_metrics(y_true_log, y_true, log_mu, var_log, label_prefix=\"best_model, random hold-out\")\n",
        "\n",
        "    # store for residual diagnostics\n",
        "    y_true_log_eval = y_true_log\n",
        "    mu_log_eval = log_mu\n",
        "\n",
        "# ----------------------------------------------------------\n",
        "# 5. Convergence diagnostics (train/val loss vs epoch)\n",
        "#     (robust to different layouts)\n",
        "# ----------------------------------------------------------\n",
        "def _extract_fold_histories(summary_dict):\n",
        "    \"\"\"\n",
        "    Try to recover a list of history dicts from several possible layouts:\n",
        "    - summary['vae_training_history']['fold_histories'] (multi-fold)\n",
        "    - summary['vae_training_history']['history'] (single history nested)\n",
        "    - summary['vae_training_history'] itself (flat history dict)\n",
        "    - summary['history'] at top level (e.g. direct VAETrainer.save_model)\n",
        "    \"\"\"\n",
        "    fold_histories_local = []\n",
        "\n",
        "    vae_hist = summary_dict.get(\"vae_training_history\", None)\n",
        "\n",
        "    # Case A: multi-fold\n",
        "    if isinstance(vae_hist, dict) and isinstance(vae_hist.get(\"fold_histories\"), list):\n",
        "        fold_histories_local = vae_hist[\"fold_histories\"]\n",
        "        print(f\"[Eval] Found fold_histories under results_summary['vae_training_history']['fold_histories'] \"\n",
        "              f\"(n_folds={len(fold_histories_local)}).\")\n",
        "        return fold_histories_local\n",
        "\n",
        "    # Case B: nested single\n",
        "    if isinstance(vae_hist, dict) and isinstance(vae_hist.get(\"history\"), dict):\n",
        "        fold_histories_local = [vae_hist[\"history\"]]\n",
        "        print(\"[Eval] Found single history under results_summary['vae_training_history']['history']; \"\n",
        "              \"treating it as one fold.\")\n",
        "        return fold_histories_local\n",
        "\n",
        "    # Case C: flat dict with train_/val_ keys\n",
        "    if isinstance(vae_hist, dict):\n",
        "        has_train_val = any(\n",
        "            k.startswith(\"train_\") or k.startswith(\"val_\")\n",
        "            for k in vae_hist.keys()\n",
        "        )\n",
        "        if has_train_val:\n",
        "            fold_histories_local = [vae_hist]\n",
        "            print(\"[Eval] results_summary['vae_training_history'] looks like a flat history dict; \"\n",
        "                  \"treating it as one fold.\")\n",
        "            return fold_histories_local\n",
        "\n",
        "    # Case D: top-level history\n",
        "    top_hist = summary_dict.get(\"history\", None)\n",
        "    if isinstance(top_hist, dict):\n",
        "        fold_histories_local = [top_hist]\n",
        "        print(\"[Eval] Found top-level 'history' dict; treating as one fold.\")\n",
        "        return fold_histories_local\n",
        "\n",
        "    return []  # nothing found\n",
        "\n",
        "fold_histories = _extract_fold_histories(results_summary)\n",
        "\n",
        "if not fold_histories:\n",
        "    print(\"\\n[Eval] No usable training histories found; skipping convergence plots.\")\n",
        "else:\n",
        "    n_folds = len(fold_histories)\n",
        "    print(f\"\\n[Eval] Plotting convergence diagnostics (train/val loss vs epoch, n_folds={n_folds}).\")\n",
        "\n",
        "    plt.figure(figsize=(8, 5))\n",
        "    for i, hist in enumerate(fold_histories):\n",
        "        train_key = next((k for k in [\"train_total_loss\", \"train_loss\", \"train_elbo\"] if k in hist), None)\n",
        "        val_key   = next((k for k in [\"val_total_loss\", \"val_loss\", \"val_elbo\"] if k in hist), None)\n",
        "        if train_key is None or val_key is None:\n",
        "            print(f\"[Eval] Fold {i+1}: missing train/val loss keys; keys: {list(hist.keys())}\")\n",
        "            continue\n",
        "\n",
        "        train_losses = np.array(hist[train_key], dtype=float)\n",
        "        val_losses   = np.array(hist[val_key], dtype=float)\n",
        "        epochs = np.arange(1, len(train_losses) + 1)\n",
        "\n",
        "        plt.plot(\n",
        "            epochs,\n",
        "            train_losses,\n",
        "            alpha=0.3,\n",
        "            label=\"train (fold 1)\" if i == 0 else None,\n",
        "        )\n",
        "        plt.plot(\n",
        "            epochs,\n",
        "            val_losses,\n",
        "            alpha=0.3,\n",
        "            linestyle=\"--\",\n",
        "            label=\"val (fold 1)\" if i == 0 else None,\n",
        "        )\n",
        "\n",
        "    plt.xlabel(\"Epoch\")\n",
        "    plt.ylabel(\"Total loss (≈ -ELBO)\")\n",
        "    if n_folds > 1:\n",
        "        plt.title(f\"SemiSupMIWAE convergence: train vs val ({n_folds}-fold CV)\")\n",
        "    else:\n",
        "        plt.title(\"SemiSupMIWAE convergence: train vs val (single split)\")\n",
        "    plt.legend()\n",
        "    plt.grid(True, alpha=0.3)\n",
        "    plt.show()\n",
        "\n",
        "    epochs_per_fold = []\n",
        "    for hist in fold_histories:\n",
        "        train_key = next((k for k in [\"train_total_loss\", \"train_loss\", \"train_elbo\"] if k in hist), None)\n",
        "        if train_key is not None and isinstance(hist[train_key], (list, tuple)):\n",
        "            epochs_per_fold.append(len(hist[train_key]))\n",
        "    if epochs_per_fold:\n",
        "        print(f\"[Eval] Epochs per fold (train history length): {epochs_per_fold}\")\n",
        "        print(f\"[Eval] Mean epochs to convergence (approx):    {np.mean(epochs_per_fold):.1f}\")\n",
        "\n",
        "# ----------------------------------------------------------\n",
        "# 6. Residual diagnostics in log-price space\n",
        "#     (histogram + QQ-plot vs Normal)\n",
        "# ----------------------------------------------------------\n",
        "if y_true_log_eval is not None and mu_log_eval is not None:\n",
        "    print(\"\\n[Eval] Generating residual diagnostics in log-price space.\")\n",
        "\n",
        "    resid_log = y_true_log_eval - mu_log_eval  # true - predicted (log space)\n",
        "\n",
        "    # Histogram\n",
        "    plt.figure(figsize=(7, 4))\n",
        "    plt.hist(resid_log, bins=50, density=True, alpha=0.7)\n",
        "    plt.xlabel(\"Residual (log_y_true - log_y_pred)\")\n",
        "    plt.ylabel(\"Density\")\n",
        "    plt.title(\"Residuals in log-price space\")\n",
        "    plt.grid(True, alpha=0.3)\n",
        "    plt.show()\n",
        "\n",
        "    # QQ-plot vs Normal\n",
        "    if HAVE_SCIPY:\n",
        "        plt.figure(figsize=(5, 5))\n",
        "        stats.probplot(resid_log, dist=\"norm\", plot=plt)\n",
        "        plt.title(\"QQ-plot of log-price residuals vs Normal\")\n",
        "        plt.grid(True, alpha=0.3)\n",
        "        plt.show()\n",
        "    else:\n",
        "        print(\"[Eval] Skipping QQ-plot (scipy not available).\")\n",
        "else:\n",
        "    print(\"\\n[Eval] No stored residuals; skipping residual diagnostics.\")\n",
        "\n",
        "# ----------------------------------------------------------\n",
        "# 7. Latent-space visualization\n",
        "#     - z1–z2 scatter colored by sale-price deciles\n",
        "#     - z1–z2 scatter colored by building class (if available)\n",
        "#     - marginal histograms of first 3 dims\n",
        "# ----------------------------------------------------------\n",
        "print(\"\\n[Eval] Generating latent space visualizations.\")\n",
        "\n",
        "# get_latent_representation in VAETrainer expects a NumPy array on CPU,\n",
        "# not a torch CUDA tensor.\n",
        "X_all_np = X_filled_np.astype(np.float32, copy=False)\n",
        "\n",
        "try:\n",
        "    mu_z, logvar_z = vae_trainer.get_latent_representation(\n",
        "        X_all_np,\n",
        "        batch_size=1024,\n",
        "    )\n",
        "    mu_z = np.asarray(mu_z)\n",
        "except AttributeError:\n",
        "    raise AttributeError(\n",
        "        \"[Eval] VAETrainer is missing 'get_latent_representation'. \"\n",
        "        \"Add such a method or expose model.encode() to extract latent means.\"\n",
        "    )\n",
        "\n",
        "if mu_z.ndim != 2 or mu_z.shape[1] < 2:\n",
        "    raise ValueError(\n",
        "        f\"[Eval] Latent representation has shape {mu_z.shape}; \"\n",
        "        \"need at least 2 dims for scatter.\"\n",
        "    )\n",
        "\n",
        "# --- 7a. z1–z2 colored by sale-price deciles (Figure 4) ---\n",
        "log_price_all = df_pred[log_y_col].astype(float).values\n",
        "mask_finite_price = np.isfinite(log_price_all)\n",
        "if mask_finite_price.sum() > 0:\n",
        "    # Compute decile edges on finite values\n",
        "    decile_edges = np.quantile(log_price_all[mask_finite_price], np.linspace(0, 1, 11))\n",
        "    # Map each point to a decile index 0–9; NaNs get -1\n",
        "    decile_idx = np.full_like(log_price_all, fill_value=-1, dtype=int)\n",
        "    decile_idx[mask_finite_price] = np.searchsorted(decile_edges[1:-1],\n",
        "                                                    log_price_all[mask_finite_price],\n",
        "                                                    side=\"right\")\n",
        "\n",
        "    valid_mask = decile_idx >= 0\n",
        "    plt.figure(figsize=(7, 6))\n",
        "    sc = plt.scatter(\n",
        "        mu_z[valid_mask, 0],\n",
        "        mu_z[valid_mask, 1],\n",
        "        c=decile_idx[valid_mask],\n",
        "        s=5,\n",
        "        alpha=0.6,\n",
        "        cmap=\"viridis\",\n",
        "    )\n",
        "    cbar = plt.colorbar(sc)\n",
        "    cbar.set_label(\"Sale-price decile (0 = lowest, 9 = highest)\")\n",
        "    plt.xlabel(\"z1\")\n",
        "    plt.ylabel(\"z2\")\n",
        "    plt.title(\"Latent space (z1 vs z2) colored by sale-price deciles\")\n",
        "    plt.grid(True, alpha=0.3)\n",
        "    plt.show()\n",
        "else:\n",
        "    print(\"[Eval] No finite log-price values; skipping decile-colored latent scatter.\")\n",
        "\n",
        "# --- 7b. z1–z2 colored by building class (Figure 5) ---\n",
        "bldg_class_candidates = [\n",
        "    \"bldg_class\", \"building_class\", \"bldg_class_group\", \"bldgclass\"\n",
        "]\n",
        "bldg_class_col = next((c for c in bldg_class_candidates if c in df_pred.columns), None)\n",
        "\n",
        "if bldg_class_col is not None:\n",
        "    print(f\"[Eval] Using '{bldg_class_col}' as building class column for latent viz.\")\n",
        "    bldg_series = df_pred[bldg_class_col].astype(str)\n",
        "    codes, uniques = pd.factorize(bldg_series)\n",
        "\n",
        "    plt.figure(figsize=(7, 6))\n",
        "    # If many classes, this may be busy, but it matches the \"colored by building class\" idea.\n",
        "    for code, label in enumerate(uniques):\n",
        "        mask = codes == code\n",
        "        if not np.any(mask):\n",
        "            continue\n",
        "        plt.scatter(\n",
        "            mu_z[mask, 0],\n",
        "            mu_z[mask, 1],\n",
        "            s=5,\n",
        "            alpha=0.5,\n",
        "            label=label,\n",
        "        )\n",
        "    plt.xlabel(\"z1\")\n",
        "    plt.ylabel(\"z2\")\n",
        "    plt.title(\"Latent space (z1 vs z2) colored by building class\")\n",
        "    plt.grid(True, alpha=0.3)\n",
        "    # Limit legend size if many classes\n",
        "    n_classes = len(uniques)\n",
        "    if n_classes <= 10:\n",
        "        plt.legend(title=\"Building class\", fontsize=8)\n",
        "    else:\n",
        "        plt.legend(title=\"Building class (truncated)\", fontsize=6, ncol=2)\n",
        "    plt.show()\n",
        "else:\n",
        "    print(\"[Eval] No building-class column found; skipping building-class latent scatter.\")\n",
        "\n",
        "# --- 7c. Marginal histograms of first 3 latent dimensions ---\n",
        "num_latent_dims = mu_z.shape[1]\n",
        "plt.figure(figsize=(12, 3))\n",
        "for d in range(min(num_latent_dims, 3)):\n",
        "    plt.subplot(1, min(num_latent_dims, 3), d + 1)\n",
        "    plt.hist(mu_z[:, d], bins=40, alpha=0.7)\n",
        "    plt.xlabel(f\"z{d+1}\")\n",
        "    plt.ylabel(\"count\")\n",
        "    plt.title(f\"Latent dim z{d+1} marginal\")\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(\"[Eval] Latent visualizations generated (convergence, residuals, z-scatter, marginals).\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "07N9lq3dlC-z",
        "outputId": "2e48fb03-e988-483c-b874-76a587a96cd3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[Eval] Using latest run dir: /content/drive/MyDrive/e6691_2025Spring_nyre_local/results/2025-10-12_01-58-16\n",
            "INFO:MIWAE_Eval:[Eval] Using latest run dir: /content/drive/MyDrive/e6691_2025Spring_nyre_local/results/2025-10-12_01-58-16\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "[Eval] Using device: cuda\n",
            "[Eval] RUN_DIR = /content/drive/MyDrive/e6691_2025Spring_nyre_local/results/2025-10-12_01-58-16\n",
            "[Eval] RESULTS_HISTORY_PKL: /content/drive/MyDrive/e6691_2025Spring_nyre_local/results/2025-10-12_01-58-16/miwae_analysis_results.pkl\n",
            "[Eval] PRED_PARQUET_PATH:   /content/drive/MyDrive/e6691_2025Spring_nyre_local/results/2025-10-12_01-58-16/final_data_with_miwae_predictions.parquet\n",
            "[Eval] PRED_CSV_PATH:       /content/drive/MyDrive/e6691_2025Spring_nyre_local/results/2025-10-12_01-58-16/final_data_with_miwae_predictions.csv\n",
            "[Eval] BEST_MODEL_PATH:   /content/drive/MyDrive/e6691_2025Spring_nyre_local/results/2025-10-12_01-58-16/vae_results/best_model.pth\n",
            "[Eval] Loaded results_summary from pickle.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2025-11-23 03:23:30,456 - __main__.prepare_vae_input - INFO - Preparing data and masks for VAE. Input features: 44, Target column: log_sale_price, Scaler type for X: standard\n",
            "2025-11-23 03:23:30,456 - __main__.prepare_vae_input - INFO - Preparing data and masks for VAE. Input features: 44, Target column: log_sale_price, Scaler type for X: standard\n",
            "INFO:__main__.prepare_vae_input:Preparing data and masks for VAE. Input features: 44, Target column: log_sale_price, Scaler type for X: standard\n",
            "2025-11-23 03:23:30,511 - __main__.prepare_vae_input - INFO - X data shape: (122712, 43). X mask shape: (122712, 43). Number of missing values in X: 0 (0.00%).\n",
            "2025-11-23 03:23:30,511 - __main__.prepare_vae_input - INFO - X data shape: (122712, 43). X mask shape: (122712, 43). Number of missing values in X: 0 (0.00%).\n",
            "INFO:__main__.prepare_vae_input:X data shape: (122712, 43). X mask shape: (122712, 43). Number of missing values in X: 0 (0.00%).\n",
            "2025-11-23 03:23:30,513 - __main__.prepare_vae_input - INFO - Applying standard scaling to X features (NaNs are typically ignored by fit and transform)...\n",
            "2025-11-23 03:23:30,513 - __main__.prepare_vae_input - INFO - Applying standard scaling to X features (NaNs are typically ignored by fit and transform)...\n",
            "INFO:__main__.prepare_vae_input:Applying standard scaling to X features (NaNs are typically ignored by fit and transform)...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Eval] Loaded predictions from Parquet. Shape: (122712, 150)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2025-11-23 03:23:30,628 - __main__.prepare_vae_input - INFO - Y target ('log_sale_price') data shape: (122712, 1). Y mask shape: (122712, 1). Number of missing values in Y: 30678 (25.00%).\n",
            "2025-11-23 03:23:30,628 - __main__.prepare_vae_input - INFO - Y target ('log_sale_price') data shape: (122712, 1). Y mask shape: (122712, 1). Number of missing values in Y: 30678 (25.00%).\n",
            "INFO:__main__.prepare_vae_input:Y target ('log_sale_price') data shape: (122712, 1). Y mask shape: (122712, 1). Number of missing values in Y: 30678 (25.00%).\n",
            "2025-11-23 03:23:30,630 - __main__.prepare_vae_input - INFO - VAE input preparation complete. Shapes: X_filled: (122712, 43), X_mask: (122712, 43), Y_filled: (122712, 1), Y_mask: (122712, 1)\n",
            "2025-11-23 03:23:30,630 - __main__.prepare_vae_input - INFO - VAE input preparation complete. Shapes: X_filled: (122712, 43), X_mask: (122712, 43), Y_filled: (122712, 1), Y_mask: (122712, 1)\n",
            "INFO:__main__.prepare_vae_input:VAE input preparation complete. Shapes: X_filled: (122712, 43), X_mask: (122712, 43), Y_filled: (122712, 1), Y_mask: (122712, 1)\n",
            "2025-11-23 03:23:30,657 - __main__.create_vae_from_artifacts - INFO - Creating VAE model. Target column for supervision: log_sale_price.\n",
            "2025-11-23 03:23:30,657 - __main__.create_vae_from_artifacts - INFO - Creating VAE model. Target column for supervision: log_sale_price.\n",
            "INFO:__main__.create_vae_from_artifacts:Creating VAE model. Target column for supervision: log_sale_price.\n",
            "2025-11-23 03:23:30,659 - __main__.create_vae_from_artifacts - INFO - Number of input features for VAE (X): 43 (Features: ['yearbuilt', 'assess_total', 'exempt_total', 'assessland', 'lotarea']...)\n",
            "2025-11-23 03:23:30,659 - __main__.create_vae_from_artifacts - INFO - Number of input features for VAE (X): 43 (Features: ['yearbuilt', 'assess_total', 'exempt_total', 'assessland', 'lotarea']...)\n",
            "INFO:__main__.create_vae_from_artifacts:Number of input features for VAE (X): 43 (Features: ['yearbuilt', 'assess_total', 'exempt_total', 'assessland', 'lotarea']...)\n",
            "2025-11-23 03:23:30,661 - __main__.create_vae_from_artifacts - INFO - Semi-supervised mode: Target 'log_sale_price' (y_dim=1). Alpha for price loss: 10.0.\n",
            "2025-11-23 03:23:30,661 - __main__.create_vae_from_artifacts - INFO - Semi-supervised mode: Target 'log_sale_price' (y_dim=1). Alpha for price loss: 10.0.\n",
            "INFO:__main__.create_vae_from_artifacts:Semi-supervised mode: Target 'log_sale_price' (y_dim=1). Alpha for price loss: 10.0.\n",
            "2025-11-23 03:23:30,663 - __main__.create_vae_from_artifacts - INFO - Using overridden latent_dim: 3\n",
            "2025-11-23 03:23:30,663 - __main__.create_vae_from_artifacts - INFO - Using overridden latent_dim: 3\n",
            "INFO:__main__.create_vae_from_artifacts:Using overridden latent_dim: 3\n",
            "2025-11-23 03:23:30,664 - __main__.create_vae_from_artifacts - INFO - Final effective Latent Dimension: 3\n",
            "2025-11-23 03:23:30,664 - __main__.create_vae_from_artifacts - INFO - Final effective Latent Dimension: 3\n",
            "INFO:__main__.create_vae_from_artifacts:Final effective Latent Dimension: 3\n",
            "2025-11-23 03:23:30,665 - __main__.create_vae_from_artifacts - INFO - Encoder layers: [21], Decoder layers: [21]\n",
            "2025-11-23 03:23:30,665 - __main__.create_vae_from_artifacts - INFO - Encoder layers: [21], Decoder layers: [21]\n",
            "INFO:__main__.create_vae_from_artifacts:Encoder layers: [21], Decoder layers: [21]\n",
            "2025-11-23 03:23:30,667 - __main__.create_vae_from_artifacts - INFO - Using determined prior: Type=student_t_mixture, K=5 (from overrides or artifact inference).\n",
            "2025-11-23 03:23:30,667 - __main__.create_vae_from_artifacts - INFO - Using determined prior: Type=student_t_mixture, K=5 (from overrides or artifact inference).\n",
            "INFO:__main__.create_vae_from_artifacts:Using determined prior: Type=student_t_mixture, K=5 (from overrides or artifact inference).\n",
            "2025-11-23 03:23:30,669 - __main__.create_vae_from_artifacts - INFO - Initializing parameters for student_t_mixture with K=5 components...\n",
            "2025-11-23 03:23:30,669 - __main__.create_vae_from_artifacts - INFO - Initializing parameters for student_t_mixture with K=5 components...\n",
            "INFO:__main__.create_vae_from_artifacts:Initializing parameters for student_t_mixture with K=5 components...\n",
            "2025-11-23 03:23:30,670 - __main__.create_vae_from_artifacts - WARNING - DP-GMM artifacts are unsuitable or insufficient for initializing mixture prior parameters. Default parameters (e.g., zeros for means/logits, identity for covariances) will be used by the VAE.\n",
            "2025-11-23 03:23:30,670 - __main__.create_vae_from_artifacts - WARNING - DP-GMM artifacts are unsuitable or insufficient for initializing mixture prior parameters. Default parameters (e.g., zeros for means/logits, identity for covariances) will be used by the VAE.\n",
            "WARNING:__main__.create_vae_from_artifacts:DP-GMM artifacts are unsuitable or insufficient for initializing mixture prior parameters. Default parameters (e.g., zeros for means/logits, identity for covariances) will be used by the VAE.\n",
            "2025-11-23 03:23:30,672 - __main__.create_vae_from_artifacts - INFO - Setting Student-t df to 4.0 for prior.\n",
            "2025-11-23 03:23:30,672 - __main__.create_vae_from_artifacts - INFO - Setting Student-t df to 4.0 for prior.\n",
            "INFO:__main__.create_vae_from_artifacts:Setting Student-t df to 4.0 for prior.\n",
            "2025-11-23 03:23:30,673 - __main__.create_vae_from_artifacts - INFO - Instantiating SemiSupMIWAE model...\n",
            "2025-11-23 03:23:30,673 - __main__.create_vae_from_artifacts - INFO - Instantiating SemiSupMIWAE model...\n",
            "INFO:__main__.create_vae_from_artifacts:Instantiating SemiSupMIWAE model...\n",
            "2025-11-23 03:23:30,674 - __main__.create_vae_from_artifacts - INFO - Price head layers for SemiSupMIWAE: [8, 4]\n",
            "2025-11-23 03:23:30,674 - __main__.create_vae_from_artifacts - INFO - Price head layers for SemiSupMIWAE: [8, 4]\n",
            "INFO:__main__.create_vae_from_artifacts:Price head layers for SemiSupMIWAE: [8, 4]\n",
            "2025-11-23 03:23:30,685 - SemiSupMIWAE(X:43,Y:1,Lat:3) - INFO - SemiSupMIWAE Initialized: InputXDim=43, TargetYDim=1, LatentDim=3, AlphaPriceLoss=10.0, Device=cuda\n",
            "INFO:SemiSupMIWAE(X:43,Y:1,Lat:3):SemiSupMIWAE Initialized: InputXDim=43, TargetYDim=1, LatentDim=3, AlphaPriceLoss=10.0, Device=cuda\n",
            "2025-11-23 03:23:30,686 - __main__.create_vae_from_artifacts - INFO - SemiSupMIWAE model created successfully on device: cuda.\n",
            "2025-11-23 03:23:30,686 - __main__.create_vae_from_artifacts - INFO - SemiSupMIWAE model created successfully on device: cuda.\n",
            "INFO:__main__.create_vae_from_artifacts:SemiSupMIWAE model created successfully on device: cuda.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Eval] VAE input shapes: X=(122712, 43), Y=(122712, 1)\n",
            "[Eval] Loaded model_state_dict into VAE model from best_model.pth\n",
            "\n",
            "[Eval] cv_* columns not found; falling back to direct predictions from best_model on a random hold-out split.\n",
            "[Eval] Evaluating on random hold-out: 18407 / 92034 observed rows.\n",
            "\n",
            "=== Posterior Predictive Metrics (best_model, random hold-out) ===\n",
            "Average log posterior predictive (log p(y_log | x)): -1.1483\n",
            "Average NLL (log-price space):                     1.1483\n",
            "RMSE (log-price):                                  1.8419\n",
            "MAE  (log-price):                                  0.9667\n",
            "RMSE (price):                                      24,551,264.3100\n",
            "MAE  (price):                                      2,184,720.6044\n",
            "MAPE (price, y_true > 0):                          169,590.75%\n",
            "Median |y - ŷ| (log):                              0.2309\n",
            "95th pct |y - ŷ| (log):                            4.0910\n",
            "[Eval] results_summary['vae_training_history'] looks like a flat history dict; treating it as one fold.\n",
            "\n",
            "[Eval] Plotting convergence diagnostics (train/val loss vs epoch, n_folds=1).\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 800x500 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Eval] Epochs per fold (train history length): [100]\n",
            "[Eval] Mean epochs to convergence (approx):    100.0\n",
            "\n",
            "[Eval] Generating residual diagnostics in log-price space.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 700x400 with 1 Axes>"
            ],
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAmcAAAGKCAYAAABJgObQAAAAOnRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjEwLjAsIGh0dHBzOi8vbWF0cGxvdGxpYi5vcmcvlHJYcgAAAAlwSFlzAAAPYQAAD2EBqD+naQAATYZJREFUeJzt3XlYVHXfBvB7YFCEwUFRMjHERJEEFMGNNPf0eRNRNLFMXDAqdykrN8gFQs1UJBNxaVMjFTWSUEsfK00zLdxwA01UFHAQZgRZhnn/8GFyHNAZnGEOw/25Lit+85tzvufLCW7PNiKVSqUCEREREQmChakLICIiIqJ/MZwRERERCQjDGREREZGAMJwRERERCQjDGREREZGAMJwRERERCQjDGREREZGAMJwRERERCQjDGREREZGAMJwR1RJjxoyBm5ubxh8PDw8MHDgQCxYsgFwuN+j6Vq9ejRdeeOGxcz788EMMGDDAoOs9duwY3Nzc8Oeff1br/devX4ebmxt27979VHW4ublhzZo1T7UMYzLUdhKR8IhNXQAR6c7X1xcrV65Uf11cXIzU1FR8/PHHuH79OuLj4w22rgkTJuC1114z2PJqyrPPPovffvsNDRs2NHUpRlVXtpOoLmI4I6pFrKys0LRpU42xFi1aQCQSYebMmUhNTUWHDh0Msi5bW1vY2toaZFk1ydLSUqtH5qiubCdRXcTTmkRmoG3btgCArKws9diJEycQHByMLl26wNfXF9OnT8ft27fVr8tkMrz//vvo0aMHPD09MWDAAKxbt079+qOnNW/cuIHx48fDy8sLPXv21JgLVH2abdCgQfjwww/VXx8/fhxjxoxBx44d4e3tjaCgIBw7dqzKbcvMzMTkyZPRvXt3eHl5YfDgwdi+fXuV8x+tY/Xq1XjppZdw+vRpjBw5Eh06dMCAAQOwa9euKpdRmaNHj2LUqFHw8vKCt7c3xo4di1OnTqlfV6lU+PTTT9G9e3d4e3tj+vTpOHr06BNP0SYmJsLNzQ2nTp3CiBEj4OnpiV69eiEhIUE9p2IbEhMT0bVrV6xcubLSfu/duxdDhgyBl5cXXn75ZWzatEljXT///DOCgoLQqVMndOvWDfPmzXvi6fBvvvkG//nPf+Dl5YVu3bph+vTpyM7OBvDvKejffvsN48aNg5eXF/z8/BAbG6uxjO+++w6DBw+Gh4cHunXrhsmTJ+P69esac4xRO1FtxXBGZAYuXboEAHBycgIApKenY8KECZBKpfjmm28QHx+PzMxMTJw4EUqlEgAQGRmJ8+fPY82aNdi7dy9mzJiBNWvWVHkN08yZM3Ht2jVs3LgRmzZtwtWrV/Hrr7/qVadcLkdoaCieffZZ7Ny5Ezt37oSbmxsmTZqEO3fuVPqeWbNmQaFQYNOmTfjxxx/x2muvYd68eXpdk1ZUVISlS5fivffew+7du/HCCy9g3rx5GmH1cc6fP4+JEyeibdu22LFjB7Zu3YoGDRpg3Lhx6mVs3boV69atQ0hICHbs2AFPT0989NFHOte4aNEizJw5E7t27UKvXr0QERGhEf6Ki4uxZ88ebNmyBePGjdN6/++//46ZM2di2LBhSEpKwowZM/Dpp59i8+bNAB4EqSlTpsDd3R3bt2/Hp59+iqNHjyIsLKzKmn777TdERkbirbfewo8//oh169bh9u3beP/99zXmRUVF4bXXXsP333+P1157DatXr0ZKSgoA4MiRI5g/fz5effVV7Nu3Dxs3bsSdO3c01muM2olqM57WJKrFysvLcebMGSxfvhwdOnSAp6cnAOCrr76Cra0tli9fjnr16gEAoqOj4e/vj19++QV9+vRBWloaunbtCi8vLwBA8+bN8fzzz8PBwUFrPVeuXEFqaio+/fRT+Pr6AngQJvr06aNXvdbW1khMTETTpk0hkUgAAKGhoUhISMDff/+Nfv36ab0nLS0NU6dORbt27QAAo0ePhqenJ5ydnXVeb0FBAWbMmAEfHx8AQEhICFJSUnD+/Hk888wzT3z/5s2b0aRJE0RERMDS0hIA8Mknn8DPzw+7d+9GaGgovv/+e3Tv3h0TJ04EADz//PNIT0/HlStXdKrx1VdfxYsvvggAmDdvHpKSkvDjjz+qvz93797FpEmT0Lp1awCAQqHQeP+mTZvQuXNnjB8/HgDQsmVLZGVl4d69ewCA+Ph4tG3bVh0Yn3/+ecydOxdvv/02Ll68qD76+rC0tDTY2NjA398flpaWcHJyQkxMDGQymca8AQMGYODAgQCAqVOn4ocffkBycjIGDRqEDh06ICkpSb385s2bY+TIkZg9ezbkcjns7OyMUjtRbcZwRlSL/PHHH/D29lZ/XVpaCgAYOHAg5s2bpx4/deoUOnXqpA5mwINTn/b29khLS0OfPn3Qu3dvfPnll1AqlejXrx86d+4Md3f3Stebnp4OAOqABDy45snT0xMXL17UuX4rKytkZWVh8eLFuHjxIhQKBVQqFQAgPz+/0vf07t0bsbGxuHPnDnr16oVOnTqpA4s+KoIrADRq1Oix63zUmTNn4OXlpQ5mACCRSNCqVSucPXsWwIPTr4MHD9Z4X8+ePZGYmKjTOh6+VrBevXpo06YNbt68qTGnffv2j60xICBAYywkJET936dOncKwYcM0Xu/cuTOAByGssoDj5+eHmJgYjB49GsOHD4efnx+cnJzg6OhYZe0A8MILLyAzMxMAYGNjgxMnTmDOnDm4fv06iouLUVZWBuBB/+3s7IxSO1FtxnBGVIt4eXlhyZIl6q/j4+Px888/Y/78+bC3t1ePKxQKHDx4UCPIAQ9O7+Xm5gIA3nvvPTg7O2Pnzp3Ytm0b6tWrh2HDhmH27NmoX7++xvsqjtI0aNBAY9zGxkav+k+dOoWQkBD07t0bK1asQJMmTXD37l0EBQVV+Z4lS5bg66+/RlJSEjZu3AiJRILg4GBMnToVFha6XZlhaWmpEVRFIhEAqIPhkygUikpvjrC1tVX35u7du7Czs9N4/eHvCQCN70fz5s2xZ88e9dePvtfGxkbjmipLS0tYW1tXWWNBQcFjvx8KhQKbN2/Gd999p/VaxT7xqPbt22Pz5s3YuHEjoqOjoVAo4O3tjY8++kgjqD+u9o0bN2Lp0qV46623MHDgQEgkEvz3v/9FVFSUUWsnqs0YzohqEWtra7Rs2VL99axZs/DTTz9h6dKlGr/s7Ozs0KNHD8yZM0drGRWnEy0sLDBq1CiMGjUKMpkMSUlJWL58OSQSCd577z2N91T84iwqKtIYfzg8VBV4CgsL1f+dnJwMa2trrFq1Sh2Wzp0798RtfvPNN/Hmm2/i9u3bSEhIwOeffw5HR8cae9SHnZ2d1mlE4MH2VxxFqlevHoqLizVev3v3rsbXD9+EIBZr/vh9tLf37t1TX0Ooi0aNGlVaYwU7OzsMHDhQ44hUBalUWuX7vLy8sHLlSpSWluKPP/7A0qVL8eabb+LQoUPqOQ9/jytqr3jEx549e/Diiy9qXB/26D5irNqJaiveEEBUi0mlUoSFhSExMVHjAnlPT09cvXoVzs7OaNmypfpPaWkpGjdujPv372PPnj3qcNW4cWOMHTsWL774Ii5fvqy1nlatWgEATp8+rR67f/8+Tp48qf66IvQVFBSox7KystR39gEPTsPa2tpqHMVKSkoCUPlRrPz8fOzevVt9E8MzzzyDadOmoU2bNpXWaSweHh5ITU1V11FR25UrV9SnS11cXNSnOCvs27dP4+uHvxePBq+Hv38lJSW4fPmyuu+6aN++vcb3AwDWrl2LuXPnAniwT2RmZmrU0KJFC5SVlWkd4atw8uRJ9U0JVlZWePHFFzFlyhRkZ2drnBI+ceKExvvOnj2rrr20tFR9Ghl48H1+9HtujNqJajOGM6JabsSIEWjfvj0iIiJQUlIC4MGnCdy8eRPh4eG4cOECMjIy8Mknn2Do0KG4fPkyxGIxli1bhtmzZ+Ps2bPIysrCwYMHcfLkSfW1PA9r06aN+on5J0+exMWLFzFv3jyN02xSqRQtWrTAjh07cP78eZw9exZz585F8+bN1XO8vLyQk5OD7du3IzMzE59//jny8vJgZWWF06dPax1pUqlU+Oijj7BgwQJcunQJN2/exO7du3HlypVK6zSW4OBg5OXlYd68eUhPT8fZs2cxc+ZMSCQS9bVQgwYNwu+//46tW7fi6tWriI+P1ytAJiQk4JdffkFGRgYWL16M+/fva13D9jjjxo3D6dOnERsbi+vXryMlJQVr165VX481YcIEHD16FKtWrUJ6ejouXLiAefPmqY+cVubgwYOYPHkyDh48iJs3b+L8+fP47rvv0KZNG43AtW/fPiQnJ+Pq1auIiYnBtWvX1NeQeXl54bfffsOJEydw+fJlzJw5U31K9MSJEygsLDRK7US1GU9rEtVyFhYWiIiIwMiRI7F+/XpMmjQJrq6u2LRpE1asWIGRI0dCpVLB09MTGzZsQJs2bQAAGzZswJIlSzBu3Djcv38fzZs3x5gxYyp9TAMArFq1CvPnz0dwcDDs7e3xxhtvoFGjRvjvf/+rnrNkyRIsWLAAI0eORPPmzTFr1ix88cUX6tcHDx6Mv//+G8uWLYNKpcLLL7+Mjz76CA0bNlQ/nqJ3797q+fb29li/fj1WrlyJUaNGoaysDM7Ozvjggw8waNAgI3Szcq6urli/fj1WrFiBYcOGQSwWw9fXF9988w0aN24M4EGAuHHjBj755BNYWlpiwIABCAsLwzvvvKN1DV9lwsLC8Nlnn+HMmTNwcHBAVFSU+s5MXXTr1g3Lli3D559/jrVr1+LZZ5/F5MmTERwcDADq54999tlniI+Ph5WVFXx9ffH111+rt+FR06ZNg1KpxIIFC5CbmwupVAofHx+tj7WaNm0adu7cidmzZ8PW1hbvvvuu+s7TGTNm4Pbt25g4cSIaNmyI8ePHY/To0UhPT8f8+fNha2uLAQMGGLx2otpMpNL1ilgiIqqSUqlEXl4emjRpoh7bunUrPvroIxw9elTjSNPDEhMTMXv2bBw6dAjNmjWrqXIN4tixYwgODsbmzZvVj1ghoqfH05pERAbw7bffolevXkhMTMSNGzdw5MgRrFu3Dv369asymBERVYanNYmIDOD111/HvXv3sHbtWkRERKBJkybo2bMnn2JPRHrjaU0iIiIiAeFpTSIiIiIBYTgjIiIiEhCGMyIiIiIBqbM3BJSVlSE/Px/169fX+fP5iIiIiKqjvLwcxcXFkEqlWh/f9qg6G87y8/Nx9epVU5dBREREdYiLiwscHBweO6fOhrOKJ3a7uLigQYMGJq5GfyqVCgqFAhKJRP2B02Q47K9xsb/Gxf4aF/trXOba36KiIly9elWnTwyps+Gs4lRmgwYNYGNjY+Jq9KdSqVBaWgobGxuz2nmFgv01LvbXuNhf42J/jcvc+6vLpVS82IqIiIhIQBjOiIiIiASE4YyIiIhIQBjOiIiIiASE4YyIiIhIQBjOiIiIiASE4YyIiIhIQBjOiIiIiASE4YyIiIhIQBjOiIiIiASE4YyIiIhIQOrsZ2sSEZmjkC+OQwWgrLQUYisrVPXJhBvGda7JsohIDyYPZwqFApGRkThy5AiKiorg6emJuXPn4vnnn9ea6+npqTWmVCoREBCAjz/+uCbKJSIiIjIqk4ez8PBw3LhxA1u2bIFUKsXKlSsRGhqK5ORk1KtXT2Pu6dOnNb6+d+8eXnnlFfj7+9dkyURERERGY9JrzmQyGVJSUjB9+nQ4OTlBIpEgLCwMt2/fxuHDh5/4/hUrVqBTp07w8/OrgWqJiIiIjM+kR87S0tKgVCrh5eWlHrOxsYGrqytSU1PRp0+fKt+bnp6O7du3Y+/evY9dR3Z2NnJycrTGy8vLAQAqlQoqlaqaW2A6FXXXxtprA/bXuNhf41E99E9ABVUVV52x99XH/de4zLW/+myPScOZTCaDpaUlJBKJxrhUKoVMJnvse1evXo3AwEA888wzj52XkJCA2NhYrXEXFxdERUVBoVCgtLRU/+JNTKVSobCwEAAgElV1yS9VF/trXOyv8ZT97+eZUql87Lz8/PyaKMcscf81LnPtb3Fxsc5zTRrOqmr6k9JlZmYm9u7di3379j1xHUFBQejbt6/WeHl5OUpKSiCRSGBjY6NbwQJS0SOpVGpWO69QsL/Gxf4aj9jKChVHzsRWYqCKI2dSqbTmijIz3H+Ny1z7WxE4dWHScObg4AClUgm5XA47Ozv1eF5eHnx8fKp8348//gh3d3c899xzT1yHo6MjHB0dtcYLCwuRlpYGkUhUa7/5FbXX1vqFjv01LvbXOETAQ6cyRVU+SoN9fzrcf43LHPurz7aY9IYAd3d3iMVipKamqscKCgqQnp6Ojh07Vvm+ffv2PfZ6NCIiIqLayqThzN7eHv7+/oiJiUFWVhbkcjmio6Ph4uICPz8/7N+/H0OGDNF4T2lpKc6fPw83NzcTVU1ERERkPCb/+KaIiAi0bdsWAQEB6NmzJ3JzcxEXFwexWAy5XI4rV65ozL979y5KS0vRqFEjE1VMREREZDwmfwhtgwYNsHjxYixevFjrtcDAQAQGBmqMNW3aFBcuXKip8oiIiIhqlMmPnBERERHRvxjOiIiIiASE4YyIiIhIQBjOiIiIiASE4YyIiIhIQBjOiIiIiASE4YyIiIhIQBjOiIiIiASE4YyIiIhIQBjOiIiIiASE4YyIiIhIQBjOiIiIiASE4YyIiIhIQBjOiIiIiASE4YyIiIhIQBjOiIiIiASE4YyIiIhIQBjOiIiIiASE4YyIiIhIQBjOiIiIiASE4YyIiIhIQBjOiIiIiASE4YyIiIhIQBjOiIiIiASE4YyIiIhIQBjOiIiIiASE4YyIiIhIQBjOiIiIiASE4YyIiIhIQBjOiIiIiATE5OFMoVBg9uzZ6NWrF7p06YKQkBBkZGRUOf/YsWMIDAyEl5cXevfujXXr1tVgtURERETGZfJwFh4ejoyMDGzZsgUHDhxAq1atEBoaipKSEq256enpeOeddxAcHIzjx49j5cqV2L17N06dOmWCyomIiIgMz6ThTCaTISUlBdOnT4eTkxMkEgnCwsJw+/ZtHD58WGv+l19+iZdeeglDhw5F/fr10bFjR+zZswdeXl4mqJ6IiIjI8MSmXHlaWhqUSqVGuLKxsYGrqytSU1PRp08fjfnHjh3DwIEDMWnSJBw7dgyOjo4ICQnBiBEjqlxHdnY2cnJytMbLy8sBACqVCiqVykBbVHMq6q6NtdcG7K9xsb/Go3ron4AKKogqn8feVxv3X+My1/7qsz0mDWcymQyWlpaQSCQa41KpFDKZTGv+rVu3sH37dqxcuRKffvopkpOTMWfOHLRo0QLdunWrdB0JCQmIjY3VGndxcUFUVBQUCgVKS0sNs0E1SKVSobCwEAAgElX+w5eqj/01LvbXeMr+9/NMqVQ+dl5+fn5NlGOWuP8al7n2t7i4WOe5Jg1nVTW9qnSpUqnQv39/dOnSBQAQGBiInTt3IikpqcpwFhQUhL59+2qNl5eXo6SkBBKJBDY2NtXcAtOp6JFUKjWrnVco2F/jYn+NR2xlhYojZ2IrMVDFkTOpVFpzRZkZ7r/GZa79rQicujBpOHNwcIBSqYRcLoednZ16PC8vDz4+PlrzHR0dYW9vrzHm5ORU6WnLh9/j6OioNV5YWIi0tDSIRKJa+82vqL221i907K9xsb/GIQIeOpUpqiKamdcRCVPg/mtc5thffbbFpDcEuLu7QywWIzU1VT1WUFCA9PR0dOzYUWt+27ZtcfbsWY2xa9euoUWLFsYulYiIiKhGmDSc2dvbw9/fHzExMcjKyoJcLkd0dDRcXFzg5+eH/fv3Y8iQIer5Y8aMwZEjR5CYmIiSkhJ8//33+Ouvvx57QwARERFRbWLy55xFRESgbdu2CAgIQM+ePZGbm4u4uDiIxWLI5XJcuXJFPbd79+5YunQp4uLi0KlTJ3z22WeIjY3FCy+8YMItICIiIjIckcrc7lXVUcU1Z+7u7rX2hoD8/Hyzu2BSKNhf42J/jSfki+NQ4cFdm2IrqyqvOdswrnNNlmVWuP8al7n2V5/cYfIjZ0RERET0L4YzIiIiIgFhOCMiIiISEIYzIiIiIgFhOCMiIiISEIYzIiIiIgFhOCMiIiISEIYzIiIiIgFhOCMiIiISEIYzIiIiIgFhOCMiIiISEIYzIiIiIgFhOCMiIiISEIYzIiIiIgFhOCMiIiISEIYzIiIiIgFhOCMiIiISEIYzIiIiIgFhOCMiIiISEIYzIiIiIgFhOCMiIiISEIYzIiIiIgFhOCMiIiISEIYzIiIiIgFhOCMiIiISEIYzIiIiIgFhOCMiIiISEIYzIiIiIgFhOCMiIiISEIYzIiIiIgERm7oAhUKByMhIHDlyBEVFRfD09MTcuXPx/PPPa80dM2YMTpw4AUtLS43x5ORkPPfcczVVMhEREZHRmDychYeH48aNG9iyZQukUilWrlyJ0NBQJCcno169elrz33nnHUydOtUElRIREREZn0lPa8pkMqSkpGD69OlwcnKCRCJBWFgYbt++jcOHD5uyNCIiIiKTMOmRs7S0NCiVSnh5eanHbGxs4OrqitTUVPTp00frPUePHsVPP/2E69evw9nZGVOmTEG/fv2qXEd2djZycnK0xsvLywEAKpUKKpXKAFtTsyrqro211wbsr3Gxv8ajeuifgAoqiCqfx95XG/df4zLX/uqzPSYNZzKZDJaWlpBIJBrjUqkUMplMa36rVq1QWlqKGTNmQCKRYMuWLZgyZQq2bdsGDw+PSteRkJCA2NhYrXEXFxdERUVBoVCgtLTUMBtUg1QqFQoLCwEAIlHlP3yp+thf42J/jafsfz/PlErlY+fl5+fXRDlmifuvcZlrf4uLi3Wea9JwVlXTq0qXCxcu1Pj6zTffRHJyMhITE6sMZ0FBQejbt6/WeHl5OUpKSiCRSGBjY6Nn5aZX0SOpVGpWO69QsL/Gxf4aj9jKChVHzsRWYqCKI2dSqbTmijIz3H+Ny1z7WxE4dWHScObg4AClUgm5XA47Ozv1eF5eHnx8fHRahrOzM3Jzc6t83dHREY6OjlrjhYWFSEtLg0gkqrXf/Iraa2v9Qsf+Ghf7axwi4KFTmaIqopl5HZEwBe6/xmWO/dVnW0x6Q4C7uzvEYjFSU1PVYwUFBUhPT0fHjh015srlcixevBiZmZka4xkZGXB2dq6JcomIiIiMzqThzN7eHv7+/oiJiUFWVhbkcjmio6Ph4uICPz8/7N+/H0OGDAEA2NnZ4eTJk1iwYAFycnJQVFSENWvW4OrVqwgKCjLlZhAREREZjMk/ISAiIgJt27ZFQEAAevbsidzcXMTFxUEsFkMul+PKlSvquZ9//jns7OwQEBCAbt264ZdffsHXX3/NB9ASERGR2RCpzO1eVR1VXHPm7u5ea28IyM/PN7sLJoWC/TUu9td4Qr44DhUe3LUptrKq8pqzDeM612RZZoX7r3GZa3/1yR0mP3JGRERERP9iOCMiIiISEIYzIiIiIgFhOCMiIiISEIYzIiIiIgFhOCMiIiISEIYzIiIiIgFhOCMiIiISEJN+8DkREekm5Ivjpi6BiGoIj5wRERERCQjDGREREZGAMJwRERERCQjDGREREZGAMJwRERERCQjDGREREZGAMJwRERERCQjDGREREZGAMJwRERERCQjDGREREZGAMJwRERERCQjDGREREZGAMJwRERERCUi1wtnRo0cNXQcRERERoZrhbNy4cejbty9iY2ORmZlp6JqIiIiI6qxqhbMdO3bglVdeQVJSEgYOHIgxY8Zg165dKCoqMnR9RERERHVKtcJZ+/bt8e6772Lv3r1ITEyEj48P1q1bhxdffBFz5szBiRMnDF0nERERUZ3w1DcEtGvXDjNmzEBiYiLGjBmD77//Hm+88QaGDRuGI0eOGKJGIiIiojrjqcKZSqXCr7/+infffRfdu3fHtm3b8MYbb2DLli3o3r07QkNDsW3bNkPVSkRERGT2xNV5U1paGnbv3o0ffvgBd+7cgZ+fH6KiotC/f39YWVkBALy9vdGqVSusWbMGr776qkGLJiIiIjJX1Qpnw4YNQ7NmzTBy5EgMHz4cTk5Olc576aWXsGDBgqcqkIiIiKguqVY4i4uLg5+fn/oo2cPu3LmDP//8EwMHDsQzzzyDM2fOPHWRRERERHVFta45e/vttyGXyyt9LScnBx9++KHOy1IoFJg9ezZ69eqFLl26ICQkBBkZGU98X2ZmJjp27KjXuoiIiIiETq8jZ7Nnzwbw4EaAxYsXo379+lpzzp07V+l4VcLDw3Hjxg1s2bIFUqkUK1euRGhoKJKTk1GvXr0q3zdv3jyIxdU68EdEREQkWHqlm1atWiE1NRUAcPbsWVhYaB94k0qlWLhwoU7Lk8lkSElJwfr169XXrYWFhSEhIQGHDx9Gnz59Kn3ftm3bkJ+fj969e+tTPhEREZHg6RXOQkNDAQBjxoxBbGwspFLpU608LS0NSqUSXl5e6jEbGxu4uroiNTW10nB2+/ZtfPLJJ9iwYQO++eabJ64jOzsbOTk5WuPl5eUAHhwFVKlUT7EVplFRd22svTZgf42L/dWffp1Sqf+tgqjyGex9tXH/NS5z7a8+21Ot84Jff/11dd6mRSaTwdLSEhKJRGNcKpVCJpNV+p4FCxZg+PDh8PDw0GkdCQkJiI2N1Rp3cXFBVFQUFAoFSktL9S/exFQqFQoLCwEAIlHlP3yp+thf42J/9Vem588ppVL52Nfz8/Ofppw6jfuvcZlrf4uLi3Weq3M48/DwwC+//ILGjRujffv2T2yYLndpVrWMqtJlcnIyLl26hE8//fTJBf9PUFAQ+vbtqzVeXl6OkpISSCQS2NjY6Lw8oajokVQqNaudVyjYX+Nif/UnruTu+Kqp/vceMVDFkbOnPfNRl3H/NS5z7W9F4NSFzuHs7bffRoMGDdT/bYiGOTg4QKlUQi6Xw87OTj2el5cHHx8fjbl5eXmIjIzE8uXLYW1trfM6HB0d4ejoqDVeWFiItLQ0iESiWvvNr6i9ttYvdOyvcbG/+tGnS/+eyhRV+T72/elw/zUuc+yvPtuiczibMmWK+r+nTp2qX0VVcHd3h1gsRmpqKnr06AEAKCgoQHp6Ot577z2Nuf/9738hk8kwffp09VhFCj148CCOHTtmkJqIiIiITKnaz6JISkpC9+7d0aRJE5SUlCA2NhaXLl1C165dMXbsWJ0Sor29Pfz9/RETE4PWrVtDIpEgOjoaLi4u8PPzw/79+7F69Wp8//33GDRoELp3767x/o8//hjAv4/4ICIiIqrtqvUQ2vj4eISHhyM3NxfAg5D05ZdfQiQS4fPPP8fnn3+u87IiIiLQtm1bBAQEoGfPnsjNzUVcXBzEYjHkcjmuXLkCAGjQoAGaNWum8adBgwbqcSIiIiJzIFJV417Vl19+GTNmzMD//d//oaioCN27d8e7776LMWPG4ODBg1iyZAlSUlKMUa/BVFxz5u7uXmtvCMjPzze7CyaFgv01LvZXfyFfHNd5rgoP7u4UW1lVec3ZhnGdDVJXXcT917jMtb/65I5qHTm7desWOnXqBAA4fvw4SktL4e/vD+DBdWRZWVnVWSwRERFRnVetcNawYUP1Kc2ff/4Znp6esLe3B/Dg2WX63E1JRERERP+q1g0Bfn5+mDdvHnx8fLBjxw6Eh4cDePAh5mvXroW3t7dBiyQiIiKqK6p15Gz27Nlo3bo1jh49irFjx2LkyJEAgF9++QWnT5/G+++/b9AiiYiIiOqKah05a9SoEZYvX6413q9fPwwcOBCWlpZPXRgRERFRXVTt55zl5OQgLS0NBQUFlX7cUsUNAkRERESku2qFs927d2P+/PkoKSmp9HWRSMRwRkRERFQN1Qpnn332Gfr27YsJEyagcePGZvUcEiIiIiJTqlY4y87Oxvr16+Hs7GzoeoiIiIjqtGrdrenm5obs7GxD10JERERU51UrnM2fPx8xMTH4+++/UVpaauiaiIiIiOqsap3WnDx5MoqKivDaa68BQKWPzjhz5szTVUZERERUB1UrnL366quGroOIiIiIUM1wNmXKFEPXQURERER4iofQqlQq/PLLL0hLS0Nubi4mTZqExo0bIz09Ha1btzZkjURERER1RrUfpfHmm2/iwoULsLe3R0FBAcaOHYu8vDwMGzYMGzduhK+vr6FrJSIiIjJ71bpbMyoqCmKxGElJSTh69Cjq168PAGjdujWCg4OxatUqgxZJREREVFdUK5wdPnwY4eHhaNOmjdZrw4cPx+nTp5+6MCIiIqK6qFrhzMLCAra2tpW+Vlpayo9zIiIiIqqmaoWzdu3aYfXq1SgrK9N67ZtvvoGHh8dTF0ZERERUF1XrhoBp06YhJCQEvXv3RqdOnVBaWorIyEj8888/uHnzJjZt2mToOomIiIjqhGodOfPx8UFiYiL69++P69evo3nz5sjOzkbXrl2RmJiITp06GbpOIiIiojpB7yNnpaWlSElJwfHjx5GTkwMnJyc8++yzePHFF9GrVy9j1EhERERUZ+gVztLS0jBlyhTcuHEDzZo1Q7NmzVBWVoY//vgDX3/9Ndq0aYMVK1bwIbRERERE1aRzOMvNzUVISAjatm2Lzz//HG3bttV4/fjx44iOjsbYsWOxe/duODg4GLxYIiIiInOn8zVnX375JZ599lmsX79eK5gBQOfOnfHNN9+gSZMmiIuLM2iRRERERHWFzuHs0KFDCAkJgVhc9cG2Bg0aYNKkSThw4IBBiiMiIiKqa3QOZ5mZmTo9v8zDwwO3bt16qqKIiIiI6iqdw1lRUVGVnwrwsHr16kGpVD5VUURERER1lV7POePHMhEREREZl16P0pg+fTqsrKweO6e0tFSvAhQKBSIjI3HkyBEUFRXB09MTc+fOxfPPP681Ny8vD0uXLsWhQ4dQVFSEFi1aYPz48QgMDNRrnURERERCpfORs86dOwN4EL4e9wcAfH19dS4gPDwcGRkZ2LJlCw4cOIBWrVohNDQUJSUlWnPfffdd5OTkYNeuXTh+/DhCQkIwZ84c/Pnnnzqvj4iIiEjIdD5y9vXXXxt85TKZDCkpKVi/fj2cnJwAAGFhYUhISMDhw4fRp08fjfn+/v7o3LkzHB0dAQBDhw5FZGQkLly4oFcgJCIiIhKqan3wuaGkpaVBqVTCy8tLPWZjYwNXV1ekpqZqhbNhw4ap//vevXvYtm0bRCIRXnrppSrXkZ2djZycHK3x8vJyAIBKpYJKpXraTalxFXXXxtprA/bXuNhf/enXKZX63ypUfq0we1993H+Ny1z7q8/2mDScyWQyWFpaQiKRaIxLpVLIZLIq3/fqq6/i1KlTcHFxQVxcHJ577rkq5yYkJCA2NlZr3MXFBVFRUVAoFHpfJycEKpUKhYWFAHijhjGwv8bF/uqvTM+fU0+6az4/P/9pyqnTuP8al7n2t7i4WOe5Jg1nVTX9Sely27ZtUCgU2L17NyZOnIj169fD29u70rlBQUHo27ev1nh5eTlKSkogkUhgY2Ojf/EmVtEjqVRqVjuvULC/xsX+6k/8hJuxNKn+9x4xUMWRM6lU+vRF1VHcf43LXPtbETh1YdJw5uDgAKVSCblcDjs7O/V4Xl4efHx8HvteiUSC0aNH47fffsPmzZurDGeOjo7qa9QeVlhYiLS0NIhEolr7za+ovbbWL3Tsr3Gxv/rRp0v/nsoUVfk+9v3pcP81LnPsrz7botdzzgzN3d0dYrEYqamp6rGCggKkp6ejY8eOGnNzcnLQp08fjbkAUFJS8sTHexARERHVFiYNZ/b29vD390dMTAyysrIgl8sRHR0NFxcX+Pn5Yf/+/RgyZAgAoGnTpmjevDmio6Nx48YNlJWVISUlBb///jv69etnys0gIiIiMhiThjMAiIiIQNu2bREQEICePXsiNzcXcXFxEIvFkMvluHLlinru6tWr4eLigsDAQPj6+uKzzz7DokWL0L9/fxNuAREREZHhiFTmdq+qjiquOXN3d6+1NwTk5+eb3QWTQsH+Ghf7q7+QL47rPFeFB3d3iq2sqrzmbMO4zgapqy7i/mtc5tpffXKHyY+cEREREdG/GM6IiIiIBIThjIiIiEhAGM6IiIiIBIThjIiIiEhAGM6IiIiIBIThjIiIiEhAGM6IiIiIBIThjIiIiEhAGM6IiIiIBIThjIiIiEhAGM6IiIiIBIThjIiIiEhAGM6IiIiIBIThjIiIiEhAGM6IiIiIBIThjIiIiEhAGM6IiIiIBIThjIiIiEhAGM6IiIiIBIThjIiIiEhAGM6IiIiIBIThjIiIiEhAGM6IiIiIBIThjIiIiEhAGM6IiIiIBIThjIiIiEhAGM6IiIiIBIThjIiIiEhAGM6IiIiIBMTk4UyhUGD27Nno1asXunTpgpCQEGRkZFQ6t7S0FCtWrEDfvn3h7e2NwYMH44cffqjhiomIiIiMx+ThLDw8HBkZGdiyZQsOHDiAVq1aITQ0FCUlJVpzV65cieTkZMTHx+P48eMYN24cZs2ahbNnz5qgciIiIiLDM2k4k8lkSElJwfTp0+Hk5ASJRIKwsDDcvn0bhw8f1ppvZWWF2bNno3Xr1hCLxRgxYgSaNGmC48ePm6B6IiIiIsMTm3LlaWlpUCqV8PLyUo/Z2NjA1dUVqamp6NOnj8b8GTNmaHwtl8tRUFCAZs2aVbmO7Oxs5OTkaI2Xl5cDAFQqFVQq1VNshWlU1F0ba68N2F/jYn/1p1+nVOp/qyCqfAZ7X23cf43LXPurz/aYNJzJZDJYWlpCIpFojEulUshksse+V6lUYs6cOWjVqhX69+9f5byEhATExsZqjbu4uCAqKgoKhQKlpaXV2wATUqlUKCwsBACIRJX/8KXqY3+Ni/3VX5meP6eUSuVjX8/Pz3+acuo07r/GZa79LS4u1nmuScNZVU1/Urq8d+8e3n33XVy7dg2bNm2CWFz1ZgQFBaFv375a4+Xl5SgpKYFEIoGNjY1+hQtARY+kUqlZ7bxCwf4aF/urP7GVlR6zVf97jxio4siZVCp9+qLqKO6/xmWu/a0InLowaThzcHCAUqmEXC6HnZ2dejwvLw8+Pj6Vvic3NxcTJ05E48aNsXXr1if+gHF0dISjo6PWeGFhIdLS0iASiWrtN7+i9tpav9Cxv8bF/upHny79eypTVOX72Penw/3XuMyxv/psi0lvCHB3d4dYLEZqaqp6rKCgAOnp6ejYsaPW/IKCAowfPx7u7u6Ij4/n3/yIiIjI7Jg0nNnb28Pf3x8xMTHIysqCXC5HdHQ0XFxc4Ofnh/3792PIkCHq+StWrECTJk0QGRkJS0tLE1ZOREREZBwmPa0JABEREYiMjERAQABKSkrQpUsXxMXFQSwWQy6X48qVK+q5CQkJEIlE6NChg8YyAgICsHjx4pounYiIiMjgTB7OGjRogMWLF1cargIDAxEYGKj++ty5czVZGhEREVGNM/knBBARERHRvxjOiIiIiASE4YyIiIhIQBjOiIiIiASE4YyIiIhIQBjOiIiIiASE4YyIiIhIQBjOiIiIiASE4YyIiIhIQBjOiIiIiASE4YyIiIhIQBjOiIiIiASE4YyIiIhIQBjOiIiIiASE4YyIiIhIQBjOiIiIiASE4YyIiIhIQBjOiIiIiASE4YyIiIhIQBjOiIiIiASE4YyIiIhIQBjOiIiIiASE4YyIiIhIQBjOiIiIiASE4YyIiIhIQBjOiIiIiASE4YyIiIhIQBjOiIiIiASE4YyIiIhIQBjOiIiIiASE4YyIiIhIQEwezhQKBWbPno1evXqhS5cuCAkJQUZGRpXz5XI5Zs2aBTc3Nxw7dqwGKyUiIiIyPpOHs/DwcGRkZGDLli04cOAAWrVqhdDQUJSUlGjNzcjIQEBAAOrXr2+CSomIiIiMT2zKlctkMqSkpGD9+vVwcnICAISFhSEhIQGHDx9Gnz59NObn5uZi4cKFcHNzw7Zt23RaR3Z2NnJycrTGy8vLAQAqlQoqleopt6TmVdRdG2uvDdhf42J/NYV8+aeBl6hS/1sFUeUz2Ptq4/5rXObaX322x6ThLC0tDUqlEl5eXuoxGxsbuLq6IjU1VSucdenSBQAqDVtVSUhIQGxsrNa4i4sLoqKioFAoUFpaWs0tMB2VSoXCwkIAgEhU+Q9fqj7217jYX01lRvgZpFQqH/t6fn6+wddZV3D/NS5z7W9xcbHOc01+5MzS0hISiURjXCqVQiaTGWQdQUFB6Nu3r9Z4eXk5SkpKIJFIYGNjY5B11aSKBC6VSs1q5xUK9te42F9NYisrAy9R9b/lioEqjpxJpVIDr7Pu4P5rXOba34rAqQuThrOqmm7IQ5mOjo5wdHTUGi8sLERaWhpEIlGt/eZX1F5b6xc69te42N9/GboD/57KFFW5bPb96XD/NS5z7K8+22LSGwIcHBygVCohl8s1xvPy8tCkSRMTVUVERERkOiYNZ+7u7hCLxUhNTVWPFRQUID09HR07djRdYUREREQmYtJwZm9vD39/f8TExCArKwtyuRzR0dFwcXGBn58f9u/fjyFDhpiyRCIiIqIaZfLnnEVERKBt27YICAhAz549kZubi7i4OIjFYsjlcly5ckU9d968efD09FRf4B8SEgJPT0/MmzfPVOUTERERGZRIZW4PEtFRxQ0B7u7utfZuzfz8fLO7m0Uo2F/jYn81hXxx3KDLU+HB4znEVlZV3hCwYVxng66zLuH+a1zm2l99codJ79YkIqqNDB2miIgeZvLTmkRERET0L4YzIiIiIgFhOCMiIiISEIYzIiIiIgFhOCMiIiISEIYzIiIiIgFhOCMiIiISEIYzIiIiIgFhOCMiIiISEIYzIiIiIgFhOCMiIiISEH62JhHR//AzM4lICHjkjIiIiEhAGM6IiIiIBIThjIiIiEhAGM6IiIiIBIThjIiIiEhAeLcmEVEdpOudqRvGdTZyJUT0KB45IyIiIhIQhjMiIiIiAWE4IyIiIhIQXnNGRGaPT/4notqER86IiIiIBIThjIiIiEhAeFqTiGotnq40PkP3WNdHc+iyXj7mg8wVwxkREdUYQ4Y9PquNzBXDGRERmTWGOKpteM0ZERERkYDwyBkRERF0P8K2fqyvkSuhus7k4UyhUCAyMhJHjhxBUVERPD09MXfuXDz//PNac1UqFdatW4fExERkZ2ejVatWmDp1Kvr06WOCyomoOkK+OA4VgLLSUoitrCAydUFEegr58k+d9l+eJqXqMvlpzfDwcGRkZGDLli04cOAAWrVqhdDQUJSUlGjNTUhIwMaNG7FkyRIcO3YM48ePx9SpU5Genm6CyomIiIgMz6RHzmQyGVJSUrB+/Xo4OTkBAMLCwpCQkIDDhw9rHRHbunUrRo0ahY4dOwIA/P39sXXrVmzfvh0ffPBBTZdPVKvxMRRExmWqx5BQ7WfScJaWlgalUgkvLy/1mI2NDVxdXZGamqoRzoqLi3Hx4kVMmzZNYxmenp5ITU2tch3Z2dnIycnRGlcqlQCAoqIiqFSqp92UGqdSqXD//n2IxWKIRNU7MbQg6ZzB6onwf6HG16nPevVliP7qytA90ZWDtUlW+z8qKMUiWIpVAE9sGgH7a1ym6e/739aVv1CpENbbuUZ+/tak+/fvAwDKy8ufONfkR84sLS0hkUg0xqVSKWQymcbY3bt3UV5eDqlUqjX3zp07Va4jISEBsbGxWuN+fn6YMmUKrl69Wv0NqOWC2hjurPb58+drfJ36rFfIDN2T2sPS1AWYOfbXuNhfY7px44apSzCa4uJirdzzKJOGs6oScWVHsvSZ+7CgoCD07dtXa7y8vBwSiQRNmzaFhUXt++WYnp6O9957D5988glat25t6nLMDvtrXOyvcbG/xsX+Gpe59re8vBzFxcVaB5kqY9Jw5uDgAKVSCblcDjs7O/V4Xl4efHx8NOba29vD0tISd+/e1RjPy8tD06ZNq1yHo6MjHB0dDVq3EFhYWODq1auwsLCAjY2NqcsxO+yvcbG/xsX+Ghf7a1zm3N8nHTGrYNJDRu7u7hCLxRrXjBUUFCA9PV190X+FevXqoV27djh16pTG+MmTJ7XmEhEREdVWJg1n9vb28Pf3R0xMDLKysiCXyxEdHQ0XFxf4+flh//79GDJkiHr+G2+8gYSEBKSmpqK4uBgJCQm4fPkygoKCTLgVRERERIZj8ofQRkREIDIyEgEBASgpKUGXLl0QFxcHsVgMuVyOK1euqOcGBgbi7t27mD59Ou7cuQNXV1fExcXhueeeM+EWEBERERmOycNZgwYNsHjxYixevFjrtcDAQAQGBmqMTZgwARMmTKip8oiIiIhqVO27TZEAAE2bNsWUKVMeezMEVR/7a1zsr3Gxv8bF/hoX+wuIVLXxCaxEREREZopHzoiIiIgEhOGMiIiISEAYzoiIiIgEhOGMiIiISEAYzmqZ8vJyxMfHw8PDA6tXr9Z4LTExEW5ubvD09NT4U9kHv1PlHtdf4MEnUowePRq+vr7o27cvoqOjUVZWZoJKa7/Vq1ejXbt2WvtrYmKiqUur1RQKBWbPno1evXqhS5cuCAkJQUZGhqnLMhtubm7w8PDQ2GcHDBhg6rJqrYyMDAwfPhxubm4a46Wlpfj444/Rr18/+Pj44PXXX8dff/1loiprnsmfc0a6KykpwYQJE9CgQQM4ODhUOsfJyQkHDhyo4crMw5P6e+fOHYSGhiIkJATx8fHIzs7GW2+9BWtra8yYMaPmCzYDnTt3xtdff23qMsxKeHg4bty4gS1btkAqlWLlypUIDQ1FcnIy6tWrZ+ryzMKGDRvQtWtXU5dR6+3btw8LFy5E586dcebMGY3XYmJicPDgQcTFxaFFixb4+uuvERoair1796Jx48Ymqrjm8MhZLXL//n306dMH69atQ4MGDUxdjtl5Un+TkpIgkUjw9ttvw8bGBi4uLpg4cSK+/fZb8Ik0JAQymQwpKSmYPn06nJycIJFIEBYWhtu3b+Pw4cOmLo9IQ0FBAbZs2YL+/ftrjJeXlyMhIQFvvvkmXF1dYW1tjYkTJ0IikSA5OdlE1dYshrNapGHDhggJCYFIJKpyzr179zBp0iR069ZNfdqtuLi4BqusvZ7U39OnT8PDw0PjdU9PT+Tl5SEzM7OmyjQrWVlZGDduHDp37oyXX34Za9euhVKpNHVZtVZaWhqUSiW8vLzUYzY2NnB1dUVqaqoJKzMvX331Ffr3748uXbpgwoQJuHz5sqlLqpVGjBgBZ2dnrfFr164hPz8fnp6e6jGRSIT27dvXmf2YpzUFpKysDIWFhZW+ZmFhAYlE8tj3N2rUCK1bt0ZwcDBWrVqF06dPY/r06SgrK8O8efOMUXKt8rT9zcvLw7PPPqsxZm9vD+DBKc/KfsjUZU/qt6OjI1q2bImwsDC0adMGv//+O2bMmAFra2uMGzeuZos1EzKZDJaWllr7slQqhUwmM1FV5qV9+/bw9PTEkiVLcP/+fSxcuBATJkzAjz/+CFtbW1OXZxYq9tWKn68VpFIpbt26ZYKKah7DmYCcOHECwcHBlb7m7OyM/fv3P/b9ffr0QZ8+fdRfd+rUCW+++SZWrFjBcIan729lR9R4OrNquvQ7KChIPdarVy+MHDkS27dvZzirpqqO+nI/NZyHb1iRSCRYvHgxunbtil9//RWDBg0yYWXmg/sxw5mgdO3aFRcuXDDoMlu2bInCwkIoFIonHhkyd0/b38aNG+Pu3bsaY3l5eQCAJk2aPE1pZqk6/XZ2dsauXbuMU1Ad4ODgAKVSCblcDjs7O/V4Xl4efHx8TFiZ+WrYsCHs7e2Rm5tr6lLMRsUNWXfv3kWzZs3U43l5eXXmZy2vOTMjCQkJ2L17t8ZYeno6mjRpUueDmSF4eXnh9OnTKC8vV4+dPHkSTZs2RYsWLUxYWe0UFxeHQ4cOaYxlZGSgZcuWJqqo9nN3d4dYLNa4LqegoADp6eno2LGj6QozE2lpaYiKitI4giOTyZCXl8fLGgyoRYsWaNSokcZ+rFQqcerUqTqzHzOcmZHi4mIsWrQIf/75J8rKynDixAmsX78eY8aMMXVpZsHf3x/FxcVYu3YtioqKcOnSJWzYsAFjxox57E0aVDmZTIYFCxbg/PnzKC0txcGDB7F9+3bur0/B3t4e/v7+iImJQVZWFuRyOaKjo+Hi4gI/Pz9Tl1frNWrUCNu3b8fKlStRVFSEO3fuYN68eWjdujX7a0AWFhYYPXo04uPjkZ6ejqKiIsTGxkKlUuGVV14xdXk1QqSqSydxa7ldu3Zh/vz5AB48oM/CwgKWlpZo3rw59u7dCwBYv349vvvuO9y6dQsODg544403MG7cOFhaWpqy9FpBl/6ePn0akZGROHv2LKRSKUaMGIFp06bBwoJ/z9FXSUkJYmJikJycjJycHDRv3hxvvfUWAgMDTV1arVZUVITIyEjs27cPJSUl6NKlC8LDw3l010D++usvLF++HOfPn0d5eTl69uyJOXPm4JlnnjF1abXOwIEDcfPmTahUKpSWlqqfw7do0SL4+/tjxYoV2LlzJ+RyOTw9PTFv3jy4u7ubuOqawXBGREREJCD86z4RERGRgDCcEREREQkIwxkRERGRgDCcEREREQkIwxkRERGRgDCcEREREQkIwxkRERGRgDCcEREREQkIwxmRAY0ZMwZubm4afzw8PDBw4EAsWLAAcrncoOtbvXo1XnjhhcfO+fDDDzFgwACDrvfYsWNwc3PDn3/++dh5KpUKkydPxuTJkwE86M+4ceMMWgtpSkxMhJubG27dumXqUmqFin2ypKQEw4YNw5IlS0xdEhHEpi6AyNz4+vpi5cqV6q+Li4uRmpqKjz/+GNevX0d8fLzB1jVhwgS89tprBlueocXHx+PcuXPYvXu3qUt5akqlEr6+vkhKSuJHIZmhevXqYeXKlRg6dCh8fHzQv39/U5dEdRjDGZGBWVlZoWnTphpjLVq0gEgkwsyZM5GamooOHToYZF22trawtbU1yLIMLScnB2vWrMHcuXPRsGFDU5fz1C5evIjCwkJTl0FG1LJlS4waNQpRUVHo1asXrKysTF0S1VE8rUlUQ9q2bQsAyMrKUo+dOHECwcHB6NKlC3x9fTF9+nTcvn1b/bpMJsP777+PHj16wNPTEwMGDMC6devUrz96WvPGjRsYP348vLy80LNnT425AHD9+nW4ublpHckaNGgQPvzwQ/XXx48fx5gxY9CxY0d4e3sjKCgIx44d02t7v/jiC9ja2mLo0KFVzrl+/TqmTp2Kzp07w8PDA/7+/vj+++815uzZswcDBgyAp6cngoKCcOHCBfj6+mLNmjU61XHo0CG4ubnh7NmzGuNnz56Fm5sbfv/99ycu49ixY+rt6NevH8aMGQMA6Nu3Lz755BNMnjwZXl5euHr1aqWnkffs2QM3Nzdcv35dPbZ9+3YMHToUHTt2RI8ePbB06VKUlJTotE36UCqViI2NRd++feHh4YEePXpgwYIFuHfvnnrOjRs3MGHCBHh5eeGll17CN998g8jISJ1PhyuVSvTo0QOLFy/Wes3f3x+zZs3SaTkV++cPP/yAadOmoWPHjujcuTMWLlyIsrIyAP+eUk9OTsaAAQMwevRoAEBJSQmWLl2KgQMHwtPTE4MGDcL27ds1ln/+/Hm8+uqr8PT0RL9+/bBjxw6tGkJCQnDr1i2t/ZCoJjGcEdWQS5cuAQCcnJwAAOnp6ZgwYQKkUim++eYbxMfHIzMzExMnToRSqQQAREZG4vz581izZg327t2LGTNmYM2aNVWeJpw5cyauXbuGjRs3YtOmTbh69Sp+/fVXveqUy+UIDQ3Fs88+i507d2Lnzp1wc3PDpEmTcOfOHZ2Xc/DgQbz44otVHn0oKirC2LFjkZubi/j4ePzwww/o168fZs2ahQMHDgB4cLRq1qxZ6NChAxITExESEoI5c+agqKhI5zp69uyJZs2aafUsJSUFzZs3R7du3Z64DG9vbyxYsAAAsG3bNqxevVr92o8//gh3d3f18nSxc+dOzJ07F/3798euXbsQERGBxMREREVF6bxdulqxYgU2bNiAsLAwJCcnY8GCBdi3bx9mz56tnjNjxgxcuXIFcXFxWLduHQ4ePIhDhw7pvA5LS0sEBgZiz5496hAFAFeuXMHFixcRGBioV82ffvopevbsid27d2PmzJnYunUrvvzyS405GzduRFRUFFasWAEAiIiIwLZt2zBlyhQkJSVh5MiRmD9/PpKTkwE8CG/vvPMOysvL8e233+Kzzz7D/v37cfnyZY3lNmnSBC+88IJ6HyQyBYYzIiMrLy/HqVOnsHz5cnTo0AGenp4AgK+++gq2trZYvnw52rZtC29vb0RHR+PixYv45ZdfAABpaWnw8fGBl5cXmjdvjldeeQVbt25F9+7dtdZz5coVpKamIiwsDL6+vnB1dcWiRYtgaWmpV73W1tZITExEeHg4WrVqBRcXF4SGhkKhUODvv//WaRl3795Feno6fHx8qpzz008/4fr16/j444/RsWNHuLi4YMaMGfD29sbmzZsBPDjiZG1tjcjISLRp0wYvv/wyxowZoxEAnsTCwkIdHCpCLwDs27cPQ4cOhUgkeuIy6tWrBzs7OwBA48aNYW9vr37N0tISU6ZMQfPmzVGvXj2dalq3bh369euHKVOmwMXFBQMGDMCUKVOwbds2FBQU6LxtT1JSUoLNmzcjODgYgwcPhrOzM/r164dp06Zh3759yM7OxtWrV3Hq1CnMnDkT3bt3R7t27bBq1Sq9gjgAjBgxAnl5eRp/GUhJSYGTk5NOAfhh3t7eePXVV9GyZUu8/vrr6N69uzpkVejXrx86d+4MR0dH3L59G7t27cLkyZPh7+8PFxcXTJgwAQMGDMD69esBAH/88Qdu3ryJ+fPno3379mjXrh2WLVsGhUKhtX4fH58n3uxCZEwMZ0QG9scff8Db21v9x8vLC6+//jo6dOiAuLg49bxTp06hU6dOGr/Q27ZtC3t7e6SlpQEAevfuje+++w7h4eE4dOgQCgsL4e7uDkdHR631pqenAwDatWunHrO0tFSHQV1ZWVkhKysL06dPR8+ePeHt7Y3BgwcDAPLz83VaRk5ODoAHRyGqcubMGdjb28PFxUVj3NPTE+fOnQMAXLt2DS1btkT9+vXVr/fs2VOfzQEADB8+HHfu3MFvv/0G4MHprX/++QfDhg3Te1mPetLdso9SKBTIyMjQCixdunRBWVkZLl68+NQ1VcjIyEBhYSE6duyoMe7l5QWVSoW0tDRcu3YNwL+n3QFAIpGgU6dOeq3L2dkZXbp00ThCuXfvXgQEBOgUgB/2aL0vvPACbt68qTVW4cyZMygvL6+0pxcvXoRKpVIfIXN3d1e/bmdnh9atW2utv2nTprh7965RTjMT6YI3BBAZmJeXl8bt+PHx8fj5558xf/58jSMuCoUCBw8ehLe3t8b7i4qKkJubCwB477334OzsjJ07d2Lbtm2oV68ehg0bhtmzZ2sElorlAUCDBg00xm1sbPSq/9SpUwgJCUHv3r2xYsUKNGnSBHfv3kVQUJDOy6g4+iORSKqco1AoKn3d1tZWvS13797VmiOVSnWuo0KLFi3g5+eH3bt3o1evXkhJSYGvry+cnZ31XlZl9eqjYtuWLVumPiUHPHjsCAD19/5R4eHhSEpKUn8dHx8PX19fndb1aA8ralYoFOqjiRVHBitUp8+vvvoq5s6dC7lcjry8PJw/f17jFLCuHq3FxsZG6zE0D/e9YjtHjRqlEQTLyspQWlqKvLw83Lt3DyKRSOv/m8r+/6i4gUUul8PBwUHv+omeFsMZkYFZW1ujZcuW6q9nzZqFn376CUuXLtW4psjOzg49evTAnDlztJZR8cvUwsICo0aNwqhRoyCTyZCUlITly5dDIpHgvffe03hPxS+ZR6/HeviXWsUvroogUOHhuxCTk5NhbW2NVatWqY/qVRzJ0lXFL7fKThlVsLOzq/R1hUKh/uVcv359rTsk7969q1ctFUaMGKG+Xm3v3r2YOHFitZbzJCKR6LH9rfjevv322+ojkg+rKgxMnz4dISEh6q+feeaZJ9ZS0cdHg03F1xKJBPfv3wfw4JEvD6tOn19++WUsWrQI+/fvR25uLnx9ffHcc8/pvZxHv+f37t177B2/FdsZGxtb6foaNmwIGxsbqFQqFBcXawS0ygJYxV8uHg2JRDWFpzWJjEwqlSIsLAyJiYka17F4enri6tWrcHZ2RsuWLdV/SktL0bhxY9y/fx979uxR/yJt3Lgxxo4dixdffFHrImYAaNWqFQDg9OnT6rH79+/j5MmT6q8rgsHD1zVlZWUhOztb/XVpaSlsbW01TrdWHLF5NHRUpeJ05uOuW/Lw8FBfm/awv/76S30qtmXLlrh06ZJGcNi3b59ONTyqf//+sLa2xtq1a3Hr1i0MGjSoWst5Ug9sbW21rhtLTU1V/7dEIsHzzz+PrKwsje9706ZNYWlpWeXRRgcHB4351tbWT6y1VatWsLW11dgHAODvv/+GhYUF2rdvr/6LxMN3s8pkMpw4ceKJy39U/fr1MWTIEOzbtw8//PCD3jcCVHh03WfPnlXv35Xx8PCAhYUFZDKZVo/s7e0hFosr/f/j9u3bWvsf8OC0vFQq1fkaQiJDYzgjqgEjRoxA+/btERERob6OZcyYMbh58ybCw8Nx4cIFZGRk4JNPPsHQoUNx+fJliMViLFu2DLNnz8bZs2eRlZWFgwcP4uTJk+jcubPWOtq0aQM3NzesWbMGJ0+exMWLFzFv3jyNX+JSqRQtWrTAjh07cP78eZw9exZz587VuMvQy8sLOTk52L59OzIzM/H5558jLy8PVlZWOH36tE5HVBo1aoTWrVs/9hf8gAED0LJlS3zwwQc4deoUMjIysGTJEpw9e1b9KQIDBw7EvXv3EBkZiYyMDOzbt0/j1J4+6tWrh4CAAKxfvx4DBw7U+3RkxZGbQ4cO4cKFC1XO8/DwQH5+Pr766itkZmbi22+/1QhnwIPHNezatQtffvkl/vnnH5w+fRozZ87E2LFjDXqdU7169RAcHIzNmzdj165dyMzMxN69e7F69WoEBASgSZMmcHNzQ6tWrRAbG4uTJ0/i/PnzePfdd9GsWbNqrXPEiBH49ddfkZmZiYEDB1ZrGSdPnsTWrVvxzz//YMuWLTh27BgCAgKqnO/o6Ah/f38sXbpUfaPJ4cOHERwcjEWLFgEAunXrhiZNmmDp0qU4d+4czp07hzlz5qBx48aVrv9Jp4yJjInhjKgGWFhYICIiAunp6eq7x1xdXbFp0yZkZGRg5MiRGDp0KP766y9s2LABbdq0gVgsxoYNG1BSUoJx48bh5ZdfRnR09GM/AmnVqlVwdHREcHAwJkyYAFdXV61fkEuWLEF5eTlGjhyJd999F6NHj1Y/3gMABg8ejNdffx3Lli3D8OHDcePGDXz00Ud4/fXXsWPHDo2bGh6nd+/e+O2331BaWlrp6/Xr18emTZvQrFkzjB8/HgEBATh27BjWrFmjvhu1U6dOmD9/Pg4ePIjAwEBs3bpVfWr40WuHdDFw4ECUlZVh+PDher+3a9eu6NatG6KjozUeQ/GowYMHIygoCJ999hkCAgLw559/IiwsTGPOiBEjEBERgYSEBLzyyisYP348rK2t8cUXXxj8aM20adMQEhKCVatWYdCgQYiKikJgYCAWLlwI4MFp2JiYGEilUgQHB2PatGkYNmwYPDw8qtXjdu3aoUWLFhg0aFC1H5A8ceJE/Pnnnxg6dChWrFiB4OBgjBgx4rHvWbx4Mfz9/bFw4UK8/PLL+OCDD9C3b1/1s9esra2xZs0alJWVYeTIkZgyZQr+85//wMvLS2M5MpkMZ8+eRb9+/apVO5EhiFS6nqcgItJDdnY2BgwYgPnz5z/xF2tVVCoV7ty5g8aNG8PC4sHfJdPT0/F///d/iImJ0fvIzNKlS3H48GGz+DgpQyosLIRSqdS4xmrkyJFo3Lgx1q5dq9eyLl26BH9/fyQmJup9J+v169fRr18/LF269LFHyoxp2bJl+PHHH5GSksLTmmQyPHJGREbh6OiId955B2vWrKn2B75fvHgRPXv2RHR0NP755x+cO3cOixYtwjPPPKPXIzVu3bqFbdu24csvv9S6kYKA4OBgvPHGG/j777+RmZmJ9evXIzU1Va9QnZ+fr35e2iuvvKJ3MBOCzMxMbNmyBR988AGDGZkUj5wRkdGoVCpMnjwZlpaW1XqkAgD8/PPPWLNmDTIyMmBtbQ1PT0+8//77cHV1xcSJE5944bq/vz927NiBJk2aYNKkSVqPBNF1GRWnAc3R7du38fHHH+Po0aO4f/8+nnvuOYwbNw7Dhw/H999/j4iIiCcu45VXXsGePXvQp08fLFy4UOPGBl2XkZSUZLIjZyUlJRg1ahS6du2KDz74oEbXTfQohjMiqrVu376tfhREVSQSyWOfVWWIZZgzhUKh06cFPPz4GGMsg6guYTgjIiIiEhBec0ZEREQkIAxnRERERALCcEZEREQkIAxnRERERALCcEZEREQkIAxnRERERALCcEZEREQkIP8PAFugL4dCGJ4AAAAASUVORK5CYII=\n"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 500x500 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "[Eval] Generating latent space visualizations.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 700x600 with 2 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Eval] Using 'bldgclass' as building class column for latent viz.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 700x600 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1200x300 with 3 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Eval] Latent visualizations generated (convergence, residuals, z-scatter, marginals).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "from sklearn.ensemble import HistGradientBoostingRegressor\n",
        "from sklearn.metrics import r2_score\n",
        "\n",
        "TARGET_COL = \"log_sale_price\"  # Target is log-price\n",
        "\n",
        "# ---------------------------------------------------------\n",
        "# 0. Helper: call SemiSupMIWAE.predict_price (or forward)\n",
        "# ---------------------------------------------------------\n",
        "\n",
        "def get_y_log_pred_from_model(X_obs_np, batch_size=1024):\n",
        "    \"\"\"\n",
        "    Run the trained SemiSupMIWAE on X_obs_np and return the\n",
        "    predictive mean for log_sale_price as a 1D numpy array.\n",
        "\n",
        "    Uses model.predict_price(x) if available; otherwise falls\n",
        "    back to model(x) and assumes it returns:\n",
        "        (recon_x, mu, logvar, y_pred_mu, y_pred_logvar).\n",
        "    \"\"\"\n",
        "    model = vae_trainer.model\n",
        "    model.eval()\n",
        "\n",
        "    preds = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for start in range(0, X_obs_np.shape[0], batch_size):\n",
        "            xb_np = X_obs_np[start:start + batch_size]\n",
        "            xb = torch.from_numpy(xb_np).float().to(DEVICE)\n",
        "\n",
        "            # Preferred path: explicit predictor\n",
        "            if hasattr(model, \"predict_price\"):\n",
        "                y_mu_batch, y_logvar_batch = model.predict_price(xb)\n",
        "            else:\n",
        "                # Fallback: use forward tuple\n",
        "                out = model(xb)\n",
        "                # Expect: (recon_x, mu_z, logvar_z, y_pred_mu, y_pred_logvar)\n",
        "                if not isinstance(out, (tuple, list)) or len(out) < 4:\n",
        "                    raise RuntimeError(\n",
        "                        \"Model forward did not return a 4/5-tuple as expected. \"\n",
        "                        \"Please adapt get_y_log_pred_from_model to your model.\"\n",
        "                    )\n",
        "                y_mu_batch = out[3]  # index 3 = y_pred_mu\n",
        "\n",
        "            preds.append(y_mu_batch.detach().cpu().numpy().reshape(-1))\n",
        "\n",
        "    return np.concatenate(preds, axis=0)\n",
        "\n",
        "\n",
        "# ---------------------------------------------------------\n",
        "# 1. Build observed subset from df_pred and X_filled_np\n",
        "# ---------------------------------------------------------\n",
        "\n",
        "if \"df_pred\" not in globals():\n",
        "    raise NameError(\n",
        "        \"df_pred not found. Make sure you loaded \"\n",
        "        \"final_data_with_miwae_predictions.parquet into df_pred.\"\n",
        "    )\n",
        "\n",
        "if \"X_filled_np\" not in globals():\n",
        "    raise NameError(\n",
        "        \"X_filled_np not found. Make sure you ran the eval cell that \"\n",
        "        \"calls prepare_vae_input and assigns its first output to X_filled_np.\"\n",
        "    )\n",
        "\n",
        "mask_obs = df_pred[TARGET_COL].notna().values\n",
        "X_all     = X_filled_np\n",
        "y_log_all = df_pred[TARGET_COL].to_numpy(dtype=float)\n",
        "\n",
        "X_obs     = X_all[mask_obs]\n",
        "y_log_obs = y_log_all[mask_obs]\n",
        "\n",
        "print(f\"[Diag] Observed rows for residual analysis: {X_obs.shape[0]}\")\n",
        "\n",
        "# ---------------------------------------------------------\n",
        "# 2. Get MIWAE predictions on observed rows and compute residuals\n",
        "# ---------------------------------------------------------\n",
        "\n",
        "print(\"[Diag] Getting MIWAE predictions for observed rows...\")\n",
        "y_log_pred_obs = get_y_log_pred_from_model(X_obs, batch_size=1024)\n",
        "\n",
        "y_log_true_obs = y_log_obs.astype(float).reshape(-1)\n",
        "resid_log      = y_log_true_obs - y_log_pred_obs\n",
        "\n",
        "print(f\"[Diag] Residuals (log-price): mean={resid_log.mean():.4f}, std={resid_log.std():.4f}\")\n",
        "\n",
        "# ---------------------------------------------------------\n",
        "# 3. Residual structure vs original X\n",
        "# ---------------------------------------------------------\n",
        "\n",
        "gbr_resid_x = HistGradientBoostingRegressor(\n",
        "    loss=\"squared_error\",\n",
        "    max_depth=3,\n",
        "    learning_rate=0.05,\n",
        "    max_iter=200,\n",
        "    random_state=0,\n",
        ")\n",
        "\n",
        "n   = X_obs.shape[0]\n",
        "rng = np.random.RandomState(0)\n",
        "idx = rng.permutation(n)\n",
        "split = int(0.8 * n)\n",
        "idx_train, idx_test = idx[:split], idx[split:]\n",
        "\n",
        "X_resid_train, X_resid_test = X_obs[idx_train], X_obs[idx_test]\n",
        "r_train, r_test             = resid_log[idx_train], resid_log[idx_test]\n",
        "\n",
        "print(\"[Diag] Fitting Residual->X regressor...\")\n",
        "gbr_resid_x.fit(X_resid_train, r_train)\n",
        "r_pred_test_x = gbr_resid_x.predict(X_resid_test)\n",
        "\n",
        "r2_x = r2_score(r_test, r_pred_test_x)\n",
        "print(f\"\\n[Diag] R^2(residual | X) on held-out: {r2_x:.4f}\")\n",
        "\n",
        "# # ---------------------------------------------------------\n",
        "# 4. Residual structure vs latent codes z (mu_z)\n",
        "# ---------------------------------------------------------\n",
        "\n",
        "print(\"[Diag] Computing latent codes for observed rows...\")\n",
        "\n",
        "# get_latent_representation expects a *numpy* array, not a torch tensor\n",
        "mu_z, logvar_z = vae_trainer.get_latent_representation(\n",
        "    X_obs.astype(np.float32), batch_size=1024\n",
        ")\n",
        "\n",
        "# Ensure we have a numpy array for downstream sklearn\n",
        "if isinstance(mu_z, torch.Tensor):\n",
        "    mu_z = mu_z.cpu().numpy()\n",
        "else:\n",
        "    mu_z = np.asarray(mu_z, dtype=float)\n",
        "\n",
        "print(f\"[Diag] Latent mu_z shape: {mu_z.shape}\")\n",
        "\n",
        "gbr_resid_z = HistGradientBoostingRegressor(\n",
        "    loss=\"squared_error\",\n",
        "    max_depth=3,\n",
        "    learning_rate=0.05,\n",
        "    max_iter=200,\n",
        "    random_state=1,\n",
        ")\n",
        "\n",
        "Z_train, Z_test = mu_z[idx_train], mu_z[idx_test]\n",
        "\n",
        "print(\"[Diag] Fitting Residual->z regressor...\")\n",
        "gbr_resid_z.fit(Z_train, r_train)\n",
        "r_pred_test_z = gbr_resid_z.predict(Z_test)\n",
        "\n",
        "r2_z = r2_score(r_test, r_pred_test_z)\n",
        "print(f\"\\n[Diag] R^2(residual | z_mu) on held-out: {r2_z:.4f}\")\n",
        "\n",
        "print(\n",
        "    \"\\n[Diag Summary]\\n\"\n",
        "    f\"  R^2(residual | X)    = {r2_x:.4f}\\n\"\n",
        "    f\"  R^2(residual | z_mu) = {r2_z:.4f}\\n\\n\"\n",
        "    \"Interpretation:\\n\"\n",
        "    \"  - If both are near 0, residuals look close to white noise given the inputs, and the\\n\"\n",
        "    \"    MIWAE is likely close to the noise floor for this feature set.\\n\"\n",
        "    \"  - If R^2(residual | X) is noticeably > 0, there is still signal in the raw features\\n\"\n",
        "    \"    that the MIWAE has not exploited (representation or capacity limit).\\n\"\n",
        "    \"  - If R^2(residual | z_mu) is noticeably > 0, the latent representation has unused\\n\"\n",
        "    \"    predictive structure that the price head / likelihood is leaving on the table.\"\n",
        ")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0wPtxKPQ4AUv",
        "outputId": "e35e6098-9737-4aea-aa7c-6574844e9dec"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Diag] Observed rows for residual analysis: 92034\n",
            "[Diag] Getting MIWAE predictions for observed rows...\n",
            "[Diag] Residuals (log-price): mean=-0.2633, std=1.8151\n",
            "[Diag] Fitting Residual->X regressor...\n",
            "\n",
            "[Diag] R^2(residual | X) on held-out: 0.4972\n",
            "[Diag] Computing latent codes for observed rows...\n",
            "[Diag] Latent mu_z shape: (92034, 3)\n",
            "[Diag] Fitting Residual->z regressor...\n",
            "\n",
            "[Diag] R^2(residual | z_mu) on held-out: 0.2729\n",
            "\n",
            "[Diag Summary]\n",
            "  R^2(residual | X)    = 0.4972\n",
            "  R^2(residual | z_mu) = 0.2729\n",
            "\n",
            "Interpretation:\n",
            "  - If both are near 0, residuals look close to white noise given the inputs, and the\n",
            "    MIWAE is likely close to the noise floor for this feature set.\n",
            "  - If R^2(residual | X) is noticeably > 0, there is still signal in the raw features\n",
            "    that the MIWAE has not exploited (representation or capacity limit).\n",
            "  - If R^2(residual | z_mu) is noticeably > 0, the latent representation has unused\n",
            "    predictive structure that the price head / likelihood is leaving on the table.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.inspection import permutation_importance\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# =========================================================\n",
        "# Permutation importances for residual | X and residual | z\n",
        "# using the held-out (test) split you already defined\n",
        "# =========================================================\n",
        "\n",
        "# Use the test split as validation\n",
        "X_resid_valid = X_resid_test\n",
        "Z_resid_valid = Z_test\n",
        "res_valid     = r_test\n",
        "\n",
        "# -------------------------------\n",
        "# 1) Residual | X importances\n",
        "# -------------------------------\n",
        "r_x = permutation_importance(\n",
        "    gbr_resid_x,\n",
        "    X_resid_valid,   # features\n",
        "    res_valid,       # residuals on held-out\n",
        "    n_repeats=10,\n",
        "    random_state=0,\n",
        "    n_jobs=-1,\n",
        ")\n",
        "\n",
        "imp_x = r_x.importances_mean\n",
        "order_x = np.argsort(imp_x)[::-1]\n",
        "top_k = min(20, X_resid_valid.shape[1])\n",
        "\n",
        "# Try to recover meaningful feature names if possible\n",
        "if \"FEATURE_COLS\" in globals():\n",
        "    feat_names_X = np.array(FEATURE_COLS)\n",
        "elif \"vae_input_cols\" in globals():\n",
        "    feat_names_X = np.array(vae_input_cols)\n",
        "else:\n",
        "    # Fallback: generic labels\n",
        "    feat_names_X = np.array([f\"x[{j}]\" for j in range(X_resid_valid.shape[1])])\n",
        "\n",
        "plt.figure(figsize=(8, 6))\n",
        "plt.bar(range(top_k), imp_x[order_x][:top_k])\n",
        "plt.xticks(range(top_k), feat_names_X[order_x][:top_k], rotation=90)\n",
        "plt.title(\"Top features explaining residuals (X → residual)\")\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# -------------------------------\n",
        "# 2) Residual | z_mu importances\n",
        "# -------------------------------\n",
        "r_z = permutation_importance(\n",
        "    gbr_resid_z,\n",
        "    Z_resid_valid,\n",
        "    res_valid,\n",
        "    n_repeats=10,\n",
        "    random_state=0,\n",
        "    n_jobs=-1,\n",
        ")\n",
        "\n",
        "imp_z = r_z.importances_mean\n",
        "order_z = np.argsort(imp_z)[::-1]\n",
        "feat_names_Z = np.array([f\"z_mu[{j}]\" for j in range(Z_resid_valid.shape[1])])\n",
        "\n",
        "print(\"Permutation importances for residual | z_mu (test split):\")\n",
        "for j in order_z:\n",
        "    print(f\"  {feat_names_Z[j]}: {imp_z[j]:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 648
        },
        "id": "402fDmsNm53x",
        "outputId": "da737506-021c-457f-8f59-db08705c269b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 800x600 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Permutation importances for residual | z_mu (test split):\n",
            "  z_mu[1]: 1.4831\n",
            "  z_mu[2]: 0.7673\n",
            "  z_mu[0]: 0.6646\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "from sklearn.ensemble import HistGradientBoostingRegressor\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.metrics import r2_score, mean_squared_error, mean_absolute_error\n",
        "\n",
        "# Optional: SciPy for kurtosis / skew; you can comment this out if you prefer.\n",
        "try:\n",
        "    from scipy.stats import kurtosis, skew\n",
        "    HAVE_SCIPY = True\n",
        "except ImportError:\n",
        "    HAVE_SCIPY = False\n",
        "\n",
        "# --------------------------------------------------------------------\n",
        "# 0. Guards / setup: assume df_pred, X_filled_np, vae_trainer exist\n",
        "# --------------------------------------------------------------------\n",
        "TARGET_COL = \"log_sale_price\"\n",
        "\n",
        "if \"df_pred\" not in globals():\n",
        "    raise NameError(\n",
        "        \"df_pred not found. Make sure you loaded \"\n",
        "        \"final_data_with_miwae_predictions.parquet into df_pred.\"\n",
        "    )\n",
        "\n",
        "if \"X_filled_np\" not in globals():\n",
        "    raise NameError(\n",
        "        \"X_filled_np not found. Make sure you ran the eval cell that \"\n",
        "        \"calls prepare_vae_input and assigns its first output to X_filled_np.\"\n",
        "    )\n",
        "\n",
        "if \"vae_trainer\" not in globals():\n",
        "    raise NameError(\"vae_trainer not found. Make sure your trainer object is in scope.\")\n",
        "\n",
        "if \"DEVICE\" not in globals():\n",
        "    DEVICE = next(vae_trainer.model.parameters()).device\n",
        "\n",
        "model = vae_trainer.model\n",
        "model.eval()\n",
        "\n",
        "# --------------------------------------------------------------------\n",
        "# 1. Build observed subset\n",
        "# --------------------------------------------------------------------\n",
        "mask_obs = df_pred[TARGET_COL].notna().values\n",
        "X_all = X_filled_np\n",
        "y_log_all = df_pred[TARGET_COL].to_numpy(dtype=float)\n",
        "\n",
        "X_obs = X_all[mask_obs]\n",
        "y_log_obs = y_log_all[mask_obs]\n",
        "\n",
        "print(f\"[Profile+] Observed rows with true {TARGET_COL}: {X_obs.shape[0]}\")\n",
        "\n",
        "# --------------------------------------------------------------------\n",
        "# 2. Latent codes mu_z for observed rows (no retraining)\n",
        "# --------------------------------------------------------------------\n",
        "with torch.no_grad():\n",
        "    # get_latent_representation expects numpy, not tensor\n",
        "    mu_z, logvar_z = vae_trainer.get_latent_representation(\n",
        "        X_obs, batch_size=1024\n",
        "    )\n",
        "\n",
        "if isinstance(mu_z, torch.Tensor):\n",
        "    mu_z = mu_z.cpu().numpy()\n",
        "else:\n",
        "    mu_z = np.asarray(mu_z, dtype=float)\n",
        "\n",
        "print(f\"[Profile+] Latent mu_z shape: {mu_z.shape}\")\n",
        "\n",
        "finite_mask = np.isfinite(y_log_obs) & np.isfinite(mu_z).all(axis=1)\n",
        "X_obs = X_obs[finite_mask]\n",
        "y_log_obs = y_log_obs[finite_mask]\n",
        "mu_z = mu_z[finite_mask]\n",
        "\n",
        "print(f\"[Profile+] Rows after finite-filter: {X_obs.shape[0]}\")\n",
        "\n",
        "# --------------------------------------------------------------------\n",
        "# 3. Get MIWAE head predictions (mean & log-variance) on observed rows\n",
        "# --------------------------------------------------------------------\n",
        "def predict_head_on_X(X_np, batch_size=1024):\n",
        "    preds_mu = []\n",
        "    preds_logvar = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for start in range(0, X_np.shape[0], batch_size):\n",
        "            xb_np = X_np[start:start + batch_size]\n",
        "            xb = torch.from_numpy(xb_np).float().to(DEVICE)\n",
        "            y_mu_batch, y_logvar_batch = model.predict_price(xb)\n",
        "            preds_mu.append(y_mu_batch.detach().cpu().numpy().reshape(-1))\n",
        "            preds_logvar.append(y_logvar_batch.detach().cpu().numpy().reshape(-1))\n",
        "\n",
        "    return (\n",
        "        np.concatenate(preds_mu, axis=0),\n",
        "        np.concatenate(preds_logvar, axis=0),\n",
        "    )\n",
        "\n",
        "print(\"[Profile+] Getting MIWAE head predictions (y_mu, y_logvar) on observed rows...\")\n",
        "y_mu_obs, y_logvar_obs = predict_head_on_X(X_obs, batch_size=1024)\n",
        "\n",
        "resid_log = y_log_obs - y_mu_obs\n",
        "print(f\"[Profile+] Residuals (log-price): mean={resid_log.mean():.4f}, std={resid_log.std():.4f}\")\n",
        "\n",
        "# --------------------------------------------------------------------\n",
        "# 4. y|X (Tree), y|z_mu (Tree), y|z_mu (Head) with consistent split\n",
        "# --------------------------------------------------------------------\n",
        "rng = np.random.RandomState(0)\n",
        "n = X_obs.shape[0]\n",
        "idx = rng.permutation(n)\n",
        "split = int(0.8 * n)\n",
        "idx_train, idx_test = idx[:split], idx[split:]\n",
        "\n",
        "def rmse(y_true, y_pred):\n",
        "    return np.sqrt(mean_squared_error(y_true, y_pred))\n",
        "\n",
        "def summarize_reg(label, y_true_train, y_pred_train, y_true_test, y_pred_test):\n",
        "    r2_tr = r2_score(y_true_train, y_pred_train)\n",
        "    r2_te = r2_score(y_true_test,  y_pred_test)\n",
        "    rmse_te = rmse(y_true_test, y_pred_test)\n",
        "    mae_te  = mean_absolute_error(y_true_test, y_pred_test)\n",
        "    print(f\"\\n[{label}]\")\n",
        "    print(f\"  R^2 train: {r2_tr:.4f}\")\n",
        "    print(f\"  R^2 test : {r2_te:.4f}\")\n",
        "    print(f\"  RMSE test (log-price): {rmse_te:.4f}\")\n",
        "    print(f\"  MAE  test (log-price): {mae_te:.4f}\")\n",
        "    return r2_tr, r2_te\n",
        "\n",
        "# y | X (Tree)\n",
        "gbr_y_x = HistGradientBoostingRegressor(\n",
        "    loss=\"squared_error\",\n",
        "    max_depth=3,\n",
        "    learning_rate=0.05,\n",
        "    max_iter=300,\n",
        "    random_state=0,\n",
        ")\n",
        "\n",
        "X_tr, X_te = X_obs[idx_train], X_obs[idx_test]\n",
        "y_tr, y_te = y_log_obs[idx_train], y_log_obs[idx_test]\n",
        "\n",
        "print(\"\\n[Profile+] Fitting y | X (Tree)...\")\n",
        "gbr_y_x.fit(X_tr, y_tr)\n",
        "y_pred_tr_x = gbr_y_x.predict(X_tr)\n",
        "y_pred_te_x = gbr_y_x.predict(X_te)\n",
        "r2_y_x_tr, r2_y_x_te = summarize_reg(\"y | X (Tree)\", y_tr, y_pred_tr_x, y_te, y_pred_te_x)\n",
        "\n",
        "# y | z_mu (Tree)\n",
        "gbr_y_z = HistGradientBoostingRegressor(\n",
        "    loss=\"squared_error\",\n",
        "    max_depth=3,\n",
        "    learning_rate=0.05,\n",
        "    max_iter=300,\n",
        "    random_state=1,\n",
        ")\n",
        "\n",
        "Z_tr, Z_te = mu_z[idx_train], mu_z[idx_test]\n",
        "\n",
        "print(\"\\n[Profile+] Fitting y | z_mu (Tree)...\")\n",
        "gbr_y_z.fit(Z_tr, y_tr)\n",
        "y_pred_tr_z = gbr_y_z.predict(Z_tr)\n",
        "y_pred_te_z = gbr_y_z.predict(Z_te)\n",
        "r2_y_z_tr, r2_y_z_te = summarize_reg(\"y | z_mu (Tree)\", y_tr, y_pred_tr_z, y_te, y_pred_te_z)\n",
        "\n",
        "# y | z_mu (MIWAE head): we already have y_mu_obs\n",
        "y_mu_tr, y_mu_te = y_mu_obs[idx_train], y_mu_obs[idx_test]\n",
        "r2_head_tr, r2_head_te = summarize_reg(\"y | z_mu (MIWAE head)\", y_tr, y_mu_tr, y_te, y_mu_te)\n",
        "\n",
        "print(\"\\n[Profile+ Summary]\")\n",
        "print(f\"  R^2(y | X, Tree)       test: {r2_y_x_te:.4f}\")\n",
        "print(f\"  R^2(y | z_mu, Tree)    test: {r2_y_z_te:.4f}\")\n",
        "print(f\"  R^2(y | z_mu, Head)    test: {r2_head_te:.4f}\")\n",
        "\n",
        "# --------------------------------------------------------------------\n",
        "# 5. Per-feature reconstructability from z_mu: R^2(f | z_mu)\n",
        "# --------------------------------------------------------------------\n",
        "candidate_features = [\n",
        "    \"assess_total\", \"assessland\", \"exempt_total\",\n",
        "    \"lotarea\", \"grosssqft\", \"yearbuilt\",\n",
        "    \"num_bldgs\", \"num_floors\",\n",
        "]\n",
        "\n",
        "feat_list = [f for f in candidate_features if f in df_pred.columns]\n",
        "print(\"\\n[Profile+] Per-feature reconstructability from z_mu (LinearRegression):\")\n",
        "if not feat_list:\n",
        "    print(\"  (None of the candidate feature names were found in df_pred; edit 'candidate_features'.)\")\n",
        "else:\n",
        "    for feat in feat_list:\n",
        "        f_vals = df_pred.loc[mask_obs, feat].to_numpy(dtype=float)[finite_mask]\n",
        "        mask_finite_f = np.isfinite(f_vals)\n",
        "        f_sub = f_vals[mask_finite_f]\n",
        "        z_sub = mu_z[mask_finite_f]\n",
        "\n",
        "        if len(f_sub) < 100:\n",
        "            print(f\"  {feat}: too few non-NaN points ({len(f_sub)}).\")\n",
        "            continue\n",
        "\n",
        "        reg_f = LinearRegression().fit(z_sub, f_sub)\n",
        "        r2_f = reg_f.score(z_sub, f_sub)\n",
        "        print(f\"  R^2({feat} | z_mu) = {r2_f:.4f}\")\n",
        "\n",
        "# --------------------------------------------------------------------\n",
        "# 6. Uncertainty calibration: z-scores using y_pred_logvar\n",
        "# --------------------------------------------------------------------\n",
        "print(\"\\n[Profile+] Uncertainty calibration in log-price space:\")\n",
        "y_sigma_obs = np.sqrt(np.exp(y_logvar_obs))\n",
        "# guard against zeros\n",
        "y_sigma_obs = np.where(y_sigma_obs <= 1e-8, 1e-8, y_sigma_obs)\n",
        "\n",
        "zscores = (y_log_obs - y_mu_obs) / y_sigma_obs\n",
        "finite_z = np.isfinite(zscores)\n",
        "zs = zscores[finite_z]\n",
        "\n",
        "print(f\"  Mean z      : {zs.mean():.4f}\")\n",
        "print(f\"  Std  z      : {zs.std():.4f}\")\n",
        "print(f\"  Frac |z|<=1 : {np.mean(np.abs(zs) <= 1.0):.4f} (Normal(0,1) baseline ~0.68)\")\n",
        "print(f\"  Frac |z|<=2 : {np.mean(np.abs(zs) <= 2.0):.4f} (Normal(0,1) baseline ~0.95)\")\n",
        "print(f\"  Frac |z|<=3 : {np.mean(np.abs(zs) <= 3.0):.4f} (Normal(0,1) baseline ~0.997)\")\n",
        "\n",
        "# --------------------------------------------------------------------\n",
        "# 7. Tail behavior of residuals (log space)\n",
        "# --------------------------------------------------------------------\n",
        "print(\"\\n[Profile+] Tail behavior of residuals (log-price):\")\n",
        "abs_resid = np.abs(resid_log[np.isfinite(resid_log)])\n",
        "\n",
        "print(f\"  Median |resid| : {np.median(abs_resid):.4f}\")\n",
        "print(f\"  90th pct |resid|: {np.percentile(abs_resid, 90):.4f}\")\n",
        "print(f\"  95th pct |resid|: {np.percentile(abs_resid, 95):.4f}\")\n",
        "print(f\"  99th pct |resid|: {np.percentile(abs_resid, 99):.4f}\")\n",
        "\n",
        "if HAVE_SCIPY:\n",
        "    print(f\"  Skew(resid)    : {skew(resid_log, nan_policy='omit'):.4f}\")\n",
        "    print(f\"  Kurtosis(resid): {kurtosis(resid_log, fisher=True, nan_policy='omit'):.4f} \"\n",
        "          \"(0 = Normal; >0 = heavy tails)\")\n",
        "else:\n",
        "    print(\"  (SciPy not available: skipping skew/kurtosis.)\")\n",
        "\n",
        "# --------------------------------------------------------------------\n",
        "# 8. Prior vs posterior: mixture usage vs latent cluster usage\n",
        "# --------------------------------------------------------------------\n",
        "print(\"\\n[Profile+] Prior vs posterior usage (mixture components):\")\n",
        "\n",
        "prior_means = getattr(model, \"prior_means\", None)\n",
        "prior_logits = getattr(model, \"prior_weights_logits\", None)\n",
        "\n",
        "if prior_means is None or prior_logits is None:\n",
        "    print(\"  Model does not expose prior_means / prior_weights_logits; skipping.\")\n",
        "else:\n",
        "    prior_means_np = prior_means.detach().cpu().numpy()  # (K, latent_dim)\n",
        "    logits_np = prior_logits.detach().cpu().numpy()\n",
        "    # softmax\n",
        "    w = np.exp(logits_np - logits_np.max())\n",
        "    w /= w.sum()\n",
        "\n",
        "    K = prior_means_np.shape[0]\n",
        "    print(f\"  Number of prior components K: {K}\")\n",
        "    print(f\"  Prior weights (softmax logits): {w}\")\n",
        "\n",
        "    if K == 1:\n",
        "        print(\"  Single-component prior; mixture usage not applicable.\")\n",
        "    else:\n",
        "        # Assign each mu_z to nearest prior mean (Euclidean) as a crude proxy for responsibilities.\n",
        "        dists_sq = ((mu_z[:, None, :] - prior_means_np[None, :, :]) ** 2).sum(axis=2)  # (N, K)\n",
        "        assign = np.argmin(dists_sq, axis=1)\n",
        "        unique, counts = np.unique(assign, return_counts=True)\n",
        "        total = len(assign)\n",
        "        print(\"  Posterior nearest-component counts:\")\n",
        "        for k, c in zip(unique, counts):\n",
        "            print(f\"    comp {k}: {c} points ({c / total:.2%})\")\n",
        "\n",
        "        # Compare latent cloud variance to prior means scale\n",
        "        print(\"  Latent variance by dimension:\")\n",
        "        print(\"   \", np.var(mu_z, axis=0))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AZflSJJIo7Ga",
        "outputId": "f4a646ce-9a36-4991-e565-473ce8132f04"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Profile+] Observed rows with true log_sale_price: 92034\n",
            "[Profile+] Latent mu_z shape: (92034, 3)\n",
            "[Profile+] Rows after finite-filter: 92034\n",
            "[Profile+] Getting MIWAE head predictions (y_mu, y_logvar) on observed rows...\n",
            "[Profile+] Residuals (log-price): mean=-0.2633, std=1.8151\n",
            "\n",
            "[Profile+] Fitting y | X (Tree)...\n",
            "\n",
            "[y | X (Tree)]\n",
            "  R^2 train: 0.9681\n",
            "  R^2 test : 0.9660\n",
            "  RMSE test (log-price): 1.0815\n",
            "  MAE  test (log-price): 0.5219\n",
            "\n",
            "[Profile+] Fitting y | z_mu (Tree)...\n",
            "\n",
            "[y | z_mu (Tree)]\n",
            "  R^2 train: 0.9306\n",
            "  R^2 test : 0.9306\n",
            "  RMSE test (log-price): 1.5451\n",
            "  MAE  test (log-price): 0.7898\n",
            "\n",
            "[y | z_mu (MIWAE head)]\n",
            "  R^2 train: 0.9024\n",
            "  R^2 test : 0.9017\n",
            "  RMSE test (log-price): 1.8393\n",
            "  MAE  test (log-price): 0.9783\n",
            "\n",
            "[Profile+ Summary]\n",
            "  R^2(y | X, Tree)       test: 0.9660\n",
            "  R^2(y | z_mu, Tree)    test: 0.9306\n",
            "  R^2(y | z_mu, Head)    test: 0.9017\n",
            "\n",
            "[Profile+] Per-feature reconstructability from z_mu (LinearRegression):\n",
            "  R^2(assess_total | z_mu) = 0.1857\n",
            "  R^2(assessland | z_mu) = 0.1684\n",
            "  R^2(exempt_total | z_mu) = 0.2886\n",
            "  R^2(lotarea | z_mu) = 0.1403\n",
            "  R^2(yearbuilt | z_mu) = 0.1066\n",
            "\n",
            "[Profile+] Uncertainty calibration in log-price space:\n",
            "  Mean z      : -0.1571\n",
            "  Std  z      : 0.8748\n",
            "  Frac |z|<=1 : 0.8374 (Normal(0,1) baseline ~0.68)\n",
            "  Frac |z|<=2 : 0.9685 (Normal(0,1) baseline ~0.95)\n",
            "  Frac |z|<=3 : 0.9883 (Normal(0,1) baseline ~0.997)\n",
            "\n",
            "[Profile+] Tail behavior of residuals (log-price):\n",
            "  Median |resid| : 0.2327\n",
            "  90th pct |resid|: 2.9643\n",
            "  95th pct |resid|: 4.1445\n",
            "  99th pct |resid|: 7.0397\n",
            "  Skew(resid)    : -0.0173\n",
            "  Kurtosis(resid): 9.2721 (0 = Normal; >0 = heavy tails)\n",
            "\n",
            "[Profile+] Prior vs posterior usage (mixture components):\n",
            "  Number of prior components K: 5\n",
            "  Prior weights (softmax logits): [0.2 0.2 0.2 0.2 0.2]\n",
            "  Posterior nearest-component counts:\n",
            "    comp 0: 92034 points (100.00%)\n",
            "  Latent variance by dimension:\n",
            "    [0.00136038 0.01781355 0.00976154]\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "A100",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}